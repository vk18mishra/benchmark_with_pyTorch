Total duration: 53.5767s
File: /opt/conda/lib/python3.8/site-packages/torch/autograd/__init__.py
File duration: 31.3149s (58.45%)
Line #|      Hits|         Time| Time per hit|      %|Source code
------+----------+-------------+-------------+-------+-----------
     1|         0|            0|            0|  0.00%|"""
     2|         0|            0|            0|  0.00%|``torch.autograd`` provides classes and functions implementing automatic
     3|         0|            0|            0|  0.00%|differentiation of arbitrary scalar valued functions. It requires minimal
     4|         0|            0|            0|  0.00%|changes to the existing code - you only need to declare :class:`Tensor` s
     5|         0|            0|            0|  0.00%|for which gradients should be computed with the ``requires_grad=True`` keyword.
     6|         0|            0|            0|  0.00%|As of now, we only support autograd for floating point :class:`Tensor` types (
     7|         0|            0|            0|  0.00%|half, float, double and bfloat16) and complex :class:`Tensor` types (cfloat, cdouble).
     8|         0|            0|            0|  0.00%|"""
     9|         0|            0|            0|  0.00%|import torch
    10|         0|            0|            0|  0.00%|import warnings
    11|         0|            0|            0|  0.00%|
    12|         0|            0|            0|  0.00%|from torch.types import _TensorOrTensors
    13|         0|            0|            0|  0.00%|from typing import Any, Callable, List, Optional, Sequence, Tuple, Union
    14|         0|            0|            0|  0.00%|
    15|         0|            0|            0|  0.00%|from .variable import Variable
    16|         0|            0|            0|  0.00%|from .function import Function, NestedIOFunction
    17|         0|            0|            0|  0.00%|from .gradcheck import gradcheck, gradgradcheck
    18|         0|            0|            0|  0.00%|from .grad_mode import no_grad, enable_grad, set_grad_enabled, inference_mode
    19|         0|            0|            0|  0.00%|from .anomaly_mode import detect_anomaly, set_detect_anomaly
    20|         0|            0|            0|  0.00%|from ..overrides import has_torch_function, handle_torch_function
    21|         0|            0|            0|  0.00%|from . import functional
    22|         0|            0|            0|  0.00%|from . import forward_ad
    23|         0|            0|            0|  0.00%|
    24|         0|            0|            0|  0.00%|__all__ = ['Variable', 'Function', 'backward', 'grad_mode']
    25|         0|            0|            0|  0.00%|
    26|         0|            0|            0|  0.00%|_OptionalTensor = Optional[torch.Tensor]
    27|         0|            0|            0|  0.00%|
    28|        61|  0.000504017|  8.26257e-06|  0.00%|def _make_grads(outputs: Sequence[torch.Tensor], grads: Sequence[_OptionalTensor]) -> Tuple[_OptionalTensor, ...]:
    29|        61|  0.000329256|  5.39764e-06|  0.00%|    new_grads: List[_OptionalTensor] = []
    30|       122|  0.000866413|  7.10175e-06|  0.00%|    for out, grad in zip(outputs, grads):
    31|        61|  0.000478268|  7.84045e-06|  0.00%|        if isinstance(grad, torch.Tensor):
    32|         0|            0|            0|  0.00%|            if not out.shape == grad.shape:
    33|         0|            0|            0|  0.00%|                raise RuntimeError("Mismatch in shape: grad_output["
    34|         0|            0|            0|  0.00%|                                   + str(grads.index(grad)) + "] has a shape of "
    35|         0|            0|            0|  0.00%|                                   + str(grad.shape) + " and output["
    36|         0|            0|            0|  0.00%|                                   + str(outputs.index(out)) + "] has a shape of "
    37|         0|            0|            0|  0.00%|                                   + str(out.shape) + ".")
    38|         0|            0|            0|  0.00%|            if out.dtype.is_complex != grad.dtype.is_complex:
    39|         0|            0|            0|  0.00%|                raise RuntimeError("For complex Tensors, both grad_output and output"
    40|         0|            0|            0|  0.00%|                                   " are required to have the same dtype."
    41|         0|            0|            0|  0.00%|                                   " Mismatch in dtype: grad_output["
    42|         0|            0|            0|  0.00%|                                   + str(grads.index(grad)) + "] has a dtype of "
    43|         0|            0|            0|  0.00%|                                   + str(grad.dtype) + " and output["
    44|         0|            0|            0|  0.00%|                                   + str(outputs.index(out)) + "] has a dtype of "
    45|         0|            0|            0|  0.00%|                                   + str(out.dtype) + ".")
    46|         0|            0|            0|  0.00%|            new_grads.append(grad)
    47|        61|  0.000311375|   5.1045e-06|  0.00%|        elif grad is None:
    48|        61|  0.000419378|  6.87505e-06|  0.00%|            if out.requires_grad:
    49|        61|  0.000594139|  9.73999e-06|  0.00%|                if out.numel() != 1:
    50|         0|            0|            0|  0.00%|                    raise RuntimeError("grad can be implicitly created only for scalar outputs")
    51|        61|   0.00204563|  3.35349e-05|  0.00%|                new_grads.append(torch.ones_like(out, memory_format=torch.preserve_format))
    52|         0|            0|            0|  0.00%|            else:
    53|         0|            0|            0|  0.00%|                new_grads.append(None)
    54|         0|            0|            0|  0.00%|        else:
    55|         0|            0|            0|  0.00%|            raise TypeError("gradients can be either Tensors or None, but got " +
    56|         0|            0|            0|  0.00%|                            type(grad).__name__)
    57|        61|   0.00031209|  5.11623e-06|  0.00%|    return tuple(new_grads)
    58|         0|            0|            0|  0.00%|
    59|         0|            0|            0|  0.00%|
    60|        61|  0.000359058|   5.8862e-06|  0.00%|def _tensor_or_tensors_to_tuple(tensors: Optional[_TensorOrTensors], length: int) -> Tuple[_OptionalTensor, ...]:
    61|        61|  0.000273705|  4.48696e-06|  0.00%|    if tensors is None:
    62|        61|  0.000350237|  5.74159e-06|  0.00%|        return (None, ) * length
    63|         0|            0|            0|  0.00%|    if isinstance(tensors, torch.Tensor):
    64|         0|            0|            0|  0.00%|        return (tensors, )
    65|         0|            0|            0|  0.00%|    return tuple(tensors)
    66|         0|            0|            0|  0.00%|
    67|         0|            0|            0|  0.00%|
    68|        61|  0.000467777|  7.66848e-06|  0.00%|def backward(
    69|         0|            0|            0|  0.00%|    tensors: _TensorOrTensors,
    70|         0|            0|            0|  0.00%|    grad_tensors: Optional[_TensorOrTensors] = None,
    71|         0|            0|            0|  0.00%|    retain_graph: Optional[bool] = None,
    72|         0|            0|            0|  0.00%|    create_graph: bool = False,
    73|         0|            0|            0|  0.00%|    grad_variables: Optional[_TensorOrTensors] = None,
    74|         0|            0|            0|  0.00%|    inputs: Optional[_TensorOrTensors] = None,
    75|         0|            0|            0|  0.00%|) -> None:
    76|         0|            0|            0|  0.00%|    r"""Computes the sum of gradients of given tensors with respect to graph
    77|         0|            0|            0|  0.00%|    leaves.
    78|         0|            0|            0|  0.00%|
    79|         0|            0|            0|  0.00%|    The graph is differentiated using the chain rule. If any of ``tensors``
    80|         0|            0|            0|  0.00%|    are non-scalar (i.e. their data has more than one element) and require
    81|         0|            0|            0|  0.00%|    gradient, then the Jacobian-vector product would be computed, in this
    82|         0|            0|            0|  0.00%|    case the function additionally requires specifying ``grad_tensors``.
    83|         0|            0|            0|  0.00%|    It should be a sequence of matching length, that contains the "vector"
    84|         0|            0|            0|  0.00%|    in the Jacobian-vector product, usually the gradient of the differentiated
    85|         0|            0|            0|  0.00%|    function w.r.t. corresponding tensors (``None`` is an acceptable value for
    86|         0|            0|            0|  0.00%|    all tensors that don't need gradient tensors).
    87|         0|            0|            0|  0.00%|
    88|         0|            0|            0|  0.00%|    This function accumulates gradients in the leaves - you might need to zero
    89|         0|            0|            0|  0.00%|    ``.grad`` attributes or set them to ``None`` before calling it.
    90|         0|            0|            0|  0.00%|    See :ref:`Default gradient layouts<default-grad-layouts>`
    91|         0|            0|            0|  0.00%|    for details on the memory layout of accumulated gradients.
    92|         0|            0|            0|  0.00%|
    93|         0|            0|            0|  0.00%|    .. note::
    94|         0|            0|            0|  0.00%|        Using this method with ``create_graph=True`` will create a reference cycle
    95|         0|            0|            0|  0.00%|        between the parameter and its gradient which can cause a memory leak.
    96|         0|            0|            0|  0.00%|        We recommend using ``autograd.grad`` when creating the graph to avoid this.
    97|         0|            0|            0|  0.00%|        If you have to use this function, make sure to reset the ``.grad`` fields of your
    98|         0|            0|            0|  0.00%|        parameters to ``None`` after use to break the cycle and avoid the leak.
    99|         0|            0|            0|  0.00%|
   100|         0|            0|            0|  0.00%|    .. note::
   101|         0|            0|            0|  0.00%|
   102|         0|            0|            0|  0.00%|        If you run any forward ops, create ``grad_tensors``, and/or call ``backward``
   103|         0|            0|            0|  0.00%|        in a user-specified CUDA stream context, see
   104|         0|            0|            0|  0.00%|        :ref:`Stream semantics of backward passes<bwd-cuda-stream-semantics>`.
   105|         0|            0|            0|  0.00%|
   106|         0|            0|            0|  0.00%|    Args:
   107|         0|            0|            0|  0.00%|        tensors (Sequence[Tensor] or Tensor): Tensors of which the derivative will be
   108|         0|            0|            0|  0.00%|            computed.
   109|         0|            0|            0|  0.00%|        grad_tensors (Sequence[Tensor or None] or Tensor, optional): The "vector" in
   110|         0|            0|            0|  0.00%|            the Jacobian-vector product, usually gradients w.r.t. each element of
   111|         0|            0|            0|  0.00%|            corresponding tensors. None values can be specified for scalar Tensors or
   112|         0|            0|            0|  0.00%|            ones that don't require grad. If a None value would be acceptable for all
   113|         0|            0|            0|  0.00%|            grad_tensors, then this argument is optional.
   114|         0|            0|            0|  0.00%|        retain_graph (bool, optional): If ``False``, the graph used to compute the grad
   115|         0|            0|            0|  0.00%|            will be freed. Note that in nearly all cases setting this option to ``True``
   116|         0|            0|            0|  0.00%|            is not needed and often can be worked around in a much more efficient
   117|         0|            0|            0|  0.00%|            way. Defaults to the value of ``create_graph``.
   118|         0|            0|            0|  0.00%|        create_graph (bool, optional): If ``True``, graph of the derivative will
   119|         0|            0|            0|  0.00%|            be constructed, allowing to compute higher order derivative products.
   120|         0|            0|            0|  0.00%|            Defaults to ``False``.
   121|         0|            0|            0|  0.00%|        inputs (Sequence[Tensor] or Tensor, optional): Inputs w.r.t. which the gradient
   122|         0|            0|            0|  0.00%|            be will accumulated into ``.grad``. All other Tensors will be ignored. If
   123|         0|            0|            0|  0.00%|            not provided, the gradient is accumulated into all the leaf Tensors that
   124|         0|            0|            0|  0.00%|            were used to compute the attr::tensors. All the provided inputs must be leaf
   125|         0|            0|            0|  0.00%|            Tensors.
   126|         0|            0|            0|  0.00%|    """
   127|        61|  0.000376225|  6.16762e-06|  0.00%|    if grad_variables is not None:
   128|         0|            0|            0|  0.00%|        warnings.warn("'grad_variables' is deprecated. Use 'grad_tensors' instead.")
   129|         0|            0|            0|  0.00%|        if grad_tensors is None:
   130|         0|            0|            0|  0.00%|            grad_tensors = grad_variables
   131|         0|            0|            0|  0.00%|        else:
   132|         0|            0|            0|  0.00%|            raise RuntimeError("'grad_tensors' and 'grad_variables' (deprecated) "
   133|         0|            0|            0|  0.00%|                               "arguments both passed to backward(). Please only "
   134|         0|            0|            0|  0.00%|                               "use 'grad_tensors'.")
   135|        61|   0.00029397|  4.81918e-06|  0.00%|    if inputs is not None and len(inputs) == 0:
   136|         0|            0|            0|  0.00%|        raise RuntimeError("'inputs' argument to backward() cannot be empty.")
   137|         0|            0|            0|  0.00%|
   138|        61|  0.000472307|  7.74274e-06|  0.00%|    tensors = (tensors,) if isinstance(tensors, torch.Tensor) else tuple(tensors)
   139|       122|  0.000818968|  6.71285e-06|  0.00%|    inputs = (inputs,) if isinstance(inputs, torch.Tensor) else \
   140|        61|   0.00036478|  5.98001e-06|  0.00%|        tuple(inputs) if inputs is not None else tuple()
   141|         0|            0|            0|  0.00%|
   142|        61|    0.0011127|   1.8241e-05|  0.00%|    grad_tensors_ = _tensor_or_tensors_to_tuple(grad_tensors, len(tensors))
(call)|        61|     0.000983|  1.61148e-05|  0.00%|# /opt/conda/lib/python3.8/site-packages/torch/autograd/__init__.py:60 _tensor_or_tensors_to_tuple
   143|        61|   0.00109744|  1.79908e-05|  0.00%|    grad_tensors_ = _make_grads(tensors, grad_tensors_)
(call)|        61|   0.00586057|  9.60749e-05|  0.01%|# /opt/conda/lib/python3.8/site-packages/torch/autograd/__init__.py:28 _make_grads
   144|        61|  0.000311375|   5.1045e-06|  0.00%|    if retain_graph is None:
   145|        61|  0.000288248|  4.72538e-06|  0.00%|        retain_graph = create_graph
   146|         0|            0|            0|  0.00%|
   147|       122|      31.3019|     0.256573| 58.42%|    Variable._execution_engine.run_backward(
   148|        61|  0.000290871|  4.76837e-06|  0.00%|        tensors, grad_tensors_, retain_graph, create_graph, inputs,
   149|        61|  0.000298977|  4.90126e-06|  0.00%|        allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag
   150|         0|            0|            0|  0.00%|
   151|         0|            0|            0|  0.00%|
   152|         0|            0|            0|  0.00%|def grad(
   153|         0|            0|            0|  0.00%|    outputs: _TensorOrTensors,
   154|         0|            0|            0|  0.00%|    inputs: _TensorOrTensors,
   155|         0|            0|            0|  0.00%|    grad_outputs: Optional[_TensorOrTensors] = None,
   156|         0|            0|            0|  0.00%|    retain_graph: Optional[bool] = None,
   157|         0|            0|            0|  0.00%|    create_graph: bool = False,
   158|         0|            0|            0|  0.00%|    only_inputs: bool = True,
   159|         0|            0|            0|  0.00%|    allow_unused: bool = False
   160|         0|            0|            0|  0.00%|) -> Tuple[torch.Tensor, ...]:
   161|         0|            0|            0|  0.00%|    r"""Computes and returns the sum of gradients of outputs with respect to
   162|         0|            0|            0|  0.00%|    the inputs.
   163|         0|            0|            0|  0.00%|
   164|         0|            0|            0|  0.00%|    ``grad_outputs`` should be a sequence of length matching ``output``
   165|         0|            0|            0|  0.00%|    containing the "vector" in Jacobian-vector product, usually the pre-computed
   166|         0|            0|            0|  0.00%|    gradients w.r.t. each of the outputs. If an output doesn't require_grad,
   167|         0|            0|            0|  0.00%|    then the gradient can be ``None``).
   168|         0|            0|            0|  0.00%|
   169|         0|            0|            0|  0.00%|    If ``only_inputs`` is ``True``, the function will only return a list of gradients
   170|         0|            0|            0|  0.00%|    w.r.t the specified inputs. If it's ``False``, then gradient w.r.t. all remaining
   171|         0|            0|            0|  0.00%|    leaves will still be computed, and will be accumulated into their ``.grad``
   172|         0|            0|            0|  0.00%|    attribute.
   173|         0|            0|            0|  0.00%|
   174|         0|            0|            0|  0.00%|    .. note::
   175|         0|            0|            0|  0.00%|
   176|         0|            0|            0|  0.00%|        If you run any forward ops, create ``grad_outputs``, and/or call ``grad``
   177|         0|            0|            0|  0.00%|        in a user-specified CUDA stream context, see
   178|         0|            0|            0|  0.00%|        :ref:`Stream semantics of backward passes<bwd-cuda-stream-semantics>`.
   179|         0|            0|            0|  0.00%|
   180|         0|            0|            0|  0.00%|    Args:
   181|         0|            0|            0|  0.00%|        outputs (sequence of Tensor): outputs of the differentiated function.
   182|         0|            0|            0|  0.00%|        inputs (sequence of Tensor): Inputs w.r.t. which the gradient will be
   183|         0|            0|            0|  0.00%|            returned (and not accumulated into ``.grad``).
   184|         0|            0|            0|  0.00%|        grad_outputs (sequence of Tensor): The "vector" in the Jacobian-vector product.
   185|         0|            0|            0|  0.00%|            Usually gradients w.r.t. each output. None values can be specified for scalar
   186|         0|            0|            0|  0.00%|            Tensors or ones that don't require grad. If a None value would be acceptable
   187|         0|            0|            0|  0.00%|            for all grad_tensors, then this argument is optional. Default: None.
   188|         0|            0|            0|  0.00%|        retain_graph (bool, optional): If ``False``, the graph used to compute the grad
   189|         0|            0|            0|  0.00%|            will be freed. Note that in nearly all cases setting this option to ``True``
   190|         0|            0|            0|  0.00%|            is not needed and often can be worked around in a much more efficient
   191|         0|            0|            0|  0.00%|            way. Defaults to the value of ``create_graph``.
   192|         0|            0|            0|  0.00%|        create_graph (bool, optional): If ``True``, graph of the derivative will
   193|         0|            0|            0|  0.00%|            be constructed, allowing to compute higher order derivative products.
   194|         0|            0|            0|  0.00%|            Default: ``False``.
   195|         0|            0|            0|  0.00%|        allow_unused (bool, optional): If ``False``, specifying inputs that were not
   196|         0|            0|            0|  0.00%|            used when computing outputs (and therefore their grad is always zero)
   197|         0|            0|            0|  0.00%|            is an error. Defaults to ``False``.
   198|         0|            0|            0|  0.00%|    """
   199|         0|            0|            0|  0.00%|    outputs = (outputs,) if isinstance(outputs, torch.Tensor) else tuple(outputs)
   200|         0|            0|            0|  0.00%|    inputs = (inputs,) if isinstance(inputs, torch.Tensor) else tuple(inputs)
   201|         0|            0|            0|  0.00%|    overridable_args = outputs + inputs
   202|         0|            0|            0|  0.00%|    if has_torch_function(overridable_args):
   203|         0|            0|            0|  0.00%|        return handle_torch_function(
   204|         0|            0|            0|  0.00%|            grad,
   205|         0|            0|            0|  0.00%|            overridable_args,
   206|         0|            0|            0|  0.00%|            outputs,
   207|         0|            0|            0|  0.00%|            inputs,
   208|         0|            0|            0|  0.00%|            grad_outputs=grad_outputs,
   209|         0|            0|            0|  0.00%|            retain_graph=retain_graph,
   210|         0|            0|            0|  0.00%|            create_graph=create_graph,
   211|         0|            0|            0|  0.00%|            only_inputs=only_inputs,
   212|         0|            0|            0|  0.00%|            allow_unused=allow_unused,
   213|         0|            0|            0|  0.00%|        )
   214|         0|            0|            0|  0.00%|
   215|         0|            0|            0|  0.00%|    if not only_inputs:
   216|         0|            0|            0|  0.00%|        warnings.warn("only_inputs argument is deprecated and is ignored now "
   217|         0|            0|            0|  0.00%|                      "(defaults to True). To accumulate gradient for other "
   218|         0|            0|            0|  0.00%|                      "parts of the graph, please use torch.autograd.backward.")
   219|         0|            0|            0|  0.00%|
   220|         0|            0|            0|  0.00%|    grad_outputs_ = _tensor_or_tensors_to_tuple(grad_outputs, len(outputs))
   221|         0|            0|            0|  0.00%|    grad_outputs_ = _make_grads(outputs, grad_outputs_)
   222|         0|            0|            0|  0.00%|
   223|         0|            0|            0|  0.00%|    if retain_graph is None:
   224|         0|            0|            0|  0.00%|        retain_graph = create_graph
   225|         0|            0|            0|  0.00%|
   226|         0|            0|            0|  0.00%|    return Variable._execution_engine.run_backward(
   227|         0|            0|            0|  0.00%|        outputs, grad_outputs_, retain_graph, create_graph,
   228|         0|            0|            0|  0.00%|        inputs, allow_unused, accumulate_grad=False)
   229|         0|            0|            0|  0.00%|
   230|         0|            0|            0|  0.00%|
   231|         0|            0|            0|  0.00%|# This function applies in case of gradient checkpointing for memory
   232|         0|            0|            0|  0.00%|# optimization. Currently, gradient checkpointing is supported only if the
   233|         0|            0|            0|  0.00%|# execution engine is invoked through torch.autograd.backward() and its
   234|         0|            0|            0|  0.00%|# inputs argument is not passed. It is not supported for torch.autograd.grad().
   235|         0|            0|            0|  0.00%|# This is because if inputs are specified, the gradient won't be calculated for
   236|         0|            0|            0|  0.00%|# anything else e.g. model parameters like weights, bias etc.
   237|         0|            0|            0|  0.00%|#
   238|         0|            0|            0|  0.00%|# This function returns whether the checkpointing is valid i.e. torch.autograd.backward
   239|         0|            0|            0|  0.00%|# or not i.e. torch.autograd.grad. The implementation works by maintaining a thread
   240|         0|            0|            0|  0.00%|# local variable in torch/csrc/autograd/engine.cpp which looks at the NodeTask
   241|         0|            0|            0|  0.00%|# in the stack and before a NodeTask is executed in evaluate_function, it
   242|         0|            0|            0|  0.00%|# checks for whether reentrant backwards is imperative or not.
   243|         0|            0|            0|  0.00%|# See https://github.com/pytorch/pytorch/pull/4594 for more discussion/context
   244|         0|            0|            0|  0.00%|def _is_checkpoint_valid():
   245|         0|            0|            0|  0.00%|    return Variable._execution_engine.is_checkpoint_valid()
   246|         0|            0|            0|  0.00%|
   247|         0|            0|            0|  0.00%|
   248|         0|            0|            0|  0.00%|def variable(*args, **kwargs):
   249|         0|            0|            0|  0.00%|    warnings.warn("torch.autograd.variable(...) is deprecated, use torch.tensor(...) instead")
   250|         0|            0|            0|  0.00%|    return torch.tensor(*args, **kwargs)
   251|         0|            0|            0|  0.00%|
   252|         0|            0|            0|  0.00%|if not torch._C._autograd_init():
   253|         0|            0|            0|  0.00%|    raise RuntimeError("autograd initialization failed")
   254|         0|            0|            0|  0.00%|
   255|         0|            0|            0|  0.00%|# Import all native method/classes
   256|         0|            0|            0|  0.00%|from torch._C._autograd import (DeviceType, ProfilerActivity, ProfilerState, ProfilerConfig, ProfilerEvent,
   257|         0|            0|            0|  0.00%|                                _enable_profiler_legacy, _disable_profiler_legacy, _profiler_enabled,
   258|         0|            0|            0|  0.00%|                                _enable_record_function, _set_empty_test_observer, kineto_available,
   259|         0|            0|            0|  0.00%|                                _supported_kineto_activities, _add_metadata_json)
   260|         0|            0|            0|  0.00%|
   261|         0|            0|            0|  0.00%|if kineto_available():
   262|         0|            0|            0|  0.00%|    from torch._C._autograd import (_ProfilerResult, _KinetoEvent,
   263|         0|            0|            0|  0.00%|                                    _prepare_profiler, _enable_profiler, _disable_profiler)
   264|         0|            0|            0|  0.00%|
   265|         0|            0|            0|  0.00%|from . import profiler
File: /opt/conda/lib/python3.8/site-packages/torch/nn/modules/conv.py
File duration: 11.4566s (21.38%)
Line #|      Hits|         Time| Time per hit|      %|Source code
------+----------+-------------+-------------+-------+-----------
     1|         0|            0|            0|  0.00%|# -*- coding: utf-8 -*-
     2|         0|            0|            0|  0.00%|import math
     3|         0|            0|            0|  0.00%|import warnings
     4|         0|            0|            0|  0.00%|
     5|         0|            0|            0|  0.00%|import torch
     6|         0|            0|            0|  0.00%|from torch import Tensor
     7|         0|            0|            0|  0.00%|from torch.nn.parameter import Parameter, UninitializedParameter
     8|         0|            0|            0|  0.00%|from .. import functional as F
     9|         0|            0|            0|  0.00%|from .. import init
    10|         0|            0|            0|  0.00%|from .lazy import LazyModuleMixin
    11|         0|            0|            0|  0.00%|from .module import Module
    12|         0|            0|            0|  0.00%|from .utils import _single, _pair, _triple, _reverse_repeat_tuple
    13|         0|            0|            0|  0.00%|from torch._torch_docs import reproducibility_notes
    14|         0|            0|            0|  0.00%|
    15|         0|            0|            0|  0.00%|from ..common_types import _size_1_t, _size_2_t, _size_3_t
    16|         0|            0|            0|  0.00%|from typing import Optional, List, Tuple, Union
    17|         0|            0|            0|  0.00%|
    18|         0|            0|            0|  0.00%|convolution_notes = \
    19|         0|            0|            0|  0.00%|    {"groups_note": r"""* :attr:`groups` controls the connections between inputs and outputs.
    20|         0|            0|            0|  0.00%|      :attr:`in_channels` and :attr:`out_channels` must both be divisible by
    21|         0|            0|            0|  0.00%|      :attr:`groups`. For example,
    22|         0|            0|            0|  0.00%|
    23|         0|            0|            0|  0.00%|        * At groups=1, all inputs are convolved to all outputs.
    24|         0|            0|            0|  0.00%|        * At groups=2, the operation becomes equivalent to having two conv
    25|         0|            0|            0|  0.00%|          layers side by side, each seeing half the input channels
    26|         0|            0|            0|  0.00%|          and producing half the output channels, and both subsequently
    27|         0|            0|            0|  0.00%|          concatenated.
    28|         0|            0|            0|  0.00%|        * At groups= :attr:`in_channels`, each input channel is convolved with
    29|         0|            0|            0|  0.00%|          its own set of filters (of size
    30|         0|            0|            0|  0.00%|          :math:`\frac{\text{out\_channels}}{\text{in\_channels}}`).""",
    31|         0|            0|            0|  0.00%|
    32|         0|            0|            0|  0.00%|        "depthwise_separable_note": r"""When `groups == in_channels` and `out_channels == K * in_channels`,
    33|         0|            0|            0|  0.00%|        where `K` is a positive integer, this operation is also known as a "depthwise convolution".
    34|         0|            0|            0|  0.00%|
    35|         0|            0|            0|  0.00%|        In other words, for an input of size :math:`(N, C_{in}, L_{in})`,
    36|         0|            0|            0|  0.00%|        a depthwise convolution with a depthwise multiplier `K` can be performed with the arguments
    37|         0|            0|            0|  0.00%|        :math:`(C_\text{in}=C_\text{in}, C_\text{out}=C_\text{in} \times \text{K}, ..., \text{groups}=C_\text{in})`."""}  # noqa: B950
    38|         0|            0|            0|  0.00%|
    39|         0|            0|            0|  0.00%|
    40|         0|            0|            0|  0.00%|
    41|         0|            0|            0|  0.00%|
    42|         0|            0|            0|  0.00%|
    43|         0|            0|            0|  0.00%|class _ConvNd(Module):
    44|         0|            0|            0|  0.00%|
    45|         0|            0|            0|  0.00%|    __constants__ = ['stride', 'padding', 'dilation', 'groups',
    46|         0|            0|            0|  0.00%|                     'padding_mode', 'output_padding', 'in_channels',
    47|         0|            0|            0|  0.00%|                     'out_channels', 'kernel_size']
    48|         0|            0|            0|  0.00%|    __annotations__ = {'bias': Optional[torch.Tensor]}
    49|         0|            0|            0|  0.00%|
    50|         0|            0|            0|  0.00%|    def _conv_forward(self, input: Tensor, weight: Tensor, bias: Optional[Tensor]) -> Tensor:
    51|         0|            0|            0|  0.00%|        ...
    52|         0|            0|            0|  0.00%|
    53|         0|            0|            0|  0.00%|    _in_channels: int
    54|         0|            0|            0|  0.00%|    _reversed_padding_repeated_twice: List[int]
    55|         0|            0|            0|  0.00%|    out_channels: int
    56|         0|            0|            0|  0.00%|    kernel_size: Tuple[int, ...]
    57|         0|            0|            0|  0.00%|    stride: Tuple[int, ...]
    58|         0|            0|            0|  0.00%|    padding: Union[str, Tuple[int, ...]]
    59|         0|            0|            0|  0.00%|    dilation: Tuple[int, ...]
    60|         0|            0|            0|  0.00%|    transposed: bool
    61|         0|            0|            0|  0.00%|    output_padding: Tuple[int, ...]
    62|         0|            0|            0|  0.00%|    groups: int
    63|         0|            0|            0|  0.00%|    padding_mode: str
    64|         0|            0|            0|  0.00%|    weight: Tensor
    65|         0|            0|            0|  0.00%|    bias: Optional[Tensor]
    66|         0|            0|            0|  0.00%|
    67|         0|            0|            0|  0.00%|    def __init__(self,
    68|         0|            0|            0|  0.00%|                 in_channels: int,
    69|         0|            0|            0|  0.00%|                 out_channels: int,
    70|         0|            0|            0|  0.00%|                 kernel_size: Tuple[int, ...],
    71|         0|            0|            0|  0.00%|                 stride: Tuple[int, ...],
    72|         0|            0|            0|  0.00%|                 padding: Tuple[int, ...],
    73|         0|            0|            0|  0.00%|                 dilation: Tuple[int, ...],
    74|         0|            0|            0|  0.00%|                 transposed: bool,
    75|         0|            0|            0|  0.00%|                 output_padding: Tuple[int, ...],
    76|         0|            0|            0|  0.00%|                 groups: int,
    77|         0|            0|            0|  0.00%|                 bias: bool,
    78|         0|            0|            0|  0.00%|                 padding_mode: str,
    79|         0|            0|            0|  0.00%|                 device=None,
    80|         0|            0|            0|  0.00%|                 dtype=None) -> None:
    81|         0|            0|            0|  0.00%|        factory_kwargs = {'device': device, 'dtype': dtype}
    82|         0|            0|            0|  0.00%|        super(_ConvNd, self).__init__()
    83|         0|            0|            0|  0.00%|        if in_channels % groups != 0:
    84|         0|            0|            0|  0.00%|            raise ValueError('in_channels must be divisible by groups')
    85|         0|            0|            0|  0.00%|        if out_channels % groups != 0:
    86|         0|            0|            0|  0.00%|            raise ValueError('out_channels must be divisible by groups')
    87|         0|            0|            0|  0.00%|        valid_padding_strings = {'same', 'valid'}
    88|         0|            0|            0|  0.00%|        if isinstance(padding, str):
    89|         0|            0|            0|  0.00%|            if padding not in valid_padding_strings:
    90|         0|            0|            0|  0.00%|                raise ValueError(
    91|         0|            0|            0|  0.00%|                    "Invalid padding string {!r}, should be one of {}".format(
    92|         0|            0|            0|  0.00%|                        padding, valid_padding_strings))
    93|         0|            0|            0|  0.00%|            if padding == 'same' and any(s != 1 for s in stride):
    94|         0|            0|            0|  0.00%|                raise ValueError("padding='same' is not supported for strided convolutions")
    95|         0|            0|            0|  0.00%|
    96|         0|            0|            0|  0.00%|        valid_padding_modes = {'zeros', 'reflect', 'replicate', 'circular'}
    97|         0|            0|            0|  0.00%|        if padding_mode not in valid_padding_modes:
    98|         0|            0|            0|  0.00%|            raise ValueError("padding_mode must be one of {}, but got padding_mode='{}'".format(
    99|         0|            0|            0|  0.00%|                valid_padding_modes, padding_mode))
   100|         0|            0|            0|  0.00%|        self.in_channels = in_channels
   101|         0|            0|            0|  0.00%|        self.out_channels = out_channels
   102|         0|            0|            0|  0.00%|        self.kernel_size = kernel_size
   103|         0|            0|            0|  0.00%|        self.stride = stride
   104|         0|            0|            0|  0.00%|        self.padding = padding
   105|         0|            0|            0|  0.00%|        self.dilation = dilation
   106|         0|            0|            0|  0.00%|        self.transposed = transposed
   107|         0|            0|            0|  0.00%|        self.output_padding = output_padding
   108|         0|            0|            0|  0.00%|        self.groups = groups
   109|         0|            0|            0|  0.00%|        self.padding_mode = padding_mode
   110|         0|            0|            0|  0.00%|        # `_reversed_padding_repeated_twice` is the padding to be passed to
   111|         0|            0|            0|  0.00%|        # `F.pad` if needed (e.g., for non-zero padding types that are
   112|         0|            0|            0|  0.00%|        # implemented as two ops: padding + conv). `F.pad` accepts paddings in
   113|         0|            0|            0|  0.00%|        # reverse order than the dimension.
   114|         0|            0|            0|  0.00%|        if isinstance(self.padding, str):
   115|         0|            0|            0|  0.00%|            self._reversed_padding_repeated_twice = [0, 0] * len(kernel_size)
   116|         0|            0|            0|  0.00%|            if padding == 'same':
   117|         0|            0|            0|  0.00%|                for d, k, i in zip(dilation, kernel_size,
   118|         0|            0|            0|  0.00%|                                   range(len(kernel_size) - 1, -1, -1)):
   119|         0|            0|            0|  0.00%|                    total_padding = d * (k - 1)
   120|         0|            0|            0|  0.00%|                    left_pad = total_padding // 2
   121|         0|            0|            0|  0.00%|                    self._reversed_padding_repeated_twice[2 * i] = left_pad
   122|         0|            0|            0|  0.00%|                    self._reversed_padding_repeated_twice[2 * i + 1] = (
   123|         0|            0|            0|  0.00%|                        total_padding - left_pad)
   124|         0|            0|            0|  0.00%|        else:
   125|         0|            0|            0|  0.00%|            self._reversed_padding_repeated_twice = _reverse_repeat_tuple(self.padding, 2)
   126|         0|            0|            0|  0.00%|
   127|         0|            0|            0|  0.00%|        if transposed:
   128|         0|            0|            0|  0.00%|            self.weight = Parameter(torch.empty(
   129|         0|            0|            0|  0.00%|                (in_channels, out_channels // groups, *kernel_size), **factory_kwargs))
   130|         0|            0|            0|  0.00%|        else:
   131|         0|            0|            0|  0.00%|            self.weight = Parameter(torch.empty(
   132|         0|            0|            0|  0.00%|                (out_channels, in_channels // groups, *kernel_size), **factory_kwargs))
   133|         0|            0|            0|  0.00%|        if bias:
   134|         0|            0|            0|  0.00%|            self.bias = Parameter(torch.empty(out_channels, **factory_kwargs))
   135|         0|            0|            0|  0.00%|        else:
   136|         0|            0|            0|  0.00%|            self.register_parameter('bias', None)
   137|         0|            0|            0|  0.00%|
   138|         0|            0|            0|  0.00%|        self.reset_parameters()
   139|         0|            0|            0|  0.00%|
   140|         0|            0|            0|  0.00%|    def reset_parameters(self) -> None:
   141|         0|            0|            0|  0.00%|        init.kaiming_uniform_(self.weight, a=math.sqrt(5))
   142|         0|            0|            0|  0.00%|        if self.bias is not None:
   143|         0|            0|            0|  0.00%|            fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)
   144|         0|            0|            0|  0.00%|            bound = 1 / math.sqrt(fan_in)
   145|         0|            0|            0|  0.00%|            init.uniform_(self.bias, -bound, bound)
   146|         0|            0|            0|  0.00%|
   147|         0|            0|            0|  0.00%|    def extra_repr(self):
   148|         0|            0|            0|  0.00%|        s = ('{in_channels}, {out_channels}, kernel_size={kernel_size}'
   149|         0|            0|            0|  0.00%|             ', stride={stride}')
   150|         0|            0|            0|  0.00%|        if self.padding != (0,) * len(self.padding):
   151|         0|            0|            0|  0.00%|            s += ', padding={padding}'
   152|         0|            0|            0|  0.00%|        if self.dilation != (1,) * len(self.dilation):
   153|         0|            0|            0|  0.00%|            s += ', dilation={dilation}'
   154|         0|            0|            0|  0.00%|        if self.output_padding != (0,) * len(self.output_padding):
   155|         0|            0|            0|  0.00%|            s += ', output_padding={output_padding}'
   156|         0|            0|            0|  0.00%|        if self.groups != 1:
   157|         0|            0|            0|  0.00%|            s += ', groups={groups}'
   158|         0|            0|            0|  0.00%|        if self.bias is None:
   159|         0|            0|            0|  0.00%|            s += ', bias=False'
   160|         0|            0|            0|  0.00%|        if self.padding_mode != 'zeros':
   161|         0|            0|            0|  0.00%|            s += ', padding_mode={padding_mode}'
   162|         0|            0|            0|  0.00%|        return s.format(**self.__dict__)
   163|         0|            0|            0|  0.00%|
   164|         0|            0|            0|  0.00%|    def __setstate__(self, state):
   165|         0|            0|            0|  0.00%|        super(_ConvNd, self).__setstate__(state)
   166|         0|            0|            0|  0.00%|        if not hasattr(self, 'padding_mode'):
   167|         0|            0|            0|  0.00%|            self.padding_mode = 'zeros'
   168|         0|            0|            0|  0.00%|
   169|         0|            0|            0|  0.00%|
   170|         0|            0|            0|  0.00%|class Conv1d(_ConvNd):
   171|         0|            0|            0|  0.00%|    __doc__ = r"""Applies a 1D convolution over an input signal composed of several input
   172|         0|            0|            0|  0.00%|    planes.
   173|         0|            0|            0|  0.00%|
   174|         0|            0|            0|  0.00%|    In the simplest case, the output value of the layer with input size
   175|         0|            0|            0|  0.00%|    :math:`(N, C_{\text{in}}, L)` and output :math:`(N, C_{\text{out}}, L_{\text{out}})` can be
   176|         0|            0|            0|  0.00%|    precisely described as:
   177|         0|            0|            0|  0.00%|
   178|         0|            0|            0|  0.00%|    .. math::
   179|         0|            0|            0|  0.00%|        \text{out}(N_i, C_{\text{out}_j}) = \text{bias}(C_{\text{out}_j}) +
   180|         0|            0|            0|  0.00%|        \sum_{k = 0}^{C_{in} - 1} \text{weight}(C_{\text{out}_j}, k)
   181|         0|            0|            0|  0.00%|        \star \text{input}(N_i, k)
   182|         0|            0|            0|  0.00%|
   183|         0|            0|            0|  0.00%|    where :math:`\star` is the valid `cross-correlation`_ operator,
   184|         0|            0|            0|  0.00%|    :math:`N` is a batch size, :math:`C` denotes a number of channels,
   185|         0|            0|            0|  0.00%|    :math:`L` is a length of signal sequence.
   186|         0|            0|            0|  0.00%|    """ + r"""
   187|         0|            0|            0|  0.00%|
   188|         0|            0|            0|  0.00%|    This module supports :ref:`TensorFloat32<tf32_on_ampere>`.
   189|         0|            0|            0|  0.00%|
   190|         0|            0|            0|  0.00%|    * :attr:`stride` controls the stride for the cross-correlation, a single
   191|         0|            0|            0|  0.00%|      number or a one-element tuple.
   192|         0|            0|            0|  0.00%|
   193|         0|            0|            0|  0.00%|    * :attr:`padding` controls the amount of padding applied to the input. It
   194|         0|            0|            0|  0.00%|      can be either a string {{'valid', 'same'}} or a tuple of ints giving the
   195|         0|            0|            0|  0.00%|      amount of implicit padding applied on both sides.
   196|         0|            0|            0|  0.00%|
   197|         0|            0|            0|  0.00%|    * :attr:`dilation` controls the spacing between the kernel points; also
   198|         0|            0|            0|  0.00%|      known as the Ã  trous algorithm. It is harder to describe, but this `link`_
   199|         0|            0|            0|  0.00%|      has a nice visualization of what :attr:`dilation` does.
   200|         0|            0|            0|  0.00%|
   201|         0|            0|            0|  0.00%|    {groups_note}
   202|         0|            0|            0|  0.00%|
   203|         0|            0|            0|  0.00%|    Note:
   204|         0|            0|            0|  0.00%|        {depthwise_separable_note}
   205|         0|            0|            0|  0.00%|    Note:
   206|         0|            0|            0|  0.00%|        {cudnn_reproducibility_note}
   207|         0|            0|            0|  0.00%|
   208|         0|            0|            0|  0.00%|    Note:
   209|         0|            0|            0|  0.00%|        ``padding='valid'`` is the same as no padding. ``padding='same'`` pads
   210|         0|            0|            0|  0.00%|        the input so the output has the shape as the input. However, this mode
   211|         0|            0|            0|  0.00%|        doesn't support any stride values other than 1.
   212|         0|            0|            0|  0.00%|
   213|         0|            0|            0|  0.00%|    Args:
   214|         0|            0|            0|  0.00%|        in_channels (int): Number of channels in the input image
   215|         0|            0|            0|  0.00%|        out_channels (int): Number of channels produced by the convolution
   216|         0|            0|            0|  0.00%|        kernel_size (int or tuple): Size of the convolving kernel
   217|         0|            0|            0|  0.00%|        stride (int or tuple, optional): Stride of the convolution. Default: 1
   218|         0|            0|            0|  0.00%|        padding (int, tuple or str, optional): Padding added to both sides of
   219|         0|            0|            0|  0.00%|            the input. Default: 0
   220|         0|            0|            0|  0.00%|        padding_mode (string, optional): ``'zeros'``, ``'reflect'``,
   221|         0|            0|            0|  0.00%|            ``'replicate'`` or ``'circular'``. Default: ``'zeros'``
   222|         0|            0|            0|  0.00%|        dilation (int or tuple, optional): Spacing between kernel
   223|         0|            0|            0|  0.00%|            elements. Default: 1
   224|         0|            0|            0|  0.00%|        groups (int, optional): Number of blocked connections from input
   225|         0|            0|            0|  0.00%|            channels to output channels. Default: 1
   226|         0|            0|            0|  0.00%|        bias (bool, optional): If ``True``, adds a learnable bias to the
   227|         0|            0|            0|  0.00%|            output. Default: ``True``
   228|         0|            0|            0|  0.00%|
   229|         0|            0|            0|  0.00%|    """.format(**reproducibility_notes, **convolution_notes) + r"""
   230|         0|            0|            0|  0.00%|
   231|         0|            0|            0|  0.00%|    Shape:
   232|         0|            0|            0|  0.00%|        - Input: :math:`(N, C_{in}, L_{in})`
   233|         0|            0|            0|  0.00%|        - Output: :math:`(N, C_{out}, L_{out})` where
   234|         0|            0|            0|  0.00%|
   235|         0|            0|            0|  0.00%|          .. math::
   236|         0|            0|            0|  0.00%|              L_{out} = \left\lfloor\frac{L_{in} + 2 \times \text{padding} - \text{dilation}
   237|         0|            0|            0|  0.00%|                        \times (\text{kernel\_size} - 1) - 1}{\text{stride}} + 1\right\rfloor
   238|         0|            0|            0|  0.00%|
   239|         0|            0|            0|  0.00%|    Attributes:
   240|         0|            0|            0|  0.00%|        weight (Tensor): the learnable weights of the module of shape
   241|         0|            0|            0|  0.00%|            :math:`(\text{out\_channels},
   242|         0|            0|            0|  0.00%|            \frac{\text{in\_channels}}{\text{groups}}, \text{kernel\_size})`.
   243|         0|            0|            0|  0.00%|            The values of these weights are sampled from
   244|         0|            0|            0|  0.00%|            :math:`\mathcal{U}(-\sqrt{k}, \sqrt{k})` where
   245|         0|            0|            0|  0.00%|            :math:`k = \frac{groups}{C_\text{in} * \text{kernel\_size}}`
   246|         0|            0|            0|  0.00%|        bias (Tensor):   the learnable bias of the module of shape
   247|         0|            0|            0|  0.00%|            (out_channels). If :attr:`bias` is ``True``, then the values of these weights are
   248|         0|            0|            0|  0.00%|            sampled from :math:`\mathcal{U}(-\sqrt{k}, \sqrt{k})` where
   249|         0|            0|            0|  0.00%|            :math:`k = \frac{groups}{C_\text{in} * \text{kernel\_size}}`
   250|         0|            0|            0|  0.00%|
   251|         0|            0|            0|  0.00%|    Examples::
   252|         0|            0|            0|  0.00%|
   253|         0|            0|            0|  0.00%|        >>> m = nn.Conv1d(16, 33, 3, stride=2)
   254|         0|            0|            0|  0.00%|        >>> input = torch.randn(20, 16, 50)
   255|         0|            0|            0|  0.00%|        >>> output = m(input)
   256|         0|            0|            0|  0.00%|
   257|         0|            0|            0|  0.00%|    .. _cross-correlation:
   258|         0|            0|            0|  0.00%|        https://en.wikipedia.org/wiki/Cross-correlation
   259|         0|            0|            0|  0.00%|
   260|         0|            0|            0|  0.00%|    .. _link:
   261|         0|            0|            0|  0.00%|        https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md
   262|         0|            0|            0|  0.00%|    """
   263|         0|            0|            0|  0.00%|
   264|         0|            0|            0|  0.00%|    def __init__(
   265|         0|            0|            0|  0.00%|        self,
   266|         0|            0|            0|  0.00%|        in_channels: int,
   267|         0|            0|            0|  0.00%|        out_channels: int,
   268|         0|            0|            0|  0.00%|        kernel_size: _size_1_t,
   269|         0|            0|            0|  0.00%|        stride: _size_1_t = 1,
   270|         0|            0|            0|  0.00%|        padding: Union[str, _size_1_t] = 0,
   271|         0|            0|            0|  0.00%|        dilation: _size_1_t = 1,
   272|         0|            0|            0|  0.00%|        groups: int = 1,
   273|         0|            0|            0|  0.00%|        bias: bool = True,
   274|         0|            0|            0|  0.00%|        padding_mode: str = 'zeros',  # TODO: refine this type
   275|         0|            0|            0|  0.00%|        device=None,
   276|         0|            0|            0|  0.00%|        dtype=None
   277|         0|            0|            0|  0.00%|    ) -> None:
   278|         0|            0|            0|  0.00%|        factory_kwargs = {'device': device, 'dtype': dtype}
   279|         0|            0|            0|  0.00%|        # we create new variables below to make mypy happy since kernel_size has
   280|         0|            0|            0|  0.00%|        # type Union[int, Tuple[int]] and kernel_size_ has type Tuple[int]
   281|         0|            0|            0|  0.00%|        kernel_size_ = _single(kernel_size)
   282|         0|            0|            0|  0.00%|        stride_ = _single(stride)
   283|         0|            0|            0|  0.00%|        padding_ = padding if isinstance(padding, str) else _single(padding)
   284|         0|            0|            0|  0.00%|        dilation_ = _single(dilation)
   285|         0|            0|            0|  0.00%|        super(Conv1d, self).__init__(
   286|         0|            0|            0|  0.00%|            in_channels, out_channels, kernel_size_, stride_, padding_, dilation_,
   287|         0|            0|            0|  0.00%|            False, _single(0), groups, bias, padding_mode, **factory_kwargs)
   288|         0|            0|            0|  0.00%|
   289|         0|            0|            0|  0.00%|    def _conv_forward(self, input: Tensor, weight: Tensor, bias: Optional[Tensor]):
   290|         0|            0|            0|  0.00%|        if self.padding_mode != 'zeros':
   291|         0|            0|            0|  0.00%|            return F.conv1d(F.pad(input, self._reversed_padding_repeated_twice, mode=self.padding_mode),
   292|         0|            0|            0|  0.00%|                            weight, bias, self.stride,
   293|         0|            0|            0|  0.00%|                            _single(0), self.dilation, self.groups)
   294|         0|            0|            0|  0.00%|        return F.conv1d(input, weight, bias, self.stride,
   295|         0|            0|            0|  0.00%|                        self.padding, self.dilation, self.groups)
   296|         0|            0|            0|  0.00%|
   297|         0|            0|            0|  0.00%|    def forward(self, input: Tensor) -> Tensor:
   298|         0|            0|            0|  0.00%|        return self._conv_forward(input, self.weight, self.bias)
   299|         0|            0|            0|  0.00%|
   300|         0|            0|            0|  0.00%|
   301|         0|            0|            0|  0.00%|class Conv2d(_ConvNd):
   302|         0|            0|            0|  0.00%|    __doc__ = r"""Applies a 2D convolution over an input signal composed of several input
   303|         0|            0|            0|  0.00%|    planes.
   304|         0|            0|            0|  0.00%|
   305|         0|            0|            0|  0.00%|    In the simplest case, the output value of the layer with input size
   306|         0|            0|            0|  0.00%|    :math:`(N, C_{\text{in}}, H, W)` and output :math:`(N, C_{\text{out}}, H_{\text{out}}, W_{\text{out}})`
   307|         0|            0|            0|  0.00%|    can be precisely described as:
   308|         0|            0|            0|  0.00%|
   309|         0|            0|            0|  0.00%|    .. math::
   310|         0|            0|            0|  0.00%|        \text{out}(N_i, C_{\text{out}_j}) = \text{bias}(C_{\text{out}_j}) +
   311|         0|            0|            0|  0.00%|        \sum_{k = 0}^{C_{\text{in}} - 1} \text{weight}(C_{\text{out}_j}, k) \star \text{input}(N_i, k)
   312|         0|            0|            0|  0.00%|
   313|         0|            0|            0|  0.00%|
   314|         0|            0|            0|  0.00%|    where :math:`\star` is the valid 2D `cross-correlation`_ operator,
   315|         0|            0|            0|  0.00%|    :math:`N` is a batch size, :math:`C` denotes a number of channels,
   316|         0|            0|            0|  0.00%|    :math:`H` is a height of input planes in pixels, and :math:`W` is
   317|         0|            0|            0|  0.00%|    width in pixels.
   318|         0|            0|            0|  0.00%|    """ + r"""
   319|         0|            0|            0|  0.00%|
   320|         0|            0|            0|  0.00%|    This module supports :ref:`TensorFloat32<tf32_on_ampere>`.
   321|         0|            0|            0|  0.00%|
   322|         0|            0|            0|  0.00%|    * :attr:`stride` controls the stride for the cross-correlation, a single
   323|         0|            0|            0|  0.00%|      number or a tuple.
   324|         0|            0|            0|  0.00%|
   325|         0|            0|            0|  0.00%|    * :attr:`padding` controls the amount of padding applied to the input. It
   326|         0|            0|            0|  0.00%|      can be either a string {{'valid', 'same'}} or a tuple of ints giving the
   327|         0|            0|            0|  0.00%|      amount of implicit padding applied on both sides.
   328|         0|            0|            0|  0.00%|
   329|         0|            0|            0|  0.00%|    * :attr:`dilation` controls the spacing between the kernel points; also
   330|         0|            0|            0|  0.00%|      known as the Ã  trous algorithm. It is harder to describe, but this `link`_
   331|         0|            0|            0|  0.00%|      has a nice visualization of what :attr:`dilation` does.
   332|         0|            0|            0|  0.00%|
   333|         0|            0|            0|  0.00%|    {groups_note}
   334|         0|            0|            0|  0.00%|
   335|         0|            0|            0|  0.00%|    The parameters :attr:`kernel_size`, :attr:`stride`, :attr:`padding`, :attr:`dilation` can either be:
   336|         0|            0|            0|  0.00%|
   337|         0|            0|            0|  0.00%|        - a single ``int`` -- in which case the same value is used for the height and width dimension
   338|         0|            0|            0|  0.00%|        - a ``tuple`` of two ints -- in which case, the first `int` is used for the height dimension,
   339|         0|            0|            0|  0.00%|          and the second `int` for the width dimension
   340|         0|            0|            0|  0.00%|
   341|         0|            0|            0|  0.00%|    Note:
   342|         0|            0|            0|  0.00%|        {depthwise_separable_note}
   343|         0|            0|            0|  0.00%|
   344|         0|            0|            0|  0.00%|    Note:
   345|         0|            0|            0|  0.00%|        {cudnn_reproducibility_note}
   346|         0|            0|            0|  0.00%|
   347|         0|            0|            0|  0.00%|    Note:
   348|         0|            0|            0|  0.00%|        ``padding='valid'`` is the same as no padding. ``padding='same'`` pads
   349|         0|            0|            0|  0.00%|        the input so the output has the shape as the input. However, this mode
   350|         0|            0|            0|  0.00%|        doesn't support any stride values other than 1.
   351|         0|            0|            0|  0.00%|
   352|         0|            0|            0|  0.00%|    Args:
   353|         0|            0|            0|  0.00%|        in_channels (int): Number of channels in the input image
   354|         0|            0|            0|  0.00%|        out_channels (int): Number of channels produced by the convolution
   355|         0|            0|            0|  0.00%|        kernel_size (int or tuple): Size of the convolving kernel
   356|         0|            0|            0|  0.00%|        stride (int or tuple, optional): Stride of the convolution. Default: 1
   357|         0|            0|            0|  0.00%|        padding (int, tuple or str, optional): Padding added to all four sides of
   358|         0|            0|            0|  0.00%|            the input. Default: 0
   359|         0|            0|            0|  0.00%|        padding_mode (string, optional): ``'zeros'``, ``'reflect'``,
   360|         0|            0|            0|  0.00%|            ``'replicate'`` or ``'circular'``. Default: ``'zeros'``
   361|         0|            0|            0|  0.00%|        dilation (int or tuple, optional): Spacing between kernel elements. Default: 1
   362|         0|            0|            0|  0.00%|        groups (int, optional): Number of blocked connections from input
   363|         0|            0|            0|  0.00%|            channels to output channels. Default: 1
   364|         0|            0|            0|  0.00%|        bias (bool, optional): If ``True``, adds a learnable bias to the
   365|         0|            0|            0|  0.00%|            output. Default: ``True``
   366|         0|            0|            0|  0.00%|    """.format(**reproducibility_notes, **convolution_notes) + r"""
   367|         0|            0|            0|  0.00%|
   368|         0|            0|            0|  0.00%|    Shape:
   369|         0|            0|            0|  0.00%|        - Input: :math:`(N, C_{in}, H_{in}, W_{in})`
   370|         0|            0|            0|  0.00%|        - Output: :math:`(N, C_{out}, H_{out}, W_{out})` where
   371|         0|            0|            0|  0.00%|
   372|         0|            0|            0|  0.00%|          .. math::
   373|         0|            0|            0|  0.00%|              H_{out} = \left\lfloor\frac{H_{in}  + 2 \times \text{padding}[0] - \text{dilation}[0]
   374|         0|            0|            0|  0.00%|                        \times (\text{kernel\_size}[0] - 1) - 1}{\text{stride}[0]} + 1\right\rfloor
   375|         0|            0|            0|  0.00%|
   376|         0|            0|            0|  0.00%|          .. math::
   377|         0|            0|            0|  0.00%|              W_{out} = \left\lfloor\frac{W_{in}  + 2 \times \text{padding}[1] - \text{dilation}[1]
   378|         0|            0|            0|  0.00%|                        \times (\text{kernel\_size}[1] - 1) - 1}{\text{stride}[1]} + 1\right\rfloor
   379|         0|            0|            0|  0.00%|
   380|         0|            0|            0|  0.00%|    Attributes:
   381|         0|            0|            0|  0.00%|        weight (Tensor): the learnable weights of the module of shape
   382|         0|            0|            0|  0.00%|            :math:`(\text{out\_channels}, \frac{\text{in\_channels}}{\text{groups}},`
   383|         0|            0|            0|  0.00%|            :math:`\text{kernel\_size[0]}, \text{kernel\_size[1]})`.
   384|         0|            0|            0|  0.00%|            The values of these weights are sampled from
   385|         0|            0|            0|  0.00%|            :math:`\mathcal{U}(-\sqrt{k}, \sqrt{k})` where
   386|         0|            0|            0|  0.00%|            :math:`k = \frac{groups}{C_\text{in} * \prod_{i=0}^{1}\text{kernel\_size}[i]}`
   387|         0|            0|            0|  0.00%|        bias (Tensor):   the learnable bias of the module of shape
   388|         0|            0|            0|  0.00%|            (out_channels). If :attr:`bias` is ``True``,
   389|         0|            0|            0|  0.00%|            then the values of these weights are
   390|         0|            0|            0|  0.00%|            sampled from :math:`\mathcal{U}(-\sqrt{k}, \sqrt{k})` where
   391|         0|            0|            0|  0.00%|            :math:`k = \frac{groups}{C_\text{in} * \prod_{i=0}^{1}\text{kernel\_size}[i]}`
   392|         0|            0|            0|  0.00%|
   393|         0|            0|            0|  0.00%|    Examples:
   394|         0|            0|            0|  0.00%|
   395|         0|            0|            0|  0.00%|        >>> # With square kernels and equal stride
   396|         0|            0|            0|  0.00%|        >>> m = nn.Conv2d(16, 33, 3, stride=2)
   397|         0|            0|            0|  0.00%|        >>> # non-square kernels and unequal stride and with padding
   398|         0|            0|            0|  0.00%|        >>> m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2))
   399|         0|            0|            0|  0.00%|        >>> # non-square kernels and unequal stride and with padding and dilation
   400|         0|            0|            0|  0.00%|        >>> m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2), dilation=(3, 1))
   401|         0|            0|            0|  0.00%|        >>> input = torch.randn(20, 16, 50, 100)
   402|         0|            0|            0|  0.00%|        >>> output = m(input)
   403|         0|            0|            0|  0.00%|
   404|         0|            0|            0|  0.00%|    .. _cross-correlation:
   405|         0|            0|            0|  0.00%|        https://en.wikipedia.org/wiki/Cross-correlation
   406|         0|            0|            0|  0.00%|
   407|         0|            0|            0|  0.00%|    .. _link:
   408|         0|            0|            0|  0.00%|        https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md
   409|         0|            0|            0|  0.00%|    """
   410|         0|            0|            0|  0.00%|
   411|         0|            0|            0|  0.00%|    def __init__(
   412|         0|            0|            0|  0.00%|        self,
   413|         0|            0|            0|  0.00%|        in_channels: int,
   414|         0|            0|            0|  0.00%|        out_channels: int,
   415|         0|            0|            0|  0.00%|        kernel_size: _size_2_t,
   416|         0|            0|            0|  0.00%|        stride: _size_2_t = 1,
   417|         0|            0|            0|  0.00%|        padding: Union[str, _size_2_t] = 0,
   418|         0|            0|            0|  0.00%|        dilation: _size_2_t = 1,
   419|         0|            0|            0|  0.00%|        groups: int = 1,
   420|         0|            0|            0|  0.00%|        bias: bool = True,
   421|         0|            0|            0|  0.00%|        padding_mode: str = 'zeros',  # TODO: refine this type
   422|         0|            0|            0|  0.00%|        device=None,
   423|         0|            0|            0|  0.00%|        dtype=None
   424|         0|            0|            0|  0.00%|    ) -> None:
   425|         0|            0|            0|  0.00%|        factory_kwargs = {'device': device, 'dtype': dtype}
   426|         0|            0|            0|  0.00%|        kernel_size_ = _pair(kernel_size)
   427|         0|            0|            0|  0.00%|        stride_ = _pair(stride)
   428|         0|            0|            0|  0.00%|        padding_ = padding if isinstance(padding, str) else _pair(padding)
   429|         0|            0|            0|  0.00%|        dilation_ = _pair(dilation)
   430|         0|            0|            0|  0.00%|        super(Conv2d, self).__init__(
   431|         0|            0|            0|  0.00%|            in_channels, out_channels, kernel_size_, stride_, padding_, dilation_,
   432|         0|            0|            0|  0.00%|            False, _pair(0), groups, bias, padding_mode, **factory_kwargs)
   433|         0|            0|            0|  0.00%|
   434|      2000|    0.0103116|   5.1558e-06|  0.02%|    def _conv_forward(self, input: Tensor, weight: Tensor, bias: Optional[Tensor]):
   435|      2000|     0.010427|   5.2135e-06|  0.02%|        if self.padding_mode != 'zeros':
   436|         0|            0|            0|  0.00%|            return F.conv2d(F.pad(input, self._reversed_padding_repeated_twice, mode=self.padding_mode),
   437|         0|            0|            0|  0.00%|                            weight, bias, self.stride,
   438|         0|            0|            0|  0.00%|                            _pair(0), self.dilation, self.groups)
   439|      4000|      11.3276|    0.0028319| 21.14%|        return F.conv2d(input, weight, bias, self.stride,
   440|      2000|   0.00987673|  4.93836e-06|  0.02%|                        self.padding, self.dilation, self.groups)
   441|         0|            0|            0|  0.00%|
   442|      2000|   0.00991583|  4.95791e-06|  0.02%|    def forward(self, input: Tensor) -> Tensor:
   443|      2000|    0.0884655|  4.42327e-05|  0.17%|        return self._conv_forward(input, self.weight, self.bias)
(call)|      4000|    0.0943375|  2.35844e-05|  0.18%|# /opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1117 __getattr__
(call)|      2000|      11.3582|    0.0056791| 21.20%|# /opt/conda/lib/python3.8/site-packages/torch/nn/modules/conv.py:434 _conv_forward
   444|         0|            0|            0|  0.00%|
   445|         0|            0|            0|  0.00%|class Conv3d(_ConvNd):
   446|         0|            0|            0|  0.00%|    __doc__ = r"""Applies a 3D convolution over an input signal composed of several input
   447|         0|            0|            0|  0.00%|    planes.
   448|         0|            0|            0|  0.00%|
   449|         0|            0|            0|  0.00%|    In the simplest case, the output value of the layer with input size :math:`(N, C_{in}, D, H, W)`
   450|         0|            0|            0|  0.00%|    and output :math:`(N, C_{out}, D_{out}, H_{out}, W_{out})` can be precisely described as:
   451|         0|            0|            0|  0.00%|
   452|         0|            0|            0|  0.00%|    .. math::
   453|         0|            0|            0|  0.00%|        out(N_i, C_{out_j}) = bias(C_{out_j}) +
   454|         0|            0|            0|  0.00%|                                \sum_{k = 0}^{C_{in} - 1} weight(C_{out_j}, k) \star input(N_i, k)
   455|         0|            0|            0|  0.00%|
   456|         0|            0|            0|  0.00%|    where :math:`\star` is the valid 3D `cross-correlation`_ operator
   457|         0|            0|            0|  0.00%|    """ + r"""
   458|         0|            0|            0|  0.00%|
   459|         0|            0|            0|  0.00%|    This module supports :ref:`TensorFloat32<tf32_on_ampere>`.
   460|         0|            0|            0|  0.00%|
   461|         0|            0|            0|  0.00%|    * :attr:`stride` controls the stride for the cross-correlation.
   462|         0|            0|            0|  0.00%|
   463|         0|            0|            0|  0.00%|    * :attr:`padding` controls the amount of padding applied to the input. It
   464|         0|            0|            0|  0.00%|      can be either a string {{'valid', 'same'}} or a tuple of ints giving the
   465|         0|            0|            0|  0.00%|      amount of implicit padding applied on both sides.
   466|         0|            0|            0|  0.00%|
   467|         0|            0|            0|  0.00%|    * :attr:`dilation` controls the spacing between the kernel points; also known as the Ã  trous algorithm.
   468|         0|            0|            0|  0.00%|      It is harder to describe, but this `link`_ has a nice visualization of what :attr:`dilation` does.
   469|         0|            0|            0|  0.00%|
   470|         0|            0|            0|  0.00%|    {groups_note}
   471|         0|            0|            0|  0.00%|
   472|         0|            0|            0|  0.00%|    The parameters :attr:`kernel_size`, :attr:`stride`, :attr:`padding`, :attr:`dilation` can either be:
   473|         0|            0|            0|  0.00%|
   474|         0|            0|            0|  0.00%|        - a single ``int`` -- in which case the same value is used for the depth, height and width dimension
   475|         0|            0|            0|  0.00%|        - a ``tuple`` of three ints -- in which case, the first `int` is used for the depth dimension,
   476|         0|            0|            0|  0.00%|          the second `int` for the height dimension and the third `int` for the width dimension
   477|         0|            0|            0|  0.00%|
   478|         0|            0|            0|  0.00%|    Note:
   479|         0|            0|            0|  0.00%|        {depthwise_separable_note}
   480|         0|            0|            0|  0.00%|
   481|         0|            0|            0|  0.00%|    Note:
   482|         0|            0|            0|  0.00%|        {cudnn_reproducibility_note}
   483|         0|            0|            0|  0.00%|
   484|         0|            0|            0|  0.00%|    Note:
   485|         0|            0|            0|  0.00%|        ``padding='valid'`` is the same as no padding. ``padding='same'`` pads
   486|         0|            0|            0|  0.00%|        the input so the output has the shape as the input. However, this mode
   487|         0|            0|            0|  0.00%|        doesn't support any stride values other than 1.
   488|         0|            0|            0|  0.00%|
   489|         0|            0|            0|  0.00%|    Args:
   490|         0|            0|            0|  0.00%|        in_channels (int): Number of channels in the input image
   491|         0|            0|            0|  0.00%|        out_channels (int): Number of channels produced by the convolution
   492|         0|            0|            0|  0.00%|        kernel_size (int or tuple): Size of the convolving kernel
   493|         0|            0|            0|  0.00%|        stride (int or tuple, optional): Stride of the convolution. Default: 1
   494|         0|            0|            0|  0.00%|        padding (int, tuple or str, optional): Padding added to all six sides of
   495|         0|            0|            0|  0.00%|            the input. Default: 0
   496|         0|            0|            0|  0.00%|        padding_mode (string, optional): ``'zeros'``, ``'reflect'``, ``'replicate'`` or ``'circular'``. Default: ``'zeros'``
   497|         0|            0|            0|  0.00%|        dilation (int or tuple, optional): Spacing between kernel elements. Default: 1
   498|         0|            0|            0|  0.00%|        groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1
   499|         0|            0|            0|  0.00%|        bias (bool, optional): If ``True``, adds a learnable bias to the output. Default: ``True``
   500|         0|            0|            0|  0.00%|    """.format(**reproducibility_notes, **convolution_notes) + r"""
   501|         0|            0|            0|  0.00%|
   502|         0|            0|            0|  0.00%|    Shape:
   503|         0|            0|            0|  0.00%|        - Input: :math:`(N, C_{in}, D_{in}, H_{in}, W_{in})`
   504|         0|            0|            0|  0.00%|        - Output: :math:`(N, C_{out}, D_{out}, H_{out}, W_{out})` where
   505|         0|            0|            0|  0.00%|
   506|         0|            0|            0|  0.00%|          .. math::
   507|         0|            0|            0|  0.00%|              D_{out} = \left\lfloor\frac{D_{in} + 2 \times \text{padding}[0] - \text{dilation}[0]
   508|         0|            0|            0|  0.00%|                    \times (\text{kernel\_size}[0] - 1) - 1}{\text{stride}[0]} + 1\right\rfloor
   509|         0|            0|            0|  0.00%|
   510|         0|            0|            0|  0.00%|          .. math::
   511|         0|            0|            0|  0.00%|              H_{out} = \left\lfloor\frac{H_{in} + 2 \times \text{padding}[1] - \text{dilation}[1]
   512|         0|            0|            0|  0.00%|                    \times (\text{kernel\_size}[1] - 1) - 1}{\text{stride}[1]} + 1\right\rfloor
   513|         0|            0|            0|  0.00%|
   514|         0|            0|            0|  0.00%|          .. math::
   515|         0|            0|            0|  0.00%|              W_{out} = \left\lfloor\frac{W_{in} + 2 \times \text{padding}[2] - \text{dilation}[2]
   516|         0|            0|            0|  0.00%|                    \times (\text{kernel\_size}[2] - 1) - 1}{\text{stride}[2]} + 1\right\rfloor
   517|         0|            0|            0|  0.00%|
   518|         0|            0|            0|  0.00%|    Attributes:
   519|         0|            0|            0|  0.00%|        weight (Tensor): the learnable weights of the module of shape
   520|         0|            0|            0|  0.00%|                         :math:`(\text{out\_channels}, \frac{\text{in\_channels}}{\text{groups}},`
   521|         0|            0|            0|  0.00%|                         :math:`\text{kernel\_size[0]}, \text{kernel\_size[1]}, \text{kernel\_size[2]})`.
   522|         0|            0|            0|  0.00%|                         The values of these weights are sampled from
   523|         0|            0|            0|  0.00%|                         :math:`\mathcal{U}(-\sqrt{k}, \sqrt{k})` where
   524|         0|            0|            0|  0.00%|                         :math:`k = \frac{groups}{C_\text{in} * \prod_{i=0}^{2}\text{kernel\_size}[i]}`
   525|         0|            0|            0|  0.00%|        bias (Tensor):   the learnable bias of the module of shape (out_channels). If :attr:`bias` is ``True``,
   526|         0|            0|            0|  0.00%|                         then the values of these weights are
   527|         0|            0|            0|  0.00%|                         sampled from :math:`\mathcal{U}(-\sqrt{k}, \sqrt{k})` where
   528|         0|            0|            0|  0.00%|                         :math:`k = \frac{groups}{C_\text{in} * \prod_{i=0}^{2}\text{kernel\_size}[i]}`
   529|         0|            0|            0|  0.00%|
   530|         0|            0|            0|  0.00%|    Examples::
   531|         0|            0|            0|  0.00%|
   532|         0|            0|            0|  0.00%|        >>> # With square kernels and equal stride
   533|         0|            0|            0|  0.00%|        >>> m = nn.Conv3d(16, 33, 3, stride=2)
   534|         0|            0|            0|  0.00%|        >>> # non-square kernels and unequal stride and with padding
   535|         0|            0|            0|  0.00%|        >>> m = nn.Conv3d(16, 33, (3, 5, 2), stride=(2, 1, 1), padding=(4, 2, 0))
   536|         0|            0|            0|  0.00%|        >>> input = torch.randn(20, 16, 10, 50, 100)
   537|         0|            0|            0|  0.00%|        >>> output = m(input)
   538|         0|            0|            0|  0.00%|
   539|         0|            0|            0|  0.00%|    .. _cross-correlation:
   540|         0|            0|            0|  0.00%|        https://en.wikipedia.org/wiki/Cross-correlation
   541|         0|            0|            0|  0.00%|
   542|         0|            0|            0|  0.00%|    .. _link:
   543|         0|            0|            0|  0.00%|        https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md
   544|         0|            0|            0|  0.00%|    """
   545|         0|            0|            0|  0.00%|
   546|         0|            0|            0|  0.00%|    def __init__(
   547|         0|            0|            0|  0.00%|        self,
   548|         0|            0|            0|  0.00%|        in_channels: int,
   549|         0|            0|            0|  0.00%|        out_channels: int,
   550|         0|            0|            0|  0.00%|        kernel_size: _size_3_t,
   551|         0|            0|            0|  0.00%|        stride: _size_3_t = 1,
   552|         0|            0|            0|  0.00%|        padding: Union[str, _size_3_t] = 0,
   553|         0|            0|            0|  0.00%|        dilation: _size_3_t = 1,
   554|         0|            0|            0|  0.00%|        groups: int = 1,
   555|         0|            0|            0|  0.00%|        bias: bool = True,
   556|         0|            0|            0|  0.00%|        padding_mode: str = 'zeros',
   557|         0|            0|            0|  0.00%|        device=None,
   558|         0|            0|            0|  0.00%|        dtype=None
   559|         0|            0|            0|  0.00%|    ) -> None:
   560|         0|            0|            0|  0.00%|        factory_kwargs = {'device': device, 'dtype': dtype}
   561|         0|            0|            0|  0.00%|        kernel_size_ = _triple(kernel_size)
   562|         0|            0|            0|  0.00%|        stride_ = _triple(stride)
   563|         0|            0|            0|  0.00%|        padding_ = padding if isinstance(padding, str) else _triple(padding)
   564|         0|            0|            0|  0.00%|        dilation_ = _triple(dilation)
   565|         0|            0|            0|  0.00%|        super(Conv3d, self).__init__(
   566|         0|            0|            0|  0.00%|            in_channels, out_channels, kernel_size_, stride_, padding_, dilation_,
   567|         0|            0|            0|  0.00%|            False, _triple(0), groups, bias, padding_mode, **factory_kwargs)
   568|         0|            0|            0|  0.00%|
   569|         0|            0|            0|  0.00%|    def _conv_forward(self, input: Tensor, weight: Tensor, bias: Optional[Tensor]):
   570|         0|            0|            0|  0.00%|        if self.padding_mode != "zeros":
   571|         0|            0|            0|  0.00%|            return F.conv3d(
   572|         0|            0|            0|  0.00%|                F.pad(
   573|         0|            0|            0|  0.00%|                    input, self._reversed_padding_repeated_twice, mode=self.padding_mode
   574|         0|            0|            0|  0.00%|                ),
   575|         0|            0|            0|  0.00%|                weight,
   576|         0|            0|            0|  0.00%|                bias,
   577|         0|            0|            0|  0.00%|                self.stride,
   578|         0|            0|            0|  0.00%|                _triple(0),
   579|         0|            0|            0|  0.00%|                self.dilation,
   580|         0|            0|            0|  0.00%|                self.groups,
   581|         0|            0|            0|  0.00%|            )
   582|         0|            0|            0|  0.00%|        return F.conv3d(
   583|         0|            0|            0|  0.00%|            input, weight, bias, self.stride, self.padding, self.dilation, self.groups
   584|         0|            0|            0|  0.00%|        )
   585|         0|            0|            0|  0.00%|
   586|         0|            0|            0|  0.00%|    def forward(self, input: Tensor) -> Tensor:
   587|         0|            0|            0|  0.00%|        return self._conv_forward(input, self.weight, self.bias)
   588|         0|            0|            0|  0.00%|
   589|         0|            0|            0|  0.00%|
   590|         0|            0|            0|  0.00%|
   591|         0|            0|            0|  0.00%|class _ConvTransposeNd(_ConvNd):
   592|         0|            0|            0|  0.00%|    def __init__(self, in_channels, out_channels, kernel_size, stride,
   593|         0|            0|            0|  0.00%|                 padding, dilation, transposed, output_padding,
   594|         0|            0|            0|  0.00%|                 groups, bias, padding_mode, device=None, dtype=None) -> None:
   595|         0|            0|            0|  0.00%|        if padding_mode != 'zeros':
   596|         0|            0|            0|  0.00%|            raise ValueError('Only "zeros" padding mode is supported for {}'.format(self.__class__.__name__))
   597|         0|            0|            0|  0.00%|
   598|         0|            0|            0|  0.00%|        factory_kwargs = {'device': device, 'dtype': dtype}
   599|         0|            0|            0|  0.00%|        super(_ConvTransposeNd, self).__init__(
   600|         0|            0|            0|  0.00%|            in_channels, out_channels, kernel_size, stride,
   601|         0|            0|            0|  0.00%|            padding, dilation, transposed, output_padding,
   602|         0|            0|            0|  0.00%|            groups, bias, padding_mode, **factory_kwargs)
   603|         0|            0|            0|  0.00%|
   604|         0|            0|            0|  0.00%|    # dilation being an optional parameter is for backwards
   605|         0|            0|            0|  0.00%|    # compatibility
   606|         0|            0|            0|  0.00%|    def _output_padding(self, input: Tensor, output_size: Optional[List[int]],
   607|         0|            0|            0|  0.00%|                        stride: List[int], padding: List[int], kernel_size: List[int],
   608|         0|            0|            0|  0.00%|                        dilation: Optional[List[int]] = None) -> List[int]:
   609|         0|            0|            0|  0.00%|        if output_size is None:
   610|         0|            0|            0|  0.00%|            ret = _single(self.output_padding)  # converting to list if was not already
   611|         0|            0|            0|  0.00%|        else:
   612|         0|            0|            0|  0.00%|            k = input.dim() - 2
   613|         0|            0|            0|  0.00%|            if len(output_size) == k + 2:
   614|         0|            0|            0|  0.00%|                output_size = output_size[2:]
   615|         0|            0|            0|  0.00%|            if len(output_size) != k:
   616|         0|            0|            0|  0.00%|                raise ValueError(
   617|         0|            0|            0|  0.00%|                    "output_size must have {} or {} elements (got {})"
   618|         0|            0|            0|  0.00%|                    .format(k, k + 2, len(output_size)))
   619|         0|            0|            0|  0.00%|
   620|         0|            0|            0|  0.00%|            min_sizes = torch.jit.annotate(List[int], [])
   621|         0|            0|            0|  0.00%|            max_sizes = torch.jit.annotate(List[int], [])
   622|         0|            0|            0|  0.00%|            for d in range(k):
   623|         0|            0|            0|  0.00%|                dim_size = ((input.size(d + 2) - 1) * stride[d] -
   624|         0|            0|            0|  0.00%|                            2 * padding[d] +
   625|         0|            0|            0|  0.00%|                            (dilation[d] if dilation is not None else 1) * (kernel_size[d] - 1) + 1)
   626|         0|            0|            0|  0.00%|                min_sizes.append(dim_size)
   627|         0|            0|            0|  0.00%|                max_sizes.append(min_sizes[d] + stride[d] - 1)
   628|         0|            0|            0|  0.00%|
   629|         0|            0|            0|  0.00%|            for i in range(len(output_size)):
   630|         0|            0|            0|  0.00%|                size = output_size[i]
   631|         0|            0|            0|  0.00%|                min_size = min_sizes[i]
   632|         0|            0|            0|  0.00%|                max_size = max_sizes[i]
   633|         0|            0|            0|  0.00%|                if size < min_size or size > max_size:
   634|         0|            0|            0|  0.00%|                    raise ValueError((
   635|         0|            0|            0|  0.00%|                        "requested an output size of {}, but valid sizes range "
   636|         0|            0|            0|  0.00%|                        "from {} to {} (for an input of {})").format(
   637|         0|            0|            0|  0.00%|                            output_size, min_sizes, max_sizes, input.size()[2:]))
   638|         0|            0|            0|  0.00%|
   639|         0|            0|            0|  0.00%|            res = torch.jit.annotate(List[int], [])
   640|         0|            0|            0|  0.00%|            for d in range(k):
   641|         0|            0|            0|  0.00%|                res.append(output_size[d] - min_sizes[d])
   642|         0|            0|            0|  0.00%|
   643|         0|            0|            0|  0.00%|            ret = res
   644|         0|            0|            0|  0.00%|        return ret
   645|         0|            0|            0|  0.00%|
   646|         0|            0|            0|  0.00%|
   647|         0|            0|            0|  0.00%|class ConvTranspose1d(_ConvTransposeNd):
   648|         0|            0|            0|  0.00%|    __doc__ = r"""Applies a 1D transposed convolution operator over an input image
   649|         0|            0|            0|  0.00%|    composed of several input planes.
   650|         0|            0|            0|  0.00%|
   651|         0|            0|            0|  0.00%|    This module can be seen as the gradient of Conv1d with respect to its input.
   652|         0|            0|            0|  0.00%|    It is also known as a fractionally-strided convolution or
   653|         0|            0|            0|  0.00%|    a deconvolution (although it is not an actual deconvolution operation).
   654|         0|            0|            0|  0.00%|
   655|         0|            0|            0|  0.00%|    This module supports :ref:`TensorFloat32<tf32_on_ampere>`.
   656|         0|            0|            0|  0.00%|
   657|         0|            0|            0|  0.00%|    * :attr:`stride` controls the stride for the cross-correlation.
   658|         0|            0|            0|  0.00%|
   659|         0|            0|            0|  0.00%|    * :attr:`padding` controls the amount of implicit zero padding on both
   660|         0|            0|            0|  0.00%|      sides for ``dilation * (kernel_size - 1) - padding`` number of points. See note
   661|         0|            0|            0|  0.00%|      below for details.
   662|         0|            0|            0|  0.00%|
   663|         0|            0|            0|  0.00%|    * :attr:`output_padding` controls the additional size added to one side
   664|         0|            0|            0|  0.00%|      of the output shape. See note below for details.
   665|         0|            0|            0|  0.00%|
   666|         0|            0|            0|  0.00%|    * :attr:`dilation` controls the spacing between the kernel points; also known as the Ã  trous algorithm.
   667|         0|            0|            0|  0.00%|      It is harder to describe, but this `link`_ has a nice visualization of what :attr:`dilation` does.
   668|         0|            0|            0|  0.00%|
   669|         0|            0|            0|  0.00%|    {groups_note}
   670|         0|            0|            0|  0.00%|
   671|         0|            0|            0|  0.00%|    Note:
   672|         0|            0|            0|  0.00%|        The :attr:`padding` argument effectively adds ``dilation * (kernel_size - 1) - padding``
   673|         0|            0|            0|  0.00%|        amount of zero padding to both sizes of the input. This is set so that
   674|         0|            0|            0|  0.00%|        when a :class:`~torch.nn.Conv1d` and a :class:`~torch.nn.ConvTranspose1d`
   675|         0|            0|            0|  0.00%|        are initialized with same parameters, they are inverses of each other in
   676|         0|            0|            0|  0.00%|        regard to the input and output shapes. However, when ``stride > 1``,
   677|         0|            0|            0|  0.00%|        :class:`~torch.nn.Conv1d` maps multiple input shapes to the same output
   678|         0|            0|            0|  0.00%|        shape. :attr:`output_padding` is provided to resolve this ambiguity by
   679|         0|            0|            0|  0.00%|        effectively increasing the calculated output shape on one side. Note
   680|         0|            0|            0|  0.00%|        that :attr:`output_padding` is only used to find output shape, but does
   681|         0|            0|            0|  0.00%|        not actually add zero-padding to output.
   682|         0|            0|            0|  0.00%|
   683|         0|            0|            0|  0.00%|    Note:
   684|         0|            0|            0|  0.00%|        In some circumstances when using the CUDA backend with CuDNN, this operator
   685|         0|            0|            0|  0.00%|        may select a nondeterministic algorithm to increase performance. If this is
   686|         0|            0|            0|  0.00%|        undesirable, you can try to make the operation deterministic (potentially at
   687|         0|            0|            0|  0.00%|        a performance cost) by setting ``torch.backends.cudnn.deterministic =
   688|         0|            0|            0|  0.00%|        True``.
   689|         0|            0|            0|  0.00%|        Please see the notes on :doc:`/notes/randomness` for background.
   690|         0|            0|            0|  0.00%|
   691|         0|            0|            0|  0.00%|
   692|         0|            0|            0|  0.00%|    Args:
   693|         0|            0|            0|  0.00%|        in_channels (int): Number of channels in the input image
   694|         0|            0|            0|  0.00%|        out_channels (int): Number of channels produced by the convolution
   695|         0|            0|            0|  0.00%|        kernel_size (int or tuple): Size of the convolving kernel
   696|         0|            0|            0|  0.00%|        stride (int or tuple, optional): Stride of the convolution. Default: 1
   697|         0|            0|            0|  0.00%|        padding (int or tuple, optional): ``dilation * (kernel_size - 1) - padding`` zero-padding
   698|         0|            0|            0|  0.00%|            will be added to both sides of the input. Default: 0
   699|         0|            0|            0|  0.00%|        output_padding (int or tuple, optional): Additional size added to one side
   700|         0|            0|            0|  0.00%|            of the output shape. Default: 0
   701|         0|            0|            0|  0.00%|        groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1
   702|         0|            0|            0|  0.00%|        bias (bool, optional): If ``True``, adds a learnable bias to the output. Default: ``True``
   703|         0|            0|            0|  0.00%|        dilation (int or tuple, optional): Spacing between kernel elements. Default: 1
   704|         0|            0|            0|  0.00%|    """.format(**reproducibility_notes, **convolution_notes) + r"""
   705|         0|            0|            0|  0.00%|
   706|         0|            0|            0|  0.00%|    Shape:
   707|         0|            0|            0|  0.00%|        - Input: :math:`(N, C_{in}, L_{in})`
   708|         0|            0|            0|  0.00%|        - Output: :math:`(N, C_{out}, L_{out})` where
   709|         0|            0|            0|  0.00%|
   710|         0|            0|            0|  0.00%|          .. math::
   711|         0|            0|            0|  0.00%|              L_{out} = (L_{in} - 1) \times \text{stride} - 2 \times \text{padding} + \text{dilation}
   712|         0|            0|            0|  0.00%|                        \times (\text{kernel\_size} - 1) + \text{output\_padding} + 1
   713|         0|            0|            0|  0.00%|
   714|         0|            0|            0|  0.00%|    Attributes:
   715|         0|            0|            0|  0.00%|        weight (Tensor): the learnable weights of the module of shape
   716|         0|            0|            0|  0.00%|                         :math:`(\text{in\_channels}, \frac{\text{out\_channels}}{\text{groups}},`
   717|         0|            0|            0|  0.00%|                         :math:`\text{kernel\_size})`.
   718|         0|            0|            0|  0.00%|                         The values of these weights are sampled from
   719|         0|            0|            0|  0.00%|                         :math:`\mathcal{U}(-\sqrt{k}, \sqrt{k})` where
   720|         0|            0|            0|  0.00%|                         :math:`k = \frac{groups}{C_\text{out} * \text{kernel\_size}}`
   721|         0|            0|            0|  0.00%|        bias (Tensor):   the learnable bias of the module of shape (out_channels).
   722|         0|            0|            0|  0.00%|                         If :attr:`bias` is ``True``, then the values of these weights are
   723|         0|            0|            0|  0.00%|                         sampled from :math:`\mathcal{U}(-\sqrt{k}, \sqrt{k})` where
   724|         0|            0|            0|  0.00%|                         :math:`k = \frac{groups}{C_\text{out} * \text{kernel\_size}}`
   725|         0|            0|            0|  0.00%|
   726|         0|            0|            0|  0.00%|    .. _cross-correlation:
   727|         0|            0|            0|  0.00%|        https://en.wikipedia.org/wiki/Cross-correlation
   728|         0|            0|            0|  0.00%|
   729|         0|            0|            0|  0.00%|    .. _link:
   730|         0|            0|            0|  0.00%|        https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md
   731|         0|            0|            0|  0.00%|    """
   732|         0|            0|            0|  0.00%|
   733|         0|            0|            0|  0.00%|    def __init__(
   734|         0|            0|            0|  0.00%|        self,
   735|         0|            0|            0|  0.00%|        in_channels: int,
   736|         0|            0|            0|  0.00%|        out_channels: int,
   737|         0|            0|            0|  0.00%|        kernel_size: _size_1_t,
   738|         0|            0|            0|  0.00%|        stride: _size_1_t = 1,
   739|         0|            0|            0|  0.00%|        padding: _size_1_t = 0,
   740|         0|            0|            0|  0.00%|        output_padding: _size_1_t = 0,
   741|         0|            0|            0|  0.00%|        groups: int = 1,
   742|         0|            0|            0|  0.00%|        bias: bool = True,
   743|         0|            0|            0|  0.00%|        dilation: _size_1_t = 1,
   744|         0|            0|            0|  0.00%|        padding_mode: str = 'zeros',
   745|         0|            0|            0|  0.00%|        device=None,
   746|         0|            0|            0|  0.00%|        dtype=None
   747|         0|            0|            0|  0.00%|    ) -> None:
   748|         0|            0|            0|  0.00%|        factory_kwargs = {'device': device, 'dtype': dtype}
   749|         0|            0|            0|  0.00%|        kernel_size = _single(kernel_size)
   750|         0|            0|            0|  0.00%|        stride = _single(stride)
   751|         0|            0|            0|  0.00%|        padding = _single(padding)
   752|         0|            0|            0|  0.00%|        dilation = _single(dilation)
   753|         0|            0|            0|  0.00%|        output_padding = _single(output_padding)
   754|         0|            0|            0|  0.00%|        super(ConvTranspose1d, self).__init__(
   755|         0|            0|            0|  0.00%|            in_channels, out_channels, kernel_size, stride, padding, dilation,
   756|         0|            0|            0|  0.00%|            True, output_padding, groups, bias, padding_mode, **factory_kwargs)
   757|         0|            0|            0|  0.00%|
   758|         0|            0|            0|  0.00%|    def forward(self, input: Tensor, output_size: Optional[List[int]] = None) -> Tensor:
   759|         0|            0|            0|  0.00%|        if self.padding_mode != 'zeros':
   760|         0|            0|            0|  0.00%|            raise ValueError('Only `zeros` padding mode is supported for ConvTranspose1d')
   761|         0|            0|            0|  0.00%|
   762|         0|            0|            0|  0.00%|        assert isinstance(self.padding, tuple)
   763|         0|            0|            0|  0.00%|        # One cannot replace List by Tuple or Sequence in "_output_padding" because
   764|         0|            0|            0|  0.00%|        # TorchScript does not support `Sequence[T]` or `Tuple[T, ...]`.
   765|         0|            0|            0|  0.00%|        output_padding = self._output_padding(
   766|         0|            0|            0|  0.00%|            input, output_size, self.stride, self.padding, self.kernel_size, self.dilation)  # type: ignore[arg-type]
   767|         0|            0|            0|  0.00%|        return F.conv_transpose1d(
   768|         0|            0|            0|  0.00%|            input, self.weight, self.bias, self.stride, self.padding,
   769|         0|            0|            0|  0.00%|            output_padding, self.groups, self.dilation)
   770|         0|            0|            0|  0.00%|
   771|         0|            0|            0|  0.00%|
   772|         0|            0|            0|  0.00%|class ConvTranspose2d(_ConvTransposeNd):
   773|         0|            0|            0|  0.00%|    __doc__ = r"""Applies a 2D transposed convolution operator over an input image
   774|         0|            0|            0|  0.00%|    composed of several input planes.
   775|         0|            0|            0|  0.00%|
   776|         0|            0|            0|  0.00%|    This module can be seen as the gradient of Conv2d with respect to its input.
   777|         0|            0|            0|  0.00%|    It is also known as a fractionally-strided convolution or
   778|         0|            0|            0|  0.00%|    a deconvolution (although it is not an actual deconvolution operation).
   779|         0|            0|            0|  0.00%|
   780|         0|            0|            0|  0.00%|    This module supports :ref:`TensorFloat32<tf32_on_ampere>`.
   781|         0|            0|            0|  0.00%|
   782|         0|            0|            0|  0.00%|    * :attr:`stride` controls the stride for the cross-correlation.
   783|         0|            0|            0|  0.00%|
   784|         0|            0|            0|  0.00%|    * :attr:`padding` controls the amount of implicit zero padding on both
   785|         0|            0|            0|  0.00%|      sides for ``dilation * (kernel_size - 1) - padding`` number of points. See note
   786|         0|            0|            0|  0.00%|      below for details.
   787|         0|            0|            0|  0.00%|
   788|         0|            0|            0|  0.00%|    * :attr:`output_padding` controls the additional size added to one side
   789|         0|            0|            0|  0.00%|      of the output shape. See note below for details.
   790|         0|            0|            0|  0.00%|
   791|         0|            0|            0|  0.00%|    * :attr:`dilation` controls the spacing between the kernel points; also known as the Ã  trous algorithm.
   792|         0|            0|            0|  0.00%|      It is harder to describe, but this `link`_ has a nice visualization of what :attr:`dilation` does.
   793|         0|            0|            0|  0.00%|
   794|         0|            0|            0|  0.00%|    {groups_note}
   795|         0|            0|            0|  0.00%|
   796|         0|            0|            0|  0.00%|    The parameters :attr:`kernel_size`, :attr:`stride`, :attr:`padding`, :attr:`output_padding`
   797|         0|            0|            0|  0.00%|    can either be:
   798|         0|            0|            0|  0.00%|
   799|         0|            0|            0|  0.00%|        - a single ``int`` -- in which case the same value is used for the height and width dimensions
   800|         0|            0|            0|  0.00%|        - a ``tuple`` of two ints -- in which case, the first `int` is used for the height dimension,
   801|         0|            0|            0|  0.00%|          and the second `int` for the width dimension
   802|         0|            0|            0|  0.00%|
   803|         0|            0|            0|  0.00%|    Note:
   804|         0|            0|            0|  0.00%|        The :attr:`padding` argument effectively adds ``dilation * (kernel_size - 1) - padding``
   805|         0|            0|            0|  0.00%|        amount of zero padding to both sizes of the input. This is set so that
   806|         0|            0|            0|  0.00%|        when a :class:`~torch.nn.Conv2d` and a :class:`~torch.nn.ConvTranspose2d`
   807|         0|            0|            0|  0.00%|        are initialized with same parameters, they are inverses of each other in
   808|         0|            0|            0|  0.00%|        regard to the input and output shapes. However, when ``stride > 1``,
   809|         0|            0|            0|  0.00%|        :class:`~torch.nn.Conv2d` maps multiple input shapes to the same output
   810|         0|            0|            0|  0.00%|        shape. :attr:`output_padding` is provided to resolve this ambiguity by
   811|         0|            0|            0|  0.00%|        effectively increasing the calculated output shape on one side. Note
   812|         0|            0|            0|  0.00%|        that :attr:`output_padding` is only used to find output shape, but does
   813|         0|            0|            0|  0.00%|        not actually add zero-padding to output.
   814|         0|            0|            0|  0.00%|
   815|         0|            0|            0|  0.00%|    Note:
   816|         0|            0|            0|  0.00%|        {cudnn_reproducibility_note}
   817|         0|            0|            0|  0.00%|
   818|         0|            0|            0|  0.00%|    Args:
   819|         0|            0|            0|  0.00%|        in_channels (int): Number of channels in the input image
   820|         0|            0|            0|  0.00%|        out_channels (int): Number of channels produced by the convolution
   821|         0|            0|            0|  0.00%|        kernel_size (int or tuple): Size of the convolving kernel
   822|         0|            0|            0|  0.00%|        stride (int or tuple, optional): Stride of the convolution. Default: 1
   823|         0|            0|            0|  0.00%|        padding (int or tuple, optional): ``dilation * (kernel_size - 1) - padding`` zero-padding
   824|         0|            0|            0|  0.00%|            will be added to both sides of each dimension in the input. Default: 0
   825|         0|            0|            0|  0.00%|        output_padding (int or tuple, optional): Additional size added to one side
   826|         0|            0|            0|  0.00%|            of each dimension in the output shape. Default: 0
   827|         0|            0|            0|  0.00%|        groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1
   828|         0|            0|            0|  0.00%|        bias (bool, optional): If ``True``, adds a learnable bias to the output. Default: ``True``
   829|         0|            0|            0|  0.00%|        dilation (int or tuple, optional): Spacing between kernel elements. Default: 1
   830|         0|            0|            0|  0.00%|    """.format(**reproducibility_notes, **convolution_notes) + r"""
   831|         0|            0|            0|  0.00%|
   832|         0|            0|            0|  0.00%|    Shape:
   833|         0|            0|            0|  0.00%|        - Input: :math:`(N, C_{in}, H_{in}, W_{in})`
   834|         0|            0|            0|  0.00%|        - Output: :math:`(N, C_{out}, H_{out}, W_{out})` where
   835|         0|            0|            0|  0.00%|
   836|         0|            0|            0|  0.00%|        .. math::
   837|         0|            0|            0|  0.00%|              H_{out} = (H_{in} - 1) \times \text{stride}[0] - 2 \times \text{padding}[0] + \text{dilation}[0]
   838|         0|            0|            0|  0.00%|                        \times (\text{kernel\_size}[0] - 1) + \text{output\_padding}[0] + 1
   839|         0|            0|            0|  0.00%|        .. math::
   840|         0|            0|            0|  0.00%|              W_{out} = (W_{in} - 1) \times \text{stride}[1] - 2 \times \text{padding}[1] + \text{dilation}[1]
   841|         0|            0|            0|  0.00%|                        \times (\text{kernel\_size}[1] - 1) + \text{output\_padding}[1] + 1
   842|         0|            0|            0|  0.00%|
   843|         0|            0|            0|  0.00%|    Attributes:
   844|         0|            0|            0|  0.00%|        weight (Tensor): the learnable weights of the module of shape
   845|         0|            0|            0|  0.00%|                         :math:`(\text{in\_channels}, \frac{\text{out\_channels}}{\text{groups}},`
   846|         0|            0|            0|  0.00%|                         :math:`\text{kernel\_size[0]}, \text{kernel\_size[1]})`.
   847|         0|            0|            0|  0.00%|                         The values of these weights are sampled from
   848|         0|            0|            0|  0.00%|                         :math:`\mathcal{U}(-\sqrt{k}, \sqrt{k})` where
   849|         0|            0|            0|  0.00%|                         :math:`k = \frac{groups}{C_\text{out} * \prod_{i=0}^{1}\text{kernel\_size}[i]}`
   850|         0|            0|            0|  0.00%|        bias (Tensor):   the learnable bias of the module of shape (out_channels)
   851|         0|            0|            0|  0.00%|                         If :attr:`bias` is ``True``, then the values of these weights are
   852|         0|            0|            0|  0.00%|                         sampled from :math:`\mathcal{U}(-\sqrt{k}, \sqrt{k})` where
   853|         0|            0|            0|  0.00%|                         :math:`k = \frac{groups}{C_\text{out} * \prod_{i=0}^{1}\text{kernel\_size}[i]}`
   854|         0|            0|            0|  0.00%|
   855|         0|            0|            0|  0.00%|    Examples::
   856|         0|            0|            0|  0.00%|
   857|         0|            0|            0|  0.00%|        >>> # With square kernels and equal stride
   858|         0|            0|            0|  0.00%|        >>> m = nn.ConvTranspose2d(16, 33, 3, stride=2)
   859|         0|            0|            0|  0.00%|        >>> # non-square kernels and unequal stride and with padding
   860|         0|            0|            0|  0.00%|        >>> m = nn.ConvTranspose2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2))
   861|         0|            0|            0|  0.00%|        >>> input = torch.randn(20, 16, 50, 100)
   862|         0|            0|            0|  0.00%|        >>> output = m(input)
   863|         0|            0|            0|  0.00%|        >>> # exact output size can be also specified as an argument
   864|         0|            0|            0|  0.00%|        >>> input = torch.randn(1, 16, 12, 12)
   865|         0|            0|            0|  0.00%|        >>> downsample = nn.Conv2d(16, 16, 3, stride=2, padding=1)
   866|         0|            0|            0|  0.00%|        >>> upsample = nn.ConvTranspose2d(16, 16, 3, stride=2, padding=1)
   867|         0|            0|            0|  0.00%|        >>> h = downsample(input)
   868|         0|            0|            0|  0.00%|        >>> h.size()
   869|         0|            0|            0|  0.00%|        torch.Size([1, 16, 6, 6])
   870|         0|            0|            0|  0.00%|        >>> output = upsample(h, output_size=input.size())
   871|         0|            0|            0|  0.00%|        >>> output.size()
   872|         0|            0|            0|  0.00%|        torch.Size([1, 16, 12, 12])
   873|         0|            0|            0|  0.00%|
   874|         0|            0|            0|  0.00%|    .. _cross-correlation:
   875|         0|            0|            0|  0.00%|        https://en.wikipedia.org/wiki/Cross-correlation
   876|         0|            0|            0|  0.00%|
   877|         0|            0|            0|  0.00%|    .. _link:
   878|         0|            0|            0|  0.00%|        https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md
   879|         0|            0|            0|  0.00%|    """
   880|         0|            0|            0|  0.00%|
   881|         0|            0|            0|  0.00%|    def __init__(
   882|         0|            0|            0|  0.00%|        self,
   883|         0|            0|            0|  0.00%|        in_channels: int,
   884|         0|            0|            0|  0.00%|        out_channels: int,
   885|         0|            0|            0|  0.00%|        kernel_size: _size_2_t,
   886|         0|            0|            0|  0.00%|        stride: _size_2_t = 1,
   887|         0|            0|            0|  0.00%|        padding: _size_2_t = 0,
   888|         0|            0|            0|  0.00%|        output_padding: _size_2_t = 0,
   889|         0|            0|            0|  0.00%|        groups: int = 1,
   890|         0|            0|            0|  0.00%|        bias: bool = True,
   891|         0|            0|            0|  0.00%|        dilation: int = 1,
   892|         0|            0|            0|  0.00%|        padding_mode: str = 'zeros',
   893|         0|            0|            0|  0.00%|        device=None,
   894|         0|            0|            0|  0.00%|        dtype=None
   895|         0|            0|            0|  0.00%|    ) -> None:
   896|         0|            0|            0|  0.00%|        factory_kwargs = {'device': device, 'dtype': dtype}
   897|         0|            0|            0|  0.00%|        kernel_size = _pair(kernel_size)
   898|         0|            0|            0|  0.00%|        stride = _pair(stride)
   899|         0|            0|            0|  0.00%|        padding = _pair(padding)
   900|         0|            0|            0|  0.00%|        dilation = _pair(dilation)
   901|         0|            0|            0|  0.00%|        output_padding = _pair(output_padding)
   902|         0|            0|            0|  0.00%|        super(ConvTranspose2d, self).__init__(
   903|         0|            0|            0|  0.00%|            in_channels, out_channels, kernel_size, stride, padding, dilation,
   904|         0|            0|            0|  0.00%|            True, output_padding, groups, bias, padding_mode, **factory_kwargs)
   905|         0|            0|            0|  0.00%|
   906|         0|            0|            0|  0.00%|    def forward(self, input: Tensor, output_size: Optional[List[int]] = None) -> Tensor:
   907|         0|            0|            0|  0.00%|        if self.padding_mode != 'zeros':
   908|         0|            0|            0|  0.00%|            raise ValueError('Only `zeros` padding mode is supported for ConvTranspose2d')
   909|         0|            0|            0|  0.00%|
   910|         0|            0|            0|  0.00%|        assert isinstance(self.padding, tuple)
   911|         0|            0|            0|  0.00%|        # One cannot replace List by Tuple or Sequence in "_output_padding" because
   912|         0|            0|            0|  0.00%|        # TorchScript does not support `Sequence[T]` or `Tuple[T, ...]`.
   913|         0|            0|            0|  0.00%|        output_padding = self._output_padding(
   914|         0|            0|            0|  0.00%|            input, output_size, self.stride, self.padding, self.kernel_size, self.dilation)  # type: ignore[arg-type]
   915|         0|            0|            0|  0.00%|
   916|         0|            0|            0|  0.00%|        return F.conv_transpose2d(
   917|         0|            0|            0|  0.00%|            input, self.weight, self.bias, self.stride, self.padding,
   918|         0|            0|            0|  0.00%|            output_padding, self.groups, self.dilation)
   919|         0|            0|            0|  0.00%|
   920|         0|            0|            0|  0.00%|
   921|         0|            0|            0|  0.00%|class ConvTranspose3d(_ConvTransposeNd):
   922|         0|            0|            0|  0.00%|    __doc__ = r"""Applies a 3D transposed convolution operator over an input image composed of several input
   923|         0|            0|            0|  0.00%|    planes.
   924|         0|            0|            0|  0.00%|    The transposed convolution operator multiplies each input value element-wise by a learnable kernel,
   925|         0|            0|            0|  0.00%|    and sums over the outputs from all input feature planes.
   926|         0|            0|            0|  0.00%|
   927|         0|            0|            0|  0.00%|    This module can be seen as the gradient of Conv3d with respect to its input.
   928|         0|            0|            0|  0.00%|    It is also known as a fractionally-strided convolution or
   929|         0|            0|            0|  0.00%|    a deconvolution (although it is not an actual deconvolution operation).
   930|         0|            0|            0|  0.00%|
   931|         0|            0|            0|  0.00%|    This module supports :ref:`TensorFloat32<tf32_on_ampere>`.
   932|         0|            0|            0|  0.00%|
   933|         0|            0|            0|  0.00%|    * :attr:`stride` controls the stride for the cross-correlation.
   934|         0|            0|            0|  0.00%|
   935|         0|            0|            0|  0.00%|    * :attr:`padding` controls the amount of implicit zero padding on both
   936|         0|            0|            0|  0.00%|      sides for ``dilation * (kernel_size - 1) - padding`` number of points. See note
   937|         0|            0|            0|  0.00%|      below for details.
   938|         0|            0|            0|  0.00%|
   939|         0|            0|            0|  0.00%|    * :attr:`output_padding` controls the additional size added to one side
   940|         0|            0|            0|  0.00%|      of the output shape. See note below for details.
   941|         0|            0|            0|  0.00%|
   942|         0|            0|            0|  0.00%|    * :attr:`dilation` controls the spacing between the kernel points; also known as the Ã  trous algorithm.
   943|         0|            0|            0|  0.00%|      It is harder to describe, but this `link`_ has a nice visualization of what :attr:`dilation` does.
   944|         0|            0|            0|  0.00%|
   945|         0|            0|            0|  0.00%|    {groups_note}
   946|         0|            0|            0|  0.00%|
   947|         0|            0|            0|  0.00%|    The parameters :attr:`kernel_size`, :attr:`stride`, :attr:`padding`, :attr:`output_padding`
   948|         0|            0|            0|  0.00%|    can either be:
   949|         0|            0|            0|  0.00%|
   950|         0|            0|            0|  0.00%|        - a single ``int`` -- in which case the same value is used for the depth, height and width dimensions
   951|         0|            0|            0|  0.00%|        - a ``tuple`` of three ints -- in which case, the first `int` is used for the depth dimension,
   952|         0|            0|            0|  0.00%|          the second `int` for the height dimension and the third `int` for the width dimension
   953|         0|            0|            0|  0.00%|
   954|         0|            0|            0|  0.00%|    Note:
   955|         0|            0|            0|  0.00%|        The :attr:`padding` argument effectively adds ``dilation * (kernel_size - 1) - padding``
   956|         0|            0|            0|  0.00%|        amount of zero padding to both sizes of the input. This is set so that
   957|         0|            0|            0|  0.00%|        when a :class:`~torch.nn.Conv3d` and a :class:`~torch.nn.ConvTranspose3d`
   958|         0|            0|            0|  0.00%|        are initialized with same parameters, they are inverses of each other in
   959|         0|            0|            0|  0.00%|        regard to the input and output shapes. However, when ``stride > 1``,
   960|         0|            0|            0|  0.00%|        :class:`~torch.nn.Conv3d` maps multiple input shapes to the same output
   961|         0|            0|            0|  0.00%|        shape. :attr:`output_padding` is provided to resolve this ambiguity by
   962|         0|            0|            0|  0.00%|        effectively increasing the calculated output shape on one side. Note
   963|         0|            0|            0|  0.00%|        that :attr:`output_padding` is only used to find output shape, but does
   964|         0|            0|            0|  0.00%|        not actually add zero-padding to output.
   965|         0|            0|            0|  0.00%|
   966|         0|            0|            0|  0.00%|    Note:
   967|         0|            0|            0|  0.00%|        {cudnn_reproducibility_note}
   968|         0|            0|            0|  0.00%|
   969|         0|            0|            0|  0.00%|    Args:
   970|         0|            0|            0|  0.00%|        in_channels (int): Number of channels in the input image
   971|         0|            0|            0|  0.00%|        out_channels (int): Number of channels produced by the convolution
   972|         0|            0|            0|  0.00%|        kernel_size (int or tuple): Size of the convolving kernel
   973|         0|            0|            0|  0.00%|        stride (int or tuple, optional): Stride of the convolution. Default: 1
   974|         0|            0|            0|  0.00%|        padding (int or tuple, optional): ``dilation * (kernel_size - 1) - padding`` zero-padding
   975|         0|            0|            0|  0.00%|            will be added to both sides of each dimension in the input. Default: 0
   976|         0|            0|            0|  0.00%|        output_padding (int or tuple, optional): Additional size added to one side
   977|         0|            0|            0|  0.00%|            of each dimension in the output shape. Default: 0
   978|         0|            0|            0|  0.00%|        groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1
   979|         0|            0|            0|  0.00%|        bias (bool, optional): If ``True``, adds a learnable bias to the output. Default: ``True``
   980|         0|            0|            0|  0.00%|        dilation (int or tuple, optional): Spacing between kernel elements. Default: 1
   981|         0|            0|            0|  0.00%|    """.format(**reproducibility_notes, **convolution_notes) + r"""
   982|         0|            0|            0|  0.00%|
   983|         0|            0|            0|  0.00%|    Shape:
   984|         0|            0|            0|  0.00%|        - Input: :math:`(N, C_{in}, D_{in}, H_{in}, W_{in})`
   985|         0|            0|            0|  0.00%|        - Output: :math:`(N, C_{out}, D_{out}, H_{out}, W_{out})` where
   986|         0|            0|            0|  0.00%|
   987|         0|            0|            0|  0.00%|        .. math::
   988|         0|            0|            0|  0.00%|              D_{out} = (D_{in} - 1) \times \text{stride}[0] - 2 \times \text{padding}[0] + \text{dilation}[0]
   989|         0|            0|            0|  0.00%|                        \times (\text{kernel\_size}[0] - 1) + \text{output\_padding}[0] + 1
   990|         0|            0|            0|  0.00%|        .. math::
   991|         0|            0|            0|  0.00%|              H_{out} = (H_{in} - 1) \times \text{stride}[1] - 2 \times \text{padding}[1] + \text{dilation}[1]
   992|         0|            0|            0|  0.00%|                        \times (\text{kernel\_size}[1] - 1) + \text{output\_padding}[1] + 1
   993|         0|            0|            0|  0.00%|        .. math::
   994|         0|            0|            0|  0.00%|              W_{out} = (W_{in} - 1) \times \text{stride}[2] - 2 \times \text{padding}[2] + \text{dilation}[2]
   995|         0|            0|            0|  0.00%|                        \times (\text{kernel\_size}[2] - 1) + \text{output\_padding}[2] + 1
   996|         0|            0|            0|  0.00%|
   997|         0|            0|            0|  0.00%|
   998|         0|            0|            0|  0.00%|    Attributes:
   999|         0|            0|            0|  0.00%|        weight (Tensor): the learnable weights of the module of shape
  1000|         0|            0|            0|  0.00%|                         :math:`(\text{in\_channels}, \frac{\text{out\_channels}}{\text{groups}},`
  1001|         0|            0|            0|  0.00%|                         :math:`\text{kernel\_size[0]}, \text{kernel\_size[1]}, \text{kernel\_size[2]})`.
  1002|         0|            0|            0|  0.00%|                         The values of these weights are sampled from
  1003|         0|            0|            0|  0.00%|                         :math:`\mathcal{U}(-\sqrt{k}, \sqrt{k})` where
  1004|         0|            0|            0|  0.00%|                         :math:`k = \frac{groups}{C_\text{out} * \prod_{i=0}^{2}\text{kernel\_size}[i]}`
  1005|         0|            0|            0|  0.00%|        bias (Tensor):   the learnable bias of the module of shape (out_channels)
  1006|         0|            0|            0|  0.00%|                         If :attr:`bias` is ``True``, then the values of these weights are
  1007|         0|            0|            0|  0.00%|                         sampled from :math:`\mathcal{U}(-\sqrt{k}, \sqrt{k})` where
  1008|         0|            0|            0|  0.00%|                         :math:`k = \frac{groups}{C_\text{out} * \prod_{i=0}^{2}\text{kernel\_size}[i]}`
  1009|         0|            0|            0|  0.00%|
  1010|         0|            0|            0|  0.00%|    Examples::
  1011|         0|            0|            0|  0.00%|
  1012|         0|            0|            0|  0.00%|        >>> # With square kernels and equal stride
  1013|         0|            0|            0|  0.00%|        >>> m = nn.ConvTranspose3d(16, 33, 3, stride=2)
  1014|         0|            0|            0|  0.00%|        >>> # non-square kernels and unequal stride and with padding
  1015|         0|            0|            0|  0.00%|        >>> m = nn.ConvTranspose3d(16, 33, (3, 5, 2), stride=(2, 1, 1), padding=(0, 4, 2))
  1016|         0|            0|            0|  0.00%|        >>> input = torch.randn(20, 16, 10, 50, 100)
  1017|         0|            0|            0|  0.00%|        >>> output = m(input)
  1018|         0|            0|            0|  0.00%|
  1019|         0|            0|            0|  0.00%|    .. _cross-correlation:
  1020|         0|            0|            0|  0.00%|        https://en.wikipedia.org/wiki/Cross-correlation
  1021|         0|            0|            0|  0.00%|
  1022|         0|            0|            0|  0.00%|    .. _link:
  1023|         0|            0|            0|  0.00%|        https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md
  1024|         0|            0|            0|  0.00%|    """
  1025|         0|            0|            0|  0.00%|
  1026|         0|            0|            0|  0.00%|    def __init__(
  1027|         0|            0|            0|  0.00%|        self,
  1028|         0|            0|            0|  0.00%|        in_channels: int,
  1029|         0|            0|            0|  0.00%|        out_channels: int,
  1030|         0|            0|            0|  0.00%|        kernel_size: _size_3_t,
  1031|         0|            0|            0|  0.00%|        stride: _size_3_t = 1,
  1032|         0|            0|            0|  0.00%|        padding: _size_3_t = 0,
  1033|         0|            0|            0|  0.00%|        output_padding: _size_3_t = 0,
  1034|         0|            0|            0|  0.00%|        groups: int = 1,
  1035|         0|            0|            0|  0.00%|        bias: bool = True,
  1036|         0|            0|            0|  0.00%|        dilation: _size_3_t = 1,
  1037|         0|            0|            0|  0.00%|        padding_mode: str = 'zeros',
  1038|         0|            0|            0|  0.00%|        device=None,
  1039|         0|            0|            0|  0.00%|        dtype=None
  1040|         0|            0|            0|  0.00%|    ) -> None:
  1041|         0|            0|            0|  0.00%|        factory_kwargs = {'device': device, 'dtype': dtype}
  1042|         0|            0|            0|  0.00%|        kernel_size = _triple(kernel_size)
  1043|         0|            0|            0|  0.00%|        stride = _triple(stride)
  1044|         0|            0|            0|  0.00%|        padding = _triple(padding)
  1045|         0|            0|            0|  0.00%|        dilation = _triple(dilation)
  1046|         0|            0|            0|  0.00%|        output_padding = _triple(output_padding)
  1047|         0|            0|            0|  0.00%|        super(ConvTranspose3d, self).__init__(
  1048|         0|            0|            0|  0.00%|            in_channels, out_channels, kernel_size, stride, padding, dilation,
  1049|         0|            0|            0|  0.00%|            True, output_padding, groups, bias, padding_mode, **factory_kwargs)
  1050|         0|            0|            0|  0.00%|
  1051|         0|            0|            0|  0.00%|    def forward(self, input: Tensor, output_size: Optional[List[int]] = None) -> Tensor:
  1052|         0|            0|            0|  0.00%|        if self.padding_mode != 'zeros':
  1053|         0|            0|            0|  0.00%|            raise ValueError('Only `zeros` padding mode is supported for ConvTranspose3d')
  1054|         0|            0|            0|  0.00%|
  1055|         0|            0|            0|  0.00%|        assert isinstance(self.padding, tuple)
  1056|         0|            0|            0|  0.00%|        # One cannot replace List by Tuple or Sequence in "_output_padding" because
  1057|         0|            0|            0|  0.00%|        # TorchScript does not support `Sequence[T]` or `Tuple[T, ...]`.
  1058|         0|            0|            0|  0.00%|        output_padding = self._output_padding(
  1059|         0|            0|            0|  0.00%|            input, output_size, self.stride, self.padding, self.kernel_size, self.dilation)  # type: ignore[arg-type]
  1060|         0|            0|            0|  0.00%|
  1061|         0|            0|            0|  0.00%|        return F.conv_transpose3d(
  1062|         0|            0|            0|  0.00%|            input, self.weight, self.bias, self.stride, self.padding,
  1063|         0|            0|            0|  0.00%|            output_padding, self.groups, self.dilation)
  1064|         0|            0|            0|  0.00%|
  1065|         0|            0|            0|  0.00%|
  1066|         0|            0|            0|  0.00%|# TODO: Deprecate and remove the following alias `_ConvTransposeMixin`.
  1067|         0|            0|            0|  0.00%|#
  1068|         0|            0|            0|  0.00%|# `_ConvTransposeMixin` was a mixin that was removed.  It is meant to be used
  1069|         0|            0|            0|  0.00%|# with `_ConvNd` to construct actual module classes that implements conv
  1070|         0|            0|            0|  0.00%|# transpose ops:
  1071|         0|            0|            0|  0.00%|#
  1072|         0|            0|            0|  0.00%|#   class MyConvTranspose(_ConvNd, _ConvTransposeMixin):
  1073|         0|            0|            0|  0.00%|#       ...
  1074|         0|            0|            0|  0.00%|#
  1075|         0|            0|            0|  0.00%|# In PyTorch, it has been replaced by `_ConvTransposeNd`, which is a proper
  1076|         0|            0|            0|  0.00%|# subclass of `_ConvNd`.  However, some user code in the wild still (incorrectly)
  1077|         0|            0|            0|  0.00%|# use the internal class `_ConvTransposeMixin`.  Hence, we provide this alias
  1078|         0|            0|            0|  0.00%|# for BC, because it is cheap and easy for us to do so, even though that
  1079|         0|            0|            0|  0.00%|# `_ConvTransposeNd` is really not a mixin anymore (but multiple inheritance as
  1080|         0|            0|            0|  0.00%|# above would still work).
  1081|         0|            0|            0|  0.00%|class _ConvTransposeMixin(_ConvTransposeNd):
  1082|         0|            0|            0|  0.00%|    def __init__(self, *args, **kwargs):
  1083|         0|            0|            0|  0.00%|        warnings.warn(
  1084|         0|            0|            0|  0.00%|            "_ConvTransposeMixin is a deprecated internal class. "
  1085|         0|            0|            0|  0.00%|            "Please consider using public APIs.")
  1086|         0|            0|            0|  0.00%|        super(_ConvTransposeMixin, self).__init__(*args, **kwargs)
  1087|         0|            0|            0|  0.00%|
  1088|         0|            0|            0|  0.00%|
  1089|         0|            0|            0|  0.00%|# TODO: Conv2dLocal
  1090|         0|            0|            0|  0.00%|# TODO: Conv2dMap
  1091|         0|            0|            0|  0.00%|# TODO: ConvTranspose2dMap
  1092|         0|            0|            0|  0.00%|
  1093|         0|            0|            0|  0.00%|
  1094|         0|            0|            0|  0.00%|class _LazyConvXdMixin(LazyModuleMixin):
  1095|         0|            0|            0|  0.00%|    groups: int
  1096|         0|            0|            0|  0.00%|    transposed: bool
  1097|         0|            0|            0|  0.00%|    in_channels: int
  1098|         0|            0|            0|  0.00%|    out_channels: int
  1099|         0|            0|            0|  0.00%|    kernel_size: Tuple[int, ...]
  1100|         0|            0|            0|  0.00%|    weight: UninitializedParameter
  1101|         0|            0|            0|  0.00%|    bias: UninitializedParameter
  1102|         0|            0|            0|  0.00%|
  1103|         0|            0|            0|  0.00%|    def reset_parameters(self) -> None:
  1104|         0|            0|            0|  0.00%|        # has_uninitialized_params is defined in parent class and it is using a protocol on self
  1105|         0|            0|            0|  0.00%|        if not self.has_uninitialized_params() and self.in_channels != 0:  # type: ignore[misc]
  1106|         0|            0|            0|  0.00%|            # "type:ignore[..]" is required because mypy thinks that "reset_parameters" is undefined
  1107|         0|            0|            0|  0.00%|            # in super class. Turns out that it is defined in _ConvND which is inherited by any class
  1108|         0|            0|            0|  0.00%|            # that also inherits _LazyConvXdMixin
  1109|         0|            0|            0|  0.00%|            super().reset_parameters()  # type: ignore[misc]
  1110|         0|            0|            0|  0.00%|
  1111|         0|            0|            0|  0.00%|    # Signature of "initialize_parameters" is incompatible with the definition in supertype LazyModuleMixin
  1112|         0|            0|            0|  0.00%|    def initialize_parameters(self, input) -> None:  # type: ignore[override]
  1113|         0|            0|            0|  0.00%|        # defined by parent class but using a protocol
  1114|         0|            0|            0|  0.00%|        if self.has_uninitialized_params():  # type: ignore[misc]
  1115|         0|            0|            0|  0.00%|            self.in_channels = input.shape[1]
  1116|         0|            0|            0|  0.00%|            if self.in_channels % self.groups != 0:
  1117|         0|            0|            0|  0.00%|                raise ValueError('in_channels must be divisible by groups')
  1118|         0|            0|            0|  0.00%|            assert isinstance(self.weight, UninitializedParameter)
  1119|         0|            0|            0|  0.00%|            if self.transposed:
  1120|         0|            0|            0|  0.00%|                self.weight.materialize((
  1121|         0|            0|            0|  0.00%|                    self.in_channels, self.out_channels // self.groups, *self.kernel_size))
  1122|         0|            0|            0|  0.00%|            else:
  1123|         0|            0|            0|  0.00%|                self.weight.materialize((
  1124|         0|            0|            0|  0.00%|                    self.out_channels, self.in_channels // self.groups, *self.kernel_size))
  1125|         0|            0|            0|  0.00%|            if self.bias is not None:
  1126|         0|            0|            0|  0.00%|                assert isinstance(self.bias, UninitializedParameter)
  1127|         0|            0|            0|  0.00%|                self.bias.materialize((self.out_channels,))
  1128|         0|            0|            0|  0.00%|            self.reset_parameters()
  1129|         0|            0|            0|  0.00%|
  1130|         0|            0|            0|  0.00%|
  1131|         0|            0|            0|  0.00%|# LazyConv1d defines weight as a Tensor but derived class defines it as UnitializeParameter
  1132|         0|            0|            0|  0.00%|class LazyConv1d(_LazyConvXdMixin, Conv1d):  # type: ignore[misc]
  1133|         0|            0|            0|  0.00%|    r"""A :class:`torch.nn.Conv1d` module with lazy initialization of
  1134|         0|            0|            0|  0.00%|    the ``in_channels`` argument of the :class:`Conv1d` that is inferred from
  1135|         0|            0|            0|  0.00%|    the ``input.size(1)``.
  1136|         0|            0|            0|  0.00%|    The attributes that will be lazily initialized are `weight` and `bias`.
  1137|         0|            0|            0|  0.00%|
  1138|         0|            0|            0|  0.00%|    Check the :class:`torch.nn.modules.lazy.LazyModuleMixin` for further documentation
  1139|         0|            0|            0|  0.00%|    on lazy modules and their limitations.
  1140|         0|            0|            0|  0.00%|
  1141|         0|            0|            0|  0.00%|    Args:
  1142|         0|            0|            0|  0.00%|        out_channels (int): Number of channels produced by the convolution
  1143|         0|            0|            0|  0.00%|        kernel_size (int or tuple): Size of the convolving kernel
  1144|         0|            0|            0|  0.00%|        stride (int or tuple, optional): Stride of the convolution. Default: 1
  1145|         0|            0|            0|  0.00%|        padding (int or tuple, optional): Zero-padding added to both sides of
  1146|         0|            0|            0|  0.00%|            the input. Default: 0
  1147|         0|            0|            0|  0.00%|        padding_mode (string, optional): ``'zeros'``, ``'reflect'``,
  1148|         0|            0|            0|  0.00%|            ``'replicate'`` or ``'circular'``. Default: ``'zeros'``
  1149|         0|            0|            0|  0.00%|        dilation (int or tuple, optional): Spacing between kernel
  1150|         0|            0|            0|  0.00%|            elements. Default: 1
  1151|         0|            0|            0|  0.00%|        groups (int, optional): Number of blocked connections from input
  1152|         0|            0|            0|  0.00%|            channels to output channels. Default: 1
  1153|         0|            0|            0|  0.00%|        bias (bool, optional): If ``True``, adds a learnable bias to the
  1154|         0|            0|            0|  0.00%|            output. Default: ``True``
  1155|         0|            0|            0|  0.00%|
  1156|         0|            0|            0|  0.00%|    .. seealso:: :class:`torch.nn.Conv1d` and :class:`torch.nn.modules.lazy.LazyModuleMixin`
  1157|         0|            0|            0|  0.00%|    """
  1158|         0|            0|            0|  0.00%|
  1159|         0|            0|            0|  0.00%|    # super class define this variable as None. "type: ignore[..] is required
  1160|         0|            0|            0|  0.00%|    # since we are redefining the variable.
  1161|         0|            0|            0|  0.00%|    cls_to_become = Conv1d  # type: ignore[assignment]
  1162|         0|            0|            0|  0.00%|
  1163|         0|            0|            0|  0.00%|    def __init__(
  1164|         0|            0|            0|  0.00%|        self,
  1165|         0|            0|            0|  0.00%|        out_channels: int,
  1166|         0|            0|            0|  0.00%|        kernel_size: _size_1_t,
  1167|         0|            0|            0|  0.00%|        stride: _size_1_t = 1,
  1168|         0|            0|            0|  0.00%|        padding: _size_1_t = 0,
  1169|         0|            0|            0|  0.00%|        dilation: _size_1_t = 1,
  1170|         0|            0|            0|  0.00%|        groups: int = 1,
  1171|         0|            0|            0|  0.00%|        bias: bool = True,
  1172|         0|            0|            0|  0.00%|        padding_mode: str = 'zeros',
  1173|         0|            0|            0|  0.00%|        device=None,
  1174|         0|            0|            0|  0.00%|        dtype=None
  1175|         0|            0|            0|  0.00%|    ) -> None:
  1176|         0|            0|            0|  0.00%|        factory_kwargs = {'device': device, 'dtype': dtype}
  1177|         0|            0|            0|  0.00%|        super().__init__(
  1178|         0|            0|            0|  0.00%|            0,
  1179|         0|            0|            0|  0.00%|            0,
  1180|         0|            0|            0|  0.00%|            kernel_size,
  1181|         0|            0|            0|  0.00%|            stride,
  1182|         0|            0|            0|  0.00%|            padding,
  1183|         0|            0|            0|  0.00%|            dilation,
  1184|         0|            0|            0|  0.00%|            groups,
  1185|         0|            0|            0|  0.00%|            # bias is hardcoded to False to avoid creating tensor
  1186|         0|            0|            0|  0.00%|            # that will soon be overwritten.
  1187|         0|            0|            0|  0.00%|            False,
  1188|         0|            0|            0|  0.00%|            padding_mode,
  1189|         0|            0|            0|  0.00%|            **factory_kwargs
  1190|         0|            0|            0|  0.00%|        )
  1191|         0|            0|            0|  0.00%|        self.weight = UninitializedParameter(**factory_kwargs)
  1192|         0|            0|            0|  0.00%|        self.out_channels = out_channels
  1193|         0|            0|            0|  0.00%|        if bias:
  1194|         0|            0|            0|  0.00%|            self.bias = UninitializedParameter(**factory_kwargs)
  1195|         0|            0|            0|  0.00%|
  1196|         0|            0|            0|  0.00%|
  1197|         0|            0|            0|  0.00%|# LazyConv2d defines weight as a Tensor but derived class defines it as UnitializeParameter
  1198|         0|            0|            0|  0.00%|class LazyConv2d(_LazyConvXdMixin, Conv2d):  # type: ignore[misc]
  1199|         0|            0|            0|  0.00%|    r"""A :class:`torch.nn.Conv2d` module with lazy initialization of
  1200|         0|            0|            0|  0.00%|    the ``in_channels`` argument of the :class:`Conv2d` that is inferred from
  1201|         0|            0|            0|  0.00%|    the ``input.size(1)``.
  1202|         0|            0|            0|  0.00%|    The attributes that will be lazily initialized are `weight` and `bias`.
  1203|         0|            0|            0|  0.00%|
  1204|         0|            0|            0|  0.00%|    Check the :class:`torch.nn.modules.lazy.LazyModuleMixin` for further documentation
  1205|         0|            0|            0|  0.00%|    on lazy modules and their limitations.
  1206|         0|            0|            0|  0.00%|
  1207|         0|            0|            0|  0.00%|    Args:
  1208|         0|            0|            0|  0.00%|        out_channels (int): Number of channels produced by the convolution
  1209|         0|            0|            0|  0.00%|        kernel_size (int or tuple): Size of the convolving kernel
  1210|         0|            0|            0|  0.00%|        stride (int or tuple, optional): Stride of the convolution. Default: 1
  1211|         0|            0|            0|  0.00%|        padding (int or tuple, optional): Zero-padding added to both sides of
  1212|         0|            0|            0|  0.00%|            the input. Default: 0
  1213|         0|            0|            0|  0.00%|        padding_mode (string, optional): ``'zeros'``, ``'reflect'``,
  1214|         0|            0|            0|  0.00%|            ``'replicate'`` or ``'circular'``. Default: ``'zeros'``
  1215|         0|            0|            0|  0.00%|        dilation (int or tuple, optional): Spacing between kernel
  1216|         0|            0|            0|  0.00%|            elements. Default: 1
  1217|         0|            0|            0|  0.00%|        groups (int, optional): Number of blocked connections from input
  1218|         0|            0|            0|  0.00%|            channels to output channels. Default: 1
  1219|         0|            0|            0|  0.00%|        bias (bool, optional): If ``True``, adds a learnable bias to the
  1220|         0|            0|            0|  0.00%|            output. Default: ``True``
  1221|         0|            0|            0|  0.00%|
  1222|         0|            0|            0|  0.00%|    .. seealso:: :class:`torch.nn.Conv2d` and :class:`torch.nn.modules.lazy.LazyModuleMixin`
  1223|         0|            0|            0|  0.00%|    """
  1224|         0|            0|            0|  0.00%|
  1225|         0|            0|            0|  0.00%|    # super class define this variable as None. "type: ignore[..] is required
  1226|         0|            0|            0|  0.00%|    # since we are redefining the variable.
  1227|         0|            0|            0|  0.00%|    cls_to_become = Conv2d  # type: ignore[assignment]
  1228|         0|            0|            0|  0.00%|
  1229|         0|            0|            0|  0.00%|    def __init__(
  1230|         0|            0|            0|  0.00%|        self,
  1231|         0|            0|            0|  0.00%|        out_channels: int,
  1232|         0|            0|            0|  0.00%|        kernel_size: _size_2_t,
  1233|         0|            0|            0|  0.00%|        stride: _size_2_t = 1,
  1234|         0|            0|            0|  0.00%|        padding: _size_2_t = 0,
  1235|         0|            0|            0|  0.00%|        dilation: _size_2_t = 1,
  1236|         0|            0|            0|  0.00%|        groups: int = 1,
  1237|         0|            0|            0|  0.00%|        bias: bool = True,
  1238|         0|            0|            0|  0.00%|        padding_mode: str = 'zeros',  # TODO: refine this type
  1239|         0|            0|            0|  0.00%|        device=None,
  1240|         0|            0|            0|  0.00%|        dtype=None
  1241|         0|            0|            0|  0.00%|    ) -> None:
  1242|         0|            0|            0|  0.00%|        factory_kwargs = {'device': device, 'dtype': dtype}
  1243|         0|            0|            0|  0.00%|        super().__init__(
  1244|         0|            0|            0|  0.00%|            0,
  1245|         0|            0|            0|  0.00%|            0,
  1246|         0|            0|            0|  0.00%|            kernel_size,
  1247|         0|            0|            0|  0.00%|            stride,
  1248|         0|            0|            0|  0.00%|            padding,
  1249|         0|            0|            0|  0.00%|            dilation,
  1250|         0|            0|            0|  0.00%|            groups,
  1251|         0|            0|            0|  0.00%|            # bias is hardcoded to False to avoid creating tensor
  1252|         0|            0|            0|  0.00%|            # that will soon be overwritten.
  1253|         0|            0|            0|  0.00%|            False,
  1254|         0|            0|            0|  0.00%|            padding_mode,
  1255|         0|            0|            0|  0.00%|            **factory_kwargs
  1256|         0|            0|            0|  0.00%|        )
  1257|         0|            0|            0|  0.00%|        self.weight = UninitializedParameter(**factory_kwargs)
  1258|         0|            0|            0|  0.00%|        self.out_channels = out_channels
  1259|         0|            0|            0|  0.00%|        if bias:
  1260|         0|            0|            0|  0.00%|            self.bias = UninitializedParameter(**factory_kwargs)
  1261|         0|            0|            0|  0.00%|
  1262|         0|            0|            0|  0.00%|
  1263|         0|            0|            0|  0.00%|# LazyConv3d defines weight as a Tensor but derived class defines it as UnitializeParameter
  1264|         0|            0|            0|  0.00%|class LazyConv3d(_LazyConvXdMixin, Conv3d):  # type: ignore[misc]
  1265|         0|            0|            0|  0.00%|    r"""A :class:`torch.nn.Conv3d` module with lazy initialization of
  1266|         0|            0|            0|  0.00%|    the ``in_channels`` argument of the :class:`Conv3d` that is inferred from
  1267|         0|            0|            0|  0.00%|    the ``input.size(1)``.
  1268|         0|            0|            0|  0.00%|    The attributes that will be lazily initialized are `weight` and `bias`.
  1269|         0|            0|            0|  0.00%|
  1270|         0|            0|            0|  0.00%|    Check the :class:`torch.nn.modules.lazy.LazyModuleMixin` for further documentation
  1271|         0|            0|            0|  0.00%|    on lazy modules and their limitations.
  1272|         0|            0|            0|  0.00%|
  1273|         0|            0|            0|  0.00%|    Args:
  1274|         0|            0|            0|  0.00%|        out_channels (int): Number of channels produced by the convolution
  1275|         0|            0|            0|  0.00%|        kernel_size (int or tuple): Size of the convolving kernel
  1276|         0|            0|            0|  0.00%|        stride (int or tuple, optional): Stride of the convolution. Default: 1
  1277|         0|            0|            0|  0.00%|        padding (int or tuple, optional): Zero-padding added to both sides of
  1278|         0|            0|            0|  0.00%|            the input. Default: 0
  1279|         0|            0|            0|  0.00%|        padding_mode (string, optional): ``'zeros'``, ``'reflect'``,
  1280|         0|            0|            0|  0.00%|            ``'replicate'`` or ``'circular'``. Default: ``'zeros'``
  1281|         0|            0|            0|  0.00%|        dilation (int or tuple, optional): Spacing between kernel
  1282|         0|            0|            0|  0.00%|            elements. Default: 1
  1283|         0|            0|            0|  0.00%|        groups (int, optional): Number of blocked connections from input
  1284|         0|            0|            0|  0.00%|            channels to output channels. Default: 1
  1285|         0|            0|            0|  0.00%|        bias (bool, optional): If ``True``, adds a learnable bias to the
  1286|         0|            0|            0|  0.00%|            output. Default: ``True``
  1287|         0|            0|            0|  0.00%|
  1288|         0|            0|            0|  0.00%|    .. seealso:: :class:`torch.nn.Conv3d` and :class:`torch.nn.modules.lazy.LazyModuleMixin`
  1289|         0|            0|            0|  0.00%|    """
  1290|         0|            0|            0|  0.00%|
  1291|         0|            0|            0|  0.00%|    # super class define this variable as None. "type: ignore[..] is required
  1292|         0|            0|            0|  0.00%|    # since we are redefining the variable.
  1293|         0|            0|            0|  0.00%|    cls_to_become = Conv3d  # type: ignore[assignment]
  1294|         0|            0|            0|  0.00%|
  1295|         0|            0|            0|  0.00%|    def __init__(
  1296|         0|            0|            0|  0.00%|        self,
  1297|         0|            0|            0|  0.00%|        out_channels: int,
  1298|         0|            0|            0|  0.00%|        kernel_size: _size_3_t,
  1299|         0|            0|            0|  0.00%|        stride: _size_3_t = 1,
  1300|         0|            0|            0|  0.00%|        padding: _size_3_t = 0,
  1301|         0|            0|            0|  0.00%|        dilation: _size_3_t = 1,
  1302|         0|            0|            0|  0.00%|        groups: int = 1,
  1303|         0|            0|            0|  0.00%|        bias: bool = True,
  1304|         0|            0|            0|  0.00%|        padding_mode: str = 'zeros',
  1305|         0|            0|            0|  0.00%|        device=None,
  1306|         0|            0|            0|  0.00%|        dtype=None
  1307|         0|            0|            0|  0.00%|    ) -> None:
  1308|         0|            0|            0|  0.00%|        factory_kwargs = {'device': device, 'dtype': dtype}
  1309|         0|            0|            0|  0.00%|        super().__init__(
  1310|         0|            0|            0|  0.00%|            0,
  1311|         0|            0|            0|  0.00%|            0,
  1312|         0|            0|            0|  0.00%|            kernel_size,
  1313|         0|            0|            0|  0.00%|            stride,
  1314|         0|            0|            0|  0.00%|            padding,
  1315|         0|            0|            0|  0.00%|            dilation,
  1316|         0|            0|            0|  0.00%|            groups,
  1317|         0|            0|            0|  0.00%|            # bias is hardcoded to False to avoid creating tensor
  1318|         0|            0|            0|  0.00%|            # that will soon be overwritten.
  1319|         0|            0|            0|  0.00%|            False,
  1320|         0|            0|            0|  0.00%|            padding_mode,
  1321|         0|            0|            0|  0.00%|            **factory_kwargs
  1322|         0|            0|            0|  0.00%|        )
  1323|         0|            0|            0|  0.00%|        self.weight = UninitializedParameter(**factory_kwargs)
  1324|         0|            0|            0|  0.00%|        self.out_channels = out_channels
  1325|         0|            0|            0|  0.00%|        if bias:
  1326|         0|            0|            0|  0.00%|            self.bias = UninitializedParameter(**factory_kwargs)
  1327|         0|            0|            0|  0.00%|
  1328|         0|            0|            0|  0.00%|
  1329|         0|            0|            0|  0.00%|# LazyConvTranspose1d defines weight as a Tensor but derived class defines it as UnitializeParameter
  1330|         0|            0|            0|  0.00%|class LazyConvTranspose1d(_LazyConvXdMixin, ConvTranspose1d):  # type: ignore[misc]
  1331|         0|            0|            0|  0.00%|    r"""A :class:`torch.nn.ConvTranspose1d` module with lazy initialization of
  1332|         0|            0|            0|  0.00%|    the ``in_channels`` argument of the :class:`ConvTranspose1d` that is inferred from
  1333|         0|            0|            0|  0.00%|    the ``input.size(1)``.
  1334|         0|            0|            0|  0.00%|    The attributes that will be lazily initialized are `weight` and `bias`.
  1335|         0|            0|            0|  0.00%|
  1336|         0|            0|            0|  0.00%|    Check the :class:`torch.nn.modules.lazy.LazyModuleMixin` for further documentation
  1337|         0|            0|            0|  0.00%|    on lazy modules and their limitations.
  1338|         0|            0|            0|  0.00%|
  1339|         0|            0|            0|  0.00%|    Args:
  1340|         0|            0|            0|  0.00%|        out_channels (int): Number of channels produced by the convolution
  1341|         0|            0|            0|  0.00%|        kernel_size (int or tuple): Size of the convolving kernel
  1342|         0|            0|            0|  0.00%|        stride (int or tuple, optional): Stride of the convolution. Default: 1
  1343|         0|            0|            0|  0.00%|        padding (int or tuple, optional): ``dilation * (kernel_size - 1) - padding`` zero-padding
  1344|         0|            0|            0|  0.00%|            will be added to both sides of the input. Default: 0
  1345|         0|            0|            0|  0.00%|        output_padding (int or tuple, optional): Additional size added to one side
  1346|         0|            0|            0|  0.00%|            of the output shape. Default: 0
  1347|         0|            0|            0|  0.00%|        groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1
  1348|         0|            0|            0|  0.00%|        bias (bool, optional): If ``True``, adds a learnable bias to the output. Default: ``True``
  1349|         0|            0|            0|  0.00%|        dilation (int or tuple, optional): Spacing between kernel elements. Default: 1
  1350|         0|            0|            0|  0.00%|
  1351|         0|            0|            0|  0.00%|    .. seealso:: :class:`torch.nn.ConvTranspose1d` and :class:`torch.nn.modules.lazy.LazyModuleMixin`
  1352|         0|            0|            0|  0.00%|    """
  1353|         0|            0|            0|  0.00%|
  1354|         0|            0|            0|  0.00%|    # super class define this variable as None. "type: ignore[..] is required
  1355|         0|            0|            0|  0.00%|    # since we are redefining the variable.
  1356|         0|            0|            0|  0.00%|    cls_to_become = ConvTranspose1d  # type: ignore[assignment]
  1357|         0|            0|            0|  0.00%|
  1358|         0|            0|            0|  0.00%|    def __init__(
  1359|         0|            0|            0|  0.00%|        self,
  1360|         0|            0|            0|  0.00%|        out_channels: int,
  1361|         0|            0|            0|  0.00%|        kernel_size: _size_1_t,
  1362|         0|            0|            0|  0.00%|        stride: _size_1_t = 1,
  1363|         0|            0|            0|  0.00%|        padding: _size_1_t = 0,
  1364|         0|            0|            0|  0.00%|        output_padding: _size_1_t = 0,
  1365|         0|            0|            0|  0.00%|        groups: int = 1,
  1366|         0|            0|            0|  0.00%|        bias: bool = True,
  1367|         0|            0|            0|  0.00%|        dilation: _size_1_t = 1,
  1368|         0|            0|            0|  0.00%|        padding_mode: str = 'zeros',
  1369|         0|            0|            0|  0.00%|        device=None,
  1370|         0|            0|            0|  0.00%|        dtype=None
  1371|         0|            0|            0|  0.00%|    ) -> None:
  1372|         0|            0|            0|  0.00%|        factory_kwargs = {'device': device, 'dtype': dtype}
  1373|         0|            0|            0|  0.00%|        super().__init__(
  1374|         0|            0|            0|  0.00%|            0,
  1375|         0|            0|            0|  0.00%|            0,
  1376|         0|            0|            0|  0.00%|            kernel_size,
  1377|         0|            0|            0|  0.00%|            stride,
  1378|         0|            0|            0|  0.00%|            padding,
  1379|         0|            0|            0|  0.00%|            output_padding,
  1380|         0|            0|            0|  0.00%|            groups,
  1381|         0|            0|            0|  0.00%|            # bias is hardcoded to False to avoid creating tensor
  1382|         0|            0|            0|  0.00%|            # that will soon be overwritten.
  1383|         0|            0|            0|  0.00%|            False,
  1384|         0|            0|            0|  0.00%|            dilation,
  1385|         0|            0|            0|  0.00%|            padding_mode,
  1386|         0|            0|            0|  0.00%|            **factory_kwargs
  1387|         0|            0|            0|  0.00%|        )
  1388|         0|            0|            0|  0.00%|        self.weight = UninitializedParameter(**factory_kwargs)
  1389|         0|            0|            0|  0.00%|        self.out_channels = out_channels
  1390|         0|            0|            0|  0.00%|        if bias:
  1391|         0|            0|            0|  0.00%|            self.bias = UninitializedParameter(**factory_kwargs)
  1392|         0|            0|            0|  0.00%|
  1393|         0|            0|            0|  0.00%|
  1394|         0|            0|            0|  0.00%|# LazyConvTranspose2d defines weight as a Tensor but derived class defines it as UnitializeParameter
  1395|         0|            0|            0|  0.00%|class LazyConvTranspose2d(_LazyConvXdMixin, ConvTranspose2d):  # type: ignore[misc]
  1396|         0|            0|            0|  0.00%|    r"""A :class:`torch.nn.ConvTranspose2d` module with lazy initialization of
  1397|         0|            0|            0|  0.00%|    the ``in_channels`` argument of the :class:`ConvTranspose2d` that is inferred from
  1398|         0|            0|            0|  0.00%|    the ``input.size(1)``.
  1399|         0|            0|            0|  0.00%|    The attributes that will be lazily initialized are `weight` and `bias`.
  1400|         0|            0|            0|  0.00%|
  1401|         0|            0|            0|  0.00%|    Check the :class:`torch.nn.modules.lazy.LazyModuleMixin` for further documentation
  1402|         0|            0|            0|  0.00%|    on lazy modules and their limitations.
  1403|         0|            0|            0|  0.00%|
  1404|         0|            0|            0|  0.00%|    Args:
  1405|         0|            0|            0|  0.00%|        out_channels (int): Number of channels produced by the convolution
  1406|         0|            0|            0|  0.00%|        kernel_size (int or tuple): Size of the convolving kernel
  1407|         0|            0|            0|  0.00%|        stride (int or tuple, optional): Stride of the convolution. Default: 1
  1408|         0|            0|            0|  0.00%|        padding (int or tuple, optional): ``dilation * (kernel_size - 1) - padding`` zero-padding
  1409|         0|            0|            0|  0.00%|            will be added to both sides of each dimension in the input. Default: 0
  1410|         0|            0|            0|  0.00%|        output_padding (int or tuple, optional): Additional size added to one side
  1411|         0|            0|            0|  0.00%|            of each dimension in the output shape. Default: 0
  1412|         0|            0|            0|  0.00%|        groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1
  1413|         0|            0|            0|  0.00%|        bias (bool, optional): If ``True``, adds a learnable bias to the output. Default: ``True``
  1414|         0|            0|            0|  0.00%|        dilation (int or tuple, optional): Spacing between kernel elements. Default: 1
  1415|         0|            0|            0|  0.00%|
  1416|         0|            0|            0|  0.00%|    .. seealso:: :class:`torch.nn.ConvTranspose2d` and :class:`torch.nn.modules.lazy.LazyModuleMixin`
  1417|         0|            0|            0|  0.00%|    """
  1418|         0|            0|            0|  0.00%|
  1419|         0|            0|            0|  0.00%|    # super class define this variable as None. "type: ignore[..] is required
  1420|         0|            0|            0|  0.00%|    # since we are redefining the variable.
  1421|         0|            0|            0|  0.00%|    cls_to_become = ConvTranspose2d  # type: ignore[assignment]
  1422|         0|            0|            0|  0.00%|
  1423|         0|            0|            0|  0.00%|    def __init__(
  1424|         0|            0|            0|  0.00%|        self,
  1425|         0|            0|            0|  0.00%|        out_channels: int,
  1426|         0|            0|            0|  0.00%|        kernel_size: _size_2_t,
  1427|         0|            0|            0|  0.00%|        stride: _size_2_t = 1,
  1428|         0|            0|            0|  0.00%|        padding: _size_2_t = 0,
  1429|         0|            0|            0|  0.00%|        output_padding: _size_2_t = 0,
  1430|         0|            0|            0|  0.00%|        groups: int = 1,
  1431|         0|            0|            0|  0.00%|        bias: bool = True,
  1432|         0|            0|            0|  0.00%|        dilation: int = 1,
  1433|         0|            0|            0|  0.00%|        padding_mode: str = 'zeros',
  1434|         0|            0|            0|  0.00%|        device=None,
  1435|         0|            0|            0|  0.00%|        dtype=None
  1436|         0|            0|            0|  0.00%|    ) -> None:
  1437|         0|            0|            0|  0.00%|        factory_kwargs = {'device': device, 'dtype': dtype}
  1438|         0|            0|            0|  0.00%|        super().__init__(
  1439|         0|            0|            0|  0.00%|            0,
  1440|         0|            0|            0|  0.00%|            0,
  1441|         0|            0|            0|  0.00%|            kernel_size,
  1442|         0|            0|            0|  0.00%|            stride,
  1443|         0|            0|            0|  0.00%|            padding,
  1444|         0|            0|            0|  0.00%|            output_padding,
  1445|         0|            0|            0|  0.00%|            groups,
  1446|         0|            0|            0|  0.00%|            # bias is hardcoded to False to avoid creating tensor
  1447|         0|            0|            0|  0.00%|            # that will soon be overwritten.
  1448|         0|            0|            0|  0.00%|            False,
  1449|         0|            0|            0|  0.00%|            dilation,
  1450|         0|            0|            0|  0.00%|            padding_mode,
  1451|         0|            0|            0|  0.00%|            **factory_kwargs
  1452|         0|            0|            0|  0.00%|        )
  1453|         0|            0|            0|  0.00%|        self.weight = UninitializedParameter(**factory_kwargs)
  1454|         0|            0|            0|  0.00%|        self.out_channels = out_channels
  1455|         0|            0|            0|  0.00%|        if bias:
  1456|         0|            0|            0|  0.00%|            self.bias = UninitializedParameter(**factory_kwargs)
  1457|         0|            0|            0|  0.00%|
  1458|         0|            0|            0|  0.00%|
  1459|         0|            0|            0|  0.00%|# LazyConvTranspose3d defines weight as a Tensor but derived class defines it as UnitializeParameter
  1460|         0|            0|            0|  0.00%|class LazyConvTranspose3d(_LazyConvXdMixin, ConvTranspose3d):  # type: ignore[misc]
  1461|         0|            0|            0|  0.00%|    r"""A :class:`torch.nn.ConvTranspose3d` module with lazy initialization of
  1462|         0|            0|            0|  0.00%|    the ``in_channels`` argument of the :class:`ConvTranspose3d` that is inferred from
  1463|         0|            0|            0|  0.00%|    the ``input.size(1)``.
  1464|         0|            0|            0|  0.00%|    The attributes that will be lazily initialized are `weight` and `bias`.
  1465|         0|            0|            0|  0.00%|
  1466|         0|            0|            0|  0.00%|    Check the :class:`torch.nn.modules.lazy.LazyModuleMixin` for further documentation
  1467|         0|            0|            0|  0.00%|    on lazy modules and their limitations.
  1468|         0|            0|            0|  0.00%|
  1469|         0|            0|            0|  0.00%|    Args:
  1470|         0|            0|            0|  0.00%|        out_channels (int): Number of channels produced by the convolution
  1471|         0|            0|            0|  0.00%|        kernel_size (int or tuple): Size of the convolving kernel
  1472|         0|            0|            0|  0.00%|        stride (int or tuple, optional): Stride of the convolution. Default: 1
  1473|         0|            0|            0|  0.00%|        padding (int or tuple, optional): ``dilation * (kernel_size - 1) - padding`` zero-padding
  1474|         0|            0|            0|  0.00%|            will be added to both sides of each dimension in the input. Default: 0
  1475|         0|            0|            0|  0.00%|        output_padding (int or tuple, optional): Additional size added to one side
  1476|         0|            0|            0|  0.00%|            of each dimension in the output shape. Default: 0
  1477|         0|            0|            0|  0.00%|        groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1
  1478|         0|            0|            0|  0.00%|        bias (bool, optional): If ``True``, adds a learnable bias to the output. Default: ``True``
  1479|         0|            0|            0|  0.00%|        dilation (int or tuple, optional): Spacing between kernel elements. Default: 1
  1480|         0|            0|            0|  0.00%|
  1481|         0|            0|            0|  0.00%|    .. seealso:: :class:`torch.nn.ConvTranspose3d` and :class:`torch.nn.modules.lazy.LazyModuleMixin`
  1482|         0|            0|            0|  0.00%|    """
  1483|         0|            0|            0|  0.00%|
  1484|         0|            0|            0|  0.00%|    # super class define this variable as None. "type: ignore[..] is required
  1485|         0|            0|            0|  0.00%|    # since we are redefining the variable.
  1486|         0|            0|            0|  0.00%|    cls_to_become = ConvTranspose3d  # type: ignore[assignment]
  1487|         0|            0|            0|  0.00%|
  1488|         0|            0|            0|  0.00%|    def __init__(
  1489|         0|            0|            0|  0.00%|        self,
  1490|         0|            0|            0|  0.00%|        out_channels: int,
  1491|         0|            0|            0|  0.00%|        kernel_size: _size_3_t,
  1492|         0|            0|            0|  0.00%|        stride: _size_3_t = 1,
  1493|         0|            0|            0|  0.00%|        padding: _size_3_t = 0,
  1494|         0|            0|            0|  0.00%|        output_padding: _size_3_t = 0,
  1495|         0|            0|            0|  0.00%|        groups: int = 1,
  1496|         0|            0|            0|  0.00%|        bias: bool = True,
  1497|         0|            0|            0|  0.00%|        dilation: _size_3_t = 1,
  1498|         0|            0|            0|  0.00%|        padding_mode: str = 'zeros',
  1499|         0|            0|            0|  0.00%|        device=None,
  1500|         0|            0|            0|  0.00%|        dtype=None
  1501|         0|            0|            0|  0.00%|    ) -> None:
  1502|         0|            0|            0|  0.00%|        factory_kwargs = {'device': device, 'dtype': dtype}
  1503|         0|            0|            0|  0.00%|        super().__init__(
  1504|         0|            0|            0|  0.00%|            0,
  1505|         0|            0|            0|  0.00%|            0,
  1506|         0|            0|            0|  0.00%|            kernel_size,
  1507|         0|            0|            0|  0.00%|            stride,
  1508|         0|            0|            0|  0.00%|            padding,
  1509|         0|            0|            0|  0.00%|            output_padding,
  1510|         0|            0|            0|  0.00%|            groups,
  1511|         0|            0|            0|  0.00%|            # bias is hardcoded to False to avoid creating tensor
  1512|         0|            0|            0|  0.00%|            # that will soon be overwritten.
  1513|         0|            0|            0|  0.00%|            False,
  1514|         0|            0|            0|  0.00%|            dilation,
  1515|         0|            0|            0|  0.00%|            padding_mode,
  1516|         0|            0|            0|  0.00%|            **factory_kwargs
  1517|         0|            0|            0|  0.00%|        )
  1518|         0|            0|            0|  0.00%|        self.weight = UninitializedParameter(**factory_kwargs)
  1519|         0|            0|            0|  0.00%|        self.out_channels = out_channels
  1520|         0|            0|            0|  0.00%|        if bias:
  1521|         0|            0|            0|  0.00%|            self.bias = UninitializedParameter(**factory_kwargs)
File: /opt/conda/lib/python3.8/site-packages/torch/nn/functional.py
File duration: 2.38592s (4.45%)
Line #|      Hits|         Time| Time per hit|      %|Source code
------+----------+-------------+-------------+-------+-----------
     1|         0|            0|            0|  0.00%|r"""Functional interface"""
     2|         0|            0|            0|  0.00%|from typing import Callable, List, Optional, Tuple
     3|         0|            0|            0|  0.00%|import math
     4|         0|            0|            0|  0.00%|import warnings
     5|         0|            0|            0|  0.00%|
     6|         0|            0|            0|  0.00%|import torch
     7|         0|            0|            0|  0.00%|from torch import _VF
     8|         0|            0|            0|  0.00%|from torch._C import _infer_size, _add_docstr
     9|         0|            0|            0|  0.00%|from torch._torch_docs import reproducibility_notes, tf32_notes
    10|         0|            0|            0|  0.00%|
    11|         0|            0|            0|  0.00%|from .._jit_internal import boolean_dispatch, _overload, BroadcastingList1, BroadcastingList2, BroadcastingList3
    12|         0|            0|            0|  0.00%|from ..overrides import (
    13|         0|            0|            0|  0.00%|    has_torch_function, has_torch_function_unary, has_torch_function_variadic,
    14|         0|            0|            0|  0.00%|    handle_torch_function)
    15|         0|            0|            0|  0.00%|from . import _reduction as _Reduction
    16|         0|            0|            0|  0.00%|from . import grad  # noqa: F401
    17|         0|            0|            0|  0.00%|from .modules import utils
    18|         0|            0|            0|  0.00%|from .modules.utils import _single, _pair, _triple, _list_with_default
    19|         0|            0|            0|  0.00%|
    20|         0|            0|            0|  0.00%|
    21|         0|            0|            0|  0.00%|Tensor = torch.Tensor
    22|         0|            0|            0|  0.00%|
    23|         0|            0|            0|  0.00%|conv1d = _add_docstr(
    24|         0|            0|            0|  0.00%|    torch.conv1d,
    25|         0|            0|            0|  0.00%|    r"""
    26|         0|            0|            0|  0.00%|conv1d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1) -> Tensor
    27|         0|            0|            0|  0.00%|
    28|         0|            0|            0|  0.00%|Applies a 1D convolution over an input signal composed of several input
    29|         0|            0|            0|  0.00%|planes.
    30|         0|            0|            0|  0.00%|
    31|         0|            0|            0|  0.00%|{tf32_note}
    32|         0|            0|            0|  0.00%|
    33|         0|            0|            0|  0.00%|See :class:`~torch.nn.Conv1d` for details and output shape.
    34|         0|            0|            0|  0.00%|
    35|         0|            0|            0|  0.00%|Note:
    36|         0|            0|            0|  0.00%|    {cudnn_reproducibility_note}
    37|         0|            0|            0|  0.00%|""".format(
    38|         0|            0|            0|  0.00%|        **reproducibility_notes, **tf32_notes
    39|         0|            0|            0|  0.00%|    )
    40|         0|            0|            0|  0.00%|    + r"""
    41|         0|            0|            0|  0.00%|
    42|         0|            0|            0|  0.00%|Args:
    43|         0|            0|            0|  0.00%|    input: input tensor of shape :math:`(\text{minibatch} , \text{in\_channels} , iW)`
    44|         0|            0|            0|  0.00%|    weight: filters of shape :math:`(\text{out\_channels} , \frac{\text{in\_channels}}{\text{groups}} , kW)`
    45|         0|            0|            0|  0.00%|    bias: optional bias of shape :math:`(\text{out\_channels})`. Default: ``None``
    46|         0|            0|            0|  0.00%|    stride: the stride of the convolving kernel. Can be a single number or
    47|         0|            0|            0|  0.00%|      a one-element tuple `(sW,)`. Default: 1
    48|         0|            0|            0|  0.00%|    padding: implicit paddings on both sides of the input. Can be a string {'valid', 'same'},
    49|         0|            0|            0|  0.00%|      single number or a one-element tuple `(padW,)`. Default: 0
    50|         0|            0|            0|  0.00%|      ``padding='valid'`` is the same as no padding. ``padding='same'`` pads
    51|         0|            0|            0|  0.00%|      the input so the output has the shape as the input. However, this mode
    52|         0|            0|            0|  0.00%|      doesn't support any stride values other than 1.
    53|         0|            0|            0|  0.00%|
    54|         0|            0|            0|  0.00%|      .. warning::
    55|         0|            0|            0|  0.00%|          For ``padding='same'``, if the ``weight`` is even-length and
    56|         0|            0|            0|  0.00%|          ``dilation`` is odd in any dimension, a full :func:`pad` operation
    57|         0|            0|            0|  0.00%|          may be needed internally. Lowering performance.
    58|         0|            0|            0|  0.00%|    dilation: the spacing between kernel elements. Can be a single number or
    59|         0|            0|            0|  0.00%|      a one-element tuple `(dW,)`. Default: 1
    60|         0|            0|            0|  0.00%|    groups: split input into groups, :math:`\text{in\_channels}` should be divisible by
    61|         0|            0|            0|  0.00%|      the number of groups. Default: 1
    62|         0|            0|            0|  0.00%|
    63|         0|            0|            0|  0.00%|Examples::
    64|         0|            0|            0|  0.00%|
    65|         0|            0|            0|  0.00%|    >>> inputs = torch.randn(33, 16, 30)
    66|         0|            0|            0|  0.00%|    >>> filters = torch.randn(20, 16, 5)
    67|         0|            0|            0|  0.00%|    >>> F.conv1d(inputs, filters)
    68|         0|            0|            0|  0.00%|""",
    69|         0|            0|            0|  0.00%|)
    70|         0|            0|            0|  0.00%|
    71|         0|            0|            0|  0.00%|conv2d = _add_docstr(
    72|         0|            0|            0|  0.00%|    torch.conv2d,
    73|         0|            0|            0|  0.00%|    r"""
    74|         0|            0|            0|  0.00%|conv2d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1) -> Tensor
    75|         0|            0|            0|  0.00%|
    76|         0|            0|            0|  0.00%|Applies a 2D convolution over an input image composed of several input
    77|         0|            0|            0|  0.00%|planes.
    78|         0|            0|            0|  0.00%|
    79|         0|            0|            0|  0.00%|{tf32_note}
    80|         0|            0|            0|  0.00%|
    81|         0|            0|            0|  0.00%|See :class:`~torch.nn.Conv2d` for details and output shape.
    82|         0|            0|            0|  0.00%|
    83|         0|            0|            0|  0.00%|Note:
    84|         0|            0|            0|  0.00%|    {cudnn_reproducibility_note}
    85|         0|            0|            0|  0.00%|""".format(
    86|         0|            0|            0|  0.00%|        **reproducibility_notes, **tf32_notes
    87|         0|            0|            0|  0.00%|    )
    88|         0|            0|            0|  0.00%|    + r"""
    89|         0|            0|            0|  0.00%|
    90|         0|            0|            0|  0.00%|Args:
    91|         0|            0|            0|  0.00%|    input: input tensor of shape :math:`(\text{minibatch} , \text{in\_channels} , iH , iW)`
    92|         0|            0|            0|  0.00%|    weight: filters of shape :math:`(\text{out\_channels} , \frac{\text{in\_channels}}{\text{groups}} , kH , kW)`
    93|         0|            0|            0|  0.00%|    bias: optional bias tensor of shape :math:`(\text{out\_channels})`. Default: ``None``
    94|         0|            0|            0|  0.00%|    stride: the stride of the convolving kernel. Can be a single number or a
    95|         0|            0|            0|  0.00%|      tuple `(sH, sW)`. Default: 1
    96|         0|            0|            0|  0.00%|    padding: implicit paddings on both sides of the input. Can be a string {'valid', 'same'},
    97|         0|            0|            0|  0.00%|      single number or a tuple `(padH, padW)`. Default: 0
    98|         0|            0|            0|  0.00%|      ``padding='valid'`` is the same as no padding. ``padding='same'`` pads
    99|         0|            0|            0|  0.00%|      the input so the output has the shape as the input. However, this mode
   100|         0|            0|            0|  0.00%|      doesn't support any stride values other than 1.
   101|         0|            0|            0|  0.00%|
   102|         0|            0|            0|  0.00%|      .. warning::
   103|         0|            0|            0|  0.00%|          For ``padding='same'``, if the ``weight`` is even-length and
   104|         0|            0|            0|  0.00%|          ``dilation`` is odd in any dimension, a full :func:`pad` operation
   105|         0|            0|            0|  0.00%|          may be needed internally. Lowering performance.
   106|         0|            0|            0|  0.00%|
   107|         0|            0|            0|  0.00%|    dilation: the spacing between kernel elements. Can be a single number or
   108|         0|            0|            0|  0.00%|      a tuple `(dH, dW)`. Default: 1
   109|         0|            0|            0|  0.00%|    groups: split input into groups, :math:`\text{in\_channels}` should be divisible by the
   110|         0|            0|            0|  0.00%|      number of groups. Default: 1
   111|         0|            0|            0|  0.00%|
   112|         0|            0|            0|  0.00%|Examples::
   113|         0|            0|            0|  0.00%|
   114|         0|            0|            0|  0.00%|    >>> # With square kernels and equal stride
   115|         0|            0|            0|  0.00%|    >>> filters = torch.randn(8, 4, 3, 3)
   116|         0|            0|            0|  0.00%|    >>> inputs = torch.randn(1, 4, 5, 5)
   117|         0|            0|            0|  0.00%|    >>> F.conv2d(inputs, filters, padding=1)
   118|         0|            0|            0|  0.00%|""",
   119|         0|            0|            0|  0.00%|)  # noqa: E501
   120|         0|            0|            0|  0.00%|
   121|         0|            0|            0|  0.00%|conv3d = _add_docstr(
   122|         0|            0|            0|  0.00%|    torch.conv3d,
   123|         0|            0|            0|  0.00%|    r"""
   124|         0|            0|            0|  0.00%|conv3d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1) -> Tensor
   125|         0|            0|            0|  0.00%|
   126|         0|            0|            0|  0.00%|Applies a 3D convolution over an input image composed of several input
   127|         0|            0|            0|  0.00%|planes.
   128|         0|            0|            0|  0.00%|
   129|         0|            0|            0|  0.00%|{tf32_note}
   130|         0|            0|            0|  0.00%|
   131|         0|            0|            0|  0.00%|See :class:`~torch.nn.Conv3d` for details and output shape.
   132|         0|            0|            0|  0.00%|
   133|         0|            0|            0|  0.00%|Note:
   134|         0|            0|            0|  0.00%|    {cudnn_reproducibility_note}
   135|         0|            0|            0|  0.00%|""".format(
   136|         0|            0|            0|  0.00%|        **reproducibility_notes, **tf32_notes
   137|         0|            0|            0|  0.00%|    )
   138|         0|            0|            0|  0.00%|    + r"""
   139|         0|            0|            0|  0.00%|
   140|         0|            0|            0|  0.00%|Args:
   141|         0|            0|            0|  0.00%|    input: input tensor of shape :math:`(\text{minibatch} , \text{in\_channels} , iT , iH , iW)`
   142|         0|            0|            0|  0.00%|    weight: filters of shape :math:`(\text{out\_channels} , \frac{\text{in\_channels}}{\text{groups}} , kT , kH , kW)`
   143|         0|            0|            0|  0.00%|    bias: optional bias tensor of shape :math:`(\text{out\_channels})`. Default: None
   144|         0|            0|            0|  0.00%|    stride: the stride of the convolving kernel. Can be a single number or a
   145|         0|            0|            0|  0.00%|      tuple `(sT, sH, sW)`. Default: 1
   146|         0|            0|            0|  0.00%|    padding: implicit paddings on both sides of the input. Can be a string {'valid', 'same'},
   147|         0|            0|            0|  0.00%|      single number or a tuple `(padT, padH, padW)`. Default: 0
   148|         0|            0|            0|  0.00%|      ``padding='valid'`` is the same as no padding. ``padding='same'`` pads
   149|         0|            0|            0|  0.00%|      the input so the output has the shape as the input. However, this mode
   150|         0|            0|            0|  0.00%|      doesn't support any stride values other than 1.
   151|         0|            0|            0|  0.00%|
   152|         0|            0|            0|  0.00%|      .. warning::
   153|         0|            0|            0|  0.00%|          For ``padding='same'``, if the ``weight`` is even-length and
   154|         0|            0|            0|  0.00%|          ``dilation`` is odd in any dimension, a full :func:`pad` operation
   155|         0|            0|            0|  0.00%|          may be needed internally. Lowering performance.
   156|         0|            0|            0|  0.00%|
   157|         0|            0|            0|  0.00%|    dilation: the spacing between kernel elements. Can be a single number or
   158|         0|            0|            0|  0.00%|      a tuple `(dT, dH, dW)`. Default: 1
   159|         0|            0|            0|  0.00%|    groups: split input into groups, :math:`\text{in\_channels}` should be divisible by
   160|         0|            0|            0|  0.00%|      the number of groups. Default: 1
   161|         0|            0|            0|  0.00%|
   162|         0|            0|            0|  0.00%|Examples::
   163|         0|            0|            0|  0.00%|
   164|         0|            0|            0|  0.00%|    >>> filters = torch.randn(33, 16, 3, 3, 3)
   165|         0|            0|            0|  0.00%|    >>> inputs = torch.randn(20, 16, 50, 10, 20)
   166|         0|            0|            0|  0.00%|    >>> F.conv3d(inputs, filters)
   167|         0|            0|            0|  0.00%|""",
   168|         0|            0|            0|  0.00%|)  # noqa: E501
   169|         0|            0|            0|  0.00%|
   170|         0|            0|            0|  0.00%|conv_transpose1d = _add_docstr(
   171|         0|            0|            0|  0.00%|    torch.conv_transpose1d,
   172|         0|            0|            0|  0.00%|    r"""
   173|         0|            0|            0|  0.00%|conv_transpose1d(input, weight, bias=None, stride=1, padding=0, output_padding=0, groups=1, dilation=1) -> Tensor
   174|         0|            0|            0|  0.00%|
   175|         0|            0|            0|  0.00%|Applies a 1D transposed convolution operator over an input signal
   176|         0|            0|            0|  0.00%|composed of several input planes, sometimes also called "deconvolution".
   177|         0|            0|            0|  0.00%|
   178|         0|            0|            0|  0.00%|{tf32_note}
   179|         0|            0|            0|  0.00%|
   180|         0|            0|            0|  0.00%|See :class:`~torch.nn.ConvTranspose1d` for details and output shape.
   181|         0|            0|            0|  0.00%|
   182|         0|            0|            0|  0.00%|Note:
   183|         0|            0|            0|  0.00%|    {cudnn_reproducibility_note}
   184|         0|            0|            0|  0.00%|""".format(
   185|         0|            0|            0|  0.00%|        **reproducibility_notes, **tf32_notes
   186|         0|            0|            0|  0.00%|    )
   187|         0|            0|            0|  0.00%|    + r"""
   188|         0|            0|            0|  0.00%|
   189|         0|            0|            0|  0.00%|Args:
   190|         0|            0|            0|  0.00%|    input: input tensor of shape :math:`(\text{minibatch} , \text{in\_channels} , iW)`
   191|         0|            0|            0|  0.00%|    weight: filters of shape :math:`(\text{in\_channels} , \frac{\text{out\_channels}}{\text{groups}} , kW)`
   192|         0|            0|            0|  0.00%|    bias: optional bias of shape :math:`(\text{out\_channels})`. Default: None
   193|         0|            0|            0|  0.00%|    stride: the stride of the convolving kernel. Can be a single number or a
   194|         0|            0|            0|  0.00%|      tuple ``(sW,)``. Default: 1
   195|         0|            0|            0|  0.00%|    padding: ``dilation * (kernel_size - 1) - padding`` zero-padding will be added to both
   196|         0|            0|            0|  0.00%|      sides of each dimension in the input. Can be a single number or a tuple
   197|         0|            0|            0|  0.00%|      ``(padW,)``. Default: 0
   198|         0|            0|            0|  0.00%|    output_padding: additional size added to one side of each dimension in the
   199|         0|            0|            0|  0.00%|      output shape. Can be a single number or a tuple ``(out_padW)``. Default: 0
   200|         0|            0|            0|  0.00%|    groups: split input into groups, :math:`\text{in\_channels}` should be divisible by the
   201|         0|            0|            0|  0.00%|      number of groups. Default: 1
   202|         0|            0|            0|  0.00%|    dilation: the spacing between kernel elements. Can be a single number or
   203|         0|            0|            0|  0.00%|      a tuple ``(dW,)``. Default: 1
   204|         0|            0|            0|  0.00%|
   205|         0|            0|            0|  0.00%|Examples::
   206|         0|            0|            0|  0.00%|
   207|         0|            0|            0|  0.00%|    >>> inputs = torch.randn(20, 16, 50)
   208|         0|            0|            0|  0.00%|    >>> weights = torch.randn(16, 33, 5)
   209|         0|            0|            0|  0.00%|    >>> F.conv_transpose1d(inputs, weights)
   210|         0|            0|            0|  0.00%|""",
   211|         0|            0|            0|  0.00%|)
   212|         0|            0|            0|  0.00%|
   213|         0|            0|            0|  0.00%|conv_transpose2d = _add_docstr(
   214|         0|            0|            0|  0.00%|    torch.conv_transpose2d,
   215|         0|            0|            0|  0.00%|    r"""
   216|         0|            0|            0|  0.00%|conv_transpose2d(input, weight, bias=None, stride=1, padding=0, output_padding=0, groups=1, dilation=1) -> Tensor
   217|         0|            0|            0|  0.00%|
   218|         0|            0|            0|  0.00%|Applies a 2D transposed convolution operator over an input image
   219|         0|            0|            0|  0.00%|composed of several input planes, sometimes also called "deconvolution".
   220|         0|            0|            0|  0.00%|
   221|         0|            0|            0|  0.00%|{tf32_note}
   222|         0|            0|            0|  0.00%|
   223|         0|            0|            0|  0.00%|See :class:`~torch.nn.ConvTranspose2d` for details and output shape.
   224|         0|            0|            0|  0.00%|
   225|         0|            0|            0|  0.00%|Note:
   226|         0|            0|            0|  0.00%|    {cudnn_reproducibility_note}
   227|         0|            0|            0|  0.00%|""".format(
   228|         0|            0|            0|  0.00%|        **reproducibility_notes, **tf32_notes
   229|         0|            0|            0|  0.00%|    )
   230|         0|            0|            0|  0.00%|    + r"""
   231|         0|            0|            0|  0.00%|
   232|         0|            0|            0|  0.00%|Args:
   233|         0|            0|            0|  0.00%|    input: input tensor of shape :math:`(\text{minibatch} , \text{in\_channels} , iH , iW)`
   234|         0|            0|            0|  0.00%|    weight: filters of shape :math:`(\text{in\_channels} , \frac{\text{out\_channels}}{\text{groups}} , kH , kW)`
   235|         0|            0|            0|  0.00%|    bias: optional bias of shape :math:`(\text{out\_channels})`. Default: None
   236|         0|            0|            0|  0.00%|    stride: the stride of the convolving kernel. Can be a single number or a
   237|         0|            0|            0|  0.00%|      tuple ``(sH, sW)``. Default: 1
   238|         0|            0|            0|  0.00%|    padding: ``dilation * (kernel_size - 1) - padding`` zero-padding will be added to both
   239|         0|            0|            0|  0.00%|      sides of each dimension in the input. Can be a single number or a tuple
   240|         0|            0|            0|  0.00%|      ``(padH, padW)``. Default: 0
   241|         0|            0|            0|  0.00%|    output_padding: additional size added to one side of each dimension in the
   242|         0|            0|            0|  0.00%|      output shape. Can be a single number or a tuple ``(out_padH, out_padW)``.
   243|         0|            0|            0|  0.00%|      Default: 0
   244|         0|            0|            0|  0.00%|    groups: split input into groups, :math:`\text{in\_channels}` should be divisible by the
   245|         0|            0|            0|  0.00%|      number of groups. Default: 1
   246|         0|            0|            0|  0.00%|    dilation: the spacing between kernel elements. Can be a single number or
   247|         0|            0|            0|  0.00%|      a tuple ``(dH, dW)``. Default: 1
   248|         0|            0|            0|  0.00%|
   249|         0|            0|            0|  0.00%|Examples::
   250|         0|            0|            0|  0.00%|
   251|         0|            0|            0|  0.00%|    >>> # With square kernels and equal stride
   252|         0|            0|            0|  0.00%|    >>> inputs = torch.randn(1, 4, 5, 5)
   253|         0|            0|            0|  0.00%|    >>> weights = torch.randn(4, 8, 3, 3)
   254|         0|            0|            0|  0.00%|    >>> F.conv_transpose2d(inputs, weights, padding=1)
   255|         0|            0|            0|  0.00%|""",
   256|         0|            0|            0|  0.00%|)  # noqa: E501
   257|         0|            0|            0|  0.00%|
   258|         0|            0|            0|  0.00%|conv_transpose3d = _add_docstr(
   259|         0|            0|            0|  0.00%|    torch.conv_transpose3d,
   260|         0|            0|            0|  0.00%|    r"""
   261|         0|            0|            0|  0.00%|conv_transpose3d(input, weight, bias=None, stride=1, padding=0, output_padding=0, groups=1, dilation=1) -> Tensor
   262|         0|            0|            0|  0.00%|
   263|         0|            0|            0|  0.00%|Applies a 3D transposed convolution operator over an input image
   264|         0|            0|            0|  0.00%|composed of several input planes, sometimes also called "deconvolution"
   265|         0|            0|            0|  0.00%|
   266|         0|            0|            0|  0.00%|{tf32_note}
   267|         0|            0|            0|  0.00%|
   268|         0|            0|            0|  0.00%|See :class:`~torch.nn.ConvTranspose3d` for details and output shape.
   269|         0|            0|            0|  0.00%|
   270|         0|            0|            0|  0.00%|Note:
   271|         0|            0|            0|  0.00%|    {cudnn_reproducibility_note}
   272|         0|            0|            0|  0.00%|""".format(
   273|         0|            0|            0|  0.00%|        **reproducibility_notes, **tf32_notes
   274|         0|            0|            0|  0.00%|    )
   275|         0|            0|            0|  0.00%|    + r"""
   276|         0|            0|            0|  0.00%|
   277|         0|            0|            0|  0.00%|Args:
   278|         0|            0|            0|  0.00%|    input: input tensor of shape :math:`(\text{minibatch} , \text{in\_channels} , iT , iH , iW)`
   279|         0|            0|            0|  0.00%|    weight: filters of shape :math:`(\text{in\_channels} , \frac{\text{out\_channels}}{\text{groups}} , kT , kH , kW)`
   280|         0|            0|            0|  0.00%|    bias: optional bias of shape :math:`(\text{out\_channels})`. Default: None
   281|         0|            0|            0|  0.00%|    stride: the stride of the convolving kernel. Can be a single number or a
   282|         0|            0|            0|  0.00%|      tuple ``(sT, sH, sW)``. Default: 1
   283|         0|            0|            0|  0.00%|    padding: ``dilation * (kernel_size - 1) - padding`` zero-padding will be added to both
   284|         0|            0|            0|  0.00%|      sides of each dimension in the input. Can be a single number or a tuple
   285|         0|            0|            0|  0.00%|      ``(padT, padH, padW)``. Default: 0
   286|         0|            0|            0|  0.00%|    output_padding: additional size added to one side of each dimension in the
   287|         0|            0|            0|  0.00%|      output shape. Can be a single number or a tuple
   288|         0|            0|            0|  0.00%|      ``(out_padT, out_padH, out_padW)``. Default: 0
   289|         0|            0|            0|  0.00%|    groups: split input into groups, :math:`\text{in\_channels}` should be divisible by the
   290|         0|            0|            0|  0.00%|      number of groups. Default: 1
   291|         0|            0|            0|  0.00%|    dilation: the spacing between kernel elements. Can be a single number or
   292|         0|            0|            0|  0.00%|      a tuple `(dT, dH, dW)`. Default: 1
   293|         0|            0|            0|  0.00%|
   294|         0|            0|            0|  0.00%|Examples::
   295|         0|            0|            0|  0.00%|
   296|         0|            0|            0|  0.00%|    >>> inputs = torch.randn(20, 16, 50, 10, 20)
   297|         0|            0|            0|  0.00%|    >>> weights = torch.randn(16, 33, 3, 3, 3)
   298|         0|            0|            0|  0.00%|    >>> F.conv_transpose3d(inputs, weights)
   299|         0|            0|            0|  0.00%|""",
   300|         0|            0|            0|  0.00%|)  # noqa: E501
   301|         0|            0|            0|  0.00%|
   302|         0|            0|            0|  0.00%|conv_tbc = _add_docstr(
   303|         0|            0|            0|  0.00%|    torch.conv_tbc,
   304|         0|            0|            0|  0.00%|    r"""
   305|         0|            0|            0|  0.00%|Applies a 1-dimensional sequence convolution over an input sequence.
   306|         0|            0|            0|  0.00%|Input and output dimensions are (Time, Batch, Channels) - hence TBC.
   307|         0|            0|            0|  0.00%|
   308|         0|            0|            0|  0.00%|Args:
   309|         0|            0|            0|  0.00%|    input: input tensor of shape :math:`(\text{sequence length} \times batch \times \text{in\_channels})`
   310|         0|            0|            0|  0.00%|    weight: filter of shape (:math:`\text{kernel width} \times \text{in\_channels} \times \text{out\_channels}`)
   311|         0|            0|            0|  0.00%|    bias: bias of shape (:math:`\text{out\_channels}`)
   312|         0|            0|            0|  0.00%|    pad: number of timesteps to pad. Default: 0
   313|         0|            0|            0|  0.00%|""",
   314|         0|            0|            0|  0.00%|)
   315|         0|            0|            0|  0.00%|
   316|         0|            0|            0|  0.00%|
   317|         0|            0|            0|  0.00%|# Pooling
   318|         0|            0|            0|  0.00%|avg_pool1d = _add_docstr(
   319|         0|            0|            0|  0.00%|    torch.avg_pool1d,
   320|         0|            0|            0|  0.00%|    r"""
   321|         0|            0|            0|  0.00%|avg_pool1d(input, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True) -> Tensor
   322|         0|            0|            0|  0.00%|
   323|         0|            0|            0|  0.00%|Applies a 1D average pooling over an input signal composed of several
   324|         0|            0|            0|  0.00%|input planes.
   325|         0|            0|            0|  0.00%|
   326|         0|            0|            0|  0.00%|See :class:`~torch.nn.AvgPool1d` for details and output shape.
   327|         0|            0|            0|  0.00%|
   328|         0|            0|            0|  0.00%|Args:
   329|         0|            0|            0|  0.00%|    input: input tensor of shape :math:`(\text{minibatch} , \text{in\_channels} , iW)`
   330|         0|            0|            0|  0.00%|    kernel_size: the size of the window. Can be a single number or a
   331|         0|            0|            0|  0.00%|      tuple `(kW,)`
   332|         0|            0|            0|  0.00%|    stride: the stride of the window. Can be a single number or a tuple
   333|         0|            0|            0|  0.00%|      `(sW,)`. Default: :attr:`kernel_size`
   334|         0|            0|            0|  0.00%|    padding: implicit zero paddings on both sides of the input. Can be a
   335|         0|            0|            0|  0.00%|      single number or a tuple `(padW,)`. Default: 0
   336|         0|            0|            0|  0.00%|    ceil_mode: when True, will use `ceil` instead of `floor` to compute the
   337|         0|            0|            0|  0.00%|        output shape. Default: ``False``
   338|         0|            0|            0|  0.00%|    count_include_pad: when True, will include the zero-padding in the
   339|         0|            0|            0|  0.00%|        averaging calculation. Default: ``True``
   340|         0|            0|            0|  0.00%|
   341|         0|            0|            0|  0.00%|Examples::
   342|         0|            0|            0|  0.00%|
   343|         0|            0|            0|  0.00%|    >>> # pool of square window of size=3, stride=2
   344|         0|            0|            0|  0.00%|    >>> input = torch.tensor([[[1, 2, 3, 4, 5, 6, 7]]], dtype=torch.float32)
   345|         0|            0|            0|  0.00%|    >>> F.avg_pool1d(input, kernel_size=3, stride=2)
   346|         0|            0|            0|  0.00%|    tensor([[[ 2.,  4.,  6.]]])
   347|         0|            0|            0|  0.00%|
   348|         0|            0|            0|  0.00%|""",
   349|         0|            0|            0|  0.00%|)
   350|         0|            0|            0|  0.00%|
   351|         0|            0|            0|  0.00%|
   352|         0|            0|            0|  0.00%|avg_pool2d = _add_docstr(
   353|         0|            0|            0|  0.00%|    torch._C._nn.avg_pool2d,
   354|         0|            0|            0|  0.00%|    r"""
   355|         0|            0|            0|  0.00%|avg_pool2d(input, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True, divisor_override=None) -> Tensor
   356|         0|            0|            0|  0.00%|
   357|         0|            0|            0|  0.00%|Applies 2D average-pooling operation in :math:`kH \times kW` regions by step size
   358|         0|            0|            0|  0.00%|:math:`sH \times sW` steps. The number of output features is equal to the number of
   359|         0|            0|            0|  0.00%|input planes.
   360|         0|            0|            0|  0.00%|
   361|         0|            0|            0|  0.00%|See :class:`~torch.nn.AvgPool2d` for details and output shape.
   362|         0|            0|            0|  0.00%|
   363|         0|            0|            0|  0.00%|Args:
   364|         0|            0|            0|  0.00%|    input: input tensor :math:`(\text{minibatch} , \text{in\_channels} , iH , iW)`
   365|         0|            0|            0|  0.00%|    kernel_size: size of the pooling region. Can be a single number or a
   366|         0|            0|            0|  0.00%|      tuple `(kH, kW)`
   367|         0|            0|            0|  0.00%|    stride: stride of the pooling operation. Can be a single number or a
   368|         0|            0|            0|  0.00%|      tuple `(sH, sW)`. Default: :attr:`kernel_size`
   369|         0|            0|            0|  0.00%|    padding: implicit zero paddings on both sides of the input. Can be a
   370|         0|            0|            0|  0.00%|      single number or a tuple `(padH, padW)`. Default: 0
   371|         0|            0|            0|  0.00%|    ceil_mode: when True, will use `ceil` instead of `floor` in the formula
   372|         0|            0|            0|  0.00%|        to compute the output shape. Default: ``False``
   373|         0|            0|            0|  0.00%|    count_include_pad: when True, will include the zero-padding in the
   374|         0|            0|            0|  0.00%|        averaging calculation. Default: ``True``
   375|         0|            0|            0|  0.00%|    divisor_override: if specified, it will be used as divisor, otherwise
   376|         0|            0|            0|  0.00%|         size of the pooling region will be used. Default: None
   377|         0|            0|            0|  0.00%|""",
   378|         0|            0|            0|  0.00%|)
   379|         0|            0|            0|  0.00%|
   380|         0|            0|            0|  0.00%|avg_pool3d = _add_docstr(
   381|         0|            0|            0|  0.00%|    torch._C._nn.avg_pool3d,
   382|         0|            0|            0|  0.00%|    r"""
   383|         0|            0|            0|  0.00%|avg_pool3d(input, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True, divisor_override=None) -> Tensor
   384|         0|            0|            0|  0.00%|
   385|         0|            0|            0|  0.00%|Applies 3D average-pooling operation in :math:`kT \times kH \times kW` regions by step
   386|         0|            0|            0|  0.00%|size :math:`sT \times sH \times sW` steps. The number of output features is equal to
   387|         0|            0|            0|  0.00%|:math:`\lfloor\frac{\text{input planes}}{sT}\rfloor`.
   388|         0|            0|            0|  0.00%|
   389|         0|            0|            0|  0.00%|See :class:`~torch.nn.AvgPool3d` for details and output shape.
   390|         0|            0|            0|  0.00%|
   391|         0|            0|            0|  0.00%|Args:
   392|         0|            0|            0|  0.00%|    input: input tensor :math:`(\text{minibatch} , \text{in\_channels} , iT \times iH , iW)`
   393|         0|            0|            0|  0.00%|    kernel_size: size of the pooling region. Can be a single number or a
   394|         0|            0|            0|  0.00%|      tuple `(kT, kH, kW)`
   395|         0|            0|            0|  0.00%|    stride: stride of the pooling operation. Can be a single number or a
   396|         0|            0|            0|  0.00%|      tuple `(sT, sH, sW)`. Default: :attr:`kernel_size`
   397|         0|            0|            0|  0.00%|    padding: implicit zero paddings on both sides of the input. Can be a
   398|         0|            0|            0|  0.00%|      single number or a tuple `(padT, padH, padW)`, Default: 0
   399|         0|            0|            0|  0.00%|    ceil_mode: when True, will use `ceil` instead of `floor` in the formula
   400|         0|            0|            0|  0.00%|        to compute the output shape
   401|         0|            0|            0|  0.00%|    count_include_pad: when True, will include the zero-padding in the
   402|         0|            0|            0|  0.00%|        averaging calculation
   403|         0|            0|            0|  0.00%|    divisor_override: if specified, it will be used as divisor, otherwise
   404|         0|            0|            0|  0.00%|        size of the pooling region will be used. Default: None
   405|         0|            0|            0|  0.00%|""",
   406|         0|            0|            0|  0.00%|)
   407|         0|            0|            0|  0.00%|
   408|         0|            0|            0|  0.00%|
   409|         0|            0|            0|  0.00%|def fractional_max_pool2d_with_indices(
   410|         0|            0|            0|  0.00%|    input: Tensor, kernel_size: BroadcastingList2[int],
   411|         0|            0|            0|  0.00%|    output_size: Optional[BroadcastingList2[int]] = None,
   412|         0|            0|            0|  0.00%|    output_ratio: Optional[BroadcastingList2[float]] = None,
   413|         0|            0|            0|  0.00%|    return_indices: bool = False,
   414|         0|            0|            0|  0.00%|    _random_samples: Optional[Tensor] = None
   415|         0|            0|            0|  0.00%|) -> Tuple[Tensor, Tensor]:
   416|         0|            0|            0|  0.00%|    r"""Applies 2D fractional max pooling over an input signal composed of several input planes.
   417|         0|            0|            0|  0.00%|
   418|         0|            0|            0|  0.00%|    Fractional MaxPooling is described in detail in the paper `Fractional MaxPooling`_ by Ben Graham
   419|         0|            0|            0|  0.00%|
   420|         0|            0|            0|  0.00%|    The max-pooling operation is applied in :math:`kH \times kW` regions by a stochastic
   421|         0|            0|            0|  0.00%|    step size determined by the target output size.
   422|         0|            0|            0|  0.00%|    The number of output features is equal to the number of input planes.
   423|         0|            0|            0|  0.00%|
   424|         0|            0|            0|  0.00%|    Args:
   425|         0|            0|            0|  0.00%|        kernel_size: the size of the window to take a max over.
   426|         0|            0|            0|  0.00%|                     Can be a single number :math:`k` (for a square kernel of :math:`k \times k`)
   427|         0|            0|            0|  0.00%|                     or a tuple `(kH, kW)`
   428|         0|            0|            0|  0.00%|        output_size: the target output size of the image of the form :math:`oH \times oW`.
   429|         0|            0|            0|  0.00%|                     Can be a tuple `(oH, oW)` or a single number :math:`oH` for a square image :math:`oH \times oH`
   430|         0|            0|            0|  0.00%|        output_ratio: If one wants to have an output size as a ratio of the input size, this option can be given.
   431|         0|            0|            0|  0.00%|                      This has to be a number or tuple in the range (0, 1)
   432|         0|            0|            0|  0.00%|        return_indices: if ``True``, will return the indices along with the outputs.
   433|         0|            0|            0|  0.00%|                        Useful to pass to :func:`~torch.nn.functional.max_unpool2d`.
   434|         0|            0|            0|  0.00%|
   435|         0|            0|            0|  0.00%|    Examples::
   436|         0|            0|            0|  0.00%|        >>> input = torch.randn(20, 16, 50, 32)
   437|         0|            0|            0|  0.00%|        >>> # pool of square window of size=3, and target output size 13x12
   438|         0|            0|            0|  0.00%|        >>> F.fractional_max_pool2d(input, 3, output_size=(13, 12))
   439|         0|            0|            0|  0.00%|        >>> # pool of square window and target output size being half of input image size
   440|         0|            0|            0|  0.00%|        >>> F.fractional_max_pool2d(input, 3, output_ratio=(0.5, 0.5))
   441|         0|            0|            0|  0.00%|
   442|         0|            0|            0|  0.00%|    .. _Fractional MaxPooling:
   443|         0|            0|            0|  0.00%|        http://arxiv.org/abs/1412.6071
   444|         0|            0|            0|  0.00%|    """
   445|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):
   446|         0|            0|            0|  0.00%|        return handle_torch_function(
   447|         0|            0|            0|  0.00%|            fractional_max_pool2d_with_indices,
   448|         0|            0|            0|  0.00%|            (input,),
   449|         0|            0|            0|  0.00%|            input,
   450|         0|            0|            0|  0.00%|            kernel_size,
   451|         0|            0|            0|  0.00%|            output_size=output_size,
   452|         0|            0|            0|  0.00%|            output_ratio=output_ratio,
   453|         0|            0|            0|  0.00%|            return_indices=return_indices,
   454|         0|            0|            0|  0.00%|            _random_samples=_random_samples,
   455|         0|            0|            0|  0.00%|        )
   456|         0|            0|            0|  0.00%|    if output_size is None and output_ratio is None:
   457|         0|            0|            0|  0.00%|        raise ValueError("fractional_max_pool2d requires specifying either " "an output_size or an output_ratio")
   458|         0|            0|            0|  0.00%|    if output_size is None:
   459|         0|            0|            0|  0.00%|        assert output_ratio is not None
   460|         0|            0|            0|  0.00%|        _output_ratio = _pair(output_ratio)
   461|         0|            0|            0|  0.00%|        output_size = [int(input.size(2) * _output_ratio[0]), int(input.size(3) * _output_ratio[1])]
   462|         0|            0|            0|  0.00%|
   463|         0|            0|            0|  0.00%|    if _random_samples is None:
   464|         0|            0|            0|  0.00%|        _random_samples = torch.rand(input.size(0), input.size(1), 2, dtype=input.dtype, device=input.device)
   465|         0|            0|            0|  0.00%|    return torch._C._nn.fractional_max_pool2d(input, kernel_size, output_size, _random_samples)
   466|         0|            0|            0|  0.00%|
   467|         0|            0|            0|  0.00%|
   468|         0|            0|            0|  0.00%|def _fractional_max_pool2d(
   469|         0|            0|            0|  0.00%|    input: Tensor, kernel_size: BroadcastingList2[int],
   470|         0|            0|            0|  0.00%|    output_size: Optional[BroadcastingList2[int]] = None,
   471|         0|            0|            0|  0.00%|    output_ratio: Optional[BroadcastingList2[float]] = None,
   472|         0|            0|            0|  0.00%|    return_indices: bool = False,
   473|         0|            0|            0|  0.00%|    _random_samples: Optional[Tensor] = None
   474|         0|            0|            0|  0.00%|) -> Tensor:
   475|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):
   476|         0|            0|            0|  0.00%|        return handle_torch_function(
   477|         0|            0|            0|  0.00%|            fractional_max_pool2d,
   478|         0|            0|            0|  0.00%|            (input,),
   479|         0|            0|            0|  0.00%|            input,
   480|         0|            0|            0|  0.00%|            kernel_size,
   481|         0|            0|            0|  0.00%|            output_size=output_size,
   482|         0|            0|            0|  0.00%|            output_ratio=output_ratio,
   483|         0|            0|            0|  0.00%|            return_indices=return_indices,
   484|         0|            0|            0|  0.00%|            _random_samples=_random_samples,
   485|         0|            0|            0|  0.00%|        )
   486|         0|            0|            0|  0.00%|    return fractional_max_pool2d_with_indices(
   487|         0|            0|            0|  0.00%|        input, kernel_size, output_size, output_ratio, return_indices, _random_samples
   488|         0|            0|            0|  0.00%|    )[0]
   489|         0|            0|            0|  0.00%|
   490|         0|            0|            0|  0.00%|
   491|         0|            0|            0|  0.00%|fractional_max_pool2d = boolean_dispatch(
   492|         0|            0|            0|  0.00%|    arg_name="return_indices",
   493|         0|            0|            0|  0.00%|    arg_index=4,
   494|         0|            0|            0|  0.00%|    default=False,
   495|         0|            0|            0|  0.00%|    if_true=fractional_max_pool2d_with_indices,
   496|         0|            0|            0|  0.00%|    if_false=_fractional_max_pool2d,
   497|         0|            0|            0|  0.00%|    module_name=__name__,
   498|         0|            0|            0|  0.00%|    func_name="fractional_max_pool2d",
   499|         0|            0|            0|  0.00%|)
   500|         0|            0|            0|  0.00%|
   501|         0|            0|            0|  0.00%|
   502|         0|            0|            0|  0.00%|def fractional_max_pool3d_with_indices(
   503|         0|            0|            0|  0.00%|    input: Tensor, kernel_size: BroadcastingList3[int],
   504|         0|            0|            0|  0.00%|    output_size: Optional[BroadcastingList3[int]] = None,
   505|         0|            0|            0|  0.00%|    output_ratio: Optional[BroadcastingList3[float]] = None,
   506|         0|            0|            0|  0.00%|    return_indices: bool = False,
   507|         0|            0|            0|  0.00%|    _random_samples: Optional[Tensor] = None
   508|         0|            0|            0|  0.00%|) -> Tuple[Tensor, Tensor]:
   509|         0|            0|            0|  0.00%|    r"""Applies 3D fractional max pooling over an input signal composed of several input planes.
   510|         0|            0|            0|  0.00%|
   511|         0|            0|            0|  0.00%|    Fractional MaxPooling is described in detail in the paper `Fractional MaxPooling`_ by Ben Graham
   512|         0|            0|            0|  0.00%|
   513|         0|            0|            0|  0.00%|    The max-pooling operation is applied in :math:`kT \times kH \times kW` regions by a stochastic
   514|         0|            0|            0|  0.00%|    step size determined by the target output size.
   515|         0|            0|            0|  0.00%|    The number of output features is equal to the number of input planes.
   516|         0|            0|            0|  0.00%|
   517|         0|            0|            0|  0.00%|    Args:
   518|         0|            0|            0|  0.00%|        kernel_size: the size of the window to take a max over.
   519|         0|            0|            0|  0.00%|                     Can be a single number :math:`k` (for a square kernel of :math:`k \times k \times k`)
   520|         0|            0|            0|  0.00%|                     or a tuple `(kT, kH, kW)`
   521|         0|            0|            0|  0.00%|        output_size: the target output size of the form :math:`oT \times oH \times oW`.
   522|         0|            0|            0|  0.00%|                     Can be a tuple `(oT, oH, oW)` or a single number :math:`oH` for a cubic output
   523|         0|            0|            0|  0.00%|                     :math:`oH \times oH \times oH`
   524|         0|            0|            0|  0.00%|        output_ratio: If one wants to have an output size as a ratio of the input size, this option can be given.
   525|         0|            0|            0|  0.00%|                      This has to be a number or tuple in the range (0, 1)
   526|         0|            0|            0|  0.00%|        return_indices: if ``True``, will return the indices along with the outputs.
   527|         0|            0|            0|  0.00%|                        Useful to pass to :func:`~torch.nn.functional.max_unpool3d`.
   528|         0|            0|            0|  0.00%|
   529|         0|            0|            0|  0.00%|    Examples::
   530|         0|            0|            0|  0.00%|        >>> input = torch.randn(20, 16, 50, 32, 16)
   531|         0|            0|            0|  0.00%|        >>> # pool of cubic window of size=3, and target output size 13x12x11
   532|         0|            0|            0|  0.00%|        >>> F.fractional_max_pool3d(input, 3, output_size=(13, 12, 11))
   533|         0|            0|            0|  0.00%|        >>> # pool of cubic window and target output size being half of input size
   534|         0|            0|            0|  0.00%|        >>> F.fractional_max_pool3d(input, 3, output_ratio=(0.5, 0.5, 0.5))
   535|         0|            0|            0|  0.00%|
   536|         0|            0|            0|  0.00%|    .. _Fractional MaxPooling:
   537|         0|            0|            0|  0.00%|        http://arxiv.org/abs/1412.6071
   538|         0|            0|            0|  0.00%|    """
   539|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):
   540|         0|            0|            0|  0.00%|        return handle_torch_function(
   541|         0|            0|            0|  0.00%|            fractional_max_pool3d_with_indices,
   542|         0|            0|            0|  0.00%|            (input,),
   543|         0|            0|            0|  0.00%|            input,
   544|         0|            0|            0|  0.00%|            kernel_size,
   545|         0|            0|            0|  0.00%|            output_size=output_size,
   546|         0|            0|            0|  0.00%|            output_ratio=output_ratio,
   547|         0|            0|            0|  0.00%|            return_indices=return_indices,
   548|         0|            0|            0|  0.00%|            _random_samples=_random_samples,
   549|         0|            0|            0|  0.00%|        )
   550|         0|            0|            0|  0.00%|    if output_size is None and output_ratio is None:
   551|         0|            0|            0|  0.00%|        raise ValueError("fractional_max_pool3d requires specifying either " "an output_size or an output_ratio")
   552|         0|            0|            0|  0.00%|    if output_size is None:
   553|         0|            0|            0|  0.00%|        assert output_ratio is not None
   554|         0|            0|            0|  0.00%|        _output_ratio = _triple(output_ratio)
   555|         0|            0|            0|  0.00%|        output_size = [
   556|         0|            0|            0|  0.00%|            int(input.size(2) * _output_ratio[0]),
   557|         0|            0|            0|  0.00%|            int(input.size(3) * _output_ratio[1]),
   558|         0|            0|            0|  0.00%|            int(input.size(4) * _output_ratio[2]),
   559|         0|            0|            0|  0.00%|        ]
   560|         0|            0|            0|  0.00%|
   561|         0|            0|            0|  0.00%|    if _random_samples is None:
   562|         0|            0|            0|  0.00%|        _random_samples = torch.rand(input.size(0), input.size(1), 3, dtype=input.dtype, device=input.device)
   563|         0|            0|            0|  0.00%|    return torch._C._nn.fractional_max_pool3d(input, kernel_size, output_size, _random_samples)
   564|         0|            0|            0|  0.00%|
   565|         0|            0|            0|  0.00%|
   566|         0|            0|            0|  0.00%|def _fractional_max_pool3d(
   567|         0|            0|            0|  0.00%|    input: Tensor, kernel_size: BroadcastingList3[int],
   568|         0|            0|            0|  0.00%|    output_size: Optional[BroadcastingList3[int]] = None,
   569|         0|            0|            0|  0.00%|    output_ratio: Optional[BroadcastingList3[float]] = None,
   570|         0|            0|            0|  0.00%|    return_indices: bool = False,
   571|         0|            0|            0|  0.00%|    _random_samples: Optional[Tensor] = None
   572|         0|            0|            0|  0.00%|) -> Tensor:
   573|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):
   574|         0|            0|            0|  0.00%|        return handle_torch_function(
   575|         0|            0|            0|  0.00%|            fractional_max_pool3d,
   576|         0|            0|            0|  0.00%|            (input,),
   577|         0|            0|            0|  0.00%|            input,
   578|         0|            0|            0|  0.00%|            kernel_size,
   579|         0|            0|            0|  0.00%|            output_size=output_size,
   580|         0|            0|            0|  0.00%|            output_ratio=output_ratio,
   581|         0|            0|            0|  0.00%|            return_indices=return_indices,
   582|         0|            0|            0|  0.00%|            _random_samples=_random_samples,
   583|         0|            0|            0|  0.00%|        )
   584|         0|            0|            0|  0.00%|    return fractional_max_pool3d_with_indices(
   585|         0|            0|            0|  0.00%|        input, kernel_size, output_size, output_ratio, return_indices, _random_samples
   586|         0|            0|            0|  0.00%|    )[0]
   587|         0|            0|            0|  0.00%|
   588|         0|            0|            0|  0.00%|
   589|         0|            0|            0|  0.00%|fractional_max_pool3d = boolean_dispatch(
   590|         0|            0|            0|  0.00%|    arg_name="return_indices",
   591|         0|            0|            0|  0.00%|    arg_index=4,
   592|         0|            0|            0|  0.00%|    default=False,
   593|         0|            0|            0|  0.00%|    if_true=fractional_max_pool3d_with_indices,
   594|         0|            0|            0|  0.00%|    if_false=_fractional_max_pool3d,
   595|         0|            0|            0|  0.00%|    module_name=__name__,
   596|         0|            0|            0|  0.00%|    func_name="fractional_max_pool3d",
   597|         0|            0|            0|  0.00%|)
   598|         0|            0|            0|  0.00%|
   599|         0|            0|            0|  0.00%|
   600|         0|            0|            0|  0.00%|def max_pool1d_with_indices(
   601|         0|            0|            0|  0.00%|    input: Tensor, kernel_size: BroadcastingList1[int],
   602|         0|            0|            0|  0.00%|    stride: Optional[BroadcastingList1[int]] = None,
   603|         0|            0|            0|  0.00%|    padding: BroadcastingList1[int] = 0,
   604|         0|            0|            0|  0.00%|    dilation: BroadcastingList1[int] = 1,
   605|         0|            0|            0|  0.00%|    ceil_mode: bool = False,
   606|         0|            0|            0|  0.00%|    return_indices: bool = False
   607|         0|            0|            0|  0.00%|) -> Tuple[Tensor, Tensor]:
   608|         0|            0|            0|  0.00%|    r"""Applies a 1D max pooling over an input signal composed of several input
   609|         0|            0|            0|  0.00%|    planes.
   610|         0|            0|            0|  0.00%|
   611|         0|            0|            0|  0.00%|    See :class:`~torch.nn.MaxPool1d` for details.
   612|         0|            0|            0|  0.00%|    """
   613|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):
   614|         0|            0|            0|  0.00%|        return handle_torch_function(
   615|         0|            0|            0|  0.00%|            max_pool1d_with_indices,
   616|         0|            0|            0|  0.00%|            (input,),
   617|         0|            0|            0|  0.00%|            input,
   618|         0|            0|            0|  0.00%|            kernel_size,
   619|         0|            0|            0|  0.00%|            stride=stride,
   620|         0|            0|            0|  0.00%|            padding=padding,
   621|         0|            0|            0|  0.00%|            dilation=dilation,
   622|         0|            0|            0|  0.00%|            ceil_mode=ceil_mode,
   623|         0|            0|            0|  0.00%|            return_indices=return_indices,
   624|         0|            0|            0|  0.00%|        )
   625|         0|            0|            0|  0.00%|    if stride is None:
   626|         0|            0|            0|  0.00%|        stride = torch.jit.annotate(List[int], [])
   627|         0|            0|            0|  0.00%|    return torch.max_pool1d_with_indices(input, kernel_size, stride, padding, dilation, ceil_mode)
   628|         0|            0|            0|  0.00%|
   629|         0|            0|            0|  0.00%|
   630|         0|            0|            0|  0.00%|def _max_pool1d(
   631|         0|            0|            0|  0.00%|    input: Tensor, kernel_size: BroadcastingList1[int],
   632|         0|            0|            0|  0.00%|    stride: Optional[BroadcastingList1[int]] = None,
   633|         0|            0|            0|  0.00%|    padding: BroadcastingList1[int] = 0,
   634|         0|            0|            0|  0.00%|    dilation: BroadcastingList1[int] = 1,
   635|         0|            0|            0|  0.00%|    ceil_mode: bool = False,
   636|         0|            0|            0|  0.00%|    return_indices: bool = False
   637|         0|            0|            0|  0.00%|) -> Tensor:
   638|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):
   639|         0|            0|            0|  0.00%|        return handle_torch_function(
   640|         0|            0|            0|  0.00%|            max_pool1d,
   641|         0|            0|            0|  0.00%|            (input,),
   642|         0|            0|            0|  0.00%|            input,
   643|         0|            0|            0|  0.00%|            kernel_size,
   644|         0|            0|            0|  0.00%|            stride=stride,
   645|         0|            0|            0|  0.00%|            padding=padding,
   646|         0|            0|            0|  0.00%|            dilation=dilation,
   647|         0|            0|            0|  0.00%|            ceil_mode=ceil_mode,
   648|         0|            0|            0|  0.00%|            return_indices=return_indices,
   649|         0|            0|            0|  0.00%|        )
   650|         0|            0|            0|  0.00%|    if stride is None:
   651|         0|            0|            0|  0.00%|        stride = torch.jit.annotate(List[int], [])
   652|         0|            0|            0|  0.00%|    return torch.max_pool1d(input, kernel_size, stride, padding, dilation, ceil_mode)
   653|         0|            0|            0|  0.00%|
   654|         0|            0|            0|  0.00%|
   655|         0|            0|            0|  0.00%|max_pool1d = boolean_dispatch(
   656|         0|            0|            0|  0.00%|    arg_name="return_indices",
   657|         0|            0|            0|  0.00%|    arg_index=6,
   658|         0|            0|            0|  0.00%|    default=False,
   659|         0|            0|            0|  0.00%|    if_true=max_pool1d_with_indices,
   660|         0|            0|            0|  0.00%|    if_false=_max_pool1d,
   661|         0|            0|            0|  0.00%|    module_name=__name__,
   662|         0|            0|            0|  0.00%|    func_name="max_pool1d",
   663|         0|            0|            0|  0.00%|)
   664|         0|            0|            0|  0.00%|
   665|         0|            0|            0|  0.00%|
   666|         0|            0|            0|  0.00%|def max_pool2d_with_indices(
   667|         0|            0|            0|  0.00%|    input: Tensor, kernel_size: BroadcastingList2[int],
   668|         0|            0|            0|  0.00%|    stride: Optional[BroadcastingList2[int]] = None,
   669|         0|            0|            0|  0.00%|    padding: BroadcastingList2[int] = 0,
   670|         0|            0|            0|  0.00%|    dilation: BroadcastingList2[int] = 1,
   671|         0|            0|            0|  0.00%|    ceil_mode: bool = False,
   672|         0|            0|            0|  0.00%|    return_indices: bool = False
   673|         0|            0|            0|  0.00%|) -> Tuple[Tensor, Tensor]:
   674|         0|            0|            0|  0.00%|    r"""Applies a 2D max pooling over an input signal composed of several input
   675|         0|            0|            0|  0.00%|    planes.
   676|         0|            0|            0|  0.00%|
   677|         0|            0|            0|  0.00%|    See :class:`~torch.nn.MaxPool2d` for details.
   678|         0|            0|            0|  0.00%|    """
   679|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):
   680|         0|            0|            0|  0.00%|        return handle_torch_function(
   681|         0|            0|            0|  0.00%|            max_pool2d_with_indices,
   682|         0|            0|            0|  0.00%|            (input,),
   683|         0|            0|            0|  0.00%|            input,
   684|         0|            0|            0|  0.00%|            kernel_size,
   685|         0|            0|            0|  0.00%|            stride=stride,
   686|         0|            0|            0|  0.00%|            padding=padding,
   687|         0|            0|            0|  0.00%|            dilation=dilation,
   688|         0|            0|            0|  0.00%|            ceil_mode=ceil_mode,
   689|         0|            0|            0|  0.00%|            return_indices=return_indices,
   690|         0|            0|            0|  0.00%|        )
   691|         0|            0|            0|  0.00%|    if stride is None:
   692|         0|            0|            0|  0.00%|        stride = torch.jit.annotate(List[int], [])
   693|         0|            0|            0|  0.00%|    return torch._C._nn.max_pool2d_with_indices(input, kernel_size, stride, padding, dilation, ceil_mode)
   694|         0|            0|            0|  0.00%|
   695|         0|            0|            0|  0.00%|
   696|       100|  0.000670433|  6.70433e-06|  0.00%|def _max_pool2d(
   697|         0|            0|            0|  0.00%|    input: Tensor, kernel_size: BroadcastingList2[int],
   698|         0|            0|            0|  0.00%|    stride: Optional[BroadcastingList2[int]] = None,
   699|         0|            0|            0|  0.00%|    padding: BroadcastingList2[int] = 0,
   700|         0|            0|            0|  0.00%|    dilation: BroadcastingList2[int] = 1,
   701|         0|            0|            0|  0.00%|    ceil_mode: bool = False,
   702|         0|            0|            0|  0.00%|    return_indices: bool = False
   703|         0|            0|            0|  0.00%|) -> Tensor:
   704|       100|  0.000732899|  7.32899e-06|  0.00%|    if has_torch_function_unary(input):
   705|         0|            0|            0|  0.00%|        return handle_torch_function(
   706|         0|            0|            0|  0.00%|            max_pool2d,
   707|         0|            0|            0|  0.00%|            (input,),
   708|         0|            0|            0|  0.00%|            input,
   709|         0|            0|            0|  0.00%|            kernel_size,
   710|         0|            0|            0|  0.00%|            stride=stride,
   711|         0|            0|            0|  0.00%|            padding=padding,
   712|         0|            0|            0|  0.00%|            dilation=dilation,
   713|         0|            0|            0|  0.00%|            ceil_mode=ceil_mode,
   714|         0|            0|            0|  0.00%|            return_indices=return_indices,
   715|         0|            0|            0|  0.00%|        )
   716|       100|  0.000475645|  4.75645e-06|  0.00%|    if stride is None:
   717|         0|            0|            0|  0.00%|        stride = torch.jit.annotate(List[int], [])
   718|       100|     0.692982|   0.00692982|  1.29%|    return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
(call)|         1|  0.000146627|  0.000146627|  0.00%|# /opt/conda/lib/python3.8/warnings.py:403 __init__
(call)|         1|   0.00438714|   0.00438714|  0.01%|# /opt/conda/lib/python3.8/warnings.py:96 _showwarnmsg
   719|         0|            0|            0|  0.00%|
   720|         0|            0|            0|  0.00%|
   721|         0|            0|            0|  0.00%|max_pool2d = boolean_dispatch(
   722|         0|            0|            0|  0.00%|    arg_name="return_indices",
   723|         0|            0|            0|  0.00%|    arg_index=6,
   724|         0|            0|            0|  0.00%|    default=False,
   725|         0|            0|            0|  0.00%|    if_true=max_pool2d_with_indices,
   726|         0|            0|            0|  0.00%|    if_false=_max_pool2d,
   727|         0|            0|            0|  0.00%|    module_name=__name__,
   728|         0|            0|            0|  0.00%|    func_name="max_pool2d",
   729|         0|            0|            0|  0.00%|)
   730|         0|            0|            0|  0.00%|
   731|         0|            0|            0|  0.00%|
   732|         0|            0|            0|  0.00%|def max_pool3d_with_indices(
   733|         0|            0|            0|  0.00%|    input: Tensor, kernel_size: BroadcastingList3[int],
   734|         0|            0|            0|  0.00%|    stride: Optional[BroadcastingList3[int]] = None,
   735|         0|            0|            0|  0.00%|    padding: BroadcastingList3[int] = 0,
   736|         0|            0|            0|  0.00%|    dilation: BroadcastingList3[int] = 1,
   737|         0|            0|            0|  0.00%|    ceil_mode: bool = False,
   738|         0|            0|            0|  0.00%|    return_indices: bool = False
   739|         0|            0|            0|  0.00%|) -> Tuple[Tensor, Tensor]:
   740|         0|            0|            0|  0.00%|    r"""Applies a 3D max pooling over an input signal composed of several input
   741|         0|            0|            0|  0.00%|    planes.
   742|         0|            0|            0|  0.00%|
   743|         0|            0|            0|  0.00%|    See :class:`~torch.nn.MaxPool3d` for details.
   744|         0|            0|            0|  0.00%|    """
   745|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):
   746|         0|            0|            0|  0.00%|        return handle_torch_function(
   747|         0|            0|            0|  0.00%|            max_pool3d_with_indices,
   748|         0|            0|            0|  0.00%|            (input,),
   749|         0|            0|            0|  0.00%|            input,
   750|         0|            0|            0|  0.00%|            kernel_size,
   751|         0|            0|            0|  0.00%|            stride=stride,
   752|         0|            0|            0|  0.00%|            padding=padding,
   753|         0|            0|            0|  0.00%|            dilation=dilation,
   754|         0|            0|            0|  0.00%|            ceil_mode=ceil_mode,
   755|         0|            0|            0|  0.00%|            return_indices=return_indices,
   756|         0|            0|            0|  0.00%|        )
   757|         0|            0|            0|  0.00%|    if stride is None:
   758|         0|            0|            0|  0.00%|        stride = torch.jit.annotate(List[int], [])
   759|         0|            0|            0|  0.00%|    return torch._C._nn.max_pool3d_with_indices(input, kernel_size, stride, padding, dilation, ceil_mode)
   760|         0|            0|            0|  0.00%|
   761|         0|            0|            0|  0.00%|
   762|         0|            0|            0|  0.00%|def _max_pool3d(
   763|         0|            0|            0|  0.00%|    input: Tensor, kernel_size: BroadcastingList3[int],
   764|         0|            0|            0|  0.00%|    stride: Optional[BroadcastingList3[int]] = None,
   765|         0|            0|            0|  0.00%|    padding: BroadcastingList3[int] = 0,
   766|         0|            0|            0|  0.00%|    dilation: BroadcastingList3[int] = 1,
   767|         0|            0|            0|  0.00%|    ceil_mode: bool = False,
   768|         0|            0|            0|  0.00%|    return_indices: bool = False
   769|         0|            0|            0|  0.00%|) -> Tensor:
   770|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):
   771|         0|            0|            0|  0.00%|        return handle_torch_function(
   772|         0|            0|            0|  0.00%|            max_pool3d,
   773|         0|            0|            0|  0.00%|            (input,),
   774|         0|            0|            0|  0.00%|            input,
   775|         0|            0|            0|  0.00%|            kernel_size,
   776|         0|            0|            0|  0.00%|            stride=stride,
   777|         0|            0|            0|  0.00%|            padding=padding,
   778|         0|            0|            0|  0.00%|            dilation=dilation,
   779|         0|            0|            0|  0.00%|            ceil_mode=ceil_mode,
   780|         0|            0|            0|  0.00%|            return_indices=return_indices,
   781|         0|            0|            0|  0.00%|        )
   782|         0|            0|            0|  0.00%|    if stride is None:
   783|         0|            0|            0|  0.00%|        stride = torch.jit.annotate(List[int], [])
   784|         0|            0|            0|  0.00%|    return torch.max_pool3d(input, kernel_size, stride, padding, dilation, ceil_mode)
   785|         0|            0|            0|  0.00%|
   786|         0|            0|            0|  0.00%|
   787|         0|            0|            0|  0.00%|max_pool3d = boolean_dispatch(
   788|         0|            0|            0|  0.00%|    arg_name="return_indices",
   789|         0|            0|            0|  0.00%|    arg_index=6,
   790|         0|            0|            0|  0.00%|    default=False,
   791|         0|            0|            0|  0.00%|    if_true=max_pool3d_with_indices,
   792|         0|            0|            0|  0.00%|    if_false=_max_pool3d,
   793|         0|            0|            0|  0.00%|    module_name=__name__,
   794|         0|            0|            0|  0.00%|    func_name="max_pool3d",
   795|         0|            0|            0|  0.00%|)
   796|         0|            0|            0|  0.00%|
   797|         0|            0|            0|  0.00%|
   798|         0|            0|            0|  0.00%|def _unpool_output_size(
   799|         0|            0|            0|  0.00%|    input: Tensor, kernel_size: List[int], stride: List[int], padding: List[int], output_size: Optional[List[int]]
   800|         0|            0|            0|  0.00%|) -> List[int]:
   801|         0|            0|            0|  0.00%|    input_size = input.size()
   802|         0|            0|            0|  0.00%|    default_size = torch.jit.annotate(List[int], [])
   803|         0|            0|            0|  0.00%|    for d in range(len(kernel_size)):
   804|         0|            0|            0|  0.00%|        default_size.append((input_size[d + 2] - 1) * stride[d] + kernel_size[d] - 2 * padding[d])
   805|         0|            0|            0|  0.00%|    if output_size is None:
   806|         0|            0|            0|  0.00%|        ret = default_size
   807|         0|            0|            0|  0.00%|    else:
   808|         0|            0|            0|  0.00%|        if len(output_size) == len(kernel_size) + 2:
   809|         0|            0|            0|  0.00%|            output_size = output_size[2:]
   810|         0|            0|            0|  0.00%|        if len(output_size) != len(kernel_size):
   811|         0|            0|            0|  0.00%|            raise ValueError(
   812|         0|            0|            0|  0.00%|                "output_size should be a sequence containing "
   813|         0|            0|            0|  0.00%|                "{} or {} elements, but it has a length of '{}'".format(
   814|         0|            0|            0|  0.00%|                    len(kernel_size), len(kernel_size) + 2, len(output_size)
   815|         0|            0|            0|  0.00%|                )
   816|         0|            0|            0|  0.00%|            )
   817|         0|            0|            0|  0.00%|        for d in range(len(kernel_size)):
   818|         0|            0|            0|  0.00%|            min_size = default_size[d] - stride[d]
   819|         0|            0|            0|  0.00%|            max_size = default_size[d] + stride[d]
   820|         0|            0|            0|  0.00%|            if not (min_size < output_size[d] < max_size):
   821|         0|            0|            0|  0.00%|                raise ValueError(
   822|         0|            0|            0|  0.00%|                    'invalid output_size "{}" (dim {} must be between {} and {})'.format(
   823|         0|            0|            0|  0.00%|                        output_size, d, min_size, max_size
   824|         0|            0|            0|  0.00%|                    )
   825|         0|            0|            0|  0.00%|                )
   826|         0|            0|            0|  0.00%|
   827|         0|            0|            0|  0.00%|        ret = output_size
   828|         0|            0|            0|  0.00%|    return ret
   829|         0|            0|            0|  0.00%|
   830|         0|            0|            0|  0.00%|
   831|         0|            0|            0|  0.00%|def max_unpool1d(
   832|         0|            0|            0|  0.00%|    input: Tensor, indices: Tensor,
   833|         0|            0|            0|  0.00%|    kernel_size: BroadcastingList1[int],
   834|         0|            0|            0|  0.00%|    stride: Optional[BroadcastingList1[int]] = None,
   835|         0|            0|            0|  0.00%|    padding: BroadcastingList1[int] = 0,
   836|         0|            0|            0|  0.00%|    output_size: Optional[BroadcastingList1[int]] = None
   837|         0|            0|            0|  0.00%|) -> Tensor:
   838|         0|            0|            0|  0.00%|    r"""Computes a partial inverse of :class:`MaxPool1d`.
   839|         0|            0|            0|  0.00%|
   840|         0|            0|            0|  0.00%|    See :class:`~torch.nn.MaxUnpool1d` for details.
   841|         0|            0|            0|  0.00%|    """
   842|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):
   843|         0|            0|            0|  0.00%|        return handle_torch_function(
   844|         0|            0|            0|  0.00%|            max_unpool1d,
   845|         0|            0|            0|  0.00%|            (input,),
   846|         0|            0|            0|  0.00%|            input,
   847|         0|            0|            0|  0.00%|            indices,
   848|         0|            0|            0|  0.00%|            kernel_size,
   849|         0|            0|            0|  0.00%|            stride=stride,
   850|         0|            0|            0|  0.00%|            padding=padding,
   851|         0|            0|            0|  0.00%|            output_size=output_size,
   852|         0|            0|            0|  0.00%|        )
   853|         0|            0|            0|  0.00%|    kernel_size = _single(kernel_size)
   854|         0|            0|            0|  0.00%|    if stride is not None:
   855|         0|            0|            0|  0.00%|        _stride = _single(stride)
   856|         0|            0|            0|  0.00%|    else:
   857|         0|            0|            0|  0.00%|        _stride = kernel_size
   858|         0|            0|            0|  0.00%|    padding = _single(padding)
   859|         0|            0|            0|  0.00%|    output_size = _unpool_output_size(input, kernel_size, _stride, padding, output_size)
   860|         0|            0|            0|  0.00%|    if isinstance(output_size, list):
   861|         0|            0|            0|  0.00%|        output_size = output_size + [1]
   862|         0|            0|            0|  0.00%|    else:
   863|         0|            0|            0|  0.00%|        output_size = output_size + (1,)
   864|         0|            0|            0|  0.00%|    return torch._C._nn.max_unpool2d(input.unsqueeze(3), indices.unsqueeze(3), output_size).squeeze(3)
   865|         0|            0|            0|  0.00%|
   866|         0|            0|            0|  0.00%|
   867|         0|            0|            0|  0.00%|def max_unpool2d(
   868|         0|            0|            0|  0.00%|    input: Tensor, indices: Tensor,
   869|         0|            0|            0|  0.00%|    kernel_size: BroadcastingList2[int],
   870|         0|            0|            0|  0.00%|    stride: Optional[BroadcastingList2[int]] = None,
   871|         0|            0|            0|  0.00%|    padding: BroadcastingList2[int] = 0,
   872|         0|            0|            0|  0.00%|    output_size: Optional[BroadcastingList2[int]] = None
   873|         0|            0|            0|  0.00%|) -> Tensor:
   874|         0|            0|            0|  0.00%|    r"""Computes a partial inverse of :class:`MaxPool2d`.
   875|         0|            0|            0|  0.00%|
   876|         0|            0|            0|  0.00%|    See :class:`~torch.nn.MaxUnpool2d` for details.
   877|         0|            0|            0|  0.00%|    """
   878|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):
   879|         0|            0|            0|  0.00%|        return handle_torch_function(
   880|         0|            0|            0|  0.00%|            max_unpool2d,
   881|         0|            0|            0|  0.00%|            (input,),
   882|         0|            0|            0|  0.00%|            input,
   883|         0|            0|            0|  0.00%|            indices,
   884|         0|            0|            0|  0.00%|            kernel_size,
   885|         0|            0|            0|  0.00%|            stride=stride,
   886|         0|            0|            0|  0.00%|            padding=padding,
   887|         0|            0|            0|  0.00%|            output_size=output_size,
   888|         0|            0|            0|  0.00%|        )
   889|         0|            0|            0|  0.00%|    kernel_size = _pair(kernel_size)
   890|         0|            0|            0|  0.00%|    if stride is not None:
   891|         0|            0|            0|  0.00%|        _stride = _pair(stride)
   892|         0|            0|            0|  0.00%|    else:
   893|         0|            0|            0|  0.00%|        _stride = kernel_size
   894|         0|            0|            0|  0.00%|    padding = _pair(padding)
   895|         0|            0|            0|  0.00%|    output_size = _unpool_output_size(input, kernel_size, _stride, padding, output_size)
   896|         0|            0|            0|  0.00%|    return torch._C._nn.max_unpool2d(input, indices, output_size)
   897|         0|            0|            0|  0.00%|
   898|         0|            0|            0|  0.00%|
   899|         0|            0|            0|  0.00%|def max_unpool3d(
   900|         0|            0|            0|  0.00%|    input: Tensor, indices: Tensor,
   901|         0|            0|            0|  0.00%|    kernel_size: BroadcastingList3[int],
   902|         0|            0|            0|  0.00%|    stride: Optional[BroadcastingList3[int]] = None,
   903|         0|            0|            0|  0.00%|    padding: BroadcastingList3[int] = 0,
   904|         0|            0|            0|  0.00%|    output_size: Optional[BroadcastingList3[int]] = None
   905|         0|            0|            0|  0.00%|) -> Tensor:
   906|         0|            0|            0|  0.00%|    r"""Computes a partial inverse of :class:`MaxPool3d`.
   907|         0|            0|            0|  0.00%|
   908|         0|            0|            0|  0.00%|    See :class:`~torch.nn.MaxUnpool3d` for details.
   909|         0|            0|            0|  0.00%|    """
   910|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):
   911|         0|            0|            0|  0.00%|        return handle_torch_function(
   912|         0|            0|            0|  0.00%|            max_unpool3d,
   913|         0|            0|            0|  0.00%|            (input,),
   914|         0|            0|            0|  0.00%|            input,
   915|         0|            0|            0|  0.00%|            indices,
   916|         0|            0|            0|  0.00%|            kernel_size,
   917|         0|            0|            0|  0.00%|            stride=stride,
   918|         0|            0|            0|  0.00%|            padding=padding,
   919|         0|            0|            0|  0.00%|            output_size=output_size,
   920|         0|            0|            0|  0.00%|        )
   921|         0|            0|            0|  0.00%|    kernel_size = _triple(kernel_size)
   922|         0|            0|            0|  0.00%|    if stride is not None:
   923|         0|            0|            0|  0.00%|        _stride = _triple(stride)
   924|         0|            0|            0|  0.00%|    else:
   925|         0|            0|            0|  0.00%|        _stride = kernel_size
   926|         0|            0|            0|  0.00%|    padding = _triple(padding)
   927|         0|            0|            0|  0.00%|    output_size = _unpool_output_size(input, kernel_size, _stride, padding, output_size)
   928|         0|            0|            0|  0.00%|    return torch._C._nn.max_unpool3d(input, indices, output_size, _stride, padding)
   929|         0|            0|            0|  0.00%|
   930|         0|            0|            0|  0.00%|
   931|         0|            0|            0|  0.00%|def lp_pool2d(
   932|         0|            0|            0|  0.00%|    input: Tensor, norm_type: float,
   933|         0|            0|            0|  0.00%|    kernel_size: int,
   934|         0|            0|            0|  0.00%|    stride: Optional[BroadcastingList2[int]] = None,
   935|         0|            0|            0|  0.00%|    ceil_mode: bool = False
   936|         0|            0|            0|  0.00%|) -> Tensor:
   937|         0|            0|            0|  0.00%|    r"""Applies a 2D power-average pooling over an input signal composed of
   938|         0|            0|            0|  0.00%|    several input planes. If the sum of all inputs to the power of `p` is
   939|         0|            0|            0|  0.00%|    zero, the gradient is set to zero as well.
   940|         0|            0|            0|  0.00%|
   941|         0|            0|            0|  0.00%|    See :class:`~torch.nn.LPPool2d` for details.
   942|         0|            0|            0|  0.00%|    """
   943|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):
   944|         0|            0|            0|  0.00%|        return handle_torch_function(
   945|         0|            0|            0|  0.00%|            lp_pool2d, (input,), input, norm_type, kernel_size, stride=stride, ceil_mode=ceil_mode
   946|         0|            0|            0|  0.00%|        )
   947|         0|            0|            0|  0.00%|    kw, kh = utils._pair(kernel_size)
   948|         0|            0|            0|  0.00%|    if stride is not None:
   949|         0|            0|            0|  0.00%|        out = avg_pool2d(input.pow(norm_type), kernel_size, stride, 0, ceil_mode)
   950|         0|            0|            0|  0.00%|    else:
   951|         0|            0|            0|  0.00%|        out = avg_pool2d(input.pow(norm_type), kernel_size, padding=0, ceil_mode=ceil_mode)
   952|         0|            0|            0|  0.00%|
   953|         0|            0|            0|  0.00%|    return (torch.sign(out) * relu(torch.abs(out))).mul(kw * kh).pow(1.0 / norm_type)
   954|         0|            0|            0|  0.00%|
   955|         0|            0|            0|  0.00%|
   956|         0|            0|            0|  0.00%|def lp_pool1d(
   957|         0|            0|            0|  0.00%|    input: Tensor, norm_type: float,
   958|         0|            0|            0|  0.00%|    kernel_size: int,
   959|         0|            0|            0|  0.00%|    stride: Optional[BroadcastingList1[int]] = None,
   960|         0|            0|            0|  0.00%|    ceil_mode: bool = False
   961|         0|            0|            0|  0.00%|) -> Tensor:
   962|         0|            0|            0|  0.00%|    r"""Applies a 1D power-average pooling over an input signal composed of
   963|         0|            0|            0|  0.00%|    several input planes. If the sum of all inputs to the power of `p` is
   964|         0|            0|            0|  0.00%|    zero, the gradient is set to zero as well.
   965|         0|            0|            0|  0.00%|
   966|         0|            0|            0|  0.00%|    See :class:`~torch.nn.LPPool1d` for details.
   967|         0|            0|            0|  0.00%|    """
   968|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):
   969|         0|            0|            0|  0.00%|        return handle_torch_function(
   970|         0|            0|            0|  0.00%|            lp_pool1d, (input,), input, norm_type, kernel_size, stride=stride, ceil_mode=ceil_mode
   971|         0|            0|            0|  0.00%|        )
   972|         0|            0|            0|  0.00%|    if stride is not None:
   973|         0|            0|            0|  0.00%|        out = avg_pool1d(input.pow(norm_type), kernel_size, stride, 0, ceil_mode)
   974|         0|            0|            0|  0.00%|    else:
   975|         0|            0|            0|  0.00%|        out = avg_pool1d(input.pow(norm_type), kernel_size, padding=0, ceil_mode=ceil_mode)
   976|         0|            0|            0|  0.00%|
   977|         0|            0|            0|  0.00%|    return (torch.sign(out) * relu(torch.abs(out))).mul(kernel_size).pow(1.0 / norm_type)
   978|         0|            0|            0|  0.00%|
   979|         0|            0|            0|  0.00%|
   980|         0|            0|            0|  0.00%|def adaptive_max_pool1d_with_indices(
   981|         0|            0|            0|  0.00%|    input: Tensor, output_size: BroadcastingList1[int], return_indices: bool = False
   982|         0|            0|            0|  0.00%|) -> Tuple[Tensor, Tensor]:
   983|         0|            0|            0|  0.00%|    r"""Applies a 1D adaptive max pooling over an input signal composed of
   984|         0|            0|            0|  0.00%|    several input planes.
   985|         0|            0|            0|  0.00%|
   986|         0|            0|            0|  0.00%|    See :class:`~torch.nn.AdaptiveMaxPool1d` for details and output shape.
   987|         0|            0|            0|  0.00%|
   988|         0|            0|            0|  0.00%|    Args:
   989|         0|            0|            0|  0.00%|        output_size: the target output size (single integer)
   990|         0|            0|            0|  0.00%|        return_indices: whether to return pooling indices. Default: ``False``
   991|         0|            0|            0|  0.00%|    """
   992|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):
   993|         0|            0|            0|  0.00%|        return handle_torch_function(
   994|         0|            0|            0|  0.00%|            adaptive_max_pool1d_with_indices, (input,), input, output_size, return_indices=return_indices
   995|         0|            0|            0|  0.00%|        )
   996|         0|            0|            0|  0.00%|    return torch.adaptive_max_pool1d(input, output_size)
   997|         0|            0|            0|  0.00%|
   998|         0|            0|            0|  0.00%|
   999|         0|            0|            0|  0.00%|def _adaptive_max_pool1d(input: Tensor, output_size: BroadcastingList1[int], return_indices: bool = False) -> Tensor:
  1000|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):
  1001|         0|            0|            0|  0.00%|        return handle_torch_function(
  1002|         0|            0|            0|  0.00%|            adaptive_max_pool1d, (input,), input, output_size, return_indices=return_indices
  1003|         0|            0|            0|  0.00%|        )
  1004|         0|            0|            0|  0.00%|    return adaptive_max_pool1d_with_indices(input, output_size)[0]
  1005|         0|            0|            0|  0.00%|
  1006|         0|            0|            0|  0.00%|
  1007|         0|            0|            0|  0.00%|adaptive_max_pool1d = boolean_dispatch(
  1008|         0|            0|            0|  0.00%|    arg_name="return_indices",
  1009|         0|            0|            0|  0.00%|    arg_index=2,
  1010|         0|            0|            0|  0.00%|    default=False,
  1011|         0|            0|            0|  0.00%|    if_true=adaptive_max_pool1d_with_indices,
  1012|         0|            0|            0|  0.00%|    if_false=_adaptive_max_pool1d,
  1013|         0|            0|            0|  0.00%|    module_name=__name__,
  1014|         0|            0|            0|  0.00%|    func_name="adaptive_max_pool1d",
  1015|         0|            0|            0|  0.00%|)
  1016|         0|            0|            0|  0.00%|
  1017|         0|            0|            0|  0.00%|
  1018|         0|            0|            0|  0.00%|def adaptive_max_pool2d_with_indices(
  1019|         0|            0|            0|  0.00%|    input: Tensor, output_size: BroadcastingList2[int],
  1020|         0|            0|            0|  0.00%|    return_indices: bool = False
  1021|         0|            0|            0|  0.00%|) -> Tuple[Tensor, Tensor]:
  1022|         0|            0|            0|  0.00%|    r"""Applies a 2D adaptive max pooling over an input signal composed of
  1023|         0|            0|            0|  0.00%|    several input planes.
  1024|         0|            0|            0|  0.00%|
  1025|         0|            0|            0|  0.00%|    See :class:`~torch.nn.AdaptiveMaxPool2d` for details and output shape.
  1026|         0|            0|            0|  0.00%|
  1027|         0|            0|            0|  0.00%|    Args:
  1028|         0|            0|            0|  0.00%|        output_size: the target output size (single integer or
  1029|         0|            0|            0|  0.00%|            double-integer tuple)
  1030|         0|            0|            0|  0.00%|        return_indices: whether to return pooling indices. Default: ``False``
  1031|         0|            0|            0|  0.00%|    """
  1032|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):
  1033|         0|            0|            0|  0.00%|        return handle_torch_function(
  1034|         0|            0|            0|  0.00%|            adaptive_max_pool2d_with_indices, (input,), input, output_size, return_indices=return_indices
  1035|         0|            0|            0|  0.00%|        )
  1036|         0|            0|            0|  0.00%|    output_size = _list_with_default(output_size, input.size())
  1037|         0|            0|            0|  0.00%|    return torch._C._nn.adaptive_max_pool2d(input, output_size)
  1038|         0|            0|            0|  0.00%|
  1039|         0|            0|            0|  0.00%|
  1040|         0|            0|            0|  0.00%|def _adaptive_max_pool2d(input: Tensor, output_size: BroadcastingList2[int], return_indices: bool = False) -> Tensor:
  1041|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):
  1042|         0|            0|            0|  0.00%|        return handle_torch_function(
  1043|         0|            0|            0|  0.00%|            adaptive_max_pool2d, (input,), input, output_size, return_indices=return_indices
  1044|         0|            0|            0|  0.00%|        )
  1045|         0|            0|            0|  0.00%|    return adaptive_max_pool2d_with_indices(input, output_size)[0]
  1046|         0|            0|            0|  0.00%|
  1047|         0|            0|            0|  0.00%|
  1048|         0|            0|            0|  0.00%|adaptive_max_pool2d = boolean_dispatch(
  1049|         0|            0|            0|  0.00%|    arg_name="return_indices",
  1050|         0|            0|            0|  0.00%|    arg_index=2,
  1051|         0|            0|            0|  0.00%|    default=False,
  1052|         0|            0|            0|  0.00%|    if_true=adaptive_max_pool2d_with_indices,
  1053|         0|            0|            0|  0.00%|    if_false=_adaptive_max_pool2d,
  1054|         0|            0|            0|  0.00%|    module_name=__name__,
  1055|         0|            0|            0|  0.00%|    func_name="adaptive_max_pool2d",
  1056|         0|            0|            0|  0.00%|)
  1057|         0|            0|            0|  0.00%|
  1058|         0|            0|            0|  0.00%|
  1059|         0|            0|            0|  0.00%|def adaptive_max_pool3d_with_indices(
  1060|         0|            0|            0|  0.00%|    input: Tensor, output_size: BroadcastingList3[int],
  1061|         0|            0|            0|  0.00%|    return_indices: bool = False
  1062|         0|            0|            0|  0.00%|) -> Tuple[Tensor, Tensor]:
  1063|         0|            0|            0|  0.00%|    r"""Applies a 3D adaptive max pooling over an input signal composed of
  1064|         0|            0|            0|  0.00%|    several input planes.
  1065|         0|            0|            0|  0.00%|
  1066|         0|            0|            0|  0.00%|    See :class:`~torch.nn.AdaptiveMaxPool3d` for details and output shape.
  1067|         0|            0|            0|  0.00%|
  1068|         0|            0|            0|  0.00%|    Args:
  1069|         0|            0|            0|  0.00%|        output_size: the target output size (single integer or
  1070|         0|            0|            0|  0.00%|            triple-integer tuple)
  1071|         0|            0|            0|  0.00%|        return_indices: whether to return pooling indices. Default: ``False``
  1072|         0|            0|            0|  0.00%|    """
  1073|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):
  1074|         0|            0|            0|  0.00%|        return handle_torch_function(
  1075|         0|            0|            0|  0.00%|            adaptive_max_pool3d_with_indices, (input,), input, output_size, return_indices=return_indices
  1076|         0|            0|            0|  0.00%|        )
  1077|         0|            0|            0|  0.00%|    output_size = _list_with_default(output_size, input.size())
  1078|         0|            0|            0|  0.00%|    return torch._C._nn.adaptive_max_pool3d(input, output_size)
  1079|         0|            0|            0|  0.00%|
  1080|         0|            0|            0|  0.00%|
  1081|         0|            0|            0|  0.00%|def _adaptive_max_pool3d(input: Tensor, output_size: BroadcastingList3[int], return_indices: bool = False) -> Tensor:
  1082|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):
  1083|         0|            0|            0|  0.00%|        return handle_torch_function(
  1084|         0|            0|            0|  0.00%|            adaptive_max_pool3d, (input,), input, output_size, return_indices=return_indices
  1085|         0|            0|            0|  0.00%|        )
  1086|         0|            0|            0|  0.00%|    return adaptive_max_pool3d_with_indices(input, output_size)[0]
  1087|         0|            0|            0|  0.00%|
  1088|         0|            0|            0|  0.00%|
  1089|         0|            0|            0|  0.00%|adaptive_max_pool3d = boolean_dispatch(
  1090|         0|            0|            0|  0.00%|    arg_name="return_indices",
  1091|         0|            0|            0|  0.00%|    arg_index=2,
  1092|         0|            0|            0|  0.00%|    default=False,
  1093|         0|            0|            0|  0.00%|    if_true=adaptive_max_pool3d_with_indices,
  1094|         0|            0|            0|  0.00%|    if_false=_adaptive_max_pool3d,
  1095|         0|            0|            0|  0.00%|    module_name=__name__,
  1096|         0|            0|            0|  0.00%|    func_name="adaptive_max_pool3d",
  1097|         0|            0|            0|  0.00%|)
  1098|         0|            0|            0|  0.00%|
  1099|         0|            0|            0|  0.00%|
  1100|         0|            0|            0|  0.00%|adaptive_avg_pool1d = _add_docstr(
  1101|         0|            0|            0|  0.00%|    torch.adaptive_avg_pool1d,
  1102|         0|            0|            0|  0.00%|    r"""
  1103|         0|            0|            0|  0.00%|adaptive_avg_pool1d(input, output_size) -> Tensor
  1104|         0|            0|            0|  0.00%|
  1105|         0|            0|            0|  0.00%|Applies a 1D adaptive average pooling over an input signal composed of
  1106|         0|            0|            0|  0.00%|several input planes.
  1107|         0|            0|            0|  0.00%|
  1108|         0|            0|            0|  0.00%|See :class:`~torch.nn.AdaptiveAvgPool1d` for details and output shape.
  1109|         0|            0|            0|  0.00%|
  1110|         0|            0|            0|  0.00%|Args:
  1111|         0|            0|            0|  0.00%|    output_size: the target output size (single integer)
  1112|         0|            0|            0|  0.00%|""",
  1113|         0|            0|            0|  0.00%|)
  1114|         0|            0|            0|  0.00%|
  1115|         0|            0|            0|  0.00%|
  1116|       100|  0.000653982|  6.53982e-06|  0.00%|def adaptive_avg_pool2d(input: Tensor, output_size: BroadcastingList2[int]) -> Tensor:
  1117|         0|            0|            0|  0.00%|    r"""
  1118|         0|            0|            0|  0.00%|    Applies a 2D adaptive average pooling over an input signal composed of
  1119|         0|            0|            0|  0.00%|    several input planes.
  1120|         0|            0|            0|  0.00%|
  1121|         0|            0|            0|  0.00%|    See :class:`~torch.nn.AdaptiveAvgPool2d` for details and output shape.
  1122|         0|            0|            0|  0.00%|
  1123|         0|            0|            0|  0.00%|    Args:
  1124|         0|            0|            0|  0.00%|        output_size: the target output size (single integer or
  1125|         0|            0|            0|  0.00%|            double-integer tuple)
  1126|         0|            0|            0|  0.00%|    """
  1127|       100|  0.000767708|  7.67708e-06|  0.00%|    if has_torch_function_unary(input):
  1128|         0|            0|            0|  0.00%|        return handle_torch_function(adaptive_avg_pool2d, (input,), input, output_size)
  1129|       100|   0.00244117|  2.44117e-05|  0.00%|    _output_size = _list_with_default(output_size, input.size())
(call)|       100|    0.0071919|   7.1919e-05|  0.01%|# /opt/conda/lib/python3.8/site-packages/torch/nn/modules/utils.py:29 _list_with_default
  1130|       100|   0.00957274|  9.57274e-05|  0.02%|    return torch._C._nn.adaptive_avg_pool2d(input, _output_size)
  1131|         0|            0|            0|  0.00%|
  1132|         0|            0|            0|  0.00%|
  1133|         0|            0|            0|  0.00%|def adaptive_avg_pool3d(input: Tensor, output_size: BroadcastingList3[int]) -> Tensor:
  1134|         0|            0|            0|  0.00%|    r"""
  1135|         0|            0|            0|  0.00%|    Applies a 3D adaptive average pooling over an input signal composed of
  1136|         0|            0|            0|  0.00%|    several input planes.
  1137|         0|            0|            0|  0.00%|
  1138|         0|            0|            0|  0.00%|    See :class:`~torch.nn.AdaptiveAvgPool3d` for details and output shape.
  1139|         0|            0|            0|  0.00%|
  1140|         0|            0|            0|  0.00%|    Args:
  1141|         0|            0|            0|  0.00%|        output_size: the target output size (single integer or
  1142|         0|            0|            0|  0.00%|            triple-integer tuple)
  1143|         0|            0|            0|  0.00%|    """
  1144|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):
  1145|         0|            0|            0|  0.00%|        return handle_torch_function(adaptive_avg_pool3d, (input,), input, output_size)
  1146|         0|            0|            0|  0.00%|    _output_size = _list_with_default(output_size, input.size())
  1147|         0|            0|            0|  0.00%|    return torch._C._nn.adaptive_avg_pool3d(input, _output_size)
  1148|         0|            0|            0|  0.00%|
  1149|         0|            0|            0|  0.00%|
  1150|         0|            0|            0|  0.00%|# Activation functions
  1151|         0|            0|            0|  0.00%|def dropout(input: Tensor, p: float = 0.5, training: bool = True, inplace: bool = False) -> Tensor:
  1152|         0|            0|            0|  0.00%|    r"""
  1153|         0|            0|            0|  0.00%|    During training, randomly zeroes some of the elements of the input
  1154|         0|            0|            0|  0.00%|    tensor with probability :attr:`p` using samples from a Bernoulli
  1155|         0|            0|            0|  0.00%|    distribution.
  1156|         0|            0|            0|  0.00%|
  1157|         0|            0|            0|  0.00%|    See :class:`~torch.nn.Dropout` for details.
  1158|         0|            0|            0|  0.00%|
  1159|         0|            0|            0|  0.00%|    Args:
  1160|         0|            0|            0|  0.00%|        p: probability of an element to be zeroed. Default: 0.5
  1161|         0|            0|            0|  0.00%|        training: apply dropout if is ``True``. Default: ``True``
  1162|         0|            0|            0|  0.00%|        inplace: If set to ``True``, will do this operation in-place. Default: ``False``
  1163|         0|            0|            0|  0.00%|    """
  1164|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):
  1165|         0|            0|            0|  0.00%|        return handle_torch_function(dropout, (input,), input, p=p, training=training, inplace=inplace)
  1166|         0|            0|            0|  0.00%|    if p < 0.0 or p > 1.0:
  1167|         0|            0|            0|  0.00%|        raise ValueError("dropout probability has to be between 0 and 1, " "but got {}".format(p))
  1168|         0|            0|            0|  0.00%|    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
  1169|         0|            0|            0|  0.00%|
  1170|         0|            0|            0|  0.00%|
  1171|         0|            0|            0|  0.00%|def alpha_dropout(input: Tensor, p: float = 0.5, training: bool = False, inplace: bool = False) -> Tensor:
  1172|         0|            0|            0|  0.00%|    r"""Applies alpha dropout to the input.
  1173|         0|            0|            0|  0.00%|
  1174|         0|            0|            0|  0.00%|    See :class:`~torch.nn.AlphaDropout` for details.
  1175|         0|            0|            0|  0.00%|    """
  1176|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):
  1177|         0|            0|            0|  0.00%|        return handle_torch_function(alpha_dropout, (input,), input, p=p, training=training, inplace=inplace)
  1178|         0|            0|            0|  0.00%|    if p < 0.0 or p > 1.0:
  1179|         0|            0|            0|  0.00%|        raise ValueError("dropout probability has to be between 0 and 1, " "but got {}".format(p))
  1180|         0|            0|            0|  0.00%|    return _VF.alpha_dropout_(input, p, training) if inplace else _VF.alpha_dropout(input, p, training)
  1181|         0|            0|            0|  0.00%|
  1182|         0|            0|            0|  0.00%|
  1183|         0|            0|            0|  0.00%|def dropout2d(input: Tensor, p: float = 0.5, training: bool = True, inplace: bool = False) -> Tensor:
  1184|         0|            0|            0|  0.00%|    r"""
  1185|         0|            0|            0|  0.00%|    Randomly zero out entire channels (a channel is a 2D feature map,
  1186|         0|            0|            0|  0.00%|    e.g., the :math:`j`-th channel of the :math:`i`-th sample in the
  1187|         0|            0|            0|  0.00%|    batched input is a 2D tensor :math:`\text{input}[i, j]`) of the input tensor).
  1188|         0|            0|            0|  0.00%|    Each channel will be zeroed out independently on every forward call with
  1189|         0|            0|            0|  0.00%|    probability :attr:`p` using samples from a Bernoulli distribution.
  1190|         0|            0|            0|  0.00%|
  1191|         0|            0|            0|  0.00%|    See :class:`~torch.nn.Dropout2d` for details.
  1192|         0|            0|            0|  0.00%|
  1193|         0|            0|            0|  0.00%|    Args:
  1194|         0|            0|            0|  0.00%|        p: probability of a channel to be zeroed. Default: 0.5
  1195|         0|            0|            0|  0.00%|        training: apply dropout if is ``True``. Default: ``True``
  1196|         0|            0|            0|  0.00%|        inplace: If set to ``True``, will do this operation in-place. Default: ``False``
  1197|         0|            0|            0|  0.00%|    """
  1198|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):
  1199|         0|            0|            0|  0.00%|        return handle_torch_function(dropout2d, (input,), input, p=p, training=training, inplace=inplace)
  1200|         0|            0|            0|  0.00%|    if p < 0.0 or p > 1.0:
  1201|         0|            0|            0|  0.00%|        raise ValueError("dropout probability has to be between 0 and 1, " "but got {}".format(p))
  1202|         0|            0|            0|  0.00%|    return _VF.feature_dropout_(input, p, training) if inplace else _VF.feature_dropout(input, p, training)
  1203|         0|            0|            0|  0.00%|
  1204|         0|            0|            0|  0.00%|
  1205|         0|            0|            0|  0.00%|def dropout3d(input: Tensor, p: float = 0.5, training: bool = True, inplace: bool = False) -> Tensor:
  1206|         0|            0|            0|  0.00%|    r"""
  1207|         0|            0|            0|  0.00%|    Randomly zero out entire channels (a channel is a 3D feature map,
  1208|         0|            0|            0|  0.00%|    e.g., the :math:`j`-th channel of the :math:`i`-th sample in the
  1209|         0|            0|            0|  0.00%|    batched input is a 3D tensor :math:`\text{input}[i, j]`) of the input tensor).
  1210|         0|            0|            0|  0.00%|    Each channel will be zeroed out independently on every forward call with
  1211|         0|            0|            0|  0.00%|    probability :attr:`p` using samples from a Bernoulli distribution.
  1212|         0|            0|            0|  0.00%|
  1213|         0|            0|            0|  0.00%|    See :class:`~torch.nn.Dropout3d` for details.
  1214|         0|            0|            0|  0.00%|
  1215|         0|            0|            0|  0.00%|    Args:
  1216|         0|            0|            0|  0.00%|        p: probability of a channel to be zeroed. Default: 0.5
  1217|         0|            0|            0|  0.00%|        training: apply dropout if is ``True``. Default: ``True``
  1218|         0|            0|            0|  0.00%|        inplace: If set to ``True``, will do this operation in-place. Default: ``False``
  1219|         0|            0|            0|  0.00%|    """
  1220|         0|            0|            0|  0.00%|    # This is 100% the same code as dropout2d. We duplicate this code so that
  1221|         0|            0|            0|  0.00%|    # stack traces are not confusing.
  1222|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):
  1223|         0|            0|            0|  0.00%|        return handle_torch_function(dropout3d, (input,), input, p=p, training=training, inplace=inplace)
  1224|         0|            0|            0|  0.00%|    if p < 0.0 or p > 1.0:
  1225|         0|            0|            0|  0.00%|        raise ValueError("dropout probability has to be between 0 and 1, " "but got {}".format(p))
  1226|         0|            0|            0|  0.00%|    return _VF.feature_dropout_(input, p, training) if inplace else _VF.feature_dropout(input, p, training)
  1227|         0|            0|            0|  0.00%|
  1228|         0|            0|            0|  0.00%|
  1229|         0|            0|            0|  0.00%|def feature_alpha_dropout(input: Tensor, p: float = 0.5, training: bool = False, inplace: bool = False) -> Tensor:
  1230|         0|            0|            0|  0.00%|    r"""
  1231|         0|            0|            0|  0.00%|    Randomly masks out entire channels (a channel is a feature map,
  1232|         0|            0|            0|  0.00%|    e.g. the :math:`j`-th channel of the :math:`i`-th sample in the batch input
  1233|         0|            0|            0|  0.00%|    is a tensor :math:`\text{input}[i, j]`) of the input tensor). Instead of
  1234|         0|            0|            0|  0.00%|    setting activations to zero, as in regular Dropout, the activations are set
  1235|         0|            0|            0|  0.00%|    to the negative saturation value of the SELU activation function.
  1236|         0|            0|            0|  0.00%|
  1237|         0|            0|            0|  0.00%|    Each element will be masked independently on every forward call with
  1238|         0|            0|            0|  0.00%|    probability :attr:`p` using samples from a Bernoulli distribution.
  1239|         0|            0|            0|  0.00%|    The elements to be masked are randomized on every forward call, and scaled
  1240|         0|            0|            0|  0.00%|    and shifted to maintain zero mean and unit variance.
  1241|         0|            0|            0|  0.00%|
  1242|         0|            0|            0|  0.00%|    See :class:`~torch.nn.FeatureAlphaDropout` for details.
  1243|         0|            0|            0|  0.00%|
  1244|         0|            0|            0|  0.00%|    Args:
  1245|         0|            0|            0|  0.00%|        p: dropout probability of a channel to be zeroed. Default: 0.5
  1246|         0|            0|            0|  0.00%|        training: apply dropout if is ``True``. Default: ``True``
  1247|         0|            0|            0|  0.00%|        inplace: If set to ``True``, will do this operation in-place. Default: ``False``
  1248|         0|            0|            0|  0.00%|    """
  1249|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):
  1250|         0|            0|            0|  0.00%|        return handle_torch_function(
  1251|         0|            0|            0|  0.00%|            feature_alpha_dropout, (input,), input, p=p, training=training, inplace=inplace
  1252|         0|            0|            0|  0.00%|        )
  1253|         0|            0|            0|  0.00%|    if p < 0.0 or p > 1.0:
  1254|         0|            0|            0|  0.00%|        raise ValueError("dropout probability has to be between 0 and 1, " "but got {}".format(p))
  1255|         0|            0|            0|  0.00%|    return _VF.feature_alpha_dropout_(input, p, training) if inplace else _VF.feature_alpha_dropout(input, p, training)
  1256|         0|            0|            0|  0.00%|
  1257|         0|            0|            0|  0.00%|
  1258|         0|            0|            0|  0.00%|def _threshold(input: Tensor, threshold: float, value: float, inplace: bool = False) -> Tensor:
  1259|         0|            0|            0|  0.00%|    r"""Thresholds each element of the input Tensor.
  1260|         0|            0|            0|  0.00%|
  1261|         0|            0|            0|  0.00%|    See :class:`~torch.nn.Threshold` for more details.
  1262|         0|            0|            0|  0.00%|    """
  1263|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):
  1264|         0|            0|            0|  0.00%|        return handle_torch_function(_threshold, (input,), input, threshold, value, inplace=inplace)
  1265|         0|            0|            0|  0.00%|    if inplace:
  1266|         0|            0|            0|  0.00%|        result = _VF.threshold_(input, threshold, value)
  1267|         0|            0|            0|  0.00%|    else:
  1268|         0|            0|            0|  0.00%|        result = _VF.threshold(input, threshold, value)
  1269|         0|            0|            0|  0.00%|    return result
  1270|         0|            0|            0|  0.00%|
  1271|         0|            0|            0|  0.00%|
  1272|         0|            0|            0|  0.00%|# We define this function as _threshold because it takes an argument
  1273|         0|            0|            0|  0.00%|# named threshold, which clobbers the recursive reference to the
  1274|         0|            0|            0|  0.00%|# function needed for __torch_function__ support
  1275|         0|            0|            0|  0.00%|threshold = _threshold
  1276|         0|            0|            0|  0.00%|
  1277|         0|            0|            0|  0.00%|threshold_ = _add_docstr(
  1278|         0|            0|            0|  0.00%|    _VF.threshold_,
  1279|         0|            0|            0|  0.00%|    r"""
  1280|         0|            0|            0|  0.00%|threshold_(input, threshold, value) -> Tensor
  1281|         0|            0|            0|  0.00%|
  1282|         0|            0|            0|  0.00%|In-place version of :func:`~threshold`.
  1283|         0|            0|            0|  0.00%|""",
  1284|         0|            0|            0|  0.00%|)
  1285|         0|            0|            0|  0.00%|
  1286|         0|            0|            0|  0.00%|
  1287|      1700|    0.0083003|  4.88253e-06|  0.02%|def relu(input: Tensor, inplace: bool = False) -> Tensor:
  1288|         0|            0|            0|  0.00%|    r"""relu(input, inplace=False) -> Tensor
  1289|         0|            0|            0|  0.00%|
  1290|         0|            0|            0|  0.00%|    Applies the rectified linear unit function element-wise. See
  1291|         0|            0|            0|  0.00%|    :class:`~torch.nn.ReLU` for more details.
  1292|         0|            0|            0|  0.00%|    """
  1293|      1700|    0.0114918|  6.75987e-06|  0.02%|    if has_torch_function_unary(input):
  1294|         0|            0|            0|  0.00%|        return handle_torch_function(relu, (input,), input, inplace=inplace)
  1295|      1700|   0.00716758|  4.21622e-06|  0.01%|    if inplace:
  1296|      1700|      0.14458|  8.50473e-05|  0.27%|        result = torch.relu_(input)
  1297|         0|            0|            0|  0.00%|    else:
  1298|         0|            0|            0|  0.00%|        result = torch.relu(input)
  1299|      1700|    0.0148652|  8.74421e-06|  0.03%|    return result
  1300|         0|            0|            0|  0.00%|
  1301|         0|            0|            0|  0.00%|
  1302|         0|            0|            0|  0.00%|relu_ = _add_docstr(
  1303|         0|            0|            0|  0.00%|    torch.relu_,
  1304|         0|            0|            0|  0.00%|    r"""
  1305|         0|            0|            0|  0.00%|relu_(input) -> Tensor
  1306|         0|            0|            0|  0.00%|
  1307|         0|            0|            0|  0.00%|In-place version of :func:`~relu`.
  1308|         0|            0|            0|  0.00%|""",
  1309|         0|            0|            0|  0.00%|)
  1310|         0|            0|            0|  0.00%|
  1311|         0|            0|            0|  0.00%|
  1312|         0|            0|            0|  0.00%|def glu(input: Tensor, dim: int = -1) -> Tensor:
  1313|         0|            0|            0|  0.00%|    r"""
  1314|         0|            0|            0|  0.00%|    glu(input, dim=-1) -> Tensor
  1315|         0|            0|            0|  0.00%|
  1316|         0|            0|            0|  0.00%|    The gated linear unit. Computes:
  1317|         0|            0|            0|  0.00%|
  1318|         0|            0|            0|  0.00%|    .. math ::
  1319|         0|            0|            0|  0.00%|        \text{GLU}(a, b) = a \otimes \sigma(b)
  1320|         0|            0|            0|  0.00%|
  1321|         0|            0|            0|  0.00%|    where `input` is split in half along `dim` to form `a` and `b`, :math:`\sigma`
  1322|         0|            0|            0|  0.00%|    is the sigmoid function and :math:`\otimes` is the element-wise product between matrices.
  1323|         0|            0|            0|  0.00%|
  1324|         0|            0|            0|  0.00%|    See `Language Modeling with Gated Convolutional Networks <https://arxiv.org/abs/1612.08083>`_.
  1325|         0|            0|            0|  0.00%|
  1326|         0|            0|            0|  0.00%|    Args:
  1327|         0|            0|            0|  0.00%|        input (Tensor): input tensor
  1328|         0|            0|            0|  0.00%|        dim (int): dimension on which to split the input. Default: -1
  1329|         0|            0|            0|  0.00%|    """
  1330|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):
  1331|         0|            0|            0|  0.00%|        return handle_torch_function(glu, (input,), input, dim=dim)
  1332|         0|            0|            0|  0.00%|    if input.dim() == 0:
  1333|         0|            0|            0|  0.00%|        raise RuntimeError("glu does not support scalars because halving size must be even")
  1334|         0|            0|            0|  0.00%|    return torch._C._nn.glu(input, dim)
  1335|         0|            0|            0|  0.00%|
  1336|         0|            0|            0|  0.00%|
  1337|         0|            0|            0|  0.00%|def hardtanh(input: Tensor, min_val: float = -1.0, max_val: float = 1.0, inplace: bool = False) -> Tensor:
  1338|         0|            0|            0|  0.00%|    r"""
  1339|         0|            0|            0|  0.00%|    hardtanh(input, min_val=-1., max_val=1., inplace=False) -> Tensor
  1340|         0|            0|            0|  0.00%|
  1341|         0|            0|            0|  0.00%|    Applies the HardTanh function element-wise. See :class:`~torch.nn.Hardtanh` for more
  1342|         0|            0|            0|  0.00%|    details.
  1343|         0|            0|            0|  0.00%|    """
  1344|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):
  1345|         0|            0|            0|  0.00%|        return handle_torch_function(hardtanh, (input,), input, min_val=min_val, max_val=max_val, inplace=inplace)
  1346|         0|            0|            0|  0.00%|    if inplace:
  1347|         0|            0|            0|  0.00%|        result = torch._C._nn.hardtanh_(input, min_val, max_val)
  1348|         0|            0|            0|  0.00%|    else:
  1349|         0|            0|            0|  0.00%|        result = torch._C._nn.hardtanh(input, min_val, max_val)
  1350|         0|            0|            0|  0.00%|    return result
  1351|         0|            0|            0|  0.00%|
  1352|         0|            0|            0|  0.00%|
  1353|         0|            0|            0|  0.00%|hardtanh_ = _add_docstr(
  1354|         0|            0|            0|  0.00%|    torch._C._nn.hardtanh_,
  1355|         0|            0|            0|  0.00%|    r"""
  1356|         0|            0|            0|  0.00%|hardtanh_(input, min_val=-1., max_val=1.) -> Tensor
  1357|         0|            0|            0|  0.00%|
  1358|         0|            0|            0|  0.00%|In-place version of :func:`~hardtanh`.
  1359|         0|            0|            0|  0.00%|""",
  1360|         0|            0|            0|  0.00%|)
  1361|         0|            0|            0|  0.00%|
  1362|         0|            0|            0|  0.00%|
  1363|         0|            0|            0|  0.00%|def relu6(input: Tensor, inplace: bool = False) -> Tensor:
  1364|         0|            0|            0|  0.00%|    r"""relu6(input, inplace=False) -> Tensor
  1365|         0|            0|            0|  0.00%|
  1366|         0|            0|            0|  0.00%|    Applies the element-wise function :math:`\text{ReLU6}(x) = \min(\max(0,x), 6)`.
  1367|         0|            0|            0|  0.00%|
  1368|         0|            0|            0|  0.00%|    See :class:`~torch.nn.ReLU6` for more details.
  1369|         0|            0|            0|  0.00%|    """
  1370|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):
  1371|         0|            0|            0|  0.00%|        return handle_torch_function(relu6, (input,), input, inplace=inplace)
  1372|         0|            0|            0|  0.00%|    if inplace:
  1373|         0|            0|            0|  0.00%|        result = torch._C._nn.relu6_(input)
  1374|         0|            0|            0|  0.00%|    else:
  1375|         0|            0|            0|  0.00%|        result = torch._C._nn.relu6(input)
  1376|         0|            0|            0|  0.00%|    return result
  1377|         0|            0|            0|  0.00%|
  1378|         0|            0|            0|  0.00%|
  1379|         0|            0|            0|  0.00%|def elu(input: Tensor, alpha: float = 1.0, inplace: bool = False) -> Tensor:
  1380|         0|            0|            0|  0.00%|    r"""Applies element-wise,
  1381|         0|            0|            0|  0.00%|    :math:`\text{ELU}(x) = \max(0,x) + \min(0, \alpha * (\exp(x) - 1))`.
  1382|         0|            0|            0|  0.00%|
  1383|         0|            0|            0|  0.00%|    See :class:`~torch.nn.ELU` for more details.
  1384|         0|            0|            0|  0.00%|    """
  1385|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):
  1386|         0|            0|            0|  0.00%|        return handle_torch_function(elu, (input,), input, alpha=alpha, inplace=inplace)
  1387|         0|            0|            0|  0.00%|    if inplace:
  1388|         0|            0|            0|  0.00%|        result = torch._C._nn.elu_(input, alpha)
  1389|         0|            0|            0|  0.00%|    else:
  1390|         0|            0|            0|  0.00%|        result = torch._C._nn.elu(input, alpha)
  1391|         0|            0|            0|  0.00%|    return result
  1392|         0|            0|            0|  0.00%|
  1393|         0|            0|            0|  0.00%|
  1394|         0|            0|            0|  0.00%|elu_ = _add_docstr(
  1395|         0|            0|            0|  0.00%|    torch._C._nn.elu_,
  1396|         0|            0|            0|  0.00%|    r"""
  1397|         0|            0|            0|  0.00%|elu_(input, alpha=1.) -> Tensor
  1398|         0|            0|            0|  0.00%|
  1399|         0|            0|            0|  0.00%|In-place version of :func:`~elu`.
  1400|         0|            0|            0|  0.00%|""",
  1401|         0|            0|            0|  0.00%|)
  1402|         0|            0|            0|  0.00%|
  1403|         0|            0|            0|  0.00%|
  1404|         0|            0|            0|  0.00%|def selu(input: Tensor, inplace: bool = False) -> Tensor:
  1405|         0|            0|            0|  0.00%|    r"""selu(input, inplace=False) -> Tensor
  1406|         0|            0|            0|  0.00%|
  1407|         0|            0|            0|  0.00%|    Applies element-wise,
  1408|         0|            0|            0|  0.00%|    :math:`\text{SELU}(x) = scale * (\max(0,x) + \min(0, \alpha * (\exp(x) - 1)))`,
  1409|         0|            0|            0|  0.00%|    with :math:`\alpha=1.6732632423543772848170429916717` and
  1410|         0|            0|            0|  0.00%|    :math:`scale=1.0507009873554804934193349852946`.
  1411|         0|            0|            0|  0.00%|
  1412|         0|            0|            0|  0.00%|    See :class:`~torch.nn.SELU` for more details.
  1413|         0|            0|            0|  0.00%|    """
  1414|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):
  1415|         0|            0|            0|  0.00%|        return handle_torch_function(selu, (input,), input, inplace=inplace)
  1416|         0|            0|            0|  0.00%|    if inplace:
  1417|         0|            0|            0|  0.00%|        result = torch.selu_(input)
  1418|         0|            0|            0|  0.00%|    else:
  1419|         0|            0|            0|  0.00%|        result = torch.selu(input)
  1420|         0|            0|            0|  0.00%|    return result
  1421|         0|            0|            0|  0.00%|
  1422|         0|            0|            0|  0.00%|
  1423|         0|            0|            0|  0.00%|selu_ = _add_docstr(
  1424|         0|            0|            0|  0.00%|    torch.selu_,
  1425|         0|            0|            0|  0.00%|    r"""
  1426|         0|            0|            0|  0.00%|selu_(input) -> Tensor
  1427|         0|            0|            0|  0.00%|
  1428|         0|            0|            0|  0.00%|In-place version of :func:`~selu`.
  1429|         0|            0|            0|  0.00%|""",
  1430|         0|            0|            0|  0.00%|)
  1431|         0|            0|            0|  0.00%|
  1432|         0|            0|            0|  0.00%|
  1433|         0|            0|            0|  0.00%|def celu(input: Tensor, alpha: float = 1.0, inplace: bool = False) -> Tensor:
  1434|         0|            0|            0|  0.00%|    r"""celu(input, alpha=1., inplace=False) -> Tensor
  1435|         0|            0|            0|  0.00%|
  1436|         0|            0|            0|  0.00%|    Applies element-wise,
  1437|         0|            0|            0|  0.00%|    :math:`\text{CELU}(x) = \max(0,x) + \min(0, \alpha * (\exp(x/\alpha) - 1))`.
  1438|         0|            0|            0|  0.00%|
  1439|         0|            0|            0|  0.00%|    See :class:`~torch.nn.CELU` for more details.
  1440|         0|            0|            0|  0.00%|    """
  1441|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):
  1442|         0|            0|            0|  0.00%|        return handle_torch_function(celu, (input,), input, alpha=alpha, inplace=inplace)
  1443|         0|            0|            0|  0.00%|    if inplace:
  1444|         0|            0|            0|  0.00%|        result = torch.celu_(input, alpha)
  1445|         0|            0|            0|  0.00%|    else:
  1446|         0|            0|            0|  0.00%|        result = torch.celu(input, alpha)
  1447|         0|            0|            0|  0.00%|    return result
  1448|         0|            0|            0|  0.00%|
  1449|         0|            0|            0|  0.00%|
  1450|         0|            0|            0|  0.00%|celu_ = _add_docstr(
  1451|         0|            0|            0|  0.00%|    torch.celu_,
  1452|         0|            0|            0|  0.00%|    r"""
  1453|         0|            0|            0|  0.00%|celu_(input, alpha=1.) -> Tensor
  1454|         0|            0|            0|  0.00%|
  1455|         0|            0|            0|  0.00%|In-place version of :func:`~celu`.
  1456|         0|            0|            0|  0.00%|""",
  1457|         0|            0|            0|  0.00%|)
  1458|         0|            0|            0|  0.00%|
  1459|         0|            0|            0|  0.00%|
  1460|         0|            0|            0|  0.00%|def leaky_relu(input: Tensor, negative_slope: float = 0.01, inplace: bool = False) -> Tensor:
  1461|         0|            0|            0|  0.00%|    r"""
  1462|         0|            0|            0|  0.00%|    leaky_relu(input, negative_slope=0.01, inplace=False) -> Tensor
  1463|         0|            0|            0|  0.00%|
  1464|         0|            0|            0|  0.00%|    Applies element-wise,
  1465|         0|            0|            0|  0.00%|    :math:`\text{LeakyReLU}(x) = \max(0, x) + \text{negative\_slope} * \min(0, x)`
  1466|         0|            0|            0|  0.00%|
  1467|         0|            0|            0|  0.00%|    See :class:`~torch.nn.LeakyReLU` for more details.
  1468|         0|            0|            0|  0.00%|    """
  1469|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):
  1470|         0|            0|            0|  0.00%|        return handle_torch_function(leaky_relu, (input,), input, negative_slope=negative_slope, inplace=inplace)
  1471|         0|            0|            0|  0.00%|    if inplace:
  1472|         0|            0|            0|  0.00%|        result = torch._C._nn.leaky_relu_(input, negative_slope)
  1473|         0|            0|            0|  0.00%|    else:
  1474|         0|            0|            0|  0.00%|        result = torch._C._nn.leaky_relu(input, negative_slope)
  1475|         0|            0|            0|  0.00%|    return result
  1476|         0|            0|            0|  0.00%|
  1477|         0|            0|            0|  0.00%|
  1478|         0|            0|            0|  0.00%|leaky_relu_ = _add_docstr(
  1479|         0|            0|            0|  0.00%|    torch._C._nn.leaky_relu_,
  1480|         0|            0|            0|  0.00%|    r"""
  1481|         0|            0|            0|  0.00%|leaky_relu_(input, negative_slope=0.01) -> Tensor
  1482|         0|            0|            0|  0.00%|
  1483|         0|            0|            0|  0.00%|In-place version of :func:`~leaky_relu`.
  1484|         0|            0|            0|  0.00%|""",
  1485|         0|            0|            0|  0.00%|)
  1486|         0|            0|            0|  0.00%|
  1487|         0|            0|            0|  0.00%|
  1488|         0|            0|            0|  0.00%|def prelu(input: Tensor, weight: Tensor) -> Tensor:
  1489|         0|            0|            0|  0.00%|    r"""prelu(input, weight) -> Tensor
  1490|         0|            0|            0|  0.00%|
  1491|         0|            0|            0|  0.00%|    Applies element-wise the function
  1492|         0|            0|            0|  0.00%|    :math:`\text{PReLU}(x) = \max(0,x) + \text{weight} * \min(0,x)` where weight is a
  1493|         0|            0|            0|  0.00%|    learnable parameter.
  1494|         0|            0|            0|  0.00%|
  1495|         0|            0|            0|  0.00%|    See :class:`~torch.nn.PReLU` for more details.
  1496|         0|            0|            0|  0.00%|    """
  1497|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):
  1498|         0|            0|            0|  0.00%|        return handle_torch_function(prelu, (input,), input, weight)
  1499|         0|            0|            0|  0.00%|    return torch.prelu(input, weight)
  1500|         0|            0|            0|  0.00%|
  1501|         0|            0|            0|  0.00%|
  1502|         0|            0|            0|  0.00%|def rrelu(
  1503|         0|            0|            0|  0.00%|    input: Tensor, lower: float = 1.0 / 8, upper: float = 1.0 / 3, training: bool = False, inplace: bool = False
  1504|         0|            0|            0|  0.00%|) -> Tensor:
  1505|         0|            0|            0|  0.00%|    r"""rrelu(input, lower=1./8, upper=1./3, training=False, inplace=False) -> Tensor
  1506|         0|            0|            0|  0.00%|
  1507|         0|            0|            0|  0.00%|    Randomized leaky ReLU.
  1508|         0|            0|            0|  0.00%|
  1509|         0|            0|            0|  0.00%|    See :class:`~torch.nn.RReLU` for more details.
  1510|         0|            0|            0|  0.00%|    """
  1511|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):
  1512|         0|            0|            0|  0.00%|        return handle_torch_function(
  1513|         0|            0|            0|  0.00%|            rrelu, (input,), input, lower=lower, upper=upper, training=training, inplace=inplace
  1514|         0|            0|            0|  0.00%|        )
  1515|         0|            0|            0|  0.00%|    if inplace:
  1516|         0|            0|            0|  0.00%|        result = torch.rrelu_(input, lower, upper, training)
  1517|         0|            0|            0|  0.00%|    else:
  1518|         0|            0|            0|  0.00%|        result = torch.rrelu(input, lower, upper, training)
  1519|         0|            0|            0|  0.00%|    return result
  1520|         0|            0|            0|  0.00%|
  1521|         0|            0|            0|  0.00%|
  1522|         0|            0|            0|  0.00%|rrelu_ = _add_docstr(
  1523|         0|            0|            0|  0.00%|    torch.rrelu_,
  1524|         0|            0|            0|  0.00%|    r"""
  1525|         0|            0|            0|  0.00%|rrelu_(input, lower=1./8, upper=1./3, training=False) -> Tensor
  1526|         0|            0|            0|  0.00%|
  1527|         0|            0|            0|  0.00%|In-place version of :func:`~rrelu`.
  1528|         0|            0|            0|  0.00%|""",
  1529|         0|            0|            0|  0.00%|)
  1530|         0|            0|            0|  0.00%|
  1531|         0|            0|            0|  0.00%|logsigmoid = _add_docstr(
  1532|         0|            0|            0|  0.00%|    torch._C._nn.log_sigmoid,
  1533|         0|            0|            0|  0.00%|    r"""
  1534|         0|            0|            0|  0.00%|logsigmoid(input) -> Tensor
  1535|         0|            0|            0|  0.00%|
  1536|         0|            0|            0|  0.00%|Applies element-wise :math:`\text{LogSigmoid}(x_i) = \log \left(\frac{1}{1 + \exp(-x_i)}\right)`
  1537|         0|            0|            0|  0.00%|
  1538|         0|            0|            0|  0.00%|See :class:`~torch.nn.LogSigmoid` for more details.
  1539|         0|            0|            0|  0.00%|""",
  1540|         0|            0|            0|  0.00%|)
  1541|         0|            0|            0|  0.00%|
  1542|         0|            0|            0|  0.00%|
  1543|         0|            0|            0|  0.00%|def gelu(input):
  1544|         0|            0|            0|  0.00%|    r"""gelu(input) -> Tensor
  1545|         0|            0|            0|  0.00%|
  1546|         0|            0|            0|  0.00%|    Applies element-wise the function
  1547|         0|            0|            0|  0.00%|    :math:`\text{GELU}(x) = x * \Phi(x)`
  1548|         0|            0|            0|  0.00%|
  1549|         0|            0|            0|  0.00%|    where :math:`\Phi(x)` is the Cumulative Distribution Function for Gaussian Distribution.
  1550|         0|            0|            0|  0.00%|
  1551|         0|            0|            0|  0.00%|    See `Gaussian Error Linear Units (GELUs) <https://arxiv.org/abs/1606.08415>`_.
  1552|         0|            0|            0|  0.00%|    """
  1553|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):
  1554|         0|            0|            0|  0.00%|        return handle_torch_function(gelu, (input,), input)
  1555|         0|            0|            0|  0.00%|    return torch._C._nn.gelu(input)
  1556|         0|            0|            0|  0.00%|
  1557|         0|            0|            0|  0.00%|
  1558|         0|            0|            0|  0.00%|def hardshrink(input: Tensor, lambd: float = 0.5) -> Tensor:
  1559|         0|            0|            0|  0.00%|    r"""
  1560|         0|            0|            0|  0.00%|    hardshrink(input, lambd=0.5) -> Tensor
  1561|         0|            0|            0|  0.00%|
  1562|         0|            0|            0|  0.00%|    Applies the hard shrinkage function element-wise
  1563|         0|            0|            0|  0.00%|
  1564|         0|            0|            0|  0.00%|    See :class:`~torch.nn.Hardshrink` for more details.
  1565|         0|            0|            0|  0.00%|    """
  1566|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):
  1567|         0|            0|            0|  0.00%|        return handle_torch_function(hardshrink, (input,), input, lambd=lambd)
  1568|         0|            0|            0|  0.00%|    return torch.hardshrink(input, lambd)
  1569|         0|            0|            0|  0.00%|
  1570|         0|            0|            0|  0.00%|
  1571|         0|            0|            0|  0.00%|def tanhshrink(input):
  1572|         0|            0|            0|  0.00%|    r"""tanhshrink(input) -> Tensor
  1573|         0|            0|            0|  0.00%|
  1574|         0|            0|            0|  0.00%|    Applies element-wise, :math:`\text{Tanhshrink}(x) = x - \text{Tanh}(x)`
  1575|         0|            0|            0|  0.00%|
  1576|         0|            0|            0|  0.00%|    See :class:`~torch.nn.Tanhshrink` for more details.
  1577|         0|            0|            0|  0.00%|    """
  1578|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):
  1579|         0|            0|            0|  0.00%|        return handle_torch_function(tanhshrink, (input,), input)
  1580|         0|            0|            0|  0.00%|    return input - input.tanh()
  1581|         0|            0|            0|  0.00%|
  1582|         0|            0|            0|  0.00%|
  1583|         0|            0|            0|  0.00%|def softsign(input):
  1584|         0|            0|            0|  0.00%|    r"""softsign(input) -> Tensor
  1585|         0|            0|            0|  0.00%|
  1586|         0|            0|            0|  0.00%|    Applies element-wise, the function :math:`\text{SoftSign}(x) = \frac{x}{1 + |x|}`
  1587|         0|            0|            0|  0.00%|
  1588|         0|            0|            0|  0.00%|    See :class:`~torch.nn.Softsign` for more details.
  1589|         0|            0|            0|  0.00%|    """
  1590|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):
  1591|         0|            0|            0|  0.00%|        return handle_torch_function(softsign, (input,), input)
  1592|         0|            0|            0|  0.00%|    return input / (input.abs() + 1)
  1593|         0|            0|            0|  0.00%|
  1594|         0|            0|            0|  0.00%|
  1595|         0|            0|            0|  0.00%|softplus = _add_docstr(
  1596|         0|            0|            0|  0.00%|    torch._C._nn.softplus,
  1597|         0|            0|            0|  0.00%|    r"""
  1598|         0|            0|            0|  0.00%|softplus(input, beta=1, threshold=20) -> Tensor
  1599|         0|            0|            0|  0.00%|
  1600|         0|            0|            0|  0.00%|Applies element-wise, the function :math:`\text{Softplus}(x) = \frac{1}{\beta} * \log(1 + \exp(\beta * x))`.
  1601|         0|            0|            0|  0.00%|
  1602|         0|            0|            0|  0.00%|For numerical stability the implementation reverts to the linear function
  1603|         0|            0|            0|  0.00%|when :math:`input \times \beta > threshold`.
  1604|         0|            0|            0|  0.00%|
  1605|         0|            0|            0|  0.00%|See :class:`~torch.nn.Softplus` for more details.
  1606|         0|            0|            0|  0.00%|""",
  1607|         0|            0|            0|  0.00%|)
  1608|         0|            0|            0|  0.00%|
  1609|         0|            0|            0|  0.00%|
  1610|         0|            0|            0|  0.00%|def _get_softmax_dim(name: str, ndim: int, stacklevel: int) -> int:
  1611|         0|            0|            0|  0.00%|    warnings.warn(
  1612|         0|            0|            0|  0.00%|        "Implicit dimension choice for {} has been deprecated. "
  1613|         0|            0|            0|  0.00%|        "Change the call to include dim=X as an argument.".format(name),
  1614|         0|            0|            0|  0.00%|        stacklevel=stacklevel,
  1615|         0|            0|            0|  0.00%|    )
  1616|         0|            0|            0|  0.00%|    if ndim == 0 or ndim == 1 or ndim == 3:
  1617|         0|            0|            0|  0.00%|        ret = 0
  1618|         0|            0|            0|  0.00%|    else:
  1619|         0|            0|            0|  0.00%|        ret = 1
  1620|         0|            0|            0|  0.00%|    return ret
  1621|         0|            0|            0|  0.00%|
  1622|         0|            0|            0|  0.00%|
  1623|         0|            0|            0|  0.00%|def softmin(input: Tensor, dim: Optional[int] = None, _stacklevel: int = 3, dtype: Optional[int] = None) -> Tensor:
  1624|         0|            0|            0|  0.00%|    r"""Applies a softmin function.
  1625|         0|            0|            0|  0.00%|
  1626|         0|            0|            0|  0.00%|    Note that :math:`\text{Softmin}(x) = \text{Softmax}(-x)`. See softmax definition for mathematical formula.
  1627|         0|            0|            0|  0.00%|
  1628|         0|            0|            0|  0.00%|    See :class:`~torch.nn.Softmin` for more details.
  1629|         0|            0|            0|  0.00%|
  1630|         0|            0|            0|  0.00%|    Args:
  1631|         0|            0|            0|  0.00%|        input (Tensor): input
  1632|         0|            0|            0|  0.00%|        dim (int): A dimension along which softmin will be computed (so every slice
  1633|         0|            0|            0|  0.00%|            along dim will sum to 1).
  1634|         0|            0|            0|  0.00%|        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.
  1635|         0|            0|            0|  0.00%|          If specified, the input tensor is casted to :attr:`dtype` before the operation
  1636|         0|            0|            0|  0.00%|          is performed. This is useful for preventing data type overflows. Default: None.
  1637|         0|            0|            0|  0.00%|    """
  1638|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):
  1639|         0|            0|            0|  0.00%|        return handle_torch_function(softmin, (input,), input, dim=dim, _stacklevel=_stacklevel, dtype=dtype)
  1640|         0|            0|            0|  0.00%|    if dim is None:
  1641|         0|            0|            0|  0.00%|        dim = _get_softmax_dim("softmin", input.dim(), _stacklevel)
  1642|         0|            0|            0|  0.00%|    if dtype is None:
  1643|         0|            0|            0|  0.00%|        ret = (-input).softmax(dim)
  1644|         0|            0|            0|  0.00%|    else:
  1645|         0|            0|            0|  0.00%|        ret = (-input).softmax(dim, dtype=dtype)
  1646|         0|            0|            0|  0.00%|    return ret
  1647|         0|            0|            0|  0.00%|
  1648|         0|            0|            0|  0.00%|
  1649|         0|            0|            0|  0.00%|def softmax(input: Tensor, dim: Optional[int] = None, _stacklevel: int = 3, dtype: Optional[int] = None) -> Tensor:
  1650|         0|            0|            0|  0.00%|    r"""Applies a softmax function.
  1651|         0|            0|            0|  0.00%|
  1652|         0|            0|            0|  0.00%|    Softmax is defined as:
  1653|         0|            0|            0|  0.00%|
  1654|         0|            0|            0|  0.00%|    :math:`\text{Softmax}(x_{i}) = \frac{\exp(x_i)}{\sum_j \exp(x_j)}`
  1655|         0|            0|            0|  0.00%|
  1656|         0|            0|            0|  0.00%|    It is applied to all slices along dim, and will re-scale them so that the elements
  1657|         0|            0|            0|  0.00%|    lie in the range `[0, 1]` and sum to 1.
  1658|         0|            0|            0|  0.00%|
  1659|         0|            0|            0|  0.00%|    See :class:`~torch.nn.Softmax` for more details.
  1660|         0|            0|            0|  0.00%|
  1661|         0|            0|            0|  0.00%|    Args:
  1662|         0|            0|            0|  0.00%|        input (Tensor): input
  1663|         0|            0|            0|  0.00%|        dim (int): A dimension along which softmax will be computed.
  1664|         0|            0|            0|  0.00%|        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.
  1665|         0|            0|            0|  0.00%|          If specified, the input tensor is casted to :attr:`dtype` before the operation
  1666|         0|            0|            0|  0.00%|          is performed. This is useful for preventing data type overflows. Default: None.
  1667|         0|            0|            0|  0.00%|
  1668|         0|            0|            0|  0.00%|    .. note::
  1669|         0|            0|            0|  0.00%|        This function doesn't work directly with NLLLoss,
  1670|         0|            0|            0|  0.00%|        which expects the Log to be computed between the Softmax and itself.
  1671|         0|            0|            0|  0.00%|        Use log_softmax instead (it's faster and has better numerical properties).
  1672|         0|            0|            0|  0.00%|
  1673|         0|            0|            0|  0.00%|    """
  1674|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):
  1675|         0|            0|            0|  0.00%|        return handle_torch_function(softmax, (input,), input, dim=dim, _stacklevel=_stacklevel, dtype=dtype)
  1676|         0|            0|            0|  0.00%|    if dim is None:
  1677|         0|            0|            0|  0.00%|        dim = _get_softmax_dim("softmax", input.dim(), _stacklevel)
  1678|         0|            0|            0|  0.00%|    if dtype is None:
  1679|         0|            0|            0|  0.00%|        ret = input.softmax(dim)
  1680|         0|            0|            0|  0.00%|    else:
  1681|         0|            0|            0|  0.00%|        ret = input.softmax(dim, dtype=dtype)
  1682|         0|            0|            0|  0.00%|    return ret
  1683|         0|            0|            0|  0.00%|
  1684|         0|            0|            0|  0.00%|
  1685|         0|            0|            0|  0.00%|def gumbel_softmax(logits: Tensor, tau: float = 1, hard: bool = False, eps: float = 1e-10, dim: int = -1) -> Tensor:
  1686|         0|            0|            0|  0.00%|    r"""
  1687|         0|            0|            0|  0.00%|    Samples from the Gumbel-Softmax distribution (`Link 1`_  `Link 2`_) and optionally discretizes.
  1688|         0|            0|            0|  0.00%|
  1689|         0|            0|            0|  0.00%|    Args:
  1690|         0|            0|            0|  0.00%|      logits: `[..., num_features]` unnormalized log probabilities
  1691|         0|            0|            0|  0.00%|      tau: non-negative scalar temperature
  1692|         0|            0|            0|  0.00%|      hard: if ``True``, the returned samples will be discretized as one-hot vectors,
  1693|         0|            0|            0|  0.00%|            but will be differentiated as if it is the soft sample in autograd
  1694|         0|            0|            0|  0.00%|      dim (int): A dimension along which softmax will be computed. Default: -1.
  1695|         0|            0|            0|  0.00%|
  1696|         0|            0|            0|  0.00%|    Returns:
  1697|         0|            0|            0|  0.00%|      Sampled tensor of same shape as `logits` from the Gumbel-Softmax distribution.
  1698|         0|            0|            0|  0.00%|      If ``hard=True``, the returned samples will be one-hot, otherwise they will
  1699|         0|            0|            0|  0.00%|      be probability distributions that sum to 1 across `dim`.
  1700|         0|            0|            0|  0.00%|
  1701|         0|            0|            0|  0.00%|    .. note::
  1702|         0|            0|            0|  0.00%|      This function is here for legacy reasons, may be removed from nn.Functional in the future.
  1703|         0|            0|            0|  0.00%|
  1704|         0|            0|            0|  0.00%|    .. note::
  1705|         0|            0|            0|  0.00%|      The main trick for `hard` is to do  `y_hard - y_soft.detach() + y_soft`
  1706|         0|            0|            0|  0.00%|
  1707|         0|            0|            0|  0.00%|      It achieves two things:
  1708|         0|            0|            0|  0.00%|      - makes the output value exactly one-hot
  1709|         0|            0|            0|  0.00%|      (since we add then subtract y_soft value)
  1710|         0|            0|            0|  0.00%|      - makes the gradient equal to y_soft gradient
  1711|         0|            0|            0|  0.00%|      (since we strip all other gradients)
  1712|         0|            0|            0|  0.00%|
  1713|         0|            0|            0|  0.00%|    Examples::
  1714|         0|            0|            0|  0.00%|        >>> logits = torch.randn(20, 32)
  1715|         0|            0|            0|  0.00%|        >>> # Sample soft categorical using reparametrization trick:
  1716|         0|            0|            0|  0.00%|        >>> F.gumbel_softmax(logits, tau=1, hard=False)
  1717|         0|            0|            0|  0.00%|        >>> # Sample hard categorical using "Straight-through" trick:
  1718|         0|            0|            0|  0.00%|        >>> F.gumbel_softmax(logits, tau=1, hard=True)
  1719|         0|            0|            0|  0.00%|
  1720|         0|            0|            0|  0.00%|    .. _Link 1:
  1721|         0|            0|            0|  0.00%|        https://arxiv.org/abs/1611.00712
  1722|         0|            0|            0|  0.00%|    .. _Link 2:
  1723|         0|            0|            0|  0.00%|        https://arxiv.org/abs/1611.01144
  1724|         0|            0|            0|  0.00%|    """
  1725|         0|            0|            0|  0.00%|    if has_torch_function_unary(logits):
  1726|         0|            0|            0|  0.00%|        return handle_torch_function(gumbel_softmax, (logits,), logits, tau=tau, hard=hard, eps=eps, dim=dim)
  1727|         0|            0|            0|  0.00%|    if eps != 1e-10:
  1728|         0|            0|            0|  0.00%|        warnings.warn("`eps` parameter is deprecated and has no effect.")
  1729|         0|            0|            0|  0.00%|
  1730|         0|            0|            0|  0.00%|    gumbels = (
  1731|         0|            0|            0|  0.00%|        -torch.empty_like(logits, memory_format=torch.legacy_contiguous_format).exponential_().log()
  1732|         0|            0|            0|  0.00%|    )  # ~Gumbel(0,1)
  1733|         0|            0|            0|  0.00%|    gumbels = (logits + gumbels) / tau  # ~Gumbel(logits,tau)
  1734|         0|            0|            0|  0.00%|    y_soft = gumbels.softmax(dim)
  1735|         0|            0|            0|  0.00%|
  1736|         0|            0|            0|  0.00%|    if hard:
  1737|         0|            0|            0|  0.00%|        # Straight through.
  1738|         0|            0|            0|  0.00%|        index = y_soft.max(dim, keepdim=True)[1]
  1739|         0|            0|            0|  0.00%|        y_hard = torch.zeros_like(logits, memory_format=torch.legacy_contiguous_format).scatter_(dim, index, 1.0)
  1740|         0|            0|            0|  0.00%|        ret = y_hard - y_soft.detach() + y_soft
  1741|         0|            0|            0|  0.00%|    else:
  1742|         0|            0|            0|  0.00%|        # Reparametrization trick.
  1743|         0|            0|            0|  0.00%|        ret = y_soft
  1744|         0|            0|            0|  0.00%|    return ret
  1745|         0|            0|            0|  0.00%|
  1746|         0|            0|            0|  0.00%|
  1747|         0|            0|            0|  0.00%|def log_softmax(input: Tensor, dim: Optional[int] = None, _stacklevel: int = 3, dtype: Optional[int] = None) -> Tensor:
  1748|         0|            0|            0|  0.00%|    r"""Applies a softmax followed by a logarithm.
  1749|         0|            0|            0|  0.00%|
  1750|         0|            0|            0|  0.00%|    While mathematically equivalent to log(softmax(x)), doing these two
  1751|         0|            0|            0|  0.00%|    operations separately is slower, and numerically unstable. This function
  1752|         0|            0|            0|  0.00%|    uses an alternative formulation to compute the output and gradient correctly.
  1753|         0|            0|            0|  0.00%|
  1754|         0|            0|            0|  0.00%|    See :class:`~torch.nn.LogSoftmax` for more details.
  1755|         0|            0|            0|  0.00%|
  1756|         0|            0|            0|  0.00%|    Args:
  1757|         0|            0|            0|  0.00%|        input (Tensor): input
  1758|         0|            0|            0|  0.00%|        dim (int): A dimension along which log_softmax will be computed.
  1759|         0|            0|            0|  0.00%|        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.
  1760|         0|            0|            0|  0.00%|          If specified, the input tensor is casted to :attr:`dtype` before the operation
  1761|         0|            0|            0|  0.00%|          is performed. This is useful for preventing data type overflows. Default: None.
  1762|         0|            0|            0|  0.00%|    """
  1763|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):
  1764|         0|            0|            0|  0.00%|        return handle_torch_function(log_softmax, (input,), input, dim=dim, _stacklevel=_stacklevel, dtype=dtype)
  1765|         0|            0|            0|  0.00%|    if dim is None:
  1766|         0|            0|            0|  0.00%|        dim = _get_softmax_dim("log_softmax", input.dim(), _stacklevel)
  1767|         0|            0|            0|  0.00%|    if dtype is None:
  1768|         0|            0|            0|  0.00%|        ret = input.log_softmax(dim)
  1769|         0|            0|            0|  0.00%|    else:
  1770|         0|            0|            0|  0.00%|        ret = input.log_softmax(dim, dtype=dtype)
  1771|         0|            0|            0|  0.00%|    return ret
  1772|         0|            0|            0|  0.00%|
  1773|         0|            0|            0|  0.00%|
  1774|         0|            0|            0|  0.00%|softshrink = _add_docstr(
  1775|         0|            0|            0|  0.00%|    torch._C._nn.softshrink,
  1776|         0|            0|            0|  0.00%|    r"""
  1777|         0|            0|            0|  0.00%|softshrink(input, lambd=0.5) -> Tensor
  1778|         0|            0|            0|  0.00%|
  1779|         0|            0|            0|  0.00%|Applies the soft shrinkage function elementwise
  1780|         0|            0|            0|  0.00%|
  1781|         0|            0|            0|  0.00%|See :class:`~torch.nn.Softshrink` for more details.
  1782|         0|            0|            0|  0.00%|""",
  1783|         0|            0|            0|  0.00%|)
  1784|         0|            0|            0|  0.00%|
  1785|         0|            0|            0|  0.00%|
  1786|         0|            0|            0|  0.00%|def tanh(input):
  1787|         0|            0|            0|  0.00%|    r"""tanh(input) -> Tensor
  1788|         0|            0|            0|  0.00%|
  1789|         0|            0|            0|  0.00%|    Applies element-wise,
  1790|         0|            0|            0|  0.00%|    :math:`\text{Tanh}(x) = \tanh(x) = \frac{\exp(x) - \exp(-x)}{\exp(x) + \exp(-x)}`
  1791|         0|            0|            0|  0.00%|
  1792|         0|            0|            0|  0.00%|    See :class:`~torch.nn.Tanh` for more details.
  1793|         0|            0|            0|  0.00%|    """
  1794|         0|            0|            0|  0.00%|    warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
  1795|         0|            0|            0|  0.00%|    return input.tanh()
  1796|         0|            0|            0|  0.00%|
  1797|         0|            0|            0|  0.00%|
  1798|         0|            0|            0|  0.00%|def sigmoid(input):
  1799|         0|            0|            0|  0.00%|    r"""sigmoid(input) -> Tensor
  1800|         0|            0|            0|  0.00%|
  1801|         0|            0|            0|  0.00%|    Applies the element-wise function :math:`\text{Sigmoid}(x) = \frac{1}{1 + \exp(-x)}`
  1802|         0|            0|            0|  0.00%|
  1803|         0|            0|            0|  0.00%|    See :class:`~torch.nn.Sigmoid` for more details.
  1804|         0|            0|            0|  0.00%|    """
  1805|         0|            0|            0|  0.00%|    warnings.warn("nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")
  1806|         0|            0|            0|  0.00%|    return input.sigmoid()
  1807|         0|            0|            0|  0.00%|
  1808|         0|            0|            0|  0.00%|
  1809|         0|            0|            0|  0.00%|def hardsigmoid(input: Tensor, inplace: bool = False) -> Tensor:
  1810|         0|            0|            0|  0.00%|    r"""Applies the element-wise function
  1811|         0|            0|            0|  0.00%|
  1812|         0|            0|            0|  0.00%|    .. math::
  1813|         0|            0|            0|  0.00%|        \text{Hardsigmoid}(x) = \begin{cases}
  1814|         0|            0|            0|  0.00%|            0 & \text{if~} x \le -3, \\
  1815|         0|            0|            0|  0.00%|            1 & \text{if~} x \ge +3, \\
  1816|         0|            0|            0|  0.00%|            x / 6 + 1 / 2 & \text{otherwise}
  1817|         0|            0|            0|  0.00%|        \end{cases}
  1818|         0|            0|            0|  0.00%|
  1819|         0|            0|            0|  0.00%|    Args:
  1820|         0|            0|            0|  0.00%|        inplace: If set to ``True``, will do this operation in-place. Default: ``False``
  1821|         0|            0|            0|  0.00%|
  1822|         0|            0|            0|  0.00%|    See :class:`~torch.nn.Hardsigmoid` for more details.
  1823|         0|            0|            0|  0.00%|    """
  1824|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):
  1825|         0|            0|            0|  0.00%|        return handle_torch_function(hardsigmoid, (input,), input, inplace=inplace)
  1826|         0|            0|            0|  0.00%|    if inplace:
  1827|         0|            0|            0|  0.00%|        return torch._C._nn.hardsigmoid_(input)
  1828|         0|            0|            0|  0.00%|    return torch._C._nn.hardsigmoid(input)
  1829|         0|            0|            0|  0.00%|
  1830|         0|            0|            0|  0.00%|
  1831|       100|  0.000584364|  5.84364e-06|  0.00%|def linear(input: Tensor, weight: Tensor, bias: Optional[Tensor] = None) -> Tensor:
  1832|         0|            0|            0|  0.00%|    r"""
  1833|         0|            0|            0|  0.00%|    Applies a linear transformation to the incoming data: :math:`y = xA^T + b`.
  1834|         0|            0|            0|  0.00%|
  1835|         0|            0|            0|  0.00%|    This operator supports :ref:`TensorFloat32<tf32_on_ampere>`.
  1836|         0|            0|            0|  0.00%|
  1837|         0|            0|            0|  0.00%|    Shape:
  1838|         0|            0|            0|  0.00%|
  1839|         0|            0|            0|  0.00%|        - Input: :math:`(N, *, in\_features)` N is the batch size, `*` means any number of
  1840|         0|            0|            0|  0.00%|          additional dimensions
  1841|         0|            0|            0|  0.00%|        - Weight: :math:`(out\_features, in\_features)`
  1842|         0|            0|            0|  0.00%|        - Bias: :math:`(out\_features)`
  1843|         0|            0|            0|  0.00%|        - Output: :math:`(N, *, out\_features)`
  1844|         0|            0|            0|  0.00%|    """
  1845|       100|  0.000846624|  8.46624e-06|  0.00%|    if has_torch_function_variadic(input, weight):
  1846|         0|            0|            0|  0.00%|        return handle_torch_function(linear, (input, weight), input, weight, bias=bias)
  1847|       100|   0.00930262|  9.30262e-05|  0.02%|    return torch._C._nn.linear(input, weight, bias)
  1848|         0|            0|            0|  0.00%|
  1849|         0|            0|            0|  0.00%|
  1850|         0|            0|            0|  0.00%|def bilinear(input1: Tensor, input2: Tensor, weight: Tensor, bias: Optional[Tensor] = None) -> Tensor:
  1851|         0|            0|            0|  0.00%|    r"""
  1852|         0|            0|            0|  0.00%|    Applies a bilinear transformation to the incoming data:
  1853|         0|            0|            0|  0.00%|    :math:`y = x_1^T A x_2 + b`
  1854|         0|            0|            0|  0.00%|
  1855|         0|            0|            0|  0.00%|    Shape:
  1856|         0|            0|            0|  0.00%|
  1857|         0|            0|            0|  0.00%|        - input1: :math:`(N, *, H_{in1})` where :math:`H_{in1}=\text{in1\_features}`
  1858|         0|            0|            0|  0.00%|          and :math:`*` means any number of additional dimensions.
  1859|         0|            0|            0|  0.00%|          All but the last dimension of the inputs should be the same.
  1860|         0|            0|            0|  0.00%|        - input2: :math:`(N, *, H_{in2})` where :math:`H_{in2}=\text{in2\_features}`
  1861|         0|            0|            0|  0.00%|        - weight: :math:`(\text{out\_features}, \text{in1\_features},
  1862|         0|            0|            0|  0.00%|          \text{in2\_features})`
  1863|         0|            0|            0|  0.00%|        - bias: :math:`(\text{out\_features})`
  1864|         0|            0|            0|  0.00%|        - output: :math:`(N, *, H_{out})` where :math:`H_{out}=\text{out\_features}`
  1865|         0|            0|            0|  0.00%|          and all but the last dimension are the same shape as the input.
  1866|         0|            0|            0|  0.00%|    """
  1867|         0|            0|            0|  0.00%|    if has_torch_function_variadic(input1, input2, weight):
  1868|         0|            0|            0|  0.00%|        return handle_torch_function(
  1869|         0|            0|            0|  0.00%|            bilinear,
  1870|         0|            0|            0|  0.00%|            (input1, input2, weight),
  1871|         0|            0|            0|  0.00%|            input1, input2, weight,
  1872|         0|            0|            0|  0.00%|            bias=bias
  1873|         0|            0|            0|  0.00%|        )
  1874|         0|            0|            0|  0.00%|    return torch.bilinear(input1, input2, weight, bias)
  1875|         0|            0|            0|  0.00%|
  1876|         0|            0|            0|  0.00%|
  1877|         0|            0|            0|  0.00%|def silu(input: Tensor, inplace: bool = False) -> Tensor:
  1878|         0|            0|            0|  0.00%|    r"""Applies the Sigmoid Linear Unit (SiLU) function, element-wise.
  1879|         0|            0|            0|  0.00%|    The SiLU function is also known as the swish function.
  1880|         0|            0|            0|  0.00%|
  1881|         0|            0|            0|  0.00%|    .. math::
  1882|         0|            0|            0|  0.00%|        \text{silu}(x) = x * \sigma(x), \text{where } \sigma(x) \text{ is the logistic sigmoid.}
  1883|         0|            0|            0|  0.00%|
  1884|         0|            0|            0|  0.00%|    .. note::
  1885|         0|            0|            0|  0.00%|        See `Gaussian Error Linear Units (GELUs) <https://arxiv.org/abs/1606.08415>`_
  1886|         0|            0|            0|  0.00%|        where the SiLU (Sigmoid Linear Unit) was originally coined, and see
  1887|         0|            0|            0|  0.00%|        `Sigmoid-Weighted Linear Units for Neural Network Function Approximation
  1888|         0|            0|            0|  0.00%|        in Reinforcement Learning <https://arxiv.org/abs/1702.03118>`_ and `Swish:
  1889|         0|            0|            0|  0.00%|        a Self-Gated Activation Function <https://arxiv.org/abs/1710.05941v1>`_
  1890|         0|            0|            0|  0.00%|        where the SiLU was experimented with later.
  1891|         0|            0|            0|  0.00%|
  1892|         0|            0|            0|  0.00%|    See :class:`~torch.nn.SiLU` for more details.
  1893|         0|            0|            0|  0.00%|    """
  1894|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):
  1895|         0|            0|            0|  0.00%|        return handle_torch_function(silu, (input,), input, inplace=inplace)
  1896|         0|            0|            0|  0.00%|    if inplace:
  1897|         0|            0|            0|  0.00%|        return torch._C._nn.silu_(input)
  1898|         0|            0|            0|  0.00%|    return torch._C._nn.silu(input)
  1899|         0|            0|            0|  0.00%|
  1900|         0|            0|            0|  0.00%|
  1901|         0|            0|            0|  0.00%|def mish(input: Tensor, inplace: bool = False) -> Tensor:
  1902|         0|            0|            0|  0.00%|    r"""Applies the Mish function, element-wise.
  1903|         0|            0|            0|  0.00%|    Mish: A Self Regularized Non-Monotonic Neural Activation Function.
  1904|         0|            0|            0|  0.00%|
  1905|         0|            0|            0|  0.00%|    .. math::
  1906|         0|            0|            0|  0.00%|        \text{Mish}(x) = x * \text{Tanh}(\text{Softplus}(x))
  1907|         0|            0|            0|  0.00%|
  1908|         0|            0|            0|  0.00%|    .. note::
  1909|         0|            0|            0|  0.00%|        See `Mish: A Self Regularized Non-Monotonic Neural Activation Function <https://arxiv.org/abs/1908.08681>`_
  1910|         0|            0|            0|  0.00%|
  1911|         0|            0|            0|  0.00%|    See :class:`~torch.nn.Mish` for more details.
  1912|         0|            0|            0|  0.00%|    """
  1913|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):
  1914|         0|            0|            0|  0.00%|        return handle_torch_function(mish, (input,), input, inplace=inplace)
  1915|         0|            0|            0|  0.00%|    if inplace:
  1916|         0|            0|            0|  0.00%|        return torch._C._nn.mish_(input)
  1917|         0|            0|            0|  0.00%|    return torch._C._nn.mish(input)
  1918|         0|            0|            0|  0.00%|
  1919|         0|            0|            0|  0.00%|
  1920|         0|            0|            0|  0.00%|def hardswish(input: Tensor, inplace: bool = False) -> Tensor:
  1921|         0|            0|            0|  0.00%|    r"""Applies the hardswish function, element-wise, as described in the paper:
  1922|         0|            0|            0|  0.00%|
  1923|         0|            0|            0|  0.00%|    `Searching for MobileNetV3`_.
  1924|         0|            0|            0|  0.00%|
  1925|         0|            0|            0|  0.00%|    .. math::
  1926|         0|            0|            0|  0.00%|        \text{Hardswish}(x) = \begin{cases}
  1927|         0|            0|            0|  0.00%|            0 & \text{if~} x \le -3, \\
  1928|         0|            0|            0|  0.00%|            x & \text{if~} x \ge +3, \\
  1929|         0|            0|            0|  0.00%|            x \cdot (x + 3) /6 & \text{otherwise}
  1930|         0|            0|            0|  0.00%|        \end{cases}
  1931|         0|            0|            0|  0.00%|
  1932|         0|            0|            0|  0.00%|    See :class:`~torch.nn.Hardswish` for more details.
  1933|         0|            0|            0|  0.00%|
  1934|         0|            0|            0|  0.00%|    .. _`Searching for MobileNetV3`:
  1935|         0|            0|            0|  0.00%|        https://arxiv.org/abs/1905.02244
  1936|         0|            0|            0|  0.00%|    """
  1937|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):
  1938|         0|            0|            0|  0.00%|        return handle_torch_function(hardswish, (input,), input, inplace=inplace)
  1939|         0|            0|            0|  0.00%|    if inplace:
  1940|         0|            0|            0|  0.00%|        return torch._C._nn.hardswish_(input)
  1941|         0|            0|            0|  0.00%|    return torch._C._nn.hardswish(input)
  1942|         0|            0|            0|  0.00%|
  1943|         0|            0|            0|  0.00%|
  1944|         0|            0|            0|  0.00%|def _no_grad_embedding_renorm_(weight: Tensor, input: Tensor, max_norm: float, norm_type: float) -> Tensor:
  1945|         0|            0|            0|  0.00%|    with torch.no_grad():
  1946|         0|            0|            0|  0.00%|        torch.embedding_renorm_(weight, input, max_norm, norm_type)
  1947|         0|            0|            0|  0.00%|
  1948|         0|            0|            0|  0.00%|
  1949|         0|            0|            0|  0.00%|def embedding(
  1950|         0|            0|            0|  0.00%|    input: Tensor,
  1951|         0|            0|            0|  0.00%|    weight: Tensor,
  1952|         0|            0|            0|  0.00%|    padding_idx: Optional[int] = None,
  1953|         0|            0|            0|  0.00%|    max_norm: Optional[float] = None,
  1954|         0|            0|            0|  0.00%|    norm_type: float = 2.0,
  1955|         0|            0|            0|  0.00%|    scale_grad_by_freq: bool = False,
  1956|         0|            0|            0|  0.00%|    sparse: bool = False,
  1957|         0|            0|            0|  0.00%|) -> Tensor:
  1958|         0|            0|            0|  0.00%|    r"""A simple lookup table that looks up embeddings in a fixed dictionary and size.
  1959|         0|            0|            0|  0.00%|
  1960|         0|            0|            0|  0.00%|    This module is often used to retrieve word embeddings using indices.
  1961|         0|            0|            0|  0.00%|    The input to the module is a list of indices, and the embedding matrix,
  1962|         0|            0|            0|  0.00%|    and the output is the corresponding word embeddings.
  1963|         0|            0|            0|  0.00%|
  1964|         0|            0|            0|  0.00%|    See :class:`torch.nn.Embedding` for more details.
  1965|         0|            0|            0|  0.00%|
  1966|         0|            0|            0|  0.00%|    Args:
  1967|         0|            0|            0|  0.00%|        input (LongTensor): Tensor containing indices into the embedding matrix
  1968|         0|            0|            0|  0.00%|        weight (Tensor): The embedding matrix with number of rows equal to the maximum possible index + 1,
  1969|         0|            0|            0|  0.00%|            and number of columns equal to the embedding size
  1970|         0|            0|            0|  0.00%|        padding_idx (int, optional): If specified, the entries at :attr:`padding_idx` do not contribute to the gradient;
  1971|         0|            0|            0|  0.00%|                                     therefore, the embedding vector at :attr:`padding_idx` is not updated during training,
  1972|         0|            0|            0|  0.00%|                                     i.e. it remains as a fixed "pad".
  1973|         0|            0|            0|  0.00%|        max_norm (float, optional): If given, each embedding vector with norm larger than :attr:`max_norm`
  1974|         0|            0|            0|  0.00%|                                    is renormalized to have norm :attr:`max_norm`.
  1975|         0|            0|            0|  0.00%|                                    Note: this will modify :attr:`weight` in-place.
  1976|         0|            0|            0|  0.00%|        norm_type (float, optional): The p of the p-norm to compute for the :attr:`max_norm` option. Default ``2``.
  1977|         0|            0|            0|  0.00%|        scale_grad_by_freq (boolean, optional): If given, this will scale gradients by the inverse of frequency of
  1978|         0|            0|            0|  0.00%|                                                the words in the mini-batch. Default ``False``.
  1979|         0|            0|            0|  0.00%|        sparse (bool, optional): If ``True``, gradient w.r.t. :attr:`weight` will be a sparse tensor. See Notes under
  1980|         0|            0|            0|  0.00%|                                 :class:`torch.nn.Embedding` for more details regarding sparse gradients.
  1981|         0|            0|            0|  0.00%|
  1982|         0|            0|            0|  0.00%|    Shape:
  1983|         0|            0|            0|  0.00%|        - Input: LongTensor of arbitrary shape containing the indices to extract
  1984|         0|            0|            0|  0.00%|        - Weight: Embedding matrix of floating point type with shape `(V, embedding_dim)`,
  1985|         0|            0|            0|  0.00%|          where V = maximum index + 1 and embedding_dim = the embedding size
  1986|         0|            0|            0|  0.00%|        - Output: `(*, embedding_dim)`, where `*` is the input shape
  1987|         0|            0|            0|  0.00%|
  1988|         0|            0|            0|  0.00%|    Examples::
  1989|         0|            0|            0|  0.00%|
  1990|         0|            0|            0|  0.00%|        >>> # a batch of 2 samples of 4 indices each
  1991|         0|            0|            0|  0.00%|        >>> input = torch.tensor([[1,2,4,5],[4,3,2,9]])
  1992|         0|            0|            0|  0.00%|        >>> # an embedding matrix containing 10 tensors of size 3
  1993|         0|            0|            0|  0.00%|        >>> embedding_matrix = torch.rand(10, 3)
  1994|         0|            0|            0|  0.00%|        >>> F.embedding(input, embedding_matrix)
  1995|         0|            0|            0|  0.00%|        tensor([[[ 0.8490,  0.9625,  0.6753],
  1996|         0|            0|            0|  0.00%|                 [ 0.9666,  0.7761,  0.6108],
  1997|         0|            0|            0|  0.00%|                 [ 0.6246,  0.9751,  0.3618],
  1998|         0|            0|            0|  0.00%|                 [ 0.4161,  0.2419,  0.7383]],
  1999|         0|            0|            0|  0.00%|
  2000|         0|            0|            0|  0.00%|                [[ 0.6246,  0.9751,  0.3618],
  2001|         0|            0|            0|  0.00%|                 [ 0.0237,  0.7794,  0.0528],
  2002|         0|            0|            0|  0.00%|                 [ 0.9666,  0.7761,  0.6108],
  2003|         0|            0|            0|  0.00%|                 [ 0.3385,  0.8612,  0.1867]]])
  2004|         0|            0|            0|  0.00%|
  2005|         0|            0|            0|  0.00%|        >>> # example with padding_idx
  2006|         0|            0|            0|  0.00%|        >>> weights = torch.rand(10, 3)
  2007|         0|            0|            0|  0.00%|        >>> weights[0, :].zero_()
  2008|         0|            0|            0|  0.00%|        >>> embedding_matrix = weights
  2009|         0|            0|            0|  0.00%|        >>> input = torch.tensor([[0,2,0,5]])
  2010|         0|            0|            0|  0.00%|        >>> F.embedding(input, embedding_matrix, padding_idx=0)
  2011|         0|            0|            0|  0.00%|        tensor([[[ 0.0000,  0.0000,  0.0000],
  2012|         0|            0|            0|  0.00%|                 [ 0.5609,  0.5384,  0.8720],
  2013|         0|            0|            0|  0.00%|                 [ 0.0000,  0.0000,  0.0000],
  2014|         0|            0|            0|  0.00%|                 [ 0.6262,  0.2438,  0.7471]]])
  2015|         0|            0|            0|  0.00%|    """
  2016|         0|            0|            0|  0.00%|
  2017|         0|            0|            0|  0.00%|    if has_torch_function_variadic(input, weight):
  2018|         0|            0|            0|  0.00%|        return handle_torch_function(
  2019|         0|            0|            0|  0.00%|            embedding, (input, weight),
  2020|         0|            0|            0|  0.00%|            input, weight, padding_idx, max_norm, norm_type,
  2021|         0|            0|            0|  0.00%|            scale_grad_by_freq, sparse
  2022|         0|            0|            0|  0.00%|        )
  2023|         0|            0|            0|  0.00%|    if padding_idx is not None:
  2024|         0|            0|            0|  0.00%|        if padding_idx > 0:
  2025|         0|            0|            0|  0.00%|            assert padding_idx < weight.size(0), "Padding_idx must be within num_embeddings"
  2026|         0|            0|            0|  0.00%|        elif padding_idx < 0:
  2027|         0|            0|            0|  0.00%|            assert padding_idx >= -weight.size(0), "Padding_idx must be within num_embeddings"
  2028|         0|            0|            0|  0.00%|            padding_idx = weight.size(0) + padding_idx
  2029|         0|            0|            0|  0.00%|    else:
  2030|         0|            0|            0|  0.00%|        padding_idx = -1
  2031|         0|            0|            0|  0.00%|    if max_norm is not None:
  2032|         0|            0|            0|  0.00%|        # Note [embedding_renorm contiguous]
  2033|         0|            0|            0|  0.00%|        # `embedding_renorm_` will call .contiguous() on input anyways, so we
  2034|         0|            0|            0|  0.00%|        # call it here and take advantage of the improved locality in the
  2035|         0|            0|            0|  0.00%|        # `embedding` call below too.
  2036|         0|            0|            0|  0.00%|        input = input.contiguous()
  2037|         0|            0|            0|  0.00%|        # Note [embedding_renorm set_grad_enabled]
  2038|         0|            0|            0|  0.00%|        # XXX: equivalent to
  2039|         0|            0|            0|  0.00%|        # with torch.no_grad():
  2040|         0|            0|            0|  0.00%|        #   torch.embedding_renorm_
  2041|         0|            0|            0|  0.00%|        # remove once script supports set_grad_enabled
  2042|         0|            0|            0|  0.00%|        _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)
  2043|         0|            0|            0|  0.00%|    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
  2044|         0|            0|            0|  0.00%|
  2045|         0|            0|            0|  0.00%|
  2046|         0|            0|            0|  0.00%|def embedding_bag(
  2047|         0|            0|            0|  0.00%|    input: Tensor,
  2048|         0|            0|            0|  0.00%|    weight: Tensor,
  2049|         0|            0|            0|  0.00%|    offsets: Optional[Tensor] = None,
  2050|         0|            0|            0|  0.00%|    max_norm: Optional[float] = None,
  2051|         0|            0|            0|  0.00%|    norm_type: float = 2,
  2052|         0|            0|            0|  0.00%|    scale_grad_by_freq: bool = False,
  2053|         0|            0|            0|  0.00%|    mode: str = "mean",
  2054|         0|            0|            0|  0.00%|    sparse: bool = False,
  2055|         0|            0|            0|  0.00%|    per_sample_weights: Optional[Tensor] = None,
  2056|         0|            0|            0|  0.00%|    include_last_offset: bool = False,
  2057|         0|            0|            0|  0.00%|    padding_idx: Optional[int] = None,
  2058|         0|            0|            0|  0.00%|) -> Tensor:
  2059|         0|            0|            0|  0.00%|    r"""Computes sums, means or maxes of `bags` of embeddings, without instantiating the
  2060|         0|            0|            0|  0.00%|    intermediate embeddings.
  2061|         0|            0|            0|  0.00%|
  2062|         0|            0|            0|  0.00%|    See :class:`torch.nn.EmbeddingBag` for more details.
  2063|         0|            0|            0|  0.00%|
  2064|         0|            0|            0|  0.00%|    Note:
  2065|         0|            0|            0|  0.00%|        {backward_reproducibility_note}
  2066|         0|            0|            0|  0.00%|
  2067|         0|            0|            0|  0.00%|    Args:
  2068|         0|            0|            0|  0.00%|        input (LongTensor): Tensor containing bags of indices into the embedding matrix
  2069|         0|            0|            0|  0.00%|        weight (Tensor): The embedding matrix with number of rows equal to the maximum possible index + 1,
  2070|         0|            0|            0|  0.00%|            and number of columns equal to the embedding size
  2071|         0|            0|            0|  0.00%|        offsets (LongTensor, optional): Only used when :attr:`input` is 1D. :attr:`offsets` determines
  2072|         0|            0|            0|  0.00%|                             the starting index position of each bag (sequence) in :attr:`input`.
  2073|         0|            0|            0|  0.00%|        max_norm (float, optional): If given, each embedding vector with norm larger than :attr:`max_norm`
  2074|         0|            0|            0|  0.00%|                                    is renormalized to have norm :attr:`max_norm`.
  2075|         0|            0|            0|  0.00%|                                    Note: this will modify :attr:`weight` in-place.
  2076|         0|            0|            0|  0.00%|        norm_type (float, optional): The ``p`` in the ``p``-norm to compute for the :attr:`max_norm` option.
  2077|         0|            0|            0|  0.00%|                                     Default ``2``.
  2078|         0|            0|            0|  0.00%|        scale_grad_by_freq (boolean, optional): if given, this will scale gradients by the inverse of frequency of
  2079|         0|            0|            0|  0.00%|                                                the words in the mini-batch. Default ``False``.
  2080|         0|            0|            0|  0.00%|                                                Note: this option is not supported when ``mode="max"``.
  2081|         0|            0|            0|  0.00%|        mode (string, optional): ``"sum"``, ``"mean"`` or ``"max"``. Specifies the way to reduce the bag.
  2082|         0|            0|            0|  0.00%|                                 Default: ``"mean"``
  2083|         0|            0|            0|  0.00%|        sparse (bool, optional): if ``True``, gradient w.r.t. :attr:`weight` will be a sparse tensor. See Notes under
  2084|         0|            0|            0|  0.00%|                                 :class:`torch.nn.Embedding` for more details regarding sparse gradients.
  2085|         0|            0|            0|  0.00%|                                 Note: this option is not supported when ``mode="max"``.
  2086|         0|            0|            0|  0.00%|        per_sample_weights (Tensor, optional): a tensor of float / double weights, or None
  2087|         0|            0|            0|  0.00%|            to indicate all weights should be taken to be 1. If specified, :attr:`per_sample_weights`
  2088|         0|            0|            0|  0.00%|            must have exactly the same shape as input and is treated as having the same
  2089|         0|            0|            0|  0.00%|            :attr:`offsets`, if those are not None.
  2090|         0|            0|            0|  0.00%|
  2091|         0|            0|            0|  0.00%|        include_last_offset (bool, optional): if ``True``, the size of offsets is equal to the number of bags + 1.
  2092|         0|            0|            0|  0.00%|            The last element is the size of the input, or the ending index position of the last bag (sequence).
  2093|         0|            0|            0|  0.00%|
  2094|         0|            0|            0|  0.00%|        padding_idx (int, optional): If specified, the entries at :attr:`padding_idx` do not contribute to the
  2095|         0|            0|            0|  0.00%|                                     gradient; therefore, the embedding vector at :attr:`padding_idx` is not updated
  2096|         0|            0|            0|  0.00%|                                     during training, i.e. it remains as a fixed "pad". Note that the embedding
  2097|         0|            0|            0|  0.00%|                                     vector at :attr:`padding_idx` is excluded from the reduction.
  2098|         0|            0|            0|  0.00%|
  2099|         0|            0|            0|  0.00%|    Shape:
  2100|         0|            0|            0|  0.00%|        - :attr:`input` (LongTensor) and :attr:`offsets` (LongTensor, optional)
  2101|         0|            0|            0|  0.00%|
  2102|         0|            0|            0|  0.00%|          - If :attr:`input` is 2D of shape `(B, N)`, it will be treated as ``B`` bags (sequences)
  2103|         0|            0|            0|  0.00%|            each of fixed length ``N``, and this will return ``B`` values aggregated in a way
  2104|         0|            0|            0|  0.00%|            depending on the :attr:`mode`. :attr:`offsets` is ignored and required to be ``None`` in this case.
  2105|         0|            0|            0|  0.00%|
  2106|         0|            0|            0|  0.00%|          - If :attr:`input` is 1D of shape `(N)`, it will be treated as a concatenation of
  2107|         0|            0|            0|  0.00%|            multiple bags (sequences). :attr:`offsets` is required to be a 1D tensor containing
  2108|         0|            0|            0|  0.00%|            the starting index positions of each bag in :attr:`input`. Therefore, for :attr:`offsets`
  2109|         0|            0|            0|  0.00%|            of shape `(B)`, :attr:`input` will be viewed as having ``B`` bags.
  2110|         0|            0|            0|  0.00%|            Empty bags (i.e., having 0-length) will have returned vectors filled by zeros.
  2111|         0|            0|            0|  0.00%|
  2112|         0|            0|            0|  0.00%|        - :attr:`weight` (Tensor): the learnable weights of the module of shape `(num_embeddings, embedding_dim)`
  2113|         0|            0|            0|  0.00%|
  2114|         0|            0|            0|  0.00%|        - :attr:`per_sample_weights` (Tensor, optional). Has the same shape as :attr:`input`.
  2115|         0|            0|            0|  0.00%|
  2116|         0|            0|            0|  0.00%|        - :attr:`output`: aggregated embedding values of shape `(B, embedding_dim)`
  2117|         0|            0|            0|  0.00%|
  2118|         0|            0|            0|  0.00%|    Examples::
  2119|         0|            0|            0|  0.00%|
  2120|         0|            0|            0|  0.00%|        >>> # an Embedding module containing 10 tensors of size 3
  2121|         0|            0|            0|  0.00%|        >>> embedding_matrix = torch.rand(10, 3)
  2122|         0|            0|            0|  0.00%|        >>> # a batch of 2 samples of 4 indices each
  2123|         0|            0|            0|  0.00%|        >>> input = torch.tensor([1,2,4,5,4,3,2,9])
  2124|         0|            0|            0|  0.00%|        >>> offsets = torch.tensor([0,4])
  2125|         0|            0|            0|  0.00%|        >>> F.embedding_bag(input, embedding_matrix, offsets)
  2126|         0|            0|            0|  0.00%|        tensor([[ 0.3397,  0.3552,  0.5545],
  2127|         0|            0|            0|  0.00%|                [ 0.5893,  0.4386,  0.5882]])
  2128|         0|            0|            0|  0.00%|
  2129|         0|            0|            0|  0.00%|        >>> # example with padding_idx
  2130|         0|            0|            0|  0.00%|        >>> embedding_matrix = torch.rand(10, 3)
  2131|         0|            0|            0|  0.00%|        >>> input = torch.tensor([2, 2, 2, 2, 4, 3, 2, 9])
  2132|         0|            0|            0|  0.00%|        >>> offsets = torch.tensor([0,4])
  2133|         0|            0|            0|  0.00%|        >>> F.embedding_bag(input, embedding_matrix, offsets, padding_idx=2, mode='sum')
  2134|         0|            0|            0|  0.00%|        tensor([[ 0.0000,  0.0000,  0.0000],
  2135|         0|            0|            0|  0.00%|                [-0.7082,  3.2145, -2.6251]])
  2136|         0|            0|            0|  0.00%|    """
  2137|         0|            0|            0|  0.00%|    if has_torch_function_variadic(input, weight):
  2138|         0|            0|            0|  0.00%|        return handle_torch_function(
  2139|         0|            0|            0|  0.00%|            embedding_bag,
  2140|         0|            0|            0|  0.00%|            (input, weight),
  2141|         0|            0|            0|  0.00%|            input,
  2142|         0|            0|            0|  0.00%|            weight,
  2143|         0|            0|            0|  0.00%|            offsets=offsets,
  2144|         0|            0|            0|  0.00%|            max_norm=max_norm,
  2145|         0|            0|            0|  0.00%|            norm_type=norm_type,
  2146|         0|            0|            0|  0.00%|            scale_grad_by_freq=scale_grad_by_freq,
  2147|         0|            0|            0|  0.00%|            mode=mode,
  2148|         0|            0|            0|  0.00%|            sparse=sparse,
  2149|         0|            0|            0|  0.00%|            per_sample_weights=per_sample_weights,
  2150|         0|            0|            0|  0.00%|            include_last_offset=include_last_offset,
  2151|         0|            0|            0|  0.00%|            padding_idx=padding_idx,
  2152|         0|            0|            0|  0.00%|        )
  2153|         0|            0|            0|  0.00%|    # Check for backward compatibility.
  2154|         0|            0|            0|  0.00%|    # Used to be embedding_bag(weight, input, ...)
  2155|         0|            0|            0|  0.00%|    # Now is     embedding_bag(input, weight, ...)
  2156|         0|            0|            0|  0.00%|    if weight.dtype == torch.long and input.is_floating_point():
  2157|         0|            0|            0|  0.00%|        warnings.warn(
  2158|         0|            0|            0|  0.00%|            "Argument order of nn.functional.embedding_bag was changed. "
  2159|         0|            0|            0|  0.00%|            "Usage `embedding_bag(weight, input, ...)` is deprecated, "
  2160|         0|            0|            0|  0.00%|            "and should now be `embedding_bag(input, weight, ...)`."
  2161|         0|            0|            0|  0.00%|        )
  2162|         0|            0|            0|  0.00%|        weight, input = input, weight
  2163|         0|            0|            0|  0.00%|
  2164|         0|            0|            0|  0.00%|    if per_sample_weights is not None and input.size() != per_sample_weights.size():
  2165|         0|            0|            0|  0.00%|        raise ValueError(
  2166|         0|            0|            0|  0.00%|            "embedding_bag: If per_sample_weights ({}) is not None, "
  2167|         0|            0|            0|  0.00%|            "then it must have the same shape as the input ({})".format(per_sample_weights.shape, input.shape)
  2168|         0|            0|            0|  0.00%|        )
  2169|         0|            0|            0|  0.00%|
  2170|         0|            0|            0|  0.00%|    if input.dim() == 2:
  2171|         0|            0|            0|  0.00%|        if offsets is not None:
  2172|         0|            0|            0|  0.00%|            type_str = "<unknown>"
  2173|         0|            0|            0|  0.00%|            # TODO: Remove this once script supports type() calls
  2174|         0|            0|            0|  0.00%|            if not torch.jit.is_scripting():
  2175|         0|            0|            0|  0.00%|                type_str = str(type(offsets))
  2176|         0|            0|            0|  0.00%|            raise ValueError(
  2177|         0|            0|            0|  0.00%|                "if input is 2D, then offsets has to be None"
  2178|         0|            0|            0|  0.00%|                ", as input is treated is a mini-batch of"
  2179|         0|            0|            0|  0.00%|                " fixed length sequences. However, found "
  2180|         0|            0|            0|  0.00%|                "offsets of type {}".format(type_str)
  2181|         0|            0|            0|  0.00%|            )
  2182|         0|            0|            0|  0.00%|        offsets = torch.arange(0, input.numel(), input.size(1), dtype=input.dtype, device=input.device)
  2183|         0|            0|            0|  0.00%|
  2184|         0|            0|            0|  0.00%|        input = input.reshape(-1)
  2185|         0|            0|            0|  0.00%|        if per_sample_weights is not None:
  2186|         0|            0|            0|  0.00%|            per_sample_weights = per_sample_weights.reshape(-1)
  2187|         0|            0|            0|  0.00%|    elif input.dim() == 1:
  2188|         0|            0|            0|  0.00%|        if offsets is None:
  2189|         0|            0|            0|  0.00%|            raise ValueError("offsets has to be a 1D Tensor but got None")
  2190|         0|            0|            0|  0.00%|        if offsets.dim() != 1:
  2191|         0|            0|            0|  0.00%|            raise ValueError("offsets has to be a 1D Tensor")
  2192|         0|            0|            0|  0.00%|    else:
  2193|         0|            0|            0|  0.00%|        raise ValueError("input has to be 1D or 2D Tensor," " but got Tensor of dimension {}".format(input.dim()))
  2194|         0|            0|            0|  0.00%|    if mode == "sum":
  2195|         0|            0|            0|  0.00%|        mode_enum = 0
  2196|         0|            0|            0|  0.00%|    elif mode == "mean":
  2197|         0|            0|            0|  0.00%|        mode_enum = 1
  2198|         0|            0|            0|  0.00%|    elif mode == "max":
  2199|         0|            0|            0|  0.00%|        mode_enum = 2
  2200|         0|            0|            0|  0.00%|
  2201|         0|            0|            0|  0.00%|        if scale_grad_by_freq:
  2202|         0|            0|            0|  0.00%|            raise ValueError("max mode does not support scaling the gradient by the frequency")
  2203|         0|            0|            0|  0.00%|
  2204|         0|            0|            0|  0.00%|        if sparse:
  2205|         0|            0|            0|  0.00%|            raise ValueError("max mode does not support sparse weights")
  2206|         0|            0|            0|  0.00%|
  2207|         0|            0|            0|  0.00%|    else:
  2208|         0|            0|            0|  0.00%|        raise ValueError("mode has to be one of sum, mean or max")
  2209|         0|            0|            0|  0.00%|
  2210|         0|            0|            0|  0.00%|    if max_norm is not None:
  2211|         0|            0|            0|  0.00%|        # XXX: equivalent to
  2212|         0|            0|            0|  0.00%|        # with torch.no_grad():
  2213|         0|            0|            0|  0.00%|        #   torch.nembedding_renorm_
  2214|         0|            0|            0|  0.00%|        # remove once script supports set_grad_enabled
  2215|         0|            0|            0|  0.00%|        _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)
  2216|         0|            0|            0|  0.00%|
  2217|         0|            0|            0|  0.00%|    if per_sample_weights is not None and mode != "sum":
  2218|         0|            0|            0|  0.00%|        raise NotImplementedError(
  2219|         0|            0|            0|  0.00%|            "embedding_bag: per_sample_weights was not None. "
  2220|         0|            0|            0|  0.00%|            "per_sample_weights is only supported for mode='sum' "
  2221|         0|            0|            0|  0.00%|            "(got mode='{}'). Please open a feature request on GitHub.".format(mode)
  2222|         0|            0|            0|  0.00%|        )
  2223|         0|            0|            0|  0.00%|
  2224|         0|            0|            0|  0.00%|    ret, _, _, _ = torch.embedding_bag(
  2225|         0|            0|            0|  0.00%|        weight, input, offsets, scale_grad_by_freq, mode_enum, sparse, per_sample_weights, include_last_offset, padding_idx
  2226|         0|            0|            0|  0.00%|    )
  2227|         0|            0|            0|  0.00%|    return ret
  2228|         0|            0|            0|  0.00%|
  2229|         0|            0|            0|  0.00%|
  2230|         0|            0|            0|  0.00%|embedding_bag.__doc__ = embedding_bag.__doc__.format(**reproducibility_notes)
  2231|         0|            0|            0|  0.00%|
  2232|         0|            0|            0|  0.00%|
  2233|      1220|   0.00634027|  5.19694e-06|  0.01%|def _verify_batch_size(size: List[int]) -> None:
  2234|         0|            0|            0|  0.00%|    # XXX: JIT script does not support the reduce from functools, and mul op is a
  2235|         0|            0|            0|  0.00%|    # builtin, which cannot be used as a value to a func yet, so rewrite this size
  2236|         0|            0|            0|  0.00%|    # check to a simple equivalent for loop
  2237|         0|            0|            0|  0.00%|    #
  2238|         0|            0|            0|  0.00%|    # TODO: make use of reduce like below when JIT is ready with the missing features:
  2239|         0|            0|            0|  0.00%|    # from operator import mul
  2240|         0|            0|            0|  0.00%|    # from functools import reduce
  2241|         0|            0|            0|  0.00%|    #
  2242|         0|            0|            0|  0.00%|    #   if reduce(mul, size[2:], size[0]) == 1
  2243|      1220|   0.00596833|  4.89208e-06|  0.01%|    size_prods = size[0]
  2244|      3660|    0.0193856|  5.29661e-06|  0.04%|    for i in range(len(size) - 2):
  2245|      2440|    0.0105643|  4.32964e-06|  0.02%|        size_prods *= size[i + 2]
  2246|      1220|   0.00466895|  3.82701e-06|  0.01%|    if size_prods == 1:
  2247|         0|            0|            0|  0.00%|        raise ValueError("Expected more than 1 value per channel when training, got input size {}".format(size))
  2248|         0|            0|            0|  0.00%|
  2249|         0|            0|            0|  0.00%|
  2250|      2000|    0.0105145|  5.25725e-06|  0.02%|def batch_norm(
  2251|         0|            0|            0|  0.00%|    input: Tensor,
  2252|         0|            0|            0|  0.00%|    running_mean: Optional[Tensor],
  2253|         0|            0|            0|  0.00%|    running_var: Optional[Tensor],
  2254|         0|            0|            0|  0.00%|    weight: Optional[Tensor] = None,
  2255|         0|            0|            0|  0.00%|    bias: Optional[Tensor] = None,
  2256|         0|            0|            0|  0.00%|    training: bool = False,
  2257|         0|            0|            0|  0.00%|    momentum: float = 0.1,
  2258|         0|            0|            0|  0.00%|    eps: float = 1e-5,
  2259|         0|            0|            0|  0.00%|) -> Tensor:
  2260|         0|            0|            0|  0.00%|    r"""Applies Batch Normalization for each channel across a batch of data.
  2261|         0|            0|            0|  0.00%|
  2262|         0|            0|            0|  0.00%|    See :class:`~torch.nn.BatchNorm1d`, :class:`~torch.nn.BatchNorm2d`,
  2263|         0|            0|            0|  0.00%|    :class:`~torch.nn.BatchNorm3d` for details.
  2264|         0|            0|            0|  0.00%|    """
  2265|      2000|     0.013731|   6.8655e-06|  0.03%|    if has_torch_function_unary(input):
  2266|         0|            0|            0|  0.00%|        return handle_torch_function(
  2267|         0|            0|            0|  0.00%|            batch_norm,
  2268|         0|            0|            0|  0.00%|            (input,),
  2269|         0|            0|            0|  0.00%|            input,
  2270|         0|            0|            0|  0.00%|            running_mean,
  2271|         0|            0|            0|  0.00%|            running_var,
  2272|         0|            0|            0|  0.00%|            weight=weight,
  2273|         0|            0|            0|  0.00%|            bias=bias,
  2274|         0|            0|            0|  0.00%|            training=training,
  2275|         0|            0|            0|  0.00%|            momentum=momentum,
  2276|         0|            0|            0|  0.00%|            eps=eps,
  2277|         0|            0|            0|  0.00%|        )
  2278|      2000|   0.00824356|  4.12178e-06|  0.02%|    if training:
  2279|      1220|    0.0227275|  1.86291e-05|  0.04%|        _verify_batch_size(input.size())
(call)|      1220|    0.0469275|  3.84651e-05|  0.09%|# /opt/conda/lib/python3.8/site-packages/torch/nn/functional.py:2233 _verify_batch_size
  2280|         0|            0|            0|  0.00%|
  2281|      4000|      1.33397|  0.000333492|  2.49%|    return torch.batch_norm(
  2282|      2000|    0.0310555|  1.55277e-05|  0.06%|        input, weight, bias, running_mean, running_var, training, momentum, eps, torch.backends.cudnn.enabled
(call)|      2000|    0.0234418|  1.17209e-05|  0.04%|# /opt/conda/lib/python3.8/site-packages/torch/backends/__init__.py:31 __get__
  2283|         0|            0|            0|  0.00%|    )
  2284|         0|            0|            0|  0.00%|
  2285|         0|            0|            0|  0.00%|
  2286|         0|            0|            0|  0.00%|def _verify_spatial_size(size: List[int]) -> None:
  2287|         0|            0|            0|  0.00%|    # Verify that there is > 1 spatial element for instance norm calculation.
  2288|         0|            0|            0|  0.00%|    size_prods = 1
  2289|         0|            0|            0|  0.00%|    for i in range(2, len(size)):
  2290|         0|            0|            0|  0.00%|        size_prods *= size[i]
  2291|         0|            0|            0|  0.00%|    if size_prods == 1:
  2292|         0|            0|            0|  0.00%|        raise ValueError("Expected more than 1 spatial element when training, got input size {}".format(size))
  2293|         0|            0|            0|  0.00%|
  2294|         0|            0|            0|  0.00%|
  2295|         0|            0|            0|  0.00%|def instance_norm(
  2296|         0|            0|            0|  0.00%|    input: Tensor,
  2297|         0|            0|            0|  0.00%|    running_mean: Optional[Tensor] = None,
  2298|         0|            0|            0|  0.00%|    running_var: Optional[Tensor] = None,
  2299|         0|            0|            0|  0.00%|    weight: Optional[Tensor] = None,
  2300|         0|            0|            0|  0.00%|    bias: Optional[Tensor] = None,
  2301|         0|            0|            0|  0.00%|    use_input_stats: bool = True,
  2302|         0|            0|            0|  0.00%|    momentum: float = 0.1,
  2303|         0|            0|            0|  0.00%|    eps: float = 1e-5,
  2304|         0|            0|            0|  0.00%|) -> Tensor:
  2305|         0|            0|            0|  0.00%|    r"""Applies Instance Normalization for each channel in each data sample in a
  2306|         0|            0|            0|  0.00%|    batch.
  2307|         0|            0|            0|  0.00%|
  2308|         0|            0|            0|  0.00%|    See :class:`~torch.nn.InstanceNorm1d`, :class:`~torch.nn.InstanceNorm2d`,
  2309|         0|            0|            0|  0.00%|    :class:`~torch.nn.InstanceNorm3d` for details.
  2310|         0|            0|            0|  0.00%|    """
  2311|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):
  2312|         0|            0|            0|  0.00%|        return handle_torch_function(
  2313|         0|            0|            0|  0.00%|            instance_norm,
  2314|         0|            0|            0|  0.00%|            (input,),
  2315|         0|            0|            0|  0.00%|            input,
  2316|         0|            0|            0|  0.00%|            running_mean=running_mean,
  2317|         0|            0|            0|  0.00%|            running_var=running_var,
  2318|         0|            0|            0|  0.00%|            weight=weight,
  2319|         0|            0|            0|  0.00%|            bias=bias,
  2320|         0|            0|            0|  0.00%|            use_input_stats=use_input_stats,
  2321|         0|            0|            0|  0.00%|            momentum=momentum,
  2322|         0|            0|            0|  0.00%|            eps=eps,
  2323|         0|            0|            0|  0.00%|        )
  2324|         0|            0|            0|  0.00%|    if use_input_stats:
  2325|         0|            0|            0|  0.00%|        _verify_spatial_size(input.size())
  2326|         0|            0|            0|  0.00%|    return torch.instance_norm(
  2327|         0|            0|            0|  0.00%|        input, weight, bias, running_mean, running_var, use_input_stats, momentum, eps, torch.backends.cudnn.enabled
  2328|         0|            0|            0|  0.00%|    )
  2329|         0|            0|            0|  0.00%|
  2330|         0|            0|            0|  0.00%|
  2331|         0|            0|            0|  0.00%|def layer_norm(
  2332|         0|            0|            0|  0.00%|    input: Tensor,
  2333|         0|            0|            0|  0.00%|    normalized_shape: List[int],
  2334|         0|            0|            0|  0.00%|    weight: Optional[Tensor] = None,
  2335|         0|            0|            0|  0.00%|    bias: Optional[Tensor] = None,
  2336|         0|            0|            0|  0.00%|    eps: float = 1e-5,
  2337|         0|            0|            0|  0.00%|) -> Tensor:
  2338|         0|            0|            0|  0.00%|    r"""Applies Layer Normalization for last certain number of dimensions.
  2339|         0|            0|            0|  0.00%|
  2340|         0|            0|            0|  0.00%|    See :class:`~torch.nn.LayerNorm` for details.
  2341|         0|            0|            0|  0.00%|    """
  2342|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):
  2343|         0|            0|            0|  0.00%|        return handle_torch_function(
  2344|         0|            0|            0|  0.00%|            layer_norm, (input,), input, normalized_shape, weight=weight, bias=bias, eps=eps
  2345|         0|            0|            0|  0.00%|        )
  2346|         0|            0|            0|  0.00%|    return torch.layer_norm(input, normalized_shape, weight, bias, eps, torch.backends.cudnn.enabled)
  2347|         0|            0|            0|  0.00%|
  2348|         0|            0|            0|  0.00%|
  2349|         0|            0|            0|  0.00%|def group_norm(
  2350|         0|            0|            0|  0.00%|    input: Tensor, num_groups: int, weight: Optional[Tensor] = None, bias: Optional[Tensor] = None, eps: float = 1e-5
  2351|         0|            0|            0|  0.00%|) -> Tensor:
  2352|         0|            0|            0|  0.00%|    r"""Applies Group Normalization for last certain number of dimensions.
  2353|         0|            0|            0|  0.00%|
  2354|         0|            0|            0|  0.00%|    See :class:`~torch.nn.GroupNorm` for details.
  2355|         0|            0|            0|  0.00%|    """
  2356|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):
  2357|         0|            0|            0|  0.00%|        return handle_torch_function(group_norm, (input,), input, num_groups, weight=weight, bias=bias, eps=eps)
  2358|         0|            0|            0|  0.00%|    _verify_batch_size([input.size(0) * input.size(1) // num_groups, num_groups] + list(input.size()[2:]))
  2359|         0|            0|            0|  0.00%|    return torch.group_norm(input, num_groups, weight, bias, eps, torch.backends.cudnn.enabled)
  2360|         0|            0|            0|  0.00%|
  2361|         0|            0|            0|  0.00%|
  2362|         0|            0|            0|  0.00%|def local_response_norm(input: Tensor, size: int, alpha: float = 1e-4, beta: float = 0.75, k: float = 1.0) -> Tensor:
  2363|         0|            0|            0|  0.00%|    r"""Applies local response normalization over an input signal composed of
  2364|         0|            0|            0|  0.00%|    several input planes, where channels occupy the second dimension.
  2365|         0|            0|            0|  0.00%|    Applies normalization across channels.
  2366|         0|            0|            0|  0.00%|
  2367|         0|            0|            0|  0.00%|    See :class:`~torch.nn.LocalResponseNorm` for details.
  2368|         0|            0|            0|  0.00%|    """
  2369|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):
  2370|         0|            0|            0|  0.00%|        return handle_torch_function(local_response_norm, (input,), input, size, alpha=alpha, beta=beta, k=k)
  2371|         0|            0|            0|  0.00%|    dim = input.dim()
  2372|         0|            0|            0|  0.00%|    if dim < 3:
  2373|         0|            0|            0|  0.00%|        raise ValueError(
  2374|         0|            0|            0|  0.00%|            "Expected 3D or higher dimensionality \
  2375|         0|            0|            0|  0.00%|                         input (got {} dimensions)".format(
  2376|         0|            0|            0|  0.00%|                dim
  2377|         0|            0|            0|  0.00%|            )
  2378|         0|            0|            0|  0.00%|        )
  2379|         0|            0|            0|  0.00%|    div = input.mul(input).unsqueeze(1)
  2380|         0|            0|            0|  0.00%|    if dim == 3:
  2381|         0|            0|            0|  0.00%|        div = pad(div, (0, 0, size // 2, (size - 1) // 2))
  2382|         0|            0|            0|  0.00%|        div = avg_pool2d(div, (size, 1), stride=1).squeeze(1)
  2383|         0|            0|            0|  0.00%|    else:
  2384|         0|            0|            0|  0.00%|        sizes = input.size()
  2385|         0|            0|            0|  0.00%|        div = div.view(sizes[0], 1, sizes[1], sizes[2], -1)
  2386|         0|            0|            0|  0.00%|        div = pad(div, (0, 0, 0, 0, size // 2, (size - 1) // 2))
  2387|         0|            0|            0|  0.00%|        div = avg_pool3d(div, (size, 1, 1), stride=1).squeeze(1)
  2388|         0|            0|            0|  0.00%|        div = div.view(sizes)
  2389|         0|            0|            0|  0.00%|    div = div.mul(alpha).add(k).pow(beta)
  2390|         0|            0|            0|  0.00%|    return input / div
  2391|         0|            0|            0|  0.00%|
  2392|         0|            0|            0|  0.00%|
  2393|         0|            0|            0|  0.00%|# loss
  2394|         0|            0|            0|  0.00%|
  2395|         0|            0|            0|  0.00%|
  2396|         0|            0|            0|  0.00%|def ctc_loss(
  2397|         0|            0|            0|  0.00%|    log_probs: Tensor,
  2398|         0|            0|            0|  0.00%|    targets: Tensor,
  2399|         0|            0|            0|  0.00%|    input_lengths: Tensor,
  2400|         0|            0|            0|  0.00%|    target_lengths: Tensor,
  2401|         0|            0|            0|  0.00%|    blank: int = 0,
  2402|         0|            0|            0|  0.00%|    reduction: str = "mean",
  2403|         0|            0|            0|  0.00%|    zero_infinity: bool = False,
  2404|         0|            0|            0|  0.00%|) -> Tensor:
  2405|         0|            0|            0|  0.00%|    r"""The Connectionist Temporal Classification loss.
  2406|         0|            0|            0|  0.00%|
  2407|         0|            0|            0|  0.00%|    See :class:`~torch.nn.CTCLoss` for details.
  2408|         0|            0|            0|  0.00%|
  2409|         0|            0|            0|  0.00%|    Note:
  2410|         0|            0|            0|  0.00%|        {cudnn_reproducibility_note}
  2411|         0|            0|            0|  0.00%|
  2412|         0|            0|            0|  0.00%|    Note:
  2413|         0|            0|            0|  0.00%|        {backward_reproducibility_note}
  2414|         0|            0|            0|  0.00%|
  2415|         0|            0|            0|  0.00%|    Args:
  2416|         0|            0|            0|  0.00%|        log_probs: :math:`(T, N, C)` where `C = number of characters in alphabet including blank`,
  2417|         0|            0|            0|  0.00%|            `T = input length`, and `N = batch size`.
  2418|         0|            0|            0|  0.00%|            The logarithmized probabilities of the outputs
  2419|         0|            0|            0|  0.00%|            (e.g. obtained with :func:`torch.nn.functional.log_softmax`).
  2420|         0|            0|            0|  0.00%|        targets: :math:`(N, S)` or `(sum(target_lengths))`.
  2421|         0|            0|            0|  0.00%|            Targets cannot be blank. In the second form, the targets are assumed to be concatenated.
  2422|         0|            0|            0|  0.00%|        input_lengths: :math:`(N)`.
  2423|         0|            0|            0|  0.00%|            Lengths of the inputs (must each be :math:`\leq T`)
  2424|         0|            0|            0|  0.00%|        target_lengths: :math:`(N)`.
  2425|         0|            0|            0|  0.00%|            Lengths of the targets
  2426|         0|            0|            0|  0.00%|        blank (int, optional):
  2427|         0|            0|            0|  0.00%|            Blank label. Default :math:`0`.
  2428|         0|            0|            0|  0.00%|        reduction (string, optional): Specifies the reduction to apply to the output:
  2429|         0|            0|            0|  0.00%|            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,
  2430|         0|            0|            0|  0.00%|            ``'mean'``: the output losses will be divided by the target lengths and
  2431|         0|            0|            0|  0.00%|            then the mean over the batch is taken, ``'sum'``: the output will be
  2432|         0|            0|            0|  0.00%|            summed. Default: ``'mean'``
  2433|         0|            0|            0|  0.00%|        zero_infinity (bool, optional):
  2434|         0|            0|            0|  0.00%|            Whether to zero infinite losses and the associated gradients.
  2435|         0|            0|            0|  0.00%|            Default: ``False``
  2436|         0|            0|            0|  0.00%|            Infinite losses mainly occur when the inputs are too short
  2437|         0|            0|            0|  0.00%|            to be aligned to the targets.
  2438|         0|            0|            0|  0.00%|
  2439|         0|            0|            0|  0.00%|    Example::
  2440|         0|            0|            0|  0.00%|
  2441|         0|            0|            0|  0.00%|        >>> log_probs = torch.randn(50, 16, 20).log_softmax(2).detach().requires_grad_()
  2442|         0|            0|            0|  0.00%|        >>> targets = torch.randint(1, 20, (16, 30), dtype=torch.long)
  2443|         0|            0|            0|  0.00%|        >>> input_lengths = torch.full((16,), 50, dtype=torch.long)
  2444|         0|            0|            0|  0.00%|        >>> target_lengths = torch.randint(10,30,(16,), dtype=torch.long)
  2445|         0|            0|            0|  0.00%|        >>> loss = F.ctc_loss(log_probs, targets, input_lengths, target_lengths)
  2446|         0|            0|            0|  0.00%|        >>> loss.backward()
  2447|         0|            0|            0|  0.00%|    """
  2448|         0|            0|            0|  0.00%|    if has_torch_function_variadic(log_probs, targets, input_lengths, target_lengths):
  2449|         0|            0|            0|  0.00%|        return handle_torch_function(
  2450|         0|            0|            0|  0.00%|            ctc_loss,
  2451|         0|            0|            0|  0.00%|            (log_probs, targets, input_lengths, target_lengths),
  2452|         0|            0|            0|  0.00%|            log_probs, targets, input_lengths, target_lengths,
  2453|         0|            0|            0|  0.00%|            blank=blank, reduction=reduction, zero_infinity=zero_infinity
  2454|         0|            0|            0|  0.00%|        )
  2455|         0|            0|            0|  0.00%|    return torch.ctc_loss(
  2456|         0|            0|            0|  0.00%|        log_probs, targets, input_lengths, target_lengths, blank, _Reduction.get_enum(reduction), zero_infinity
  2457|         0|            0|            0|  0.00%|    )
  2458|         0|            0|            0|  0.00%|
  2459|         0|            0|            0|  0.00%|
  2460|         0|            0|            0|  0.00%|ctc_loss.__doc__ = ctc_loss.__doc__.format(**reproducibility_notes)
  2461|         0|            0|            0|  0.00%|
  2462|         0|            0|            0|  0.00%|
  2463|         0|            0|            0|  0.00%|def nll_loss(
  2464|         0|            0|            0|  0.00%|    input: Tensor,
  2465|         0|            0|            0|  0.00%|    target: Tensor,
  2466|         0|            0|            0|  0.00%|    weight: Optional[Tensor] = None,
  2467|         0|            0|            0|  0.00%|    size_average: Optional[bool] = None,
  2468|         0|            0|            0|  0.00%|    ignore_index: int = -100,
  2469|         0|            0|            0|  0.00%|    reduce: Optional[bool] = None,
  2470|         0|            0|            0|  0.00%|    reduction: str = "mean",
  2471|         0|            0|            0|  0.00%|) -> Tensor:
  2472|         0|            0|            0|  0.00%|    r"""The negative log likelihood loss.
  2473|         0|            0|            0|  0.00%|
  2474|         0|            0|            0|  0.00%|    See :class:`~torch.nn.NLLLoss` for details.
  2475|         0|            0|            0|  0.00%|
  2476|         0|            0|            0|  0.00%|    Args:
  2477|         0|            0|            0|  0.00%|        input: :math:`(N, C)` where `C = number of classes` or :math:`(N, C, H, W)`
  2478|         0|            0|            0|  0.00%|            in case of 2D Loss, or :math:`(N, C, d_1, d_2, ..., d_K)` where :math:`K \geq 1`
  2479|         0|            0|            0|  0.00%|            in the case of K-dimensional loss.
  2480|         0|            0|            0|  0.00%|        target: :math:`(N)` where each value is :math:`0 \leq \text{targets}[i] \leq C-1`,
  2481|         0|            0|            0|  0.00%|            or :math:`(N, d_1, d_2, ..., d_K)` where :math:`K \geq 1` for
  2482|         0|            0|            0|  0.00%|            K-dimensional loss.
  2483|         0|            0|            0|  0.00%|        weight (Tensor, optional): a manual rescaling weight given to each
  2484|         0|            0|            0|  0.00%|            class. If given, has to be a Tensor of size `C`
  2485|         0|            0|            0|  0.00%|        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,
  2486|         0|            0|            0|  0.00%|            the losses are averaged over each loss element in the batch. Note that for
  2487|         0|            0|            0|  0.00%|            some losses, there multiple elements per sample. If the field :attr:`size_average`
  2488|         0|            0|            0|  0.00%|            is set to ``False``, the losses are instead summed for each minibatch. Ignored
  2489|         0|            0|            0|  0.00%|            when reduce is ``False``. Default: ``True``
  2490|         0|            0|            0|  0.00%|        ignore_index (int, optional): Specifies a target value that is ignored
  2491|         0|            0|            0|  0.00%|            and does not contribute to the input gradient. When :attr:`size_average` is
  2492|         0|            0|            0|  0.00%|            ``True``, the loss is averaged over non-ignored targets. Default: -100
  2493|         0|            0|            0|  0.00%|        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the
  2494|         0|            0|            0|  0.00%|            losses are averaged or summed over observations for each minibatch depending
  2495|         0|            0|            0|  0.00%|            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per
  2496|         0|            0|            0|  0.00%|            batch element instead and ignores :attr:`size_average`. Default: ``True``
  2497|         0|            0|            0|  0.00%|        reduction (string, optional): Specifies the reduction to apply to the output:
  2498|         0|            0|            0|  0.00%|            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,
  2499|         0|            0|            0|  0.00%|            ``'mean'``: the sum of the output will be divided by the number of
  2500|         0|            0|            0|  0.00%|            elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`
  2501|         0|            0|            0|  0.00%|            and :attr:`reduce` are in the process of being deprecated, and in the meantime,
  2502|         0|            0|            0|  0.00%|            specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``
  2503|         0|            0|            0|  0.00%|
  2504|         0|            0|            0|  0.00%|    Example::
  2505|         0|            0|            0|  0.00%|
  2506|         0|            0|            0|  0.00%|        >>> # input is of size N x C = 3 x 5
  2507|         0|            0|            0|  0.00%|        >>> input = torch.randn(3, 5, requires_grad=True)
  2508|         0|            0|            0|  0.00%|        >>> # each element in target has to have 0 <= value < C
  2509|         0|            0|            0|  0.00%|        >>> target = torch.tensor([1, 0, 4])
  2510|         0|            0|            0|  0.00%|        >>> output = F.nll_loss(F.log_softmax(input), target)
  2511|         0|            0|            0|  0.00%|        >>> output.backward()
  2512|         0|            0|            0|  0.00%|    """
  2513|         0|            0|            0|  0.00%|    if has_torch_function_variadic(input, target):
  2514|         0|            0|            0|  0.00%|        return handle_torch_function(
  2515|         0|            0|            0|  0.00%|            nll_loss,
  2516|         0|            0|            0|  0.00%|            (input, target),
  2517|         0|            0|            0|  0.00%|            input,
  2518|         0|            0|            0|  0.00%|            target,
  2519|         0|            0|            0|  0.00%|            weight=weight,
  2520|         0|            0|            0|  0.00%|            size_average=size_average,
  2521|         0|            0|            0|  0.00%|            ignore_index=ignore_index,
  2522|         0|            0|            0|  0.00%|            reduce=reduce,
  2523|         0|            0|            0|  0.00%|            reduction=reduction,
  2524|         0|            0|            0|  0.00%|        )
  2525|         0|            0|            0|  0.00%|    if size_average is not None or reduce is not None:
  2526|         0|            0|            0|  0.00%|        reduction = _Reduction.legacy_get_string(size_average, reduce)
  2527|         0|            0|            0|  0.00%|    return torch._C._nn.nll_loss_nd(input, target, weight, _Reduction.get_enum(reduction), ignore_index)
  2528|         0|            0|            0|  0.00%|
  2529|         0|            0|            0|  0.00%|
  2530|         0|            0|            0|  0.00%|def poisson_nll_loss(
  2531|         0|            0|            0|  0.00%|    input: Tensor,
  2532|         0|            0|            0|  0.00%|    target: Tensor,
  2533|         0|            0|            0|  0.00%|    log_input: bool = True,
  2534|         0|            0|            0|  0.00%|    full: bool = False,
  2535|         0|            0|            0|  0.00%|    size_average: Optional[bool] = None,
  2536|         0|            0|            0|  0.00%|    eps: float = 1e-8,
  2537|         0|            0|            0|  0.00%|    reduce: Optional[bool] = None,
  2538|         0|            0|            0|  0.00%|    reduction: str = "mean",
  2539|         0|            0|            0|  0.00%|) -> Tensor:
  2540|         0|            0|            0|  0.00%|    r"""Poisson negative log likelihood loss.
  2541|         0|            0|            0|  0.00%|
  2542|         0|            0|            0|  0.00%|    See :class:`~torch.nn.PoissonNLLLoss` for details.
  2543|         0|            0|            0|  0.00%|
  2544|         0|            0|            0|  0.00%|    Args:
  2545|         0|            0|            0|  0.00%|        input: expectation of underlying Poisson distribution.
  2546|         0|            0|            0|  0.00%|        target: random sample :math:`target \sim \text{Poisson}(input)`.
  2547|         0|            0|            0|  0.00%|        log_input: if ``True`` the loss is computed as
  2548|         0|            0|            0|  0.00%|            :math:`\exp(\text{input}) - \text{target} * \text{input}`, if ``False`` then loss is
  2549|         0|            0|            0|  0.00%|            :math:`\text{input} - \text{target} * \log(\text{input}+\text{eps})`. Default: ``True``
  2550|         0|            0|            0|  0.00%|        full: whether to compute full loss, i. e. to add the Stirling
  2551|         0|            0|            0|  0.00%|            approximation term. Default: ``False``
  2552|         0|            0|            0|  0.00%|            :math:`\text{target} * \log(\text{target}) - \text{target} + 0.5 * \log(2 * \pi * \text{target})`.
  2553|         0|            0|            0|  0.00%|        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,
  2554|         0|            0|            0|  0.00%|            the losses are averaged over each loss element in the batch. Note that for
  2555|         0|            0|            0|  0.00%|            some losses, there multiple elements per sample. If the field :attr:`size_average`
  2556|         0|            0|            0|  0.00%|            is set to ``False``, the losses are instead summed for each minibatch. Ignored
  2557|         0|            0|            0|  0.00%|            when reduce is ``False``. Default: ``True``
  2558|         0|            0|            0|  0.00%|        eps (float, optional): Small value to avoid evaluation of :math:`\log(0)` when
  2559|         0|            0|            0|  0.00%|            :attr:`log_input`=``False``. Default: 1e-8
  2560|         0|            0|            0|  0.00%|        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the
  2561|         0|            0|            0|  0.00%|            losses are averaged or summed over observations for each minibatch depending
  2562|         0|            0|            0|  0.00%|            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per
  2563|         0|            0|            0|  0.00%|            batch element instead and ignores :attr:`size_average`. Default: ``True``
  2564|         0|            0|            0|  0.00%|        reduction (string, optional): Specifies the reduction to apply to the output:
  2565|         0|            0|            0|  0.00%|            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,
  2566|         0|            0|            0|  0.00%|            ``'mean'``: the sum of the output will be divided by the number of
  2567|         0|            0|            0|  0.00%|            elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`
  2568|         0|            0|            0|  0.00%|            and :attr:`reduce` are in the process of being deprecated, and in the meantime,
  2569|         0|            0|            0|  0.00%|            specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``
  2570|         0|            0|            0|  0.00%|
  2571|         0|            0|            0|  0.00%|    """
  2572|         0|            0|            0|  0.00%|    if has_torch_function_variadic(input, target):
  2573|         0|            0|            0|  0.00%|        return handle_torch_function(
  2574|         0|            0|            0|  0.00%|            poisson_nll_loss,
  2575|         0|            0|            0|  0.00%|            (input, target),
  2576|         0|            0|            0|  0.00%|            input,
  2577|         0|            0|            0|  0.00%|            target,
  2578|         0|            0|            0|  0.00%|            log_input=log_input,
  2579|         0|            0|            0|  0.00%|            full=full,
  2580|         0|            0|            0|  0.00%|            size_average=size_average,
  2581|         0|            0|            0|  0.00%|            eps=eps,
  2582|         0|            0|            0|  0.00%|            reduce=reduce,
  2583|         0|            0|            0|  0.00%|            reduction=reduction,
  2584|         0|            0|            0|  0.00%|        )
  2585|         0|            0|            0|  0.00%|    if size_average is not None or reduce is not None:
  2586|         0|            0|            0|  0.00%|        reduction = _Reduction.legacy_get_string(size_average, reduce)
  2587|         0|            0|            0|  0.00%|    if reduction != "none" and reduction != "mean" and reduction != "sum":
  2588|         0|            0|            0|  0.00%|        ret = input
  2589|         0|            0|            0|  0.00%|        raise ValueError(reduction + " is not valid")
  2590|         0|            0|            0|  0.00%|
  2591|         0|            0|            0|  0.00%|    ret = torch.poisson_nll_loss(input, target, log_input, full, eps, _Reduction.get_enum(reduction))
  2592|         0|            0|            0|  0.00%|    return ret
  2593|         0|            0|            0|  0.00%|
  2594|         0|            0|            0|  0.00%|
  2595|         0|            0|            0|  0.00%|def gaussian_nll_loss(
  2596|         0|            0|            0|  0.00%|    input: Tensor,
  2597|         0|            0|            0|  0.00%|    target: Tensor,
  2598|         0|            0|            0|  0.00%|    var: Tensor,
  2599|         0|            0|            0|  0.00%|    full: bool = False,
  2600|         0|            0|            0|  0.00%|    eps: float = 1e-6,
  2601|         0|            0|            0|  0.00%|    reduction: str = "mean",
  2602|         0|            0|            0|  0.00%|) -> Tensor:
  2603|         0|            0|            0|  0.00%|    r"""Gaussian negative log likelihood loss.
  2604|         0|            0|            0|  0.00%|
  2605|         0|            0|            0|  0.00%|    See :class:`~torch.nn.GaussianNLLLoss` for details.
  2606|         0|            0|            0|  0.00%|
  2607|         0|            0|            0|  0.00%|    Args:
  2608|         0|            0|            0|  0.00%|        input: expectation of the Gaussian distribution.
  2609|         0|            0|            0|  0.00%|        target: sample from the Gaussian distribution.
  2610|         0|            0|            0|  0.00%|        var: tensor of positive variance(s), one for each of the expectations
  2611|         0|            0|            0|  0.00%|            in the input (heteroscedastic), or a single one (homoscedastic).
  2612|         0|            0|            0|  0.00%|        full (bool, optional): include the constant term in the loss calculation. Default: ``False``.
  2613|         0|            0|            0|  0.00%|        eps (float, optional): value added to var, for stability. Default: 1e-6.
  2614|         0|            0|            0|  0.00%|        reduction (string, optional): specifies the reduction to apply to the output:
  2615|         0|            0|            0|  0.00%|            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,
  2616|         0|            0|            0|  0.00%|            ``'mean'``: the output is the average of all batch member losses,
  2617|         0|            0|            0|  0.00%|            ``'sum'``: the output is the sum of all batch member losses.
  2618|         0|            0|            0|  0.00%|            Default: ``'mean'``.
  2619|         0|            0|            0|  0.00%|    """
  2620|         0|            0|            0|  0.00%|    if has_torch_function_variadic(input, target, var):
  2621|         0|            0|            0|  0.00%|        return handle_torch_function(
  2622|         0|            0|            0|  0.00%|            gaussian_nll_loss,
  2623|         0|            0|            0|  0.00%|            (input, target, var),
  2624|         0|            0|            0|  0.00%|            input,
  2625|         0|            0|            0|  0.00%|            target,
  2626|         0|            0|            0|  0.00%|            var,
  2627|         0|            0|            0|  0.00%|            full=full,
  2628|         0|            0|            0|  0.00%|            eps=eps,
  2629|         0|            0|            0|  0.00%|            reduction=reduction,
  2630|         0|            0|            0|  0.00%|        )
  2631|         0|            0|            0|  0.00%|
  2632|         0|            0|            0|  0.00%|    # Check var size
  2633|         0|            0|            0|  0.00%|    # If var.size == input.size, the case is heteroscedastic and no further checks are needed.
  2634|         0|            0|            0|  0.00%|    # Otherwise:
  2635|         0|            0|            0|  0.00%|    if var.size() != input.size():
  2636|         0|            0|            0|  0.00%|
  2637|         0|            0|            0|  0.00%|        # If var is one dimension short of input, but the sizes match otherwise, then this is a homoscedastic case.
  2638|         0|            0|            0|  0.00%|        # e.g. input.size = (10, 2, 3), var.size = (10, 2)
  2639|         0|            0|            0|  0.00%|        # -> unsqueeze var so that var.shape = (10, 2, 1)
  2640|         0|            0|            0|  0.00%|        # this is done so that broadcasting can happen in the loss calculation
  2641|         0|            0|            0|  0.00%|        if input.size()[:-1] == var.size():
  2642|         0|            0|            0|  0.00%|            var = torch.unsqueeze(var, -1)
  2643|         0|            0|            0|  0.00%|
  2644|         0|            0|            0|  0.00%|        # This checks if the sizes match up to the final dimension, and the final dimension of var is of size 1.
  2645|         0|            0|            0|  0.00%|        # This is also a homoscedastic case.
  2646|         0|            0|            0|  0.00%|        # e.g. input.size = (10, 2, 3), var.size = (10, 2, 1)
  2647|         0|            0|            0|  0.00%|        elif input.size()[:-1] == var.size()[:-1] and var.size(-1) == 1:  # Heteroscedastic case
  2648|         0|            0|            0|  0.00%|            pass
  2649|         0|            0|            0|  0.00%|
  2650|         0|            0|            0|  0.00%|        # If none of the above pass, then the size of var is incorrect.
  2651|         0|            0|            0|  0.00%|        else:
  2652|         0|            0|            0|  0.00%|            raise ValueError("var is of incorrect size")
  2653|         0|            0|            0|  0.00%|
  2654|         0|            0|            0|  0.00%|    # Check validity of reduction mode
  2655|         0|            0|            0|  0.00%|    if reduction != 'none' and reduction != 'mean' and reduction != 'sum':
  2656|         0|            0|            0|  0.00%|        raise ValueError(reduction + " is not valid")
  2657|         0|            0|            0|  0.00%|
  2658|         0|            0|            0|  0.00%|    # Entries of var must be non-negative
  2659|         0|            0|            0|  0.00%|    if torch.any(var < 0):
  2660|         0|            0|            0|  0.00%|        raise ValueError("var has negative entry/entries")
  2661|         0|            0|            0|  0.00%|
  2662|         0|            0|            0|  0.00%|    # Clamp for stability
  2663|         0|            0|            0|  0.00%|    var = var.clone()
  2664|         0|            0|            0|  0.00%|    with torch.no_grad():
  2665|         0|            0|            0|  0.00%|        var.clamp_(min=eps)
  2666|         0|            0|            0|  0.00%|
  2667|         0|            0|            0|  0.00%|    # Calculate the loss
  2668|         0|            0|            0|  0.00%|    loss = 0.5 * (torch.log(var) + (input - target)**2 / var)
  2669|         0|            0|            0|  0.00%|    if full:
  2670|         0|            0|            0|  0.00%|        loss += 0.5 * math.log(2 * math.pi)
  2671|         0|            0|            0|  0.00%|
  2672|         0|            0|            0|  0.00%|    if reduction == 'mean':
  2673|         0|            0|            0|  0.00%|        return loss.mean()
  2674|         0|            0|            0|  0.00%|    elif reduction == 'sum':
  2675|         0|            0|            0|  0.00%|        return loss.sum()
  2676|         0|            0|            0|  0.00%|    else:
  2677|         0|            0|            0|  0.00%|        return loss
  2678|         0|            0|            0|  0.00%|
  2679|         0|            0|            0|  0.00%|
  2680|         0|            0|            0|  0.00%|def kl_div(
  2681|         0|            0|            0|  0.00%|    input: Tensor,
  2682|         0|            0|            0|  0.00%|    target: Tensor,
  2683|         0|            0|            0|  0.00%|    size_average: Optional[bool] = None,
  2684|         0|            0|            0|  0.00%|    reduce: Optional[bool] = None,
  2685|         0|            0|            0|  0.00%|    reduction: str = "mean",
  2686|         0|            0|            0|  0.00%|    log_target: bool = False,
  2687|         0|            0|            0|  0.00%|) -> Tensor:
  2688|         0|            0|            0|  0.00%|    r"""The `Kullback-Leibler divergence Loss
  2689|         0|            0|            0|  0.00%|    <https://en.wikipedia.org/wiki/Kullback-Leibler_divergence>`__
  2690|         0|            0|            0|  0.00%|
  2691|         0|            0|            0|  0.00%|    See :class:`~torch.nn.KLDivLoss` for details.
  2692|         0|            0|            0|  0.00%|
  2693|         0|            0|            0|  0.00%|    Args:
  2694|         0|            0|            0|  0.00%|        input: Tensor of arbitrary shape
  2695|         0|            0|            0|  0.00%|        target: Tensor of the same shape as input
  2696|         0|            0|            0|  0.00%|        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,
  2697|         0|            0|            0|  0.00%|            the losses are averaged over each loss element in the batch. Note that for
  2698|         0|            0|            0|  0.00%|            some losses, there multiple elements per sample. If the field :attr:`size_average`
  2699|         0|            0|            0|  0.00%|            is set to ``False``, the losses are instead summed for each minibatch. Ignored
  2700|         0|            0|            0|  0.00%|            when reduce is ``False``. Default: ``True``
  2701|         0|            0|            0|  0.00%|        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the
  2702|         0|            0|            0|  0.00%|            losses are averaged or summed over observations for each minibatch depending
  2703|         0|            0|            0|  0.00%|            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per
  2704|         0|            0|            0|  0.00%|            batch element instead and ignores :attr:`size_average`. Default: ``True``
  2705|         0|            0|            0|  0.00%|        reduction (string, optional): Specifies the reduction to apply to the output:
  2706|         0|            0|            0|  0.00%|            ``'none'`` | ``'batchmean'`` | ``'sum'`` | ``'mean'``.
  2707|         0|            0|            0|  0.00%|            ``'none'``: no reduction will be applied
  2708|         0|            0|            0|  0.00%|            ``'batchmean'``: the sum of the output will be divided by the batchsize
  2709|         0|            0|            0|  0.00%|            ``'sum'``: the output will be summed
  2710|         0|            0|            0|  0.00%|            ``'mean'``: the output will be divided by the number of elements in the output
  2711|         0|            0|            0|  0.00%|            Default: ``'mean'``
  2712|         0|            0|            0|  0.00%|        log_target (bool): A flag indicating whether ``target`` is passed in the log space.
  2713|         0|            0|            0|  0.00%|            It is recommended to pass certain distributions (like ``softmax``)
  2714|         0|            0|            0|  0.00%|            in the log space to avoid numerical issues caused by explicit ``log``.
  2715|         0|            0|            0|  0.00%|            Default: ``False``
  2716|         0|            0|            0|  0.00%|
  2717|         0|            0|            0|  0.00%|    .. note::
  2718|         0|            0|            0|  0.00%|        :attr:`size_average` and :attr:`reduce` are in the process of being deprecated,
  2719|         0|            0|            0|  0.00%|        and in the meantime, specifying either of those two args will override :attr:`reduction`.
  2720|         0|            0|            0|  0.00%|
  2721|         0|            0|            0|  0.00%|    .. note::
  2722|         0|            0|            0|  0.00%|        :attr:``reduction`` = ``'mean'`` doesn't return the true kl divergence value, please use
  2723|         0|            0|            0|  0.00%|        :attr:``reduction`` = ``'batchmean'`` which aligns with KL math definition.
  2724|         0|            0|            0|  0.00%|        In the next major release, ``'mean'`` will be changed to be the same as 'batchmean'.
  2725|         0|            0|            0|  0.00%|    """
  2726|         0|            0|            0|  0.00%|    if has_torch_function_variadic(input, target):
  2727|         0|            0|            0|  0.00%|        return handle_torch_function(
  2728|         0|            0|            0|  0.00%|            kl_div,
  2729|         0|            0|            0|  0.00%|            (input, target),
  2730|         0|            0|            0|  0.00%|            input,
  2731|         0|            0|            0|  0.00%|            target,
  2732|         0|            0|            0|  0.00%|            size_average=size_average,
  2733|         0|            0|            0|  0.00%|            reduce=reduce,
  2734|         0|            0|            0|  0.00%|            reduction=reduction,
  2735|         0|            0|            0|  0.00%|            log_target=log_target,
  2736|         0|            0|            0|  0.00%|        )
  2737|         0|            0|            0|  0.00%|    if size_average is not None or reduce is not None:
  2738|         0|            0|            0|  0.00%|        reduction_enum = _Reduction.legacy_get_enum(size_average, reduce)
  2739|         0|            0|            0|  0.00%|    else:
  2740|         0|            0|            0|  0.00%|        if reduction == "mean":
  2741|         0|            0|            0|  0.00%|            warnings.warn(
  2742|         0|            0|            0|  0.00%|                "reduction: 'mean' divides the total loss by both the batch size and the support size."
  2743|         0|            0|            0|  0.00%|                "'batchmean' divides only by the batch size, and aligns with the KL div math definition."
  2744|         0|            0|            0|  0.00%|                "'mean' will be changed to behave the same as 'batchmean' in the next major release."
  2745|         0|            0|            0|  0.00%|            )
  2746|         0|            0|            0|  0.00%|
  2747|         0|            0|            0|  0.00%|        # special case for batchmean
  2748|         0|            0|            0|  0.00%|        if reduction == "batchmean":
  2749|         0|            0|            0|  0.00%|            reduction_enum = _Reduction.get_enum("sum")
  2750|         0|            0|            0|  0.00%|        else:
  2751|         0|            0|            0|  0.00%|            reduction_enum = _Reduction.get_enum(reduction)
  2752|         0|            0|            0|  0.00%|
  2753|         0|            0|            0|  0.00%|    reduced = torch.kl_div(input, target, reduction_enum, log_target=log_target)
  2754|         0|            0|            0|  0.00%|
  2755|         0|            0|            0|  0.00%|    if reduction == "batchmean" and input.dim() != 0:
  2756|         0|            0|            0|  0.00%|        reduced = reduced / input.size()[0]
  2757|         0|            0|            0|  0.00%|
  2758|         0|            0|            0|  0.00%|    return reduced
  2759|         0|            0|            0|  0.00%|
  2760|         0|            0|            0|  0.00%|
  2761|       100|  0.000674009|  6.74009e-06|  0.00%|def cross_entropy(
  2762|         0|            0|            0|  0.00%|    input: Tensor,
  2763|         0|            0|            0|  0.00%|    target: Tensor,
  2764|         0|            0|            0|  0.00%|    weight: Optional[Tensor] = None,
  2765|         0|            0|            0|  0.00%|    size_average: Optional[bool] = None,
  2766|         0|            0|            0|  0.00%|    ignore_index: int = -100,
  2767|         0|            0|            0|  0.00%|    reduce: Optional[bool] = None,
  2768|         0|            0|            0|  0.00%|    reduction: str = "mean",
  2769|         0|            0|            0|  0.00%|) -> Tensor:
  2770|         0|            0|            0|  0.00%|    r"""This criterion combines `log_softmax` and `nll_loss` in a single
  2771|         0|            0|            0|  0.00%|    function.
  2772|         0|            0|            0|  0.00%|
  2773|         0|            0|            0|  0.00%|    See :class:`~torch.nn.CrossEntropyLoss` for details.
  2774|         0|            0|            0|  0.00%|
  2775|         0|            0|            0|  0.00%|    Args:
  2776|         0|            0|            0|  0.00%|        input (Tensor) : :math:`(N, C)` where `C = number of classes` or :math:`(N, C, H, W)`
  2777|         0|            0|            0|  0.00%|            in case of 2D Loss, or :math:`(N, C, d_1, d_2, ..., d_K)` where :math:`K \geq 1`
  2778|         0|            0|            0|  0.00%|            in the case of K-dimensional loss.
  2779|         0|            0|            0|  0.00%|        target (Tensor) : :math:`(N)` where each value is :math:`0 \leq \text{targets}[i] \leq C-1`,
  2780|         0|            0|            0|  0.00%|            or :math:`(N, d_1, d_2, ..., d_K)` where :math:`K \geq 1` for
  2781|         0|            0|            0|  0.00%|            K-dimensional loss.
  2782|         0|            0|            0|  0.00%|        weight (Tensor, optional): a manual rescaling weight given to each
  2783|         0|            0|            0|  0.00%|            class. If given, has to be a Tensor of size `C`
  2784|         0|            0|            0|  0.00%|        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,
  2785|         0|            0|            0|  0.00%|            the losses are averaged over each loss element in the batch. Note that for
  2786|         0|            0|            0|  0.00%|            some losses, there multiple elements per sample. If the field :attr:`size_average`
  2787|         0|            0|            0|  0.00%|            is set to ``False``, the losses are instead summed for each minibatch. Ignored
  2788|         0|            0|            0|  0.00%|            when reduce is ``False``. Default: ``True``
  2789|         0|            0|            0|  0.00%|        ignore_index (int, optional): Specifies a target value that is ignored
  2790|         0|            0|            0|  0.00%|            and does not contribute to the input gradient. When :attr:`size_average` is
  2791|         0|            0|            0|  0.00%|            ``True``, the loss is averaged over non-ignored targets. Default: -100
  2792|         0|            0|            0|  0.00%|        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the
  2793|         0|            0|            0|  0.00%|            losses are averaged or summed over observations for each minibatch depending
  2794|         0|            0|            0|  0.00%|            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per
  2795|         0|            0|            0|  0.00%|            batch element instead and ignores :attr:`size_average`. Default: ``True``
  2796|         0|            0|            0|  0.00%|        reduction (string, optional): Specifies the reduction to apply to the output:
  2797|         0|            0|            0|  0.00%|            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,
  2798|         0|            0|            0|  0.00%|            ``'mean'``: the sum of the output will be divided by the number of
  2799|         0|            0|            0|  0.00%|            elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`
  2800|         0|            0|            0|  0.00%|            and :attr:`reduce` are in the process of being deprecated, and in the meantime,
  2801|         0|            0|            0|  0.00%|            specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``
  2802|         0|            0|            0|  0.00%|
  2803|         0|            0|            0|  0.00%|    Examples::
  2804|         0|            0|            0|  0.00%|
  2805|         0|            0|            0|  0.00%|        >>> input = torch.randn(3, 5, requires_grad=True)
  2806|         0|            0|            0|  0.00%|        >>> target = torch.randint(5, (3,), dtype=torch.int64)
  2807|         0|            0|            0|  0.00%|        >>> loss = F.cross_entropy(input, target)
  2808|         0|            0|            0|  0.00%|        >>> loss.backward()
  2809|         0|            0|            0|  0.00%|    """
  2810|       100|  0.000796795|  7.96795e-06|  0.00%|    if has_torch_function_variadic(input, target):
  2811|         0|            0|            0|  0.00%|        return handle_torch_function(
  2812|         0|            0|            0|  0.00%|            cross_entropy,
  2813|         0|            0|            0|  0.00%|            (input, target),
  2814|         0|            0|            0|  0.00%|            input,
  2815|         0|            0|            0|  0.00%|            target,
  2816|         0|            0|            0|  0.00%|            weight=weight,
  2817|         0|            0|            0|  0.00%|            size_average=size_average,
  2818|         0|            0|            0|  0.00%|            ignore_index=ignore_index,
  2819|         0|            0|            0|  0.00%|            reduce=reduce,
  2820|         0|            0|            0|  0.00%|            reduction=reduction,
  2821|         0|            0|            0|  0.00%|        )
  2822|       100|   0.00050354|   5.0354e-06|  0.00%|    if size_average is not None or reduce is not None:
  2823|         0|            0|            0|  0.00%|        reduction = _Reduction.legacy_get_string(size_average, reduce)
  2824|       100|    0.0113394|  0.000113394|  0.02%|    return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index)
(call)|       100|   0.00252485|  2.52485e-05|  0.00%|# /opt/conda/lib/python3.8/site-packages/torch/nn/_reduction.py:7 get_enum
  2825|         0|            0|            0|  0.00%|
  2826|         0|            0|            0|  0.00%|
  2827|         0|            0|            0|  0.00%|def binary_cross_entropy(
  2828|         0|            0|            0|  0.00%|    input: Tensor,
  2829|         0|            0|            0|  0.00%|    target: Tensor,
  2830|         0|            0|            0|  0.00%|    weight: Optional[Tensor] = None,
  2831|         0|            0|            0|  0.00%|    size_average: Optional[bool] = None,
  2832|         0|            0|            0|  0.00%|    reduce: Optional[bool] = None,
  2833|         0|            0|            0|  0.00%|    reduction: str = "mean",
  2834|         0|            0|            0|  0.00%|) -> Tensor:
  2835|         0|            0|            0|  0.00%|    r"""Function that measures the Binary Cross Entropy
  2836|         0|            0|            0|  0.00%|    between the target and the output.
  2837|         0|            0|            0|  0.00%|
  2838|         0|            0|            0|  0.00%|    See :class:`~torch.nn.BCELoss` for details.
  2839|         0|            0|            0|  0.00%|
  2840|         0|            0|            0|  0.00%|    Args:
  2841|         0|            0|            0|  0.00%|        input: Tensor of arbitrary shape
  2842|         0|            0|            0|  0.00%|        target: Tensor of the same shape as input
  2843|         0|            0|            0|  0.00%|        weight (Tensor, optional): a manual rescaling weight
  2844|         0|            0|            0|  0.00%|                if provided it's repeated to match input tensor shape
  2845|         0|            0|            0|  0.00%|        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,
  2846|         0|            0|            0|  0.00%|            the losses are averaged over each loss element in the batch. Note that for
  2847|         0|            0|            0|  0.00%|            some losses, there multiple elements per sample. If the field :attr:`size_average`
  2848|         0|            0|            0|  0.00%|            is set to ``False``, the losses are instead summed for each minibatch. Ignored
  2849|         0|            0|            0|  0.00%|            when reduce is ``False``. Default: ``True``
  2850|         0|            0|            0|  0.00%|        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the
  2851|         0|            0|            0|  0.00%|            losses are averaged or summed over observations for each minibatch depending
  2852|         0|            0|            0|  0.00%|            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per
  2853|         0|            0|            0|  0.00%|            batch element instead and ignores :attr:`size_average`. Default: ``True``
  2854|         0|            0|            0|  0.00%|        reduction (string, optional): Specifies the reduction to apply to the output:
  2855|         0|            0|            0|  0.00%|            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,
  2856|         0|            0|            0|  0.00%|            ``'mean'``: the sum of the output will be divided by the number of
  2857|         0|            0|            0|  0.00%|            elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`
  2858|         0|            0|            0|  0.00%|            and :attr:`reduce` are in the process of being deprecated, and in the meantime,
  2859|         0|            0|            0|  0.00%|            specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``
  2860|         0|            0|            0|  0.00%|
  2861|         0|            0|            0|  0.00%|    Examples::
  2862|         0|            0|            0|  0.00%|
  2863|         0|            0|            0|  0.00%|        >>> input = torch.randn((3, 2), requires_grad=True)
  2864|         0|            0|            0|  0.00%|        >>> target = torch.rand((3, 2), requires_grad=False)
  2865|         0|            0|            0|  0.00%|        >>> loss = F.binary_cross_entropy(F.sigmoid(input), target)
  2866|         0|            0|            0|  0.00%|        >>> loss.backward()
  2867|         0|            0|            0|  0.00%|    """
  2868|         0|            0|            0|  0.00%|    if has_torch_function_variadic(input, target):
  2869|         0|            0|            0|  0.00%|        return handle_torch_function(
  2870|         0|            0|            0|  0.00%|            binary_cross_entropy,
  2871|         0|            0|            0|  0.00%|            (input, target),
  2872|         0|            0|            0|  0.00%|            input,
  2873|         0|            0|            0|  0.00%|            target,
  2874|         0|            0|            0|  0.00%|            weight=weight,
  2875|         0|            0|            0|  0.00%|            size_average=size_average,
  2876|         0|            0|            0|  0.00%|            reduce=reduce,
  2877|         0|            0|            0|  0.00%|            reduction=reduction,
  2878|         0|            0|            0|  0.00%|        )
  2879|         0|            0|            0|  0.00%|    if size_average is not None or reduce is not None:
  2880|         0|            0|            0|  0.00%|        reduction_enum = _Reduction.legacy_get_enum(size_average, reduce)
  2881|         0|            0|            0|  0.00%|    else:
  2882|         0|            0|            0|  0.00%|        reduction_enum = _Reduction.get_enum(reduction)
  2883|         0|            0|            0|  0.00%|    if target.size() != input.size():
  2884|         0|            0|            0|  0.00%|        raise ValueError(
  2885|         0|            0|            0|  0.00%|            "Using a target size ({}) that is different to the input size ({}) is deprecated. "
  2886|         0|            0|            0|  0.00%|            "Please ensure they have the same size.".format(target.size(), input.size())
  2887|         0|            0|            0|  0.00%|        )
  2888|         0|            0|            0|  0.00%|
  2889|         0|            0|            0|  0.00%|    if weight is not None:
  2890|         0|            0|            0|  0.00%|        new_size = _infer_size(target.size(), weight.size())
  2891|         0|            0|            0|  0.00%|        weight = weight.expand(new_size)
  2892|         0|            0|            0|  0.00%|
  2893|         0|            0|            0|  0.00%|    return torch._C._nn.binary_cross_entropy(input, target, weight, reduction_enum)
  2894|         0|            0|            0|  0.00%|
  2895|         0|            0|            0|  0.00%|
  2896|         0|            0|            0|  0.00%|def binary_cross_entropy_with_logits(
  2897|         0|            0|            0|  0.00%|    input: Tensor,
  2898|         0|            0|            0|  0.00%|    target: Tensor,
  2899|         0|            0|            0|  0.00%|    weight: Optional[Tensor] = None,
  2900|         0|            0|            0|  0.00%|    size_average: Optional[bool] = None,
  2901|         0|            0|            0|  0.00%|    reduce: Optional[bool] = None,
  2902|         0|            0|            0|  0.00%|    reduction: str = "mean",
  2903|         0|            0|            0|  0.00%|    pos_weight: Optional[Tensor] = None,
  2904|         0|            0|            0|  0.00%|) -> Tensor:
  2905|         0|            0|            0|  0.00%|    r"""Function that measures Binary Cross Entropy between target and output
  2906|         0|            0|            0|  0.00%|    logits.
  2907|         0|            0|            0|  0.00%|
  2908|         0|            0|            0|  0.00%|    See :class:`~torch.nn.BCEWithLogitsLoss` for details.
  2909|         0|            0|            0|  0.00%|
  2910|         0|            0|            0|  0.00%|    Args:
  2911|         0|            0|            0|  0.00%|        input: Tensor of arbitrary shape
  2912|         0|            0|            0|  0.00%|        target: Tensor of the same shape as input
  2913|         0|            0|            0|  0.00%|        weight (Tensor, optional): a manual rescaling weight
  2914|         0|            0|            0|  0.00%|            if provided it's repeated to match input tensor shape
  2915|         0|            0|            0|  0.00%|        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,
  2916|         0|            0|            0|  0.00%|            the losses are averaged over each loss element in the batch. Note that for
  2917|         0|            0|            0|  0.00%|            some losses, there multiple elements per sample. If the field :attr:`size_average`
  2918|         0|            0|            0|  0.00%|            is set to ``False``, the losses are instead summed for each minibatch. Ignored
  2919|         0|            0|            0|  0.00%|            when reduce is ``False``. Default: ``True``
  2920|         0|            0|            0|  0.00%|        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the
  2921|         0|            0|            0|  0.00%|            losses are averaged or summed over observations for each minibatch depending
  2922|         0|            0|            0|  0.00%|            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per
  2923|         0|            0|            0|  0.00%|            batch element instead and ignores :attr:`size_average`. Default: ``True``
  2924|         0|            0|            0|  0.00%|        reduction (string, optional): Specifies the reduction to apply to the output:
  2925|         0|            0|            0|  0.00%|            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,
  2926|         0|            0|            0|  0.00%|            ``'mean'``: the sum of the output will be divided by the number of
  2927|         0|            0|            0|  0.00%|            elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`
  2928|         0|            0|            0|  0.00%|            and :attr:`reduce` are in the process of being deprecated, and in the meantime,
  2929|         0|            0|            0|  0.00%|            specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``
  2930|         0|            0|            0|  0.00%|        pos_weight (Tensor, optional): a weight of positive examples.
  2931|         0|            0|            0|  0.00%|                Must be a vector with length equal to the number of classes.
  2932|         0|            0|            0|  0.00%|
  2933|         0|            0|            0|  0.00%|    Examples::
  2934|         0|            0|            0|  0.00%|
  2935|         0|            0|            0|  0.00%|         >>> input = torch.randn(3, requires_grad=True)
  2936|         0|            0|            0|  0.00%|         >>> target = torch.empty(3).random_(2)
  2937|         0|            0|            0|  0.00%|         >>> loss = F.binary_cross_entropy_with_logits(input, target)
  2938|         0|            0|            0|  0.00%|         >>> loss.backward()
  2939|         0|            0|            0|  0.00%|    """
  2940|         0|            0|            0|  0.00%|    if has_torch_function_variadic(input, target):
  2941|         0|            0|            0|  0.00%|        return handle_torch_function(
  2942|         0|            0|            0|  0.00%|            binary_cross_entropy_with_logits,
  2943|         0|            0|            0|  0.00%|            (input, target),
  2944|         0|            0|            0|  0.00%|            input,
  2945|         0|            0|            0|  0.00%|            target,
  2946|         0|            0|            0|  0.00%|            weight=weight,
  2947|         0|            0|            0|  0.00%|            size_average=size_average,
  2948|         0|            0|            0|  0.00%|            reduce=reduce,
  2949|         0|            0|            0|  0.00%|            reduction=reduction,
  2950|         0|            0|            0|  0.00%|            pos_weight=pos_weight,
  2951|         0|            0|            0|  0.00%|        )
  2952|         0|            0|            0|  0.00%|    if size_average is not None or reduce is not None:
  2953|         0|            0|            0|  0.00%|        reduction_enum = _Reduction.legacy_get_enum(size_average, reduce)
  2954|         0|            0|            0|  0.00%|    else:
  2955|         0|            0|            0|  0.00%|        reduction_enum = _Reduction.get_enum(reduction)
  2956|         0|            0|            0|  0.00%|
  2957|         0|            0|            0|  0.00%|    if not (target.size() == input.size()):
  2958|         0|            0|            0|  0.00%|        raise ValueError("Target size ({}) must be the same as input size ({})".format(target.size(), input.size()))
  2959|         0|            0|            0|  0.00%|
  2960|         0|            0|            0|  0.00%|    return torch.binary_cross_entropy_with_logits(input, target, weight, pos_weight, reduction_enum)
  2961|         0|            0|            0|  0.00%|
  2962|         0|            0|            0|  0.00%|
  2963|         0|            0|            0|  0.00%|def smooth_l1_loss(
  2964|         0|            0|            0|  0.00%|    input: Tensor,
  2965|         0|            0|            0|  0.00%|    target: Tensor,
  2966|         0|            0|            0|  0.00%|    size_average: Optional[bool] = None,
  2967|         0|            0|            0|  0.00%|    reduce: Optional[bool] = None,
  2968|         0|            0|            0|  0.00%|    reduction: str = "mean",
  2969|         0|            0|            0|  0.00%|    beta: float = 1.0,
  2970|         0|            0|            0|  0.00%|) -> Tensor:
  2971|         0|            0|            0|  0.00%|    r"""Function that uses a squared term if the absolute
  2972|         0|            0|            0|  0.00%|    element-wise error falls below beta and an L1 term otherwise.
  2973|         0|            0|            0|  0.00%|
  2974|         0|            0|            0|  0.00%|    See :class:`~torch.nn.SmoothL1Loss` for details.
  2975|         0|            0|            0|  0.00%|    """
  2976|         0|            0|            0|  0.00%|    if has_torch_function_variadic(input, target):
  2977|         0|            0|            0|  0.00%|        return handle_torch_function(
  2978|         0|            0|            0|  0.00%|            smooth_l1_loss,
  2979|         0|            0|            0|  0.00%|            (input, target),
  2980|         0|            0|            0|  0.00%|            input,
  2981|         0|            0|            0|  0.00%|            target,
  2982|         0|            0|            0|  0.00%|            size_average=size_average,
  2983|         0|            0|            0|  0.00%|            reduce=reduce,
  2984|         0|            0|            0|  0.00%|            reduction=reduction,
  2985|         0|            0|            0|  0.00%|            beta=beta,
  2986|         0|            0|            0|  0.00%|        )
  2987|         0|            0|            0|  0.00%|    if not (target.size() == input.size()):
  2988|         0|            0|            0|  0.00%|        warnings.warn(
  2989|         0|            0|            0|  0.00%|            "Using a target size ({}) that is different to the input size ({}). "
  2990|         0|            0|            0|  0.00%|            "This will likely lead to incorrect results due to broadcasting. "
  2991|         0|            0|            0|  0.00%|            "Please ensure they have the same size.".format(target.size(), input.size()),
  2992|         0|            0|            0|  0.00%|            stacklevel=2,
  2993|         0|            0|            0|  0.00%|        )
  2994|         0|            0|            0|  0.00%|    if size_average is not None or reduce is not None:
  2995|         0|            0|            0|  0.00%|        reduction = _Reduction.legacy_get_string(size_average, reduce)
  2996|         0|            0|            0|  0.00%|
  2997|         0|            0|            0|  0.00%|    expanded_input, expanded_target = torch.broadcast_tensors(input, target)
  2998|         0|            0|            0|  0.00%|    return torch._C._nn.smooth_l1_loss(expanded_input, expanded_target, _Reduction.get_enum(reduction), beta)
  2999|         0|            0|            0|  0.00%|
  3000|         0|            0|            0|  0.00%|
  3001|         0|            0|            0|  0.00%|def huber_loss(
  3002|         0|            0|            0|  0.00%|    input: Tensor,
  3003|         0|            0|            0|  0.00%|    target: Tensor,
  3004|         0|            0|            0|  0.00%|    reduction: str = 'mean',
  3005|         0|            0|            0|  0.00%|    delta: float = 1.0,
  3006|         0|            0|            0|  0.00%|) -> Tensor:
  3007|         0|            0|            0|  0.00%|    r"""Function that uses a squared term if the absolute
  3008|         0|            0|            0|  0.00%|    element-wise error falls below delta and a delta-scaled L1 term otherwise.
  3009|         0|            0|            0|  0.00%|
  3010|         0|            0|            0|  0.00%|    See :class:`~torch.nn.HuberLoss` for details.
  3011|         0|            0|            0|  0.00%|    """
  3012|         0|            0|            0|  0.00%|    if has_torch_function_variadic(input, target):
  3013|         0|            0|            0|  0.00%|        return handle_torch_function(
  3014|         0|            0|            0|  0.00%|            huber_loss,
  3015|         0|            0|            0|  0.00%|            (input, target),
  3016|         0|            0|            0|  0.00%|            input,
  3017|         0|            0|            0|  0.00%|            target,
  3018|         0|            0|            0|  0.00%|            reduction=reduction,
  3019|         0|            0|            0|  0.00%|            delta=delta,
  3020|         0|            0|            0|  0.00%|        )
  3021|         0|            0|            0|  0.00%|    if not (target.size() == input.size()):
  3022|         0|            0|            0|  0.00%|        warnings.warn("Using a target size ({}) that is different to the input size ({}). "
  3023|         0|            0|            0|  0.00%|                      "This will likely lead to incorrect results due to broadcasting. "
  3024|         0|            0|            0|  0.00%|                      "Please ensure they have the same size.".format(target.size(), input.size()),
  3025|         0|            0|            0|  0.00%|                      stacklevel=2)
  3026|         0|            0|            0|  0.00%|
  3027|         0|            0|            0|  0.00%|    expanded_input, expanded_target = torch.broadcast_tensors(input, target)
  3028|         0|            0|            0|  0.00%|    return torch._C._nn.huber_loss(expanded_input, expanded_target, _Reduction.get_enum(reduction), delta)
  3029|         0|            0|            0|  0.00%|
  3030|         0|            0|            0|  0.00%|
  3031|         0|            0|            0|  0.00%|def l1_loss(
  3032|         0|            0|            0|  0.00%|    input: Tensor,
  3033|         0|            0|            0|  0.00%|    target: Tensor,
  3034|         0|            0|            0|  0.00%|    size_average: Optional[bool] = None,
  3035|         0|            0|            0|  0.00%|    reduce: Optional[bool] = None,
  3036|         0|            0|            0|  0.00%|    reduction: str = "mean",
  3037|         0|            0|            0|  0.00%|) -> Tensor:
  3038|         0|            0|            0|  0.00%|    r"""l1_loss(input, target, size_average=None, reduce=None, reduction='mean') -> Tensor
  3039|         0|            0|            0|  0.00%|
  3040|         0|            0|            0|  0.00%|    Function that takes the mean element-wise absolute value difference.
  3041|         0|            0|            0|  0.00%|
  3042|         0|            0|            0|  0.00%|    See :class:`~torch.nn.L1Loss` for details.
  3043|         0|            0|            0|  0.00%|    """
  3044|         0|            0|            0|  0.00%|    if has_torch_function_variadic(input, target):
  3045|         0|            0|            0|  0.00%|        return handle_torch_function(
  3046|         0|            0|            0|  0.00%|            l1_loss, (input, target), input, target, size_average=size_average, reduce=reduce, reduction=reduction
  3047|         0|            0|            0|  0.00%|        )
  3048|         0|            0|            0|  0.00%|    if not (target.size() == input.size()):
  3049|         0|            0|            0|  0.00%|        warnings.warn(
  3050|         0|            0|            0|  0.00%|            "Using a target size ({}) that is different to the input size ({}). "
  3051|         0|            0|            0|  0.00%|            "This will likely lead to incorrect results due to broadcasting. "
  3052|         0|            0|            0|  0.00%|            "Please ensure they have the same size.".format(target.size(), input.size()),
  3053|         0|            0|            0|  0.00%|            stacklevel=2,
  3054|         0|            0|            0|  0.00%|        )
  3055|         0|            0|            0|  0.00%|    if size_average is not None or reduce is not None:
  3056|         0|            0|            0|  0.00%|        reduction = _Reduction.legacy_get_string(size_average, reduce)
  3057|         0|            0|            0|  0.00%|
  3058|         0|            0|            0|  0.00%|    expanded_input, expanded_target = torch.broadcast_tensors(input, target)
  3059|         0|            0|            0|  0.00%|    return torch._C._nn.l1_loss(expanded_input, expanded_target, _Reduction.get_enum(reduction))
  3060|         0|            0|            0|  0.00%|
  3061|         0|            0|            0|  0.00%|
  3062|         0|            0|            0|  0.00%|def mse_loss(
  3063|         0|            0|            0|  0.00%|    input: Tensor,
  3064|         0|            0|            0|  0.00%|    target: Tensor,
  3065|         0|            0|            0|  0.00%|    size_average: Optional[bool] = None,
  3066|         0|            0|            0|  0.00%|    reduce: Optional[bool] = None,
  3067|         0|            0|            0|  0.00%|    reduction: str = "mean",
  3068|         0|            0|            0|  0.00%|) -> Tensor:
  3069|         0|            0|            0|  0.00%|    r"""mse_loss(input, target, size_average=None, reduce=None, reduction='mean') -> Tensor
  3070|         0|            0|            0|  0.00%|
  3071|         0|            0|            0|  0.00%|    Measures the element-wise mean squared error.
  3072|         0|            0|            0|  0.00%|
  3073|         0|            0|            0|  0.00%|    See :class:`~torch.nn.MSELoss` for details.
  3074|         0|            0|            0|  0.00%|    """
  3075|         0|            0|            0|  0.00%|    if has_torch_function_variadic(input, target):
  3076|         0|            0|            0|  0.00%|        return handle_torch_function(
  3077|         0|            0|            0|  0.00%|            mse_loss, (input, target), input, target, size_average=size_average, reduce=reduce, reduction=reduction
  3078|         0|            0|            0|  0.00%|        )
  3079|         0|            0|            0|  0.00%|    if not (target.size() == input.size()):
  3080|         0|            0|            0|  0.00%|        warnings.warn(
  3081|         0|            0|            0|  0.00%|            "Using a target size ({}) that is different to the input size ({}). "
  3082|         0|            0|            0|  0.00%|            "This will likely lead to incorrect results due to broadcasting. "
  3083|         0|            0|            0|  0.00%|            "Please ensure they have the same size.".format(target.size(), input.size()),
  3084|         0|            0|            0|  0.00%|            stacklevel=2,
  3085|         0|            0|            0|  0.00%|        )
  3086|         0|            0|            0|  0.00%|    if size_average is not None or reduce is not None:
  3087|         0|            0|            0|  0.00%|        reduction = _Reduction.legacy_get_string(size_average, reduce)
  3088|         0|            0|            0|  0.00%|
  3089|         0|            0|            0|  0.00%|    expanded_input, expanded_target = torch.broadcast_tensors(input, target)
  3090|         0|            0|            0|  0.00%|    return torch._C._nn.mse_loss(expanded_input, expanded_target, _Reduction.get_enum(reduction))
  3091|         0|            0|            0|  0.00%|
  3092|         0|            0|            0|  0.00%|
  3093|         0|            0|            0|  0.00%|def margin_ranking_loss(
  3094|         0|            0|            0|  0.00%|    input1: Tensor,
  3095|         0|            0|            0|  0.00%|    input2: Tensor,
  3096|         0|            0|            0|  0.00%|    target: Tensor,
  3097|         0|            0|            0|  0.00%|    margin: float = 0,
  3098|         0|            0|            0|  0.00%|    size_average: Optional[bool] = None,
  3099|         0|            0|            0|  0.00%|    reduce: Optional[bool] = None,
  3100|         0|            0|            0|  0.00%|    reduction: str = "mean",
  3101|         0|            0|            0|  0.00%|) -> Tensor:
  3102|         0|            0|            0|  0.00%|    r"""margin_ranking_loss(input1, input2, target, margin=0, size_average=None, reduce=None, reduction='mean') -> Tensor
  3103|         0|            0|            0|  0.00%|
  3104|         0|            0|            0|  0.00%|    See :class:`~torch.nn.MarginRankingLoss` for details.
  3105|         0|            0|            0|  0.00%|    """
  3106|         0|            0|            0|  0.00%|    if has_torch_function_variadic(input1, input2, target):
  3107|         0|            0|            0|  0.00%|        return handle_torch_function(
  3108|         0|            0|            0|  0.00%|            margin_ranking_loss,
  3109|         0|            0|            0|  0.00%|            (input1, input2, target),
  3110|         0|            0|            0|  0.00%|            input1,
  3111|         0|            0|            0|  0.00%|            input2,
  3112|         0|            0|            0|  0.00%|            target,
  3113|         0|            0|            0|  0.00%|            margin=margin,
  3114|         0|            0|            0|  0.00%|            size_average=size_average,
  3115|         0|            0|            0|  0.00%|            reduce=reduce,
  3116|         0|            0|            0|  0.00%|            reduction=reduction,
  3117|         0|            0|            0|  0.00%|        )
  3118|         0|            0|            0|  0.00%|    if size_average is not None or reduce is not None:
  3119|         0|            0|            0|  0.00%|        reduction_enum = _Reduction.legacy_get_enum(size_average, reduce)
  3120|         0|            0|            0|  0.00%|    else:
  3121|         0|            0|            0|  0.00%|        reduction_enum = _Reduction.get_enum(reduction)
  3122|         0|            0|            0|  0.00%|    if input1.dim() == 0 or input2.dim() == 0 or target.dim() == 0:
  3123|         0|            0|            0|  0.00%|        raise RuntimeError(
  3124|         0|            0|            0|  0.00%|            (
  3125|         0|            0|            0|  0.00%|                "margin_ranking_loss does not support scalars, got sizes: "
  3126|         0|            0|            0|  0.00%|                "input1: {}, input2: {}, target: {} ".format(input1.size(), input2.size(), target.size())
  3127|         0|            0|            0|  0.00%|            )
  3128|         0|            0|            0|  0.00%|        )
  3129|         0|            0|            0|  0.00%|    return torch.margin_ranking_loss(input1, input2, target, margin, reduction_enum)
  3130|         0|            0|            0|  0.00%|
  3131|         0|            0|            0|  0.00%|
  3132|         0|            0|            0|  0.00%|def hinge_embedding_loss(
  3133|         0|            0|            0|  0.00%|    input: Tensor,
  3134|         0|            0|            0|  0.00%|    target: Tensor,
  3135|         0|            0|            0|  0.00%|    margin: float = 1.0,
  3136|         0|            0|            0|  0.00%|    size_average: Optional[bool] = None,
  3137|         0|            0|            0|  0.00%|    reduce: Optional[bool] = None,
  3138|         0|            0|            0|  0.00%|    reduction: str = "mean",
  3139|         0|            0|            0|  0.00%|) -> Tensor:
  3140|         0|            0|            0|  0.00%|    r"""hinge_embedding_loss(input, target, margin=1.0, size_average=None, reduce=None, reduction='mean') -> Tensor
  3141|         0|            0|            0|  0.00%|
  3142|         0|            0|            0|  0.00%|    See :class:`~torch.nn.HingeEmbeddingLoss` for details.
  3143|         0|            0|            0|  0.00%|    """
  3144|         0|            0|            0|  0.00%|    if has_torch_function_variadic(input, target):
  3145|         0|            0|            0|  0.00%|        return handle_torch_function(
  3146|         0|            0|            0|  0.00%|            hinge_embedding_loss,
  3147|         0|            0|            0|  0.00%|            (input, target),
  3148|         0|            0|            0|  0.00%|            input,
  3149|         0|            0|            0|  0.00%|            target,
  3150|         0|            0|            0|  0.00%|            margin=margin,
  3151|         0|            0|            0|  0.00%|            size_average=size_average,
  3152|         0|            0|            0|  0.00%|            reduce=reduce,
  3153|         0|            0|            0|  0.00%|            reduction=reduction,
  3154|         0|            0|            0|  0.00%|        )
  3155|         0|            0|            0|  0.00%|    if size_average is not None or reduce is not None:
  3156|         0|            0|            0|  0.00%|        reduction_enum = _Reduction.legacy_get_enum(size_average, reduce)
  3157|         0|            0|            0|  0.00%|    else:
  3158|         0|            0|            0|  0.00%|        reduction_enum = _Reduction.get_enum(reduction)
  3159|         0|            0|            0|  0.00%|    return torch.hinge_embedding_loss(input, target, margin, reduction_enum)
  3160|         0|            0|            0|  0.00%|
  3161|         0|            0|            0|  0.00%|
  3162|         0|            0|            0|  0.00%|def multilabel_margin_loss(
  3163|         0|            0|            0|  0.00%|    input: Tensor,
  3164|         0|            0|            0|  0.00%|    target: Tensor,
  3165|         0|            0|            0|  0.00%|    size_average: Optional[bool] = None,
  3166|         0|            0|            0|  0.00%|    reduce: Optional[bool] = None,
  3167|         0|            0|            0|  0.00%|    reduction: str = "mean",
  3168|         0|            0|            0|  0.00%|) -> Tensor:
  3169|         0|            0|            0|  0.00%|    r"""multilabel_margin_loss(input, target, size_average=None, reduce=None, reduction='mean') -> Tensor
  3170|         0|            0|            0|  0.00%|
  3171|         0|            0|            0|  0.00%|    See :class:`~torch.nn.MultiLabelMarginLoss` for details.
  3172|         0|            0|            0|  0.00%|    """
  3173|         0|            0|            0|  0.00%|    if has_torch_function_variadic(input, target):
  3174|         0|            0|            0|  0.00%|        return handle_torch_function(
  3175|         0|            0|            0|  0.00%|            multilabel_margin_loss,
  3176|         0|            0|            0|  0.00%|            (input, target),
  3177|         0|            0|            0|  0.00%|            input,
  3178|         0|            0|            0|  0.00%|            target,
  3179|         0|            0|            0|  0.00%|            size_average=size_average,
  3180|         0|            0|            0|  0.00%|            reduce=reduce,
  3181|         0|            0|            0|  0.00%|            reduction=reduction,
  3182|         0|            0|            0|  0.00%|        )
  3183|         0|            0|            0|  0.00%|    if size_average is not None or reduce is not None:
  3184|         0|            0|            0|  0.00%|        reduction_enum = _Reduction.legacy_get_enum(size_average, reduce)
  3185|         0|            0|            0|  0.00%|    else:
  3186|         0|            0|            0|  0.00%|        reduction_enum = _Reduction.get_enum(reduction)
  3187|         0|            0|            0|  0.00%|    return torch._C._nn.multilabel_margin_loss(input, target, reduction_enum)
  3188|         0|            0|            0|  0.00%|
  3189|         0|            0|            0|  0.00%|
  3190|         0|            0|            0|  0.00%|def soft_margin_loss(
  3191|         0|            0|            0|  0.00%|    input: Tensor,
  3192|         0|            0|            0|  0.00%|    target: Tensor,
  3193|         0|            0|            0|  0.00%|    size_average: Optional[bool] = None,
  3194|         0|            0|            0|  0.00%|    reduce: Optional[bool] = None,
  3195|         0|            0|            0|  0.00%|    reduction: str = "mean",
  3196|         0|            0|            0|  0.00%|) -> Tensor:
  3197|         0|            0|            0|  0.00%|    r"""soft_margin_loss(input, target, size_average=None, reduce=None, reduction='mean') -> Tensor
  3198|         0|            0|            0|  0.00%|
  3199|         0|            0|            0|  0.00%|    See :class:`~torch.nn.SoftMarginLoss` for details.
  3200|         0|            0|            0|  0.00%|    """
  3201|         0|            0|            0|  0.00%|    if has_torch_function_variadic(input, target):
  3202|         0|            0|            0|  0.00%|        return handle_torch_function(
  3203|         0|            0|            0|  0.00%|            soft_margin_loss, (input, target), input, target, size_average=size_average, reduce=reduce, reduction=reduction
  3204|         0|            0|            0|  0.00%|        )
  3205|         0|            0|            0|  0.00%|    if size_average is not None or reduce is not None:
  3206|         0|            0|            0|  0.00%|        reduction_enum = _Reduction.legacy_get_enum(size_average, reduce)
  3207|         0|            0|            0|  0.00%|    else:
  3208|         0|            0|            0|  0.00%|        reduction_enum = _Reduction.get_enum(reduction)
  3209|         0|            0|            0|  0.00%|    return torch._C._nn.soft_margin_loss(input, target, reduction_enum)
  3210|         0|            0|            0|  0.00%|
  3211|         0|            0|            0|  0.00%|
  3212|         0|            0|            0|  0.00%|def multilabel_soft_margin_loss(
  3213|         0|            0|            0|  0.00%|    input: Tensor,
  3214|         0|            0|            0|  0.00%|    target: Tensor,
  3215|         0|            0|            0|  0.00%|    weight: Optional[Tensor] = None,
  3216|         0|            0|            0|  0.00%|    size_average: Optional[bool] = None,
  3217|         0|            0|            0|  0.00%|    reduce: Optional[bool] = None,
  3218|         0|            0|            0|  0.00%|    reduction: str = "mean",
  3219|         0|            0|            0|  0.00%|) -> Tensor:
  3220|         0|            0|            0|  0.00%|    r"""multilabel_soft_margin_loss(input, target, weight=None, size_average=None) -> Tensor
  3221|         0|            0|            0|  0.00%|
  3222|         0|            0|            0|  0.00%|    See :class:`~torch.nn.MultiLabelSoftMarginLoss` for details.
  3223|         0|            0|            0|  0.00%|    """
  3224|         0|            0|            0|  0.00%|    if has_torch_function_variadic(input, target):
  3225|         0|            0|            0|  0.00%|        return handle_torch_function(
  3226|         0|            0|            0|  0.00%|            multilabel_soft_margin_loss,
  3227|         0|            0|            0|  0.00%|            (input, target),
  3228|         0|            0|            0|  0.00%|            input,
  3229|         0|            0|            0|  0.00%|            target,
  3230|         0|            0|            0|  0.00%|            weight=weight,
  3231|         0|            0|            0|  0.00%|            size_average=size_average,
  3232|         0|            0|            0|  0.00%|            reduce=reduce,
  3233|         0|            0|            0|  0.00%|            reduction=reduction,
  3234|         0|            0|            0|  0.00%|        )
  3235|         0|            0|            0|  0.00%|    if size_average is not None or reduce is not None:
  3236|         0|            0|            0|  0.00%|        reduction = _Reduction.legacy_get_string(size_average, reduce)
  3237|         0|            0|            0|  0.00%|
  3238|         0|            0|            0|  0.00%|    loss = -(target * logsigmoid(input) + (1 - target) * logsigmoid(-input))
  3239|         0|            0|            0|  0.00%|
  3240|         0|            0|            0|  0.00%|    if weight is not None:
  3241|         0|            0|            0|  0.00%|        loss = loss * weight
  3242|         0|            0|            0|  0.00%|
  3243|         0|            0|            0|  0.00%|    loss = loss.sum(dim=1) / input.size(1)  # only return N loss values
  3244|         0|            0|            0|  0.00%|
  3245|         0|            0|            0|  0.00%|    if reduction == "none":
  3246|         0|            0|            0|  0.00%|        ret = loss
  3247|         0|            0|            0|  0.00%|    elif reduction == "mean":
  3248|         0|            0|            0|  0.00%|        ret = loss.mean()
  3249|         0|            0|            0|  0.00%|    elif reduction == "sum":
  3250|         0|            0|            0|  0.00%|        ret = loss.sum()
  3251|         0|            0|            0|  0.00%|    else:
  3252|         0|            0|            0|  0.00%|        ret = input
  3253|         0|            0|            0|  0.00%|        raise ValueError(reduction + " is not valid")
  3254|         0|            0|            0|  0.00%|    return ret
  3255|         0|            0|            0|  0.00%|
  3256|         0|            0|            0|  0.00%|
  3257|         0|            0|            0|  0.00%|def cosine_embedding_loss(
  3258|         0|            0|            0|  0.00%|    input1: Tensor,
  3259|         0|            0|            0|  0.00%|    input2: Tensor,
  3260|         0|            0|            0|  0.00%|    target: Tensor,
  3261|         0|            0|            0|  0.00%|    margin: float = 0,
  3262|         0|            0|            0|  0.00%|    size_average: Optional[bool] = None,
  3263|         0|            0|            0|  0.00%|    reduce: Optional[bool] = None,
  3264|         0|            0|            0|  0.00%|    reduction: str = "mean",
  3265|         0|            0|            0|  0.00%|) -> Tensor:
  3266|         0|            0|            0|  0.00%|    r"""cosine_embedding_loss(input1, input2, target, margin=0, size_average=None, reduce=None, reduction='mean') -> Tensor
  3267|         0|            0|            0|  0.00%|
  3268|         0|            0|            0|  0.00%|    See :class:`~torch.nn.CosineEmbeddingLoss` for details.
  3269|         0|            0|            0|  0.00%|    """
  3270|         0|            0|            0|  0.00%|    if has_torch_function_variadic(input1, input2, target):
  3271|         0|            0|            0|  0.00%|        return handle_torch_function(
  3272|         0|            0|            0|  0.00%|            cosine_embedding_loss,
  3273|         0|            0|            0|  0.00%|            (input1, input2, target),
  3274|         0|            0|            0|  0.00%|            input1,
  3275|         0|            0|            0|  0.00%|            input2,
  3276|         0|            0|            0|  0.00%|            target,
  3277|         0|            0|            0|  0.00%|            margin=margin,
  3278|         0|            0|            0|  0.00%|            size_average=size_average,
  3279|         0|            0|            0|  0.00%|            reduce=reduce,
  3280|         0|            0|            0|  0.00%|            reduction=reduction,
  3281|         0|            0|            0|  0.00%|        )
  3282|         0|            0|            0|  0.00%|    if size_average is not None or reduce is not None:
  3283|         0|            0|            0|  0.00%|        reduction_enum = _Reduction.legacy_get_enum(size_average, reduce)
  3284|         0|            0|            0|  0.00%|    else:
  3285|         0|            0|            0|  0.00%|        reduction_enum = _Reduction.get_enum(reduction)
  3286|         0|            0|            0|  0.00%|    return torch.cosine_embedding_loss(input1, input2, target, margin, reduction_enum)
  3287|         0|            0|            0|  0.00%|
  3288|         0|            0|            0|  0.00%|
  3289|         0|            0|            0|  0.00%|def multi_margin_loss(
  3290|         0|            0|            0|  0.00%|    input: Tensor,
  3291|         0|            0|            0|  0.00%|    target: Tensor,
  3292|         0|            0|            0|  0.00%|    p: int = 1,
  3293|         0|            0|            0|  0.00%|    margin: float = 1.0,
  3294|         0|            0|            0|  0.00%|    weight: Optional[Tensor] = None,
  3295|         0|            0|            0|  0.00%|    size_average: Optional[bool] = None,
  3296|         0|            0|            0|  0.00%|    reduce: Optional[bool] = None,
  3297|         0|            0|            0|  0.00%|    reduction: str = "mean",
  3298|         0|            0|            0|  0.00%|) -> Tensor:
  3299|         0|            0|            0|  0.00%|    r"""multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,
  3300|         0|            0|            0|  0.00%|                          reduce=None, reduction='mean') -> Tensor
  3301|         0|            0|            0|  0.00%|
  3302|         0|            0|            0|  0.00%|    See :class:`~torch.nn.MultiMarginLoss` for details.
  3303|         0|            0|            0|  0.00%|    """
  3304|         0|            0|            0|  0.00%|    if has_torch_function_variadic(input, target):
  3305|         0|            0|            0|  0.00%|        return handle_torch_function(
  3306|         0|            0|            0|  0.00%|            multi_margin_loss,
  3307|         0|            0|            0|  0.00%|            (input, target),
  3308|         0|            0|            0|  0.00%|            input,
  3309|         0|            0|            0|  0.00%|            target,
  3310|         0|            0|            0|  0.00%|            p=p,
  3311|         0|            0|            0|  0.00%|            margin=margin,
  3312|         0|            0|            0|  0.00%|            weight=weight,
  3313|         0|            0|            0|  0.00%|            size_average=size_average,
  3314|         0|            0|            0|  0.00%|            reduce=reduce,
  3315|         0|            0|            0|  0.00%|            reduction=reduction,
  3316|         0|            0|            0|  0.00%|        )
  3317|         0|            0|            0|  0.00%|    if size_average is not None or reduce is not None:
  3318|         0|            0|            0|  0.00%|        reduction_enum = _Reduction.legacy_get_enum(size_average, reduce)
  3319|         0|            0|            0|  0.00%|    else:
  3320|         0|            0|            0|  0.00%|        reduction_enum = _Reduction.get_enum(reduction)
  3321|         0|            0|            0|  0.00%|    if p != 1 and p != 2:
  3322|         0|            0|            0|  0.00%|        raise ValueError("only p == 1 and p == 2 supported")
  3323|         0|            0|            0|  0.00%|    if weight is not None:
  3324|         0|            0|            0|  0.00%|        if weight.dim() != 1:
  3325|         0|            0|            0|  0.00%|            raise ValueError("weight must be one-dimensional")
  3326|         0|            0|            0|  0.00%|
  3327|         0|            0|            0|  0.00%|    return torch._C._nn.multi_margin_loss(input, target, p, margin, weight, reduction_enum)
  3328|         0|            0|            0|  0.00%|
  3329|         0|            0|            0|  0.00%|
  3330|         0|            0|            0|  0.00%|pixel_shuffle = _add_docstr(
  3331|         0|            0|            0|  0.00%|    torch.pixel_shuffle,
  3332|         0|            0|            0|  0.00%|    r"""
  3333|         0|            0|            0|  0.00%|pixel_shuffle(input, upscale_factor) -> Tensor
  3334|         0|            0|            0|  0.00%|
  3335|         0|            0|            0|  0.00%|Rearranges elements in a tensor of shape :math:`(*, C \times r^2, H, W)` to a
  3336|         0|            0|            0|  0.00%|tensor of shape :math:`(*, C, H \times r, W \times r)`, where r is the :attr:`upscale_factor`.
  3337|         0|            0|            0|  0.00%|
  3338|         0|            0|            0|  0.00%|See :class:`~torch.nn.PixelShuffle` for details.
  3339|         0|            0|            0|  0.00%|
  3340|         0|            0|            0|  0.00%|Args:
  3341|         0|            0|            0|  0.00%|    input (Tensor): the input tensor
  3342|         0|            0|            0|  0.00%|    upscale_factor (int): factor to increase spatial resolution by
  3343|         0|            0|            0|  0.00%|
  3344|         0|            0|            0|  0.00%|Examples::
  3345|         0|            0|            0|  0.00%|
  3346|         0|            0|            0|  0.00%|    >>> input = torch.randn(1, 9, 4, 4)
  3347|         0|            0|            0|  0.00%|    >>> output = torch.nn.functional.pixel_shuffle(input, 3)
  3348|         0|            0|            0|  0.00%|    >>> print(output.size())
  3349|         0|            0|            0|  0.00%|    torch.Size([1, 1, 12, 12])
  3350|         0|            0|            0|  0.00%|""",
  3351|         0|            0|            0|  0.00%|)
  3352|         0|            0|            0|  0.00%|
  3353|         0|            0|            0|  0.00%|pixel_unshuffle = _add_docstr(
  3354|         0|            0|            0|  0.00%|    torch.pixel_unshuffle,
  3355|         0|            0|            0|  0.00%|    r"""
  3356|         0|            0|            0|  0.00%|pixel_unshuffle(input, downscale_factor) -> Tensor
  3357|         0|            0|            0|  0.00%|
  3358|         0|            0|            0|  0.00%|Reverses the :class:`~torch.nn.PixelShuffle` operation by rearranging elements in a
  3359|         0|            0|            0|  0.00%|tensor of shape :math:`(*, C, H \times r, W \times r)` to a tensor of shape
  3360|         0|            0|            0|  0.00%|:math:`(*, C \times r^2, H, W)`, where r is the :attr:`downscale_factor`.
  3361|         0|            0|            0|  0.00%|
  3362|         0|            0|            0|  0.00%|See :class:`~torch.nn.PixelUnshuffle` for details.
  3363|         0|            0|            0|  0.00%|
  3364|         0|            0|            0|  0.00%|Args:
  3365|         0|            0|            0|  0.00%|    input (Tensor): the input tensor
  3366|         0|            0|            0|  0.00%|    downscale_factor (int): factor to increase spatial resolution by
  3367|         0|            0|            0|  0.00%|
  3368|         0|            0|            0|  0.00%|Examples::
  3369|         0|            0|            0|  0.00%|
  3370|         0|            0|            0|  0.00%|    >>> input = torch.randn(1, 1, 12, 12)
  3371|         0|            0|            0|  0.00%|    >>> output = torch.nn.functional.pixel_unshuffle(input, 3)
  3372|         0|            0|            0|  0.00%|    >>> print(output.size())
  3373|         0|            0|            0|  0.00%|    torch.Size([1, 9, 4, 4])
  3374|         0|            0|            0|  0.00%|""",
  3375|         0|            0|            0|  0.00%|)
  3376|         0|            0|            0|  0.00%|
  3377|         0|            0|            0|  0.00%|channel_shuffle = _add_docstr(
  3378|         0|            0|            0|  0.00%|    torch.channel_shuffle,
  3379|         0|            0|            0|  0.00%|    r"""
  3380|         0|            0|            0|  0.00%|channel_shuffle(input, groups) -> Tensor
  3381|         0|            0|            0|  0.00%|
  3382|         0|            0|            0|  0.00%|Divide the channels in a tensor of shape :math:`(*, C , H, W)`
  3383|         0|            0|            0|  0.00%|into g groups and rearrange them as :math:`(*, C \frac g, g, H, W)`,
  3384|         0|            0|            0|  0.00%|while keeping the original tensor shape.
  3385|         0|            0|            0|  0.00%|
  3386|         0|            0|            0|  0.00%|See :class:`~torch.nn.ChannelShuffle` for details.
  3387|         0|            0|            0|  0.00%|
  3388|         0|            0|            0|  0.00%|Args:
  3389|         0|            0|            0|  0.00%|    input (Tensor): the input tensor
  3390|         0|            0|            0|  0.00%|    groups (int): number of groups to divide channels in and rearrange.
  3391|         0|            0|            0|  0.00%|
  3392|         0|            0|            0|  0.00%|Examples::
  3393|         0|            0|            0|  0.00%|
  3394|         0|            0|            0|  0.00%|    >>> input = torch.randn(1, 4, 2, 2)
  3395|         0|            0|            0|  0.00%|    >>> print(input)
  3396|         0|            0|            0|  0.00%|    [[[[1, 2],
  3397|         0|            0|            0|  0.00%|       [3, 4]],
  3398|         0|            0|            0|  0.00%|      [[5, 6],
  3399|         0|            0|            0|  0.00%|       [7, 8]],
  3400|         0|            0|            0|  0.00%|      [[9, 10],
  3401|         0|            0|            0|  0.00%|       [11, 12]],
  3402|         0|            0|            0|  0.00%|      [[13, 14],
  3403|         0|            0|            0|  0.00%|       [15, 16]],
  3404|         0|            0|            0|  0.00%|     ]]
  3405|         0|            0|            0|  0.00%|    >>> output = torch.nn.functional.channel_shuffle(input, 2)
  3406|         0|            0|            0|  0.00%|    >>> print(output)
  3407|         0|            0|            0|  0.00%|    [[[[1, 2],
  3408|         0|            0|            0|  0.00%|       [3, 4]],
  3409|         0|            0|            0|  0.00%|      [[9, 10],
  3410|         0|            0|            0|  0.00%|       [11, 12]],
  3411|         0|            0|            0|  0.00%|      [[5, 6],
  3412|         0|            0|            0|  0.00%|       [7, 8]],
  3413|         0|            0|            0|  0.00%|      [[13, 14],
  3414|         0|            0|            0|  0.00%|       [15, 16]],
  3415|         0|            0|            0|  0.00%|     ]]
  3416|         0|            0|            0|  0.00%|""",
  3417|         0|            0|            0|  0.00%|)
  3418|         0|            0|            0|  0.00%|
  3419|         0|            0|            0|  0.00%|
  3420|         0|            0|            0|  0.00%|@_overload  # noqa: F811
  3421|         0|            0|            0|  0.00%|def upsample(input: Tensor, size: Optional[int] = None, scale_factor: Optional[float] = None, mode: str = "nearest", align_corners: Optional[bool] = None) -> Tensor:  # noqa: F811
  3422|         0|            0|            0|  0.00%|    pass
  3423|         0|            0|            0|  0.00%|
  3424|         0|            0|            0|  0.00%|
  3425|         0|            0|            0|  0.00%|@_overload  # noqa: F811
  3426|         0|            0|            0|  0.00%|def upsample(input: Tensor, size: Optional[List[int]] = None, scale_factor: Optional[float] = None, mode: str = "nearest", align_corners: Optional[bool] = None) -> Tensor:  # noqa: F811
  3427|         0|            0|            0|  0.00%|    pass
  3428|         0|            0|            0|  0.00%|
  3429|         0|            0|            0|  0.00%|
  3430|         0|            0|            0|  0.00%|def upsample(input, size=None, scale_factor=None, mode="nearest", align_corners=None):  # noqa: F811
  3431|         0|            0|            0|  0.00%|    r"""Upsamples the input to either the given :attr:`size` or the given
  3432|         0|            0|            0|  0.00%|    :attr:`scale_factor`
  3433|         0|            0|            0|  0.00%|
  3434|         0|            0|            0|  0.00%|    .. warning::
  3435|         0|            0|            0|  0.00%|        This function is deprecated in favor of :func:`torch.nn.functional.interpolate`.
  3436|         0|            0|            0|  0.00%|        This is equivalent with ``nn.functional.interpolate(...)``.
  3437|         0|            0|            0|  0.00%|
  3438|         0|            0|            0|  0.00%|    Note:
  3439|         0|            0|            0|  0.00%|        {backward_reproducibility_note}
  3440|         0|            0|            0|  0.00%|
  3441|         0|            0|            0|  0.00%|    The algorithm used for upsampling is determined by :attr:`mode`.
  3442|         0|            0|            0|  0.00%|
  3443|         0|            0|            0|  0.00%|    Currently temporal, spatial and volumetric upsampling are supported, i.e.
  3444|         0|            0|            0|  0.00%|    expected inputs are 3-D, 4-D or 5-D in shape.
  3445|         0|            0|            0|  0.00%|
  3446|         0|            0|            0|  0.00%|    The input dimensions are interpreted in the form:
  3447|         0|            0|            0|  0.00%|    `mini-batch x channels x [optional depth] x [optional height] x width`.
  3448|         0|            0|            0|  0.00%|
  3449|         0|            0|            0|  0.00%|    The modes available for upsampling are: `nearest`, `linear` (3D-only),
  3450|         0|            0|            0|  0.00%|    `bilinear`, `bicubic` (4D-only), `trilinear` (5D-only)
  3451|         0|            0|            0|  0.00%|
  3452|         0|            0|            0|  0.00%|    Args:
  3453|         0|            0|            0|  0.00%|        input (Tensor): the input tensor
  3454|         0|            0|            0|  0.00%|        size (int or Tuple[int] or Tuple[int, int] or Tuple[int, int, int]):
  3455|         0|            0|            0|  0.00%|            output spatial size.
  3456|         0|            0|            0|  0.00%|        scale_factor (float or Tuple[float]): multiplier for spatial size. Has to match input size if it is a tuple.
  3457|         0|            0|            0|  0.00%|        mode (string): algorithm used for upsampling:
  3458|         0|            0|            0|  0.00%|            ``'nearest'`` | ``'linear'`` | ``'bilinear'`` | ``'bicubic'`` |
  3459|         0|            0|            0|  0.00%|            ``'trilinear'``. Default: ``'nearest'``
  3460|         0|            0|            0|  0.00%|        align_corners (bool, optional): Geometrically, we consider the pixels of the
  3461|         0|            0|            0|  0.00%|            input and output as squares rather than points.
  3462|         0|            0|            0|  0.00%|            If set to ``True``, the input and output tensors are aligned by the
  3463|         0|            0|            0|  0.00%|            center points of their corner pixels, preserving the values at the corner pixels.
  3464|         0|            0|            0|  0.00%|            If set to ``False``, the input and output tensors are aligned by the corner
  3465|         0|            0|            0|  0.00%|            points of their corner pixels, and the interpolation uses edge value padding
  3466|         0|            0|            0|  0.00%|            for out-of-boundary values, making this operation *independent* of input size
  3467|         0|            0|            0|  0.00%|            when :attr:`scale_factor` is kept the same. This only has an effect when :attr:`mode`
  3468|         0|            0|            0|  0.00%|            is ``'linear'``, ``'bilinear'``, ``'bicubic'`` or ``'trilinear'``.
  3469|         0|            0|            0|  0.00%|            Default: ``False``
  3470|         0|            0|            0|  0.00%|
  3471|         0|            0|            0|  0.00%|    .. note::
  3472|         0|            0|            0|  0.00%|        With ``mode='bicubic'``, it's possible to cause overshoot, in other words it can produce
  3473|         0|            0|            0|  0.00%|        negative values or values greater than 255 for images.
  3474|         0|            0|            0|  0.00%|        Explicitly call ``result.clamp(min=0, max=255)`` if you want to reduce the overshoot
  3475|         0|            0|            0|  0.00%|        when displaying the image.
  3476|         0|            0|            0|  0.00%|
  3477|         0|            0|            0|  0.00%|    .. warning::
  3478|         0|            0|            0|  0.00%|        With ``align_corners = True``, the linearly interpolating modes
  3479|         0|            0|            0|  0.00%|        (`linear`, `bilinear`, and `trilinear`) don't proportionally align the
  3480|         0|            0|            0|  0.00%|        output and input pixels, and thus the output values can depend on the
  3481|         0|            0|            0|  0.00%|        input size. This was the default behavior for these modes up to version
  3482|         0|            0|            0|  0.00%|        0.3.1. Since then, the default behavior is ``align_corners = False``.
  3483|         0|            0|            0|  0.00%|        See :class:`~torch.nn.Upsample` for concrete examples on how this
  3484|         0|            0|            0|  0.00%|        affects the outputs.
  3485|         0|            0|            0|  0.00%|
  3486|         0|            0|            0|  0.00%|    """
  3487|         0|            0|            0|  0.00%|    warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
  3488|         0|            0|            0|  0.00%|    return interpolate(input, size, scale_factor, mode, align_corners)
  3489|         0|            0|            0|  0.00%|
  3490|         0|            0|            0|  0.00%|
  3491|         0|            0|            0|  0.00%|upsample.__doc__ = upsample.__doc__.format(**reproducibility_notes)
  3492|         0|            0|            0|  0.00%|
  3493|         0|            0|            0|  0.00%|
  3494|         0|            0|            0|  0.00%|@_overload  # noqa: F811
  3495|         0|            0|            0|  0.00%|def interpolate(input: Tensor, size: Optional[int] = None, scale_factor: Optional[List[float]] = None, mode: str = 'nearest', align_corners: Optional[bool] = None, recompute_scale_factor: Optional[bool] = None) -> Tensor:  # noqa: F811
  3496|         0|            0|            0|  0.00%|    pass
  3497|         0|            0|            0|  0.00%|
  3498|         0|            0|            0|  0.00%|
  3499|         0|            0|            0|  0.00%|@_overload  # noqa: F811
  3500|         0|            0|            0|  0.00%|def interpolate(input: Tensor, size: Optional[List[int]] = None, scale_factor: Optional[List[float]] = None, mode: str = 'nearest', align_corners: Optional[bool] = None, recompute_scale_factor: Optional[bool] = None) -> Tensor:  # noqa: F811
  3501|         0|            0|            0|  0.00%|    pass
  3502|         0|            0|            0|  0.00%|
  3503|         0|            0|            0|  0.00%|
  3504|         0|            0|            0|  0.00%|@_overload  # noqa: F811
  3505|         0|            0|            0|  0.00%|def interpolate(input: Tensor, size: Optional[int] = None, scale_factor: Optional[float] = None, mode: str = 'nearest', align_corners: Optional[bool] = None, recompute_scale_factor: Optional[bool] = None) -> Tensor:  # noqa: F811
  3506|         0|            0|            0|  0.00%|    pass
  3507|         0|            0|            0|  0.00%|
  3508|         0|            0|            0|  0.00%|
  3509|         0|            0|            0|  0.00%|@_overload  # noqa: F811
  3510|         0|            0|            0|  0.00%|def interpolate(  # noqa: F811
  3511|         0|            0|            0|  0.00%|    input: Tensor,
  3512|         0|            0|            0|  0.00%|    size: Optional[List[int]] = None,
  3513|         0|            0|            0|  0.00%|    scale_factor: Optional[float] = None,
  3514|         0|            0|            0|  0.00%|    mode: str = "nearest",
  3515|         0|            0|            0|  0.00%|    align_corners: Optional[bool] = None,
  3516|         0|            0|            0|  0.00%|    recompute_scale_factor: Optional[bool] = None,
  3517|         0|            0|            0|  0.00%|) -> Tensor:  # noqa: F811
  3518|         0|            0|            0|  0.00%|    pass
  3519|         0|            0|            0|  0.00%|
  3520|         0|            0|            0|  0.00%|def interpolate(input: Tensor, size: Optional[int] = None, scale_factor: Optional[List[float]] = None, mode: str = 'nearest', align_corners: Optional[bool] = None, recompute_scale_factor: Optional[bool] = None) -> Tensor:  # noqa: F811
  3521|         0|            0|            0|  0.00%|    r"""Down/up samples the input to either the given :attr:`size` or the given
  3522|         0|            0|            0|  0.00%|    :attr:`scale_factor`
  3523|         0|            0|            0|  0.00%|
  3524|         0|            0|            0|  0.00%|    The algorithm used for interpolation is determined by :attr:`mode`.
  3525|         0|            0|            0|  0.00%|
  3526|         0|            0|            0|  0.00%|    Currently temporal, spatial and volumetric sampling are supported, i.e.
  3527|         0|            0|            0|  0.00%|    expected inputs are 3-D, 4-D or 5-D in shape.
  3528|         0|            0|            0|  0.00%|
  3529|         0|            0|            0|  0.00%|    The input dimensions are interpreted in the form:
  3530|         0|            0|            0|  0.00%|    `mini-batch x channels x [optional depth] x [optional height] x width`.
  3531|         0|            0|            0|  0.00%|
  3532|         0|            0|            0|  0.00%|    The modes available for resizing are: `nearest`, `linear` (3D-only),
  3533|         0|            0|            0|  0.00%|    `bilinear`, `bicubic` (4D-only), `trilinear` (5D-only), `area`
  3534|         0|            0|            0|  0.00%|
  3535|         0|            0|            0|  0.00%|    Args:
  3536|         0|            0|            0|  0.00%|        input (Tensor): the input tensor
  3537|         0|            0|            0|  0.00%|        size (int or Tuple[int] or Tuple[int, int] or Tuple[int, int, int]):
  3538|         0|            0|            0|  0.00%|            output spatial size.
  3539|         0|            0|            0|  0.00%|        scale_factor (float or Tuple[float]): multiplier for spatial size. Has to match input size if it is a tuple.
  3540|         0|            0|            0|  0.00%|        mode (str): algorithm used for upsampling:
  3541|         0|            0|            0|  0.00%|            ``'nearest'`` | ``'linear'`` | ``'bilinear'`` | ``'bicubic'`` |
  3542|         0|            0|            0|  0.00%|            ``'trilinear'`` | ``'area'``. Default: ``'nearest'``
  3543|         0|            0|            0|  0.00%|        align_corners (bool, optional): Geometrically, we consider the pixels of the
  3544|         0|            0|            0|  0.00%|            input and output as squares rather than points.
  3545|         0|            0|            0|  0.00%|            If set to ``True``, the input and output tensors are aligned by the
  3546|         0|            0|            0|  0.00%|            center points of their corner pixels, preserving the values at the corner pixels.
  3547|         0|            0|            0|  0.00%|            If set to ``False``, the input and output tensors are aligned by the corner
  3548|         0|            0|            0|  0.00%|            points of their corner pixels, and the interpolation uses edge value padding
  3549|         0|            0|            0|  0.00%|            for out-of-boundary values, making this operation *independent* of input size
  3550|         0|            0|            0|  0.00%|            when :attr:`scale_factor` is kept the same. This only has an effect when :attr:`mode`
  3551|         0|            0|            0|  0.00%|            is ``'linear'``, ``'bilinear'``, ``'bicubic'`` or ``'trilinear'``.
  3552|         0|            0|            0|  0.00%|            Default: ``False``
  3553|         0|            0|            0|  0.00%|        recompute_scale_factor (bool, optional): recompute the scale_factor for use in the
  3554|         0|            0|            0|  0.00%|            interpolation calculation.  When `scale_factor` is passed as a parameter, it is used
  3555|         0|            0|            0|  0.00%|            to compute the `output_size`.  If `recompute_scale_factor` is ``False`` or not specified,
  3556|         0|            0|            0|  0.00%|            the passed-in `scale_factor` will be used in the interpolation computation.
  3557|         0|            0|            0|  0.00%|            Otherwise, a new `scale_factor` will be computed based on the output and input sizes for
  3558|         0|            0|            0|  0.00%|            use in the interpolation computation (i.e. the computation will be identical to if the computed
  3559|         0|            0|            0|  0.00%|            `output_size` were passed-in explicitly).  Note that when `scale_factor` is floating-point,
  3560|         0|            0|            0|  0.00%|            the recomputed scale_factor may differ from the one passed in due to rounding and precision
  3561|         0|            0|            0|  0.00%|            issues.
  3562|         0|            0|            0|  0.00%|
  3563|         0|            0|            0|  0.00%|    .. note::
  3564|         0|            0|            0|  0.00%|        With ``mode='bicubic'``, it's possible to cause overshoot, in other words it can produce
  3565|         0|            0|            0|  0.00%|        negative values or values greater than 255 for images.
  3566|         0|            0|            0|  0.00%|        Explicitly call ``result.clamp(min=0, max=255)`` if you want to reduce the overshoot
  3567|         0|            0|            0|  0.00%|        when displaying the image.
  3568|         0|            0|            0|  0.00%|
  3569|         0|            0|            0|  0.00%|    .. warning::
  3570|         0|            0|            0|  0.00%|        With ``align_corners = True``, the linearly interpolating modes
  3571|         0|            0|            0|  0.00%|        (`linear`, `bilinear`, and `trilinear`) don't proportionally align the
  3572|         0|            0|            0|  0.00%|        output and input pixels, and thus the output values can depend on the
  3573|         0|            0|            0|  0.00%|        input size. This was the default behavior for these modes up to version
  3574|         0|            0|            0|  0.00%|        0.3.1. Since then, the default behavior is ``align_corners = False``.
  3575|         0|            0|            0|  0.00%|        See :class:`~torch.nn.Upsample` for concrete examples on how this
  3576|         0|            0|            0|  0.00%|        affects the outputs.
  3577|         0|            0|            0|  0.00%|
  3578|         0|            0|            0|  0.00%|    .. warning::
  3579|         0|            0|            0|  0.00%|        When scale_factor is specified, if recompute_scale_factor=True,
  3580|         0|            0|            0|  0.00%|        scale_factor is used to compute the output_size which will then
  3581|         0|            0|            0|  0.00%|        be used to infer new scales for the interpolation.
  3582|         0|            0|            0|  0.00%|        The default behavior for recompute_scale_factor changed to False
  3583|         0|            0|            0|  0.00%|        in 1.6.0, and scale_factor is used in the interpolation
  3584|         0|            0|            0|  0.00%|        calculation.
  3585|         0|            0|            0|  0.00%|
  3586|         0|            0|            0|  0.00%|    Note:
  3587|         0|            0|            0|  0.00%|        {backward_reproducibility_note}
  3588|         0|            0|            0|  0.00%|    """
  3589|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):
  3590|         0|            0|            0|  0.00%|        return handle_torch_function(
  3591|         0|            0|            0|  0.00%|            interpolate,
  3592|         0|            0|            0|  0.00%|            (input,),
  3593|         0|            0|            0|  0.00%|            input,
  3594|         0|            0|            0|  0.00%|            size=size,
  3595|         0|            0|            0|  0.00%|            scale_factor=scale_factor,
  3596|         0|            0|            0|  0.00%|            mode=mode,
  3597|         0|            0|            0|  0.00%|            align_corners=align_corners,
  3598|         0|            0|            0|  0.00%|            recompute_scale_factor=recompute_scale_factor,
  3599|         0|            0|            0|  0.00%|        )
  3600|         0|            0|            0|  0.00%|
  3601|         0|            0|            0|  0.00%|    if mode in ("nearest", "area"):
  3602|         0|            0|            0|  0.00%|        if align_corners is not None:
  3603|         0|            0|            0|  0.00%|            raise ValueError(
  3604|         0|            0|            0|  0.00%|                "align_corners option can only be set with the "
  3605|         0|            0|            0|  0.00%|                "interpolating modes: linear | bilinear | bicubic | trilinear"
  3606|         0|            0|            0|  0.00%|            )
  3607|         0|            0|            0|  0.00%|    else:
  3608|         0|            0|            0|  0.00%|        if align_corners is None:
  3609|         0|            0|            0|  0.00%|            warnings.warn(
  3610|         0|            0|            0|  0.00%|                "Default upsampling behavior when mode={} is changed "
  3611|         0|            0|            0|  0.00%|                "to align_corners=False since 0.4.0. Please specify "
  3612|         0|            0|            0|  0.00%|                "align_corners=True if the old behavior is desired. "
  3613|         0|            0|            0|  0.00%|                "See the documentation of nn.Upsample for details.".format(mode)
  3614|         0|            0|            0|  0.00%|            )
  3615|         0|            0|            0|  0.00%|            align_corners = False
  3616|         0|            0|            0|  0.00%|
  3617|         0|            0|            0|  0.00%|    dim = input.dim() - 2  # Number of spatial dimensions.
  3618|         0|            0|            0|  0.00%|
  3619|         0|            0|            0|  0.00%|    # Process size and scale_factor.  Validate that exactly one is set.
  3620|         0|            0|            0|  0.00%|    # Validate its length if it is a list, or expand it if it is a scalar.
  3621|         0|            0|            0|  0.00%|    # After this block, exactly one of output_size and scale_factors will
  3622|         0|            0|            0|  0.00%|    # be non-None, and it will be a list (or tuple).
  3623|         0|            0|            0|  0.00%|    if size is not None and scale_factor is not None:
  3624|         0|            0|            0|  0.00%|        raise ValueError("only one of size or scale_factor should be defined")
  3625|         0|            0|            0|  0.00%|    elif size is not None:
  3626|         0|            0|            0|  0.00%|        assert scale_factor is None
  3627|         0|            0|            0|  0.00%|        scale_factors = None
  3628|         0|            0|            0|  0.00%|        if isinstance(size, (list, tuple)):
  3629|         0|            0|            0|  0.00%|            if len(size) != dim:
  3630|         0|            0|            0|  0.00%|                raise ValueError(
  3631|         0|            0|            0|  0.00%|                    "size shape must match input shape. " "Input is {}D, size is {}".format(dim, len(size))
  3632|         0|            0|            0|  0.00%|                )
  3633|         0|            0|            0|  0.00%|            output_size = size
  3634|         0|            0|            0|  0.00%|        else:
  3635|         0|            0|            0|  0.00%|            output_size = [size for _ in range(dim)]
  3636|         0|            0|            0|  0.00%|    elif scale_factor is not None:
  3637|         0|            0|            0|  0.00%|        assert size is None
  3638|         0|            0|            0|  0.00%|        output_size = None
  3639|         0|            0|            0|  0.00%|        if isinstance(scale_factor, (list, tuple)):
  3640|         0|            0|            0|  0.00%|            if len(scale_factor) != dim:
  3641|         0|            0|            0|  0.00%|                raise ValueError(
  3642|         0|            0|            0|  0.00%|                    "scale_factor shape must match input shape. "
  3643|         0|            0|            0|  0.00%|                    "Input is {}D, scale_factor is {}".format(dim, len(scale_factor))
  3644|         0|            0|            0|  0.00%|                )
  3645|         0|            0|            0|  0.00%|            scale_factors = scale_factor
  3646|         0|            0|            0|  0.00%|        else:
  3647|         0|            0|            0|  0.00%|            scale_factors = [scale_factor for _ in range(dim)]
  3648|         0|            0|            0|  0.00%|    else:
  3649|         0|            0|            0|  0.00%|        raise ValueError("either size or scale_factor should be defined")
  3650|         0|            0|            0|  0.00%|
  3651|         0|            0|            0|  0.00%|    if recompute_scale_factor is None:
  3652|         0|            0|            0|  0.00%|        # only warn when the scales have floating values since
  3653|         0|            0|            0|  0.00%|        # the result for ints is the same with/without recompute_scale_factor
  3654|         0|            0|            0|  0.00%|        if scale_factors is not None:
  3655|         0|            0|            0|  0.00%|            for scale in scale_factors:
  3656|         0|            0|            0|  0.00%|                if math.floor(scale) != scale:
  3657|         0|            0|            0|  0.00%|                    warnings.warn(
  3658|         0|            0|            0|  0.00%|                        "The default behavior for interpolate/upsample with float scale_factor changed "
  3659|         0|            0|            0|  0.00%|                        "in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, "
  3660|         0|            0|            0|  0.00%|                        "instead of relying on the computed output size. "
  3661|         0|            0|            0|  0.00%|                        "If you wish to restore the old behavior, please set recompute_scale_factor=True. "
  3662|         0|            0|            0|  0.00%|                        "See the documentation of nn.Upsample for details. "
  3663|         0|            0|            0|  0.00%|                    )
  3664|         0|            0|            0|  0.00%|                    break
  3665|         0|            0|            0|  0.00%|    elif recompute_scale_factor and size is not None:
  3666|         0|            0|            0|  0.00%|        raise ValueError("recompute_scale_factor is not meaningful with an explicit size.")
  3667|         0|            0|            0|  0.00%|
  3668|         0|            0|            0|  0.00%|    # "area" mode always requires an explicit size rather than scale factor.
  3669|         0|            0|            0|  0.00%|    # Re-use the recompute_scale_factor code path.
  3670|         0|            0|            0|  0.00%|    if mode == "area" and output_size is None:
  3671|         0|            0|            0|  0.00%|        recompute_scale_factor = True
  3672|         0|            0|            0|  0.00%|
  3673|         0|            0|            0|  0.00%|    if recompute_scale_factor is not None and recompute_scale_factor:
  3674|         0|            0|            0|  0.00%|        # We compute output_size here, then un-set scale_factors.
  3675|         0|            0|            0|  0.00%|        # The C++ code will recompute it based on the (integer) output size.
  3676|         0|            0|            0|  0.00%|        if not torch.jit.is_scripting() and torch._C._get_tracing_state():
  3677|         0|            0|            0|  0.00%|            # make scale_factor a tensor in tracing so constant doesn't get baked in
  3678|         0|            0|            0|  0.00%|            output_size = [
  3679|         0|            0|            0|  0.00%|                (torch.floor((input.size(i + 2).float() * torch.tensor(scale_factors[i], dtype=torch.float32)).float()))
  3680|         0|            0|            0|  0.00%|                for i in range(dim)
  3681|         0|            0|            0|  0.00%|            ]
  3682|         0|            0|            0|  0.00%|        else:
  3683|         0|            0|            0|  0.00%|            assert scale_factors is not None
  3684|         0|            0|            0|  0.00%|            output_size = [int(math.floor(float(input.size(i + 2)) * scale_factors[i])) for i in range(dim)]
  3685|         0|            0|            0|  0.00%|        scale_factors = None
  3686|         0|            0|            0|  0.00%|
  3687|         0|            0|            0|  0.00%|    if input.dim() == 3 and mode == "nearest":
  3688|         0|            0|            0|  0.00%|        return torch._C._nn.upsample_nearest1d(input, output_size, scale_factors)
  3689|         0|            0|            0|  0.00%|    if input.dim() == 4 and mode == "nearest":
  3690|         0|            0|            0|  0.00%|        return torch._C._nn.upsample_nearest2d(input, output_size, scale_factors)
  3691|         0|            0|            0|  0.00%|    if input.dim() == 5 and mode == "nearest":
  3692|         0|            0|            0|  0.00%|        return torch._C._nn.upsample_nearest3d(input, output_size, scale_factors)
  3693|         0|            0|            0|  0.00%|
  3694|         0|            0|            0|  0.00%|    if input.dim() == 3 and mode == "area":
  3695|         0|            0|            0|  0.00%|        assert output_size is not None
  3696|         0|            0|            0|  0.00%|        return adaptive_avg_pool1d(input, output_size)
  3697|         0|            0|            0|  0.00%|    if input.dim() == 4 and mode == "area":
  3698|         0|            0|            0|  0.00%|        assert output_size is not None
  3699|         0|            0|            0|  0.00%|        return adaptive_avg_pool2d(input, output_size)
  3700|         0|            0|            0|  0.00%|    if input.dim() == 5 and mode == "area":
  3701|         0|            0|            0|  0.00%|        assert output_size is not None
  3702|         0|            0|            0|  0.00%|        return adaptive_avg_pool3d(input, output_size)
  3703|         0|            0|            0|  0.00%|
  3704|         0|            0|            0|  0.00%|    if input.dim() == 3 and mode == "linear":
  3705|         0|            0|            0|  0.00%|        assert align_corners is not None
  3706|         0|            0|            0|  0.00%|        return torch._C._nn.upsample_linear1d(input, output_size, align_corners, scale_factors)
  3707|         0|            0|            0|  0.00%|    if input.dim() == 4 and mode == "bilinear":
  3708|         0|            0|            0|  0.00%|        assert align_corners is not None
  3709|         0|            0|            0|  0.00%|        return torch._C._nn.upsample_bilinear2d(input, output_size, align_corners, scale_factors)
  3710|         0|            0|            0|  0.00%|    if input.dim() == 5 and mode == "trilinear":
  3711|         0|            0|            0|  0.00%|        assert align_corners is not None
  3712|         0|            0|            0|  0.00%|        return torch._C._nn.upsample_trilinear3d(input, output_size, align_corners, scale_factors)
  3713|         0|            0|            0|  0.00%|    if input.dim() == 4 and mode == "bicubic":
  3714|         0|            0|            0|  0.00%|        assert align_corners is not None
  3715|         0|            0|            0|  0.00%|        return torch._C._nn.upsample_bicubic2d(input, output_size, align_corners, scale_factors)
  3716|         0|            0|            0|  0.00%|
  3717|         0|            0|            0|  0.00%|    if input.dim() == 3 and mode == "bilinear":
  3718|         0|            0|            0|  0.00%|        raise NotImplementedError("Got 3D input, but bilinear mode needs 4D input")
  3719|         0|            0|            0|  0.00%|    if input.dim() == 3 and mode == "trilinear":
  3720|         0|            0|            0|  0.00%|        raise NotImplementedError("Got 3D input, but trilinear mode needs 5D input")
  3721|         0|            0|            0|  0.00%|    if input.dim() == 4 and mode == "linear":
  3722|         0|            0|            0|  0.00%|        raise NotImplementedError("Got 4D input, but linear mode needs 3D input")
  3723|         0|            0|            0|  0.00%|    if input.dim() == 4 and mode == "trilinear":
  3724|         0|            0|            0|  0.00%|        raise NotImplementedError("Got 4D input, but trilinear mode needs 5D input")
  3725|         0|            0|            0|  0.00%|    if input.dim() == 5 and mode == "linear":
  3726|         0|            0|            0|  0.00%|        raise NotImplementedError("Got 5D input, but linear mode needs 3D input")
  3727|         0|            0|            0|  0.00%|    if input.dim() == 5 and mode == "bilinear":
  3728|         0|            0|            0|  0.00%|        raise NotImplementedError("Got 5D input, but bilinear mode needs 4D input")
  3729|         0|            0|            0|  0.00%|
  3730|         0|            0|            0|  0.00%|    raise NotImplementedError(
  3731|         0|            0|            0|  0.00%|        "Input Error: Only 3D, 4D and 5D input Tensors supported"
  3732|         0|            0|            0|  0.00%|        " (got {}D) for the modes: nearest | linear | bilinear | bicubic | trilinear"
  3733|         0|            0|            0|  0.00%|        " (got {})".format(input.dim(), mode)
  3734|         0|            0|            0|  0.00%|    )
  3735|         0|            0|            0|  0.00%|
  3736|         0|            0|            0|  0.00%|
  3737|         0|            0|            0|  0.00%|interpolate.__doc__ = interpolate.__doc__.format(**reproducibility_notes)
  3738|         0|            0|            0|  0.00%|
  3739|         0|            0|            0|  0.00%|
  3740|         0|            0|            0|  0.00%|@_overload  # noqa: F811
  3741|         0|            0|            0|  0.00%|def upsample_nearest(input: Tensor, size: Optional[int] = None, scale_factor: Optional[float] = None) -> Tensor:  # noqa: F811
  3742|         0|            0|            0|  0.00%|    pass
  3743|         0|            0|            0|  0.00%|
  3744|         0|            0|            0|  0.00%|
  3745|         0|            0|            0|  0.00%|@_overload  # noqa: F811
  3746|         0|            0|            0|  0.00%|def upsample_nearest(input: Tensor, size: Optional[List[int]] = None, scale_factor: Optional[float] = None) -> Tensor:  # noqa: F811
  3747|         0|            0|            0|  0.00%|    pass
  3748|         0|            0|            0|  0.00%|
  3749|         0|            0|            0|  0.00%|
  3750|         0|            0|            0|  0.00%|def upsample_nearest(input, size=None, scale_factor=None):  # noqa: F811
  3751|         0|            0|            0|  0.00%|    r"""Upsamples the input, using nearest neighbours' pixel values.
  3752|         0|            0|            0|  0.00%|
  3753|         0|            0|            0|  0.00%|    .. warning::
  3754|         0|            0|            0|  0.00%|        This function is deprecated in favor of :func:`torch.nn.functional.interpolate`.
  3755|         0|            0|            0|  0.00%|        This is equivalent with ``nn.functional.interpolate(..., mode='nearest')``.
  3756|         0|            0|            0|  0.00%|
  3757|         0|            0|            0|  0.00%|    Currently spatial and volumetric upsampling are supported (i.e. expected
  3758|         0|            0|            0|  0.00%|    inputs are 4 or 5 dimensional).
  3759|         0|            0|            0|  0.00%|
  3760|         0|            0|            0|  0.00%|    Args:
  3761|         0|            0|            0|  0.00%|        input (Tensor): input
  3762|         0|            0|            0|  0.00%|        size (int or Tuple[int, int] or Tuple[int, int, int]): output spatia
  3763|         0|            0|            0|  0.00%|            size.
  3764|         0|            0|            0|  0.00%|        scale_factor (int): multiplier for spatial size. Has to be an integer.
  3765|         0|            0|            0|  0.00%|
  3766|         0|            0|            0|  0.00%|    Note:
  3767|         0|            0|            0|  0.00%|        {backward_reproducibility_note}
  3768|         0|            0|            0|  0.00%|    """
  3769|         0|            0|            0|  0.00%|    # DeprecationWarning is ignored by default
  3770|         0|            0|            0|  0.00%|    warnings.warn("nn.functional.upsample_nearest is deprecated. Use nn.functional.interpolate instead.")
  3771|         0|            0|            0|  0.00%|    return interpolate(input, size, scale_factor, mode="nearest")
  3772|         0|            0|            0|  0.00%|
  3773|         0|            0|            0|  0.00%|
  3774|         0|            0|            0|  0.00%|upsample_nearest.__doc__ = upsample_nearest.__doc__.format(**reproducibility_notes)
  3775|         0|            0|            0|  0.00%|
  3776|         0|            0|            0|  0.00%|
  3777|         0|            0|            0|  0.00%|@_overload  # noqa: F811
  3778|         0|            0|            0|  0.00%|def upsample_bilinear(
  3779|         0|            0|            0|  0.00%|    input: Tensor, size: Optional[int] = None, scale_factor: Optional[float] = None
  3780|         0|            0|            0|  0.00%|) -> Tensor:  # noqa: F811
  3781|         0|            0|            0|  0.00%|    pass
  3782|         0|            0|            0|  0.00%|
  3783|         0|            0|            0|  0.00%|
  3784|         0|            0|            0|  0.00%|@_overload  # noqa: F811
  3785|         0|            0|            0|  0.00%|def upsample_bilinear(  # noqa: F811
  3786|         0|            0|            0|  0.00%|    input: Tensor, size: Optional[List[int]] = None, scale_factor: Optional[float] = None
  3787|         0|            0|            0|  0.00%|) -> Tensor:  # noqa: F811
  3788|         0|            0|            0|  0.00%|    pass
  3789|         0|            0|            0|  0.00%|
  3790|         0|            0|            0|  0.00%|
  3791|         0|            0|            0|  0.00%|@_overload  # noqa: F811
  3792|         0|            0|            0|  0.00%|def upsample_bilinear(  # noqa: F811
  3793|         0|            0|            0|  0.00%|    input: Tensor, size: Optional[int] = None, scale_factor: Optional[List[float]] = None
  3794|         0|            0|            0|  0.00%|) -> Tensor:  # noqa: F811
  3795|         0|            0|            0|  0.00%|    pass
  3796|         0|            0|            0|  0.00%|
  3797|         0|            0|            0|  0.00%|
  3798|         0|            0|            0|  0.00%|@_overload  # noqa: F811
  3799|         0|            0|            0|  0.00%|def upsample_bilinear(  # noqa: F811
  3800|         0|            0|            0|  0.00%|    input: Tensor, size: Optional[List[int]] = None, scale_factor: Optional[List[float]] = None
  3801|         0|            0|            0|  0.00%|) -> Tensor:  # noqa: F811
  3802|         0|            0|            0|  0.00%|    pass
  3803|         0|            0|            0|  0.00%|
  3804|         0|            0|            0|  0.00%|
  3805|         0|            0|            0|  0.00%|def upsample_bilinear(input, size=None, scale_factor=None):  # noqa: F811
  3806|         0|            0|            0|  0.00%|    r"""Upsamples the input, using bilinear upsampling.
  3807|         0|            0|            0|  0.00%|
  3808|         0|            0|            0|  0.00%|    .. warning::
  3809|         0|            0|            0|  0.00%|        This function is deprecated in favor of :func:`torch.nn.functional.interpolate`.
  3810|         0|            0|            0|  0.00%|        This is equivalent with
  3811|         0|            0|            0|  0.00%|        ``nn.functional.interpolate(..., mode='bilinear', align_corners=True)``.
  3812|         0|            0|            0|  0.00%|
  3813|         0|            0|            0|  0.00%|    Expected inputs are spatial (4 dimensional). Use `upsample_trilinear` fo
  3814|         0|            0|            0|  0.00%|    volumetric (5 dimensional) inputs.
  3815|         0|            0|            0|  0.00%|
  3816|         0|            0|            0|  0.00%|    Args:
  3817|         0|            0|            0|  0.00%|        input (Tensor): input
  3818|         0|            0|            0|  0.00%|        size (int or Tuple[int, int]): output spatial size.
  3819|         0|            0|            0|  0.00%|        scale_factor (int or Tuple[int, int]): multiplier for spatial size
  3820|         0|            0|            0|  0.00%|
  3821|         0|            0|            0|  0.00%|    Note:
  3822|         0|            0|            0|  0.00%|        {backward_reproducibility_note}
  3823|         0|            0|            0|  0.00%|    """
  3824|         0|            0|            0|  0.00%|    # DeprecationWarning is ignored by default
  3825|         0|            0|            0|  0.00%|    warnings.warn("nn.functional.upsample_bilinear is deprecated. Use nn.functional.interpolate instead.")
  3826|         0|            0|            0|  0.00%|    return interpolate(input, size, scale_factor, mode="bilinear", align_corners=True)
  3827|         0|            0|            0|  0.00%|
  3828|         0|            0|            0|  0.00%|
  3829|         0|            0|            0|  0.00%|upsample_bilinear.__doc__ = upsample_bilinear.__doc__.format(**reproducibility_notes)
  3830|         0|            0|            0|  0.00%|
  3831|         0|            0|            0|  0.00%|GRID_SAMPLE_INTERPOLATION_MODES = {
  3832|         0|            0|            0|  0.00%|    "bilinear": 0,
  3833|         0|            0|            0|  0.00%|    "nearest": 1,
  3834|         0|            0|            0|  0.00%|    "bicubic": 2,
  3835|         0|            0|            0|  0.00%|}
  3836|         0|            0|            0|  0.00%|
  3837|         0|            0|            0|  0.00%|GRID_SAMPLE_PADDING_MODES = {
  3838|         0|            0|            0|  0.00%|    "zeros": 0,
  3839|         0|            0|            0|  0.00%|    "border": 1,
  3840|         0|            0|            0|  0.00%|    "reflection": 2,
  3841|         0|            0|            0|  0.00%|}
  3842|         0|            0|            0|  0.00%|
  3843|         0|            0|            0|  0.00%|
  3844|         0|            0|            0|  0.00%|def grid_sample(
  3845|         0|            0|            0|  0.00%|    input: Tensor,
  3846|         0|            0|            0|  0.00%|    grid: Tensor,
  3847|         0|            0|            0|  0.00%|    mode: str = "bilinear",
  3848|         0|            0|            0|  0.00%|    padding_mode: str = "zeros",
  3849|         0|            0|            0|  0.00%|    align_corners: Optional[bool] = None,
  3850|         0|            0|            0|  0.00%|) -> Tensor:
  3851|         0|            0|            0|  0.00%|    r"""Given an :attr:`input` and a flow-field :attr:`grid`, computes the
  3852|         0|            0|            0|  0.00%|    ``output`` using :attr:`input` values and pixel locations from :attr:`grid`.
  3853|         0|            0|            0|  0.00%|
  3854|         0|            0|            0|  0.00%|    Currently, only spatial (4-D) and volumetric (5-D) :attr:`input` are
  3855|         0|            0|            0|  0.00%|    supported.
  3856|         0|            0|            0|  0.00%|
  3857|         0|            0|            0|  0.00%|    In the spatial (4-D) case, for :attr:`input` with shape
  3858|         0|            0|            0|  0.00%|    :math:`(N, C, H_\text{in}, W_\text{in})` and :attr:`grid` with shape
  3859|         0|            0|            0|  0.00%|    :math:`(N, H_\text{out}, W_\text{out}, 2)`, the output will have shape
  3860|         0|            0|            0|  0.00%|    :math:`(N, C, H_\text{out}, W_\text{out})`.
  3861|         0|            0|            0|  0.00%|
  3862|         0|            0|            0|  0.00%|    For each output location ``output[n, :, h, w]``, the size-2 vector
  3863|         0|            0|            0|  0.00%|    ``grid[n, h, w]`` specifies :attr:`input` pixel locations ``x`` and ``y``,
  3864|         0|            0|            0|  0.00%|    which are used to interpolate the output value ``output[n, :, h, w]``.
  3865|         0|            0|            0|  0.00%|    In the case of 5D inputs, ``grid[n, d, h, w]`` specifies the
  3866|         0|            0|            0|  0.00%|    ``x``, ``y``, ``z`` pixel locations for interpolating
  3867|         0|            0|            0|  0.00%|    ``output[n, :, d, h, w]``. :attr:`mode` argument specifies ``nearest`` or
  3868|         0|            0|            0|  0.00%|    ``bilinear`` interpolation method to sample the input pixels.
  3869|         0|            0|            0|  0.00%|
  3870|         0|            0|            0|  0.00%|    :attr:`grid` specifies the sampling pixel locations normalized by the
  3871|         0|            0|            0|  0.00%|    :attr:`input` spatial dimensions. Therefore, it should have most values in
  3872|         0|            0|            0|  0.00%|    the range of ``[-1, 1]``. For example, values ``x = -1, y = -1`` is the
  3873|         0|            0|            0|  0.00%|    left-top pixel of :attr:`input`, and values  ``x = 1, y = 1`` is the
  3874|         0|            0|            0|  0.00%|    right-bottom pixel of :attr:`input`.
  3875|         0|            0|            0|  0.00%|
  3876|         0|            0|            0|  0.00%|    If :attr:`grid` has values outside the range of ``[-1, 1]``, the corresponding
  3877|         0|            0|            0|  0.00%|    outputs are handled as defined by :attr:`padding_mode`. Options are
  3878|         0|            0|            0|  0.00%|
  3879|         0|            0|            0|  0.00%|        * ``padding_mode="zeros"``: use ``0`` for out-of-bound grid locations,
  3880|         0|            0|            0|  0.00%|        * ``padding_mode="border"``: use border values for out-of-bound grid locations,
  3881|         0|            0|            0|  0.00%|        * ``padding_mode="reflection"``: use values at locations reflected by
  3882|         0|            0|            0|  0.00%|          the border for out-of-bound grid locations. For location far away
  3883|         0|            0|            0|  0.00%|          from the border, it will keep being reflected until becoming in bound,
  3884|         0|            0|            0|  0.00%|          e.g., (normalized) pixel location ``x = -3.5`` reflects by border ``-1``
  3885|         0|            0|            0|  0.00%|          and becomes ``x' = 1.5``, then reflects by border ``1`` and becomes
  3886|         0|            0|            0|  0.00%|          ``x'' = -0.5``.
  3887|         0|            0|            0|  0.00%|
  3888|         0|            0|            0|  0.00%|    Note:
  3889|         0|            0|            0|  0.00%|        This function is often used in conjunction with :func:`affine_grid`
  3890|         0|            0|            0|  0.00%|        to build `Spatial Transformer Networks`_ .
  3891|         0|            0|            0|  0.00%|
  3892|         0|            0|            0|  0.00%|    Note:
  3893|         0|            0|            0|  0.00%|        When using the CUDA backend, this operation may induce nondeterministic
  3894|         0|            0|            0|  0.00%|        behaviour in its backward pass that is not easily switched off.
  3895|         0|            0|            0|  0.00%|        Please see the notes on :doc:`/notes/randomness` for background.
  3896|         0|            0|            0|  0.00%|
  3897|         0|            0|            0|  0.00%|    Note:
  3898|         0|            0|            0|  0.00%|        NaN values in :attr:`grid` would be interpreted as ``-1``.
  3899|         0|            0|            0|  0.00%|
  3900|         0|            0|            0|  0.00%|    Args:
  3901|         0|            0|            0|  0.00%|        input (Tensor): input of shape :math:`(N, C, H_\text{in}, W_\text{in})` (4-D case)
  3902|         0|            0|            0|  0.00%|                        or :math:`(N, C, D_\text{in}, H_\text{in}, W_\text{in})` (5-D case)
  3903|         0|            0|            0|  0.00%|        grid (Tensor): flow-field of shape :math:`(N, H_\text{out}, W_\text{out}, 2)` (4-D case)
  3904|         0|            0|            0|  0.00%|                       or :math:`(N, D_\text{out}, H_\text{out}, W_\text{out}, 3)` (5-D case)
  3905|         0|            0|            0|  0.00%|        mode (str): interpolation mode to calculate output values
  3906|         0|            0|            0|  0.00%|            ``'bilinear'`` | ``'nearest'`` | ``'bicubic'``. Default: ``'bilinear'``
  3907|         0|            0|            0|  0.00%|            Note: ``mode='bicubic'`` supports only 4-D input.
  3908|         0|            0|            0|  0.00%|            When ``mode='bilinear'`` and the input is 5-D, the interpolation mode
  3909|         0|            0|            0|  0.00%|            used internally will actually be trilinear. However, when the input is 4-D,
  3910|         0|            0|            0|  0.00%|            the interpolation mode will legitimately be bilinear.
  3911|         0|            0|            0|  0.00%|        padding_mode (str): padding mode for outside grid values
  3912|         0|            0|            0|  0.00%|            ``'zeros'`` | ``'border'`` | ``'reflection'``. Default: ``'zeros'``
  3913|         0|            0|            0|  0.00%|        align_corners (bool, optional): Geometrically, we consider the pixels of the
  3914|         0|            0|            0|  0.00%|            input  as squares rather than points.
  3915|         0|            0|            0|  0.00%|            If set to ``True``, the extrema (``-1`` and ``1``) are considered as referring
  3916|         0|            0|            0|  0.00%|            to the center points of the input's corner pixels. If set to ``False``, they
  3917|         0|            0|            0|  0.00%|            are instead considered as referring to the corner points of the input's corner
  3918|         0|            0|            0|  0.00%|            pixels, making the sampling more resolution agnostic.
  3919|         0|            0|            0|  0.00%|            This option parallels the ``align_corners`` option in
  3920|         0|            0|            0|  0.00%|            :func:`interpolate`, and so whichever option is used here
  3921|         0|            0|            0|  0.00%|            should also be used there to resize the input image before grid sampling.
  3922|         0|            0|            0|  0.00%|            Default: ``False``
  3923|         0|            0|            0|  0.00%|
  3924|         0|            0|            0|  0.00%|    Returns:
  3925|         0|            0|            0|  0.00%|        output (Tensor): output Tensor
  3926|         0|            0|            0|  0.00%|
  3927|         0|            0|            0|  0.00%|    .. _`Spatial Transformer Networks`:
  3928|         0|            0|            0|  0.00%|        https://arxiv.org/abs/1506.02025
  3929|         0|            0|            0|  0.00%|
  3930|         0|            0|            0|  0.00%|    .. warning::
  3931|         0|            0|            0|  0.00%|        When ``align_corners = True``, the grid positions depend on the pixel
  3932|         0|            0|            0|  0.00%|        size relative to the input image size, and so the locations sampled by
  3933|         0|            0|            0|  0.00%|        :func:`grid_sample` will differ for the same input given at different
  3934|         0|            0|            0|  0.00%|        resolutions (that is, after being upsampled or downsampled).
  3935|         0|            0|            0|  0.00%|        The default behavior up to version 1.2.0 was ``align_corners = True``.
  3936|         0|            0|            0|  0.00%|        Since then, the default behavior has been changed to ``align_corners = False``,
  3937|         0|            0|            0|  0.00%|        in order to bring it in line with the default for :func:`interpolate`.
  3938|         0|            0|            0|  0.00%|
  3939|         0|            0|            0|  0.00%|    .. note::
  3940|         0|            0|            0|  0.00%|        ``mode='bicubic'`` is implemented using the `cubic convolution algorithm`_ with :math:`\alpha=-0.75`.
  3941|         0|            0|            0|  0.00%|        The constant :math:`\alpha` might be different from packages to packages.
  3942|         0|            0|            0|  0.00%|        For example, `PIL`_ and `OpenCV`_ use -0.5 and -0.75 respectively.
  3943|         0|            0|            0|  0.00%|        This algorithm may "overshoot" the range of values it's interpolating.
  3944|         0|            0|            0|  0.00%|        For example, it may produce negative values or values greater than 255 when interpolating input in [0, 255].
  3945|         0|            0|            0|  0.00%|        Clamp the results with :func: `torch.clamp` to ensure they are within the valid range.
  3946|         0|            0|            0|  0.00%|    .. _`cubic convolution algorithm`: https://en.wikipedia.org/wiki/Bicubic_interpolation
  3947|         0|            0|            0|  0.00%|    .. _`PIL`: https://github.com/python-pillow/Pillow/blob/4634eafe3c695a014267eefdce830b4a825beed7/src/libImaging/Resample.c#L51
  3948|         0|            0|            0|  0.00%|    .. _`OpenCV`: https://github.com/opencv/opencv/blob/f345ed564a06178670750bad59526cfa4033be55/modules/imgproc/src/resize.cpp#L908
  3949|         0|            0|            0|  0.00%|    """
  3950|         0|            0|            0|  0.00%|    if has_torch_function_variadic(input, grid):
  3951|         0|            0|            0|  0.00%|        return handle_torch_function(
  3952|         0|            0|            0|  0.00%|            grid_sample, (input, grid), input, grid, mode=mode, padding_mode=padding_mode, align_corners=align_corners
  3953|         0|            0|            0|  0.00%|        )
  3954|         0|            0|            0|  0.00%|    if mode != "bilinear" and mode != "nearest" and mode != "bicubic":
  3955|         0|            0|            0|  0.00%|        raise ValueError(
  3956|         0|            0|            0|  0.00%|            "nn.functional.grid_sample(): expected mode to be "
  3957|         0|            0|            0|  0.00%|            "'bilinear', 'nearest' or 'bicubic', but got: '{}'".format(mode)
  3958|         0|            0|            0|  0.00%|        )
  3959|         0|            0|            0|  0.00%|    if padding_mode != "zeros" and padding_mode != "border" and padding_mode != "reflection":
  3960|         0|            0|            0|  0.00%|        raise ValueError(
  3961|         0|            0|            0|  0.00%|            "nn.functional.grid_sample(): expected padding_mode "
  3962|         0|            0|            0|  0.00%|            "to be 'zeros', 'border', or 'reflection', "
  3963|         0|            0|            0|  0.00%|            "but got: '{}'".format(padding_mode)
  3964|         0|            0|            0|  0.00%|        )
  3965|         0|            0|            0|  0.00%|
  3966|         0|            0|            0|  0.00%|    if mode == "bilinear":
  3967|         0|            0|            0|  0.00%|        mode_enum = 0
  3968|         0|            0|            0|  0.00%|    elif mode == "nearest":
  3969|         0|            0|            0|  0.00%|        mode_enum = 1
  3970|         0|            0|            0|  0.00%|    else:  # mode == 'bicubic'
  3971|         0|            0|            0|  0.00%|        mode_enum = 2
  3972|         0|            0|            0|  0.00%|
  3973|         0|            0|            0|  0.00%|    if padding_mode == "zeros":
  3974|         0|            0|            0|  0.00%|        padding_mode_enum = 0
  3975|         0|            0|            0|  0.00%|    elif padding_mode == "border":
  3976|         0|            0|            0|  0.00%|        padding_mode_enum = 1
  3977|         0|            0|            0|  0.00%|    else:  # padding_mode == 'reflection'
  3978|         0|            0|            0|  0.00%|        padding_mode_enum = 2
  3979|         0|            0|            0|  0.00%|
  3980|         0|            0|            0|  0.00%|    if align_corners is None:
  3981|         0|            0|            0|  0.00%|        warnings.warn(
  3982|         0|            0|            0|  0.00%|            "Default grid_sample and affine_grid behavior has changed "
  3983|         0|            0|            0|  0.00%|            "to align_corners=False since 1.3.0. Please specify "
  3984|         0|            0|            0|  0.00%|            "align_corners=True if the old behavior is desired. "
  3985|         0|            0|            0|  0.00%|            "See the documentation of grid_sample for details."
  3986|         0|            0|            0|  0.00%|        )
  3987|         0|            0|            0|  0.00%|        align_corners = False
  3988|         0|            0|            0|  0.00%|
  3989|         0|            0|            0|  0.00%|    return torch.grid_sampler(input, grid, mode_enum, padding_mode_enum, align_corners)
  3990|         0|            0|            0|  0.00%|
  3991|         0|            0|            0|  0.00%|
  3992|         0|            0|            0|  0.00%|def affine_grid(theta: Tensor, size: List[int], align_corners: Optional[bool] = None) -> Tensor:
  3993|         0|            0|            0|  0.00%|    r"""Generates a 2D or 3D flow field (sampling grid), given a batch of
  3994|         0|            0|            0|  0.00%|    affine matrices :attr:`theta`.
  3995|         0|            0|            0|  0.00%|
  3996|         0|            0|            0|  0.00%|    .. note::
  3997|         0|            0|            0|  0.00%|        This function is often used in conjunction with :func:`grid_sample`
  3998|         0|            0|            0|  0.00%|        to build `Spatial Transformer Networks`_ .
  3999|         0|            0|            0|  0.00%|
  4000|         0|            0|            0|  0.00%|    Args:
  4001|         0|            0|            0|  0.00%|        theta (Tensor): input batch of affine matrices with shape
  4002|         0|            0|            0|  0.00%|            (:math:`N \times 2 \times 3`) for 2D or
  4003|         0|            0|            0|  0.00%|            (:math:`N \times 3 \times 4`) for 3D
  4004|         0|            0|            0|  0.00%|        size (torch.Size): the target output image size.
  4005|         0|            0|            0|  0.00%|            (:math:`N \times C \times H \times W` for 2D or
  4006|         0|            0|            0|  0.00%|            :math:`N \times C \times D \times H \times W` for 3D)
  4007|         0|            0|            0|  0.00%|            Example: torch.Size((32, 3, 24, 24))
  4008|         0|            0|            0|  0.00%|        align_corners (bool, optional): if ``True``, consider ``-1`` and ``1``
  4009|         0|            0|            0|  0.00%|            to refer to the centers of the corner pixels rather than the image corners.
  4010|         0|            0|            0|  0.00%|            Refer to :func:`grid_sample` for a more complete description.
  4011|         0|            0|            0|  0.00%|            A grid generated by :func:`affine_grid` should be passed to :func:`grid_sample`
  4012|         0|            0|            0|  0.00%|            with the same setting for this option.
  4013|         0|            0|            0|  0.00%|            Default: ``False``
  4014|         0|            0|            0|  0.00%|
  4015|         0|            0|            0|  0.00%|    Returns:
  4016|         0|            0|            0|  0.00%|        output (Tensor): output Tensor of size (:math:`N \times H \times W \times 2`)
  4017|         0|            0|            0|  0.00%|
  4018|         0|            0|            0|  0.00%|    .. _`Spatial Transformer Networks`:
  4019|         0|            0|            0|  0.00%|        https://arxiv.org/abs/1506.02025
  4020|         0|            0|            0|  0.00%|
  4021|         0|            0|            0|  0.00%|    .. warning::
  4022|         0|            0|            0|  0.00%|        When ``align_corners = True``, the grid positions depend on the pixel
  4023|         0|            0|            0|  0.00%|        size relative to the input image size, and so the locations sampled by
  4024|         0|            0|            0|  0.00%|        :func:`grid_sample` will differ for the same input given at different
  4025|         0|            0|            0|  0.00%|        resolutions (that is, after being upsampled or downsampled).
  4026|         0|            0|            0|  0.00%|        The default behavior up to version 1.2.0 was ``align_corners = True``.
  4027|         0|            0|            0|  0.00%|        Since then, the default behavior has been changed to ``align_corners = False``,
  4028|         0|            0|            0|  0.00%|        in order to bring it in line with the default for :func:`interpolate`.
  4029|         0|            0|            0|  0.00%|    .. warning::
  4030|         0|            0|            0|  0.00%|        When ``align_corners = True``, 2D affine transforms on 1D data and
  4031|         0|            0|            0|  0.00%|        3D affine transforms on 2D data (that is, when one of the spatial
  4032|         0|            0|            0|  0.00%|        dimensions has unit size) are ill-defined, and not an intended use case.
  4033|         0|            0|            0|  0.00%|        This is not a problem when ``align_corners = False``.
  4034|         0|            0|            0|  0.00%|        Up to version 1.2.0, all grid points along a unit dimension were
  4035|         0|            0|            0|  0.00%|        considered arbitrarily to be at ``-1``.
  4036|         0|            0|            0|  0.00%|        From version 1.3.0, under ``align_corners = True`` all grid points
  4037|         0|            0|            0|  0.00%|        along a unit dimension are considered to be at ```0``
  4038|         0|            0|            0|  0.00%|        (the center of the input image).
  4039|         0|            0|            0|  0.00%|    """
  4040|         0|            0|            0|  0.00%|    if has_torch_function_unary(theta):
  4041|         0|            0|            0|  0.00%|        return handle_torch_function(affine_grid, (theta,), theta, size, align_corners=align_corners)
  4042|         0|            0|            0|  0.00%|    if align_corners is None:
  4043|         0|            0|            0|  0.00%|        warnings.warn(
  4044|         0|            0|            0|  0.00%|            "Default grid_sample and affine_grid behavior has changed "
  4045|         0|            0|            0|  0.00%|            "to align_corners=False since 1.3.0. Please specify "
  4046|         0|            0|            0|  0.00%|            "align_corners=True if the old behavior is desired. "
  4047|         0|            0|            0|  0.00%|            "See the documentation of grid_sample for details."
  4048|         0|            0|            0|  0.00%|        )
  4049|         0|            0|            0|  0.00%|        align_corners = False
  4050|         0|            0|            0|  0.00%|
  4051|         0|            0|            0|  0.00%|    # enforce floating point dtype on theta
  4052|         0|            0|            0|  0.00%|    if not theta.is_floating_point():
  4053|         0|            0|            0|  0.00%|        raise ValueError("Expected theta to have floating point type, but got {}".format(theta.dtype))
  4054|         0|            0|            0|  0.00%|    # check that shapes and sizes match
  4055|         0|            0|            0|  0.00%|    if len(size) == 4:
  4056|         0|            0|            0|  0.00%|        if theta.dim() != 3 or theta.shape[-2] != 2 or theta.shape[-1] != 3:
  4057|         0|            0|            0|  0.00%|            raise ValueError(
  4058|         0|            0|            0|  0.00%|                "Expected a batch of 2D affine matrices of shape Nx2x3 "
  4059|         0|            0|            0|  0.00%|                "for size {}. Got {}.".format(size, theta.shape)
  4060|         0|            0|            0|  0.00%|            )
  4061|         0|            0|            0|  0.00%|        spatial_size = size[-2:]  # spatial dimension sizes
  4062|         0|            0|            0|  0.00%|    elif len(size) == 5:
  4063|         0|            0|            0|  0.00%|        if theta.dim() != 3 or theta.shape[-2] != 3 or theta.shape[-1] != 4:
  4064|         0|            0|            0|  0.00%|            raise ValueError(
  4065|         0|            0|            0|  0.00%|                "Expected a batch of 3D affine matrices of shape Nx3x4 "
  4066|         0|            0|            0|  0.00%|                "for size {}. Got {}.".format(size, theta.shape)
  4067|         0|            0|            0|  0.00%|            )
  4068|         0|            0|            0|  0.00%|        spatial_size = size[-3:]  # spatial dimension sizes
  4069|         0|            0|            0|  0.00%|    else:
  4070|         0|            0|            0|  0.00%|        raise NotImplementedError(
  4071|         0|            0|            0|  0.00%|            "affine_grid only supports 4D and 5D sizes, "
  4072|         0|            0|            0|  0.00%|            "for 2D and 3D affine transforms, respectively. "
  4073|         0|            0|            0|  0.00%|            "Got size {}.".format(size)
  4074|         0|            0|            0|  0.00%|        )
  4075|         0|            0|            0|  0.00%|    # check for empty span
  4076|         0|            0|            0|  0.00%|    if align_corners and min(spatial_size) == 1:
  4077|         0|            0|            0|  0.00%|        warnings.warn(
  4078|         0|            0|            0|  0.00%|            "Since version 1.3.0, affine_grid behavior has changed "
  4079|         0|            0|            0|  0.00%|            "for unit-size grids when align_corners=True. "
  4080|         0|            0|            0|  0.00%|            "This is not an intended use case of affine_grid. "
  4081|         0|            0|            0|  0.00%|            "See the documentation of affine_grid for details."
  4082|         0|            0|            0|  0.00%|        )
  4083|         0|            0|            0|  0.00%|    elif min(size) <= 0:
  4084|         0|            0|            0|  0.00%|        raise ValueError("Expected non-zero, positive output size. Got {}".format(size))
  4085|         0|            0|            0|  0.00%|
  4086|         0|            0|            0|  0.00%|    return torch.affine_grid_generator(theta, size, align_corners)
  4087|         0|            0|            0|  0.00%|
  4088|         0|            0|            0|  0.00%|
  4089|         0|            0|            0|  0.00%|def _pad(input: Tensor, pad: List[int], mode: str = "constant", value: float = 0) -> Tensor:
  4090|         0|            0|            0|  0.00%|    r"""Pads tensor.
  4091|         0|            0|            0|  0.00%|
  4092|         0|            0|            0|  0.00%|    Padding size:
  4093|         0|            0|            0|  0.00%|        The padding size by which to pad some dimensions of :attr:`input`
  4094|         0|            0|            0|  0.00%|        are described starting from the last dimension and moving forward.
  4095|         0|            0|            0|  0.00%|        :math:`\left\lfloor\frac{\text{len(pad)}}{2}\right\rfloor` dimensions
  4096|         0|            0|            0|  0.00%|        of ``input`` will be padded.
  4097|         0|            0|            0|  0.00%|        For example, to pad only the last dimension of the input tensor, then
  4098|         0|            0|            0|  0.00%|        :attr:`pad` has the form
  4099|         0|            0|            0|  0.00%|        :math:`(\text{padding\_left}, \text{padding\_right})`;
  4100|         0|            0|            0|  0.00%|        to pad the last 2 dimensions of the input tensor, then use
  4101|         0|            0|            0|  0.00%|        :math:`(\text{padding\_left}, \text{padding\_right},`
  4102|         0|            0|            0|  0.00%|        :math:`\text{padding\_top}, \text{padding\_bottom})`;
  4103|         0|            0|            0|  0.00%|        to pad the last 3 dimensions, use
  4104|         0|            0|            0|  0.00%|        :math:`(\text{padding\_left}, \text{padding\_right},`
  4105|         0|            0|            0|  0.00%|        :math:`\text{padding\_top}, \text{padding\_bottom}`
  4106|         0|            0|            0|  0.00%|        :math:`\text{padding\_front}, \text{padding\_back})`.
  4107|         0|            0|            0|  0.00%|
  4108|         0|            0|            0|  0.00%|    Padding mode:
  4109|         0|            0|            0|  0.00%|        See :class:`torch.nn.ConstantPad2d`, :class:`torch.nn.ReflectionPad2d`, and
  4110|         0|            0|            0|  0.00%|        :class:`torch.nn.ReplicationPad2d` for concrete examples on how each of the
  4111|         0|            0|            0|  0.00%|        padding modes works. Constant padding is implemented for arbitrary dimensions.
  4112|         0|            0|            0|  0.00%|        Replicate padding is implemented for padding the last 3 dimensions of 5D input
  4113|         0|            0|            0|  0.00%|        tensor, or the last 2 dimensions of 4D input tensor, or the last dimension of
  4114|         0|            0|            0|  0.00%|        3D input tensor. Reflect padding is only implemented for padding the last 2
  4115|         0|            0|            0|  0.00%|        dimensions of 4D input tensor, or the last dimension of 3D input tensor.
  4116|         0|            0|            0|  0.00%|
  4117|         0|            0|            0|  0.00%|    Note:
  4118|         0|            0|            0|  0.00%|        When using the CUDA backend, this operation may induce nondeterministic
  4119|         0|            0|            0|  0.00%|        behaviour in its backward pass that is not easily switched off.
  4120|         0|            0|            0|  0.00%|        Please see the notes on :doc:`/notes/randomness` for background.
  4121|         0|            0|            0|  0.00%|
  4122|         0|            0|            0|  0.00%|    Args:
  4123|         0|            0|            0|  0.00%|        input (Tensor): N-dimensional tensor
  4124|         0|            0|            0|  0.00%|        pad (tuple): m-elements tuple, where
  4125|         0|            0|            0|  0.00%|            :math:`\frac{m}{2} \leq` input dimensions and :math:`m` is even.
  4126|         0|            0|            0|  0.00%|        mode: ``'constant'``, ``'reflect'``, ``'replicate'`` or ``'circular'``.
  4127|         0|            0|            0|  0.00%|            Default: ``'constant'``
  4128|         0|            0|            0|  0.00%|        value: fill value for ``'constant'`` padding. Default: ``0``
  4129|         0|            0|            0|  0.00%|
  4130|         0|            0|            0|  0.00%|    Examples::
  4131|         0|            0|            0|  0.00%|
  4132|         0|            0|            0|  0.00%|        >>> t4d = torch.empty(3, 3, 4, 2)
  4133|         0|            0|            0|  0.00%|        >>> p1d = (1, 1) # pad last dim by 1 on each side
  4134|         0|            0|            0|  0.00%|        >>> out = F.pad(t4d, p1d, "constant", 0)  # effectively zero padding
  4135|         0|            0|            0|  0.00%|        >>> print(out.size())
  4136|         0|            0|            0|  0.00%|        torch.Size([3, 3, 4, 4])
  4137|         0|            0|            0|  0.00%|        >>> p2d = (1, 1, 2, 2) # pad last dim by (1, 1) and 2nd to last by (2, 2)
  4138|         0|            0|            0|  0.00%|        >>> out = F.pad(t4d, p2d, "constant", 0)
  4139|         0|            0|            0|  0.00%|        >>> print(out.size())
  4140|         0|            0|            0|  0.00%|        torch.Size([3, 3, 8, 4])
  4141|         0|            0|            0|  0.00%|        >>> t4d = torch.empty(3, 3, 4, 2)
  4142|         0|            0|            0|  0.00%|        >>> p3d = (0, 1, 2, 1, 3, 3) # pad by (0, 1), (2, 1), and (3, 3)
  4143|         0|            0|            0|  0.00%|        >>> out = F.pad(t4d, p3d, "constant", 0)
  4144|         0|            0|            0|  0.00%|        >>> print(out.size())
  4145|         0|            0|            0|  0.00%|        torch.Size([3, 9, 7, 3])
  4146|         0|            0|            0|  0.00%|
  4147|         0|            0|            0|  0.00%|    """
  4148|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):
  4149|         0|            0|            0|  0.00%|        return handle_torch_function(_pad, (input,), input, pad, mode=mode, value=value)
  4150|         0|            0|            0|  0.00%|    assert len(pad) % 2 == 0, "Padding length must be divisible by 2"
  4151|         0|            0|            0|  0.00%|    assert len(pad) // 2 <= input.dim(), "Padding length too large"
  4152|         0|            0|            0|  0.00%|    if mode == "constant":
  4153|         0|            0|            0|  0.00%|        return _VF.constant_pad_nd(input, pad, value)
  4154|         0|            0|            0|  0.00%|    else:
  4155|         0|            0|            0|  0.00%|        assert value == 0, 'Padding mode "{}"" doesn\'t take in value argument'.format(mode)
  4156|         0|            0|            0|  0.00%|        if input.dim() == 3:
  4157|         0|            0|            0|  0.00%|            assert len(pad) == 2, "3D tensors expect 2 values for padding"
  4158|         0|            0|            0|  0.00%|            if mode == "reflect":
  4159|         0|            0|            0|  0.00%|                return torch._C._nn.reflection_pad1d(input, pad)
  4160|         0|            0|            0|  0.00%|            elif mode == "replicate":
  4161|         0|            0|            0|  0.00%|                return torch._C._nn.replication_pad1d(input, pad)
  4162|         0|            0|            0|  0.00%|            elif mode == "circular":
  4163|         0|            0|            0|  0.00%|                return _pad_circular(input, pad)
  4164|         0|            0|            0|  0.00%|            else:
  4165|         0|            0|            0|  0.00%|                raise NotImplementedError
  4166|         0|            0|            0|  0.00%|
  4167|         0|            0|            0|  0.00%|        elif input.dim() == 4:
  4168|         0|            0|            0|  0.00%|            assert len(pad) == 4, "4D tensors expect 4 values for padding"
  4169|         0|            0|            0|  0.00%|            if mode == "reflect":
  4170|         0|            0|            0|  0.00%|                return torch._C._nn.reflection_pad2d(input, pad)
  4171|         0|            0|            0|  0.00%|            elif mode == "replicate":
  4172|         0|            0|            0|  0.00%|                return torch._C._nn.replication_pad2d(input, pad)
  4173|         0|            0|            0|  0.00%|            elif mode == "circular":
  4174|         0|            0|            0|  0.00%|                return _pad_circular(input, pad)
  4175|         0|            0|            0|  0.00%|            else:
  4176|         0|            0|            0|  0.00%|                raise NotImplementedError
  4177|         0|            0|            0|  0.00%|
  4178|         0|            0|            0|  0.00%|        elif input.dim() == 5:
  4179|         0|            0|            0|  0.00%|            assert len(pad) == 6, "5D tensors expect 6 values for padding"
  4180|         0|            0|            0|  0.00%|            if mode == "reflect":
  4181|         0|            0|            0|  0.00%|                raise NotImplementedError
  4182|         0|            0|            0|  0.00%|            elif mode == "replicate":
  4183|         0|            0|            0|  0.00%|                return torch._C._nn.replication_pad3d(input, pad)
  4184|         0|            0|            0|  0.00%|            elif mode == "circular":
  4185|         0|            0|            0|  0.00%|                return _pad_circular(input, pad)
  4186|         0|            0|            0|  0.00%|            else:
  4187|         0|            0|            0|  0.00%|                raise NotImplementedError
  4188|         0|            0|            0|  0.00%|        else:
  4189|         0|            0|            0|  0.00%|            raise NotImplementedError("Only 3D, 4D, 5D padding with non-constant padding are supported for now")
  4190|         0|            0|            0|  0.00%|
  4191|         0|            0|            0|  0.00%|
  4192|         0|            0|            0|  0.00%|# We define this function as _pad because it takes an argument
  4193|         0|            0|            0|  0.00%|# named pad, which clobbers the recursive reference to the pad
  4194|         0|            0|            0|  0.00%|# function needed for __torch_function__ support
  4195|         0|            0|            0|  0.00%|pad = _pad
  4196|         0|            0|            0|  0.00%|
  4197|         0|            0|            0|  0.00%|# distance
  4198|         0|            0|            0|  0.00%|
  4199|         0|            0|            0|  0.00%|
  4200|         0|            0|            0|  0.00%|def pairwise_distance(x1: Tensor, x2: Tensor, p: float = 2.0, eps: float = 1e-6, keepdim: bool = False) -> Tensor:
  4201|         0|            0|            0|  0.00%|    r"""
  4202|         0|            0|            0|  0.00%|    See :class:`torch.nn.PairwiseDistance` for details
  4203|         0|            0|            0|  0.00%|    """
  4204|         0|            0|            0|  0.00%|    if has_torch_function_variadic(x1, x2):
  4205|         0|            0|            0|  0.00%|        return handle_torch_function(pairwise_distance, (x1, x2), x1, x2, p=p, eps=eps, keepdim=keepdim)
  4206|         0|            0|            0|  0.00%|    return torch.pairwise_distance(x1, x2, p, eps, keepdim)
  4207|         0|            0|            0|  0.00%|
  4208|         0|            0|            0|  0.00%|
  4209|         0|            0|            0|  0.00%|pdist = _add_docstr(
  4210|         0|            0|            0|  0.00%|    torch.pdist,
  4211|         0|            0|            0|  0.00%|    r"""
  4212|         0|            0|            0|  0.00%|pdist(input, p=2) -> Tensor
  4213|         0|            0|            0|  0.00%|
  4214|         0|            0|            0|  0.00%|Computes the p-norm distance between every pair of row vectors in the input.
  4215|         0|            0|            0|  0.00%|This is identical to the upper triangular portion, excluding the diagonal, of
  4216|         0|            0|            0|  0.00%|`torch.norm(input[:, None] - input, dim=2, p=p)`. This function will be faster
  4217|         0|            0|            0|  0.00%|if the rows are contiguous.
  4218|         0|            0|            0|  0.00%|
  4219|         0|            0|            0|  0.00%|If input has shape :math:`N \times M` then the output will have shape
  4220|         0|            0|            0|  0.00%|:math:`\frac{1}{2} N (N - 1)`.
  4221|         0|            0|            0|  0.00%|
  4222|         0|            0|            0|  0.00%|This function is equivalent to `scipy.spatial.distance.pdist(input,
  4223|         0|            0|            0|  0.00%|'minkowski', p=p)` if :math:`p \in (0, \infty)`. When :math:`p = 0` it is
  4224|         0|            0|            0|  0.00%|equivalent to `scipy.spatial.distance.pdist(input, 'hamming') * M`.
  4225|         0|            0|            0|  0.00%|When :math:`p = \infty`, the closest scipy function is
  4226|         0|            0|            0|  0.00%|`scipy.spatial.distance.pdist(xn, lambda x, y: np.abs(x - y).max())`.
  4227|         0|            0|            0|  0.00%|
  4228|         0|            0|            0|  0.00%|Args:
  4229|         0|            0|            0|  0.00%|    input: input tensor of shape :math:`N \times M`.
  4230|         0|            0|            0|  0.00%|    p: p value for the p-norm distance to calculate between each vector pair
  4231|         0|            0|            0|  0.00%|        :math:`\in [0, \infty]`.
  4232|         0|            0|            0|  0.00%|""",
  4233|         0|            0|            0|  0.00%|)
  4234|         0|            0|            0|  0.00%|
  4235|         0|            0|            0|  0.00%|
  4236|         0|            0|            0|  0.00%|cosine_similarity = _add_docstr(
  4237|         0|            0|            0|  0.00%|    torch.cosine_similarity,
  4238|         0|            0|            0|  0.00%|    r"""
  4239|         0|            0|            0|  0.00%|cosine_similarity(x1, x2, dim=1, eps=1e-8) -> Tensor
  4240|         0|            0|            0|  0.00%|
  4241|         0|            0|            0|  0.00%|Returns cosine similarity between x1 and x2, computed along dim.
  4242|         0|            0|            0|  0.00%|
  4243|         0|            0|            0|  0.00%|.. math ::
  4244|         0|            0|            0|  0.00%|    \text{similarity} = \dfrac{x_1 \cdot x_2}{\max(\Vert x_1 \Vert _2 \cdot \Vert x_2 \Vert _2, \epsilon)}
  4245|         0|            0|            0|  0.00%|
  4246|         0|            0|            0|  0.00%|Args:
  4247|         0|            0|            0|  0.00%|    x1 (Tensor): First input.
  4248|         0|            0|            0|  0.00%|    x2 (Tensor): Second input (of size matching x1).
  4249|         0|            0|            0|  0.00%|    dim (int, optional): Dimension of vectors. Default: 1
  4250|         0|            0|            0|  0.00%|    eps (float, optional): Small value to avoid division by zero.
  4251|         0|            0|            0|  0.00%|        Default: 1e-8
  4252|         0|            0|            0|  0.00%|
  4253|         0|            0|            0|  0.00%|Shape:
  4254|         0|            0|            0|  0.00%|    - Input: :math:`(\ast_1, D, \ast_2)` where D is at position `dim`.
  4255|         0|            0|            0|  0.00%|    - Output: :math:`(\ast_1, \ast_2)` where 1 is at position `dim`.
  4256|         0|            0|            0|  0.00%|
  4257|         0|            0|            0|  0.00%|Example::
  4258|         0|            0|            0|  0.00%|
  4259|         0|            0|            0|  0.00%|    >>> input1 = torch.randn(100, 128)
  4260|         0|            0|            0|  0.00%|    >>> input2 = torch.randn(100, 128)
  4261|         0|            0|            0|  0.00%|    >>> output = F.cosine_similarity(input1, input2)
  4262|         0|            0|            0|  0.00%|    >>> print(output)
  4263|         0|            0|            0|  0.00%|""",
  4264|         0|            0|            0|  0.00%|)
  4265|         0|            0|            0|  0.00%|
  4266|         0|            0|            0|  0.00%|
  4267|         0|            0|            0|  0.00%|one_hot = _add_docstr(
  4268|         0|            0|            0|  0.00%|    torch._C._nn.one_hot,
  4269|         0|            0|            0|  0.00%|    r"""
  4270|         0|            0|            0|  0.00%|one_hot(tensor, num_classes=-1) -> LongTensor
  4271|         0|            0|            0|  0.00%|
  4272|         0|            0|            0|  0.00%|Takes LongTensor with index values of shape ``(*)`` and returns a tensor
  4273|         0|            0|            0|  0.00%|of shape ``(*, num_classes)`` that have zeros everywhere except where the
  4274|         0|            0|            0|  0.00%|index of last dimension matches the corresponding value of the input tensor,
  4275|         0|            0|            0|  0.00%|in which case it will be 1.
  4276|         0|            0|            0|  0.00%|
  4277|         0|            0|            0|  0.00%|See also `One-hot on Wikipedia`_ .
  4278|         0|            0|            0|  0.00%|
  4279|         0|            0|            0|  0.00%|.. _One-hot on Wikipedia:
  4280|         0|            0|            0|  0.00%|    https://en.wikipedia.org/wiki/One-hot
  4281|         0|            0|            0|  0.00%|
  4282|         0|            0|            0|  0.00%|Arguments:
  4283|         0|            0|            0|  0.00%|    tensor (LongTensor): class values of any shape.
  4284|         0|            0|            0|  0.00%|    num_classes (int):  Total number of classes. If set to -1, the number
  4285|         0|            0|            0|  0.00%|        of classes will be inferred as one greater than the largest class
  4286|         0|            0|            0|  0.00%|        value in the input tensor.
  4287|         0|            0|            0|  0.00%|
  4288|         0|            0|            0|  0.00%|Returns:
  4289|         0|            0|            0|  0.00%|    LongTensor that has one more dimension with 1 values at the
  4290|         0|            0|            0|  0.00%|    index of last dimension indicated by the input, and 0 everywhere
  4291|         0|            0|            0|  0.00%|    else.
  4292|         0|            0|            0|  0.00%|
  4293|         0|            0|            0|  0.00%|Examples:
  4294|         0|            0|            0|  0.00%|    >>> F.one_hot(torch.arange(0, 5) % 3)
  4295|         0|            0|            0|  0.00%|    tensor([[1, 0, 0],
  4296|         0|            0|            0|  0.00%|            [0, 1, 0],
  4297|         0|            0|            0|  0.00%|            [0, 0, 1],
  4298|         0|            0|            0|  0.00%|            [1, 0, 0],
  4299|         0|            0|            0|  0.00%|            [0, 1, 0]])
  4300|         0|            0|            0|  0.00%|    >>> F.one_hot(torch.arange(0, 5) % 3, num_classes=5)
  4301|         0|            0|            0|  0.00%|    tensor([[1, 0, 0, 0, 0],
  4302|         0|            0|            0|  0.00%|            [0, 1, 0, 0, 0],
  4303|         0|            0|            0|  0.00%|            [0, 0, 1, 0, 0],
  4304|         0|            0|            0|  0.00%|            [1, 0, 0, 0, 0],
  4305|         0|            0|            0|  0.00%|            [0, 1, 0, 0, 0]])
  4306|         0|            0|            0|  0.00%|    >>> F.one_hot(torch.arange(0, 6).view(3,2) % 3)
  4307|         0|            0|            0|  0.00%|    tensor([[[1, 0, 0],
  4308|         0|            0|            0|  0.00%|             [0, 1, 0]],
  4309|         0|            0|            0|  0.00%|            [[0, 0, 1],
  4310|         0|            0|            0|  0.00%|             [1, 0, 0]],
  4311|         0|            0|            0|  0.00%|            [[0, 1, 0],
  4312|         0|            0|            0|  0.00%|             [0, 0, 1]]])
  4313|         0|            0|            0|  0.00%|""",
  4314|         0|            0|            0|  0.00%|)
  4315|         0|            0|            0|  0.00%|
  4316|         0|            0|            0|  0.00%|
  4317|         0|            0|            0|  0.00%|def triplet_margin_loss(
  4318|         0|            0|            0|  0.00%|    anchor: Tensor,
  4319|         0|            0|            0|  0.00%|    positive: Tensor,
  4320|         0|            0|            0|  0.00%|    negative: Tensor,
  4321|         0|            0|            0|  0.00%|    margin: float = 1.0,
  4322|         0|            0|            0|  0.00%|    p: float = 2,
  4323|         0|            0|            0|  0.00%|    eps: float = 1e-6,
  4324|         0|            0|            0|  0.00%|    swap: bool = False,
  4325|         0|            0|            0|  0.00%|    size_average: Optional[bool] = None,
  4326|         0|            0|            0|  0.00%|    reduce: Optional[bool] = None,
  4327|         0|            0|            0|  0.00%|    reduction: str = "mean",
  4328|         0|            0|            0|  0.00%|) -> Tensor:
  4329|         0|            0|            0|  0.00%|    r"""
  4330|         0|            0|            0|  0.00%|    See :class:`~torch.nn.TripletMarginLoss` for details
  4331|         0|            0|            0|  0.00%|    """
  4332|         0|            0|            0|  0.00%|    if has_torch_function_variadic(anchor, positive, negative):
  4333|         0|            0|            0|  0.00%|        return handle_torch_function(
  4334|         0|            0|            0|  0.00%|            triplet_margin_loss,
  4335|         0|            0|            0|  0.00%|            (anchor, positive, negative),
  4336|         0|            0|            0|  0.00%|            anchor,
  4337|         0|            0|            0|  0.00%|            positive,
  4338|         0|            0|            0|  0.00%|            negative,
  4339|         0|            0|            0|  0.00%|            margin=margin,
  4340|         0|            0|            0|  0.00%|            p=p,
  4341|         0|            0|            0|  0.00%|            eps=eps,
  4342|         0|            0|            0|  0.00%|            swap=swap,
  4343|         0|            0|            0|  0.00%|            size_average=size_average,
  4344|         0|            0|            0|  0.00%|            reduce=reduce,
  4345|         0|            0|            0|  0.00%|            reduction=reduction,
  4346|         0|            0|            0|  0.00%|        )
  4347|         0|            0|            0|  0.00%|    if size_average is not None or reduce is not None:
  4348|         0|            0|            0|  0.00%|        reduction_enum = _Reduction.legacy_get_enum(size_average, reduce)
  4349|         0|            0|            0|  0.00%|    else:
  4350|         0|            0|            0|  0.00%|        reduction_enum = _Reduction.get_enum(reduction)
  4351|         0|            0|            0|  0.00%|    return torch.triplet_margin_loss(anchor, positive, negative, margin, p, eps, swap, reduction_enum)
  4352|         0|            0|            0|  0.00%|
  4353|         0|            0|            0|  0.00%|
  4354|         0|            0|            0|  0.00%|def triplet_margin_with_distance_loss(
  4355|         0|            0|            0|  0.00%|    anchor: Tensor,
  4356|         0|            0|            0|  0.00%|    positive: Tensor,
  4357|         0|            0|            0|  0.00%|    negative: Tensor,
  4358|         0|            0|            0|  0.00%|    *,
  4359|         0|            0|            0|  0.00%|    distance_function: Optional[Callable[[Tensor, Tensor], Tensor]] = None,
  4360|         0|            0|            0|  0.00%|    margin: float = 1.0,
  4361|         0|            0|            0|  0.00%|    swap: bool = False,
  4362|         0|            0|            0|  0.00%|    reduction: str = "mean"
  4363|         0|            0|            0|  0.00%|) -> Tensor:
  4364|         0|            0|            0|  0.00%|    r"""
  4365|         0|            0|            0|  0.00%|    See :class:`~torch.nn.TripletMarginWithDistanceLoss` for details.
  4366|         0|            0|            0|  0.00%|    """
  4367|         0|            0|            0|  0.00%|    if torch.jit.is_scripting():
  4368|         0|            0|            0|  0.00%|        raise NotImplementedError(
  4369|         0|            0|            0|  0.00%|            "F.triplet_margin_with_distance_loss does not support JIT scripting: "
  4370|         0|            0|            0|  0.00%|            "functions requiring Callables cannot be scripted."
  4371|         0|            0|            0|  0.00%|        )
  4372|         0|            0|            0|  0.00%|
  4373|         0|            0|            0|  0.00%|    if has_torch_function_variadic(anchor, positive, negative):
  4374|         0|            0|            0|  0.00%|        return handle_torch_function(
  4375|         0|            0|            0|  0.00%|            triplet_margin_with_distance_loss,
  4376|         0|            0|            0|  0.00%|            (anchor, positive, negative),
  4377|         0|            0|            0|  0.00%|            anchor,
  4378|         0|            0|            0|  0.00%|            positive,
  4379|         0|            0|            0|  0.00%|            negative,
  4380|         0|            0|            0|  0.00%|            distance_function=distance_function,
  4381|         0|            0|            0|  0.00%|            margin=margin,
  4382|         0|            0|            0|  0.00%|            swap=swap,
  4383|         0|            0|            0|  0.00%|            reduction=reduction,
  4384|         0|            0|            0|  0.00%|        )
  4385|         0|            0|            0|  0.00%|
  4386|         0|            0|            0|  0.00%|    distance_function = distance_function if distance_function is not None else pairwise_distance
  4387|         0|            0|            0|  0.00%|
  4388|         0|            0|            0|  0.00%|    positive_dist = distance_function(anchor, positive)
  4389|         0|            0|            0|  0.00%|    negative_dist = distance_function(anchor, negative)
  4390|         0|            0|            0|  0.00%|
  4391|         0|            0|            0|  0.00%|    if swap:
  4392|         0|            0|            0|  0.00%|        swap_dist = distance_function(positive, negative)
  4393|         0|            0|            0|  0.00%|        negative_dist = torch.min(negative_dist, swap_dist)
  4394|         0|            0|            0|  0.00%|
  4395|         0|            0|            0|  0.00%|    output = torch.clamp(positive_dist - negative_dist + margin, min=0.0)
  4396|         0|            0|            0|  0.00%|
  4397|         0|            0|            0|  0.00%|    reduction_enum = _Reduction.get_enum(reduction)
  4398|         0|            0|            0|  0.00%|    if reduction_enum == 1:
  4399|         0|            0|            0|  0.00%|        return output.mean()
  4400|         0|            0|            0|  0.00%|    elif reduction_enum == 2:
  4401|         0|            0|            0|  0.00%|        return output.sum()
  4402|         0|            0|            0|  0.00%|    else:
  4403|         0|            0|            0|  0.00%|        return output
  4404|         0|            0|            0|  0.00%|
  4405|         0|            0|            0|  0.00%|
  4406|         0|            0|            0|  0.00%|def normalize(input: Tensor, p: float = 2.0, dim: int = 1, eps: float = 1e-12, out: Optional[Tensor] = None) -> Tensor:
  4407|         0|            0|            0|  0.00%|    r"""Performs :math:`L_p` normalization of inputs over specified dimension.
  4408|         0|            0|            0|  0.00%|
  4409|         0|            0|            0|  0.00%|    For a tensor :attr:`input` of sizes :math:`(n_0, ..., n_{dim}, ..., n_k)`, each
  4410|         0|            0|            0|  0.00%|    :math:`n_{dim}` -element vector :math:`v` along dimension :attr:`dim` is transformed as
  4411|         0|            0|            0|  0.00%|
  4412|         0|            0|            0|  0.00%|    .. math::
  4413|         0|            0|            0|  0.00%|        v = \frac{v}{\max(\lVert v \rVert_p, \epsilon)}.
  4414|         0|            0|            0|  0.00%|
  4415|         0|            0|            0|  0.00%|    With the default arguments it uses the Euclidean norm over vectors along dimension :math:`1` for normalization.
  4416|         0|            0|            0|  0.00%|
  4417|         0|            0|            0|  0.00%|    Args:
  4418|         0|            0|            0|  0.00%|        input: input tensor of any shape
  4419|         0|            0|            0|  0.00%|        p (float): the exponent value in the norm formulation. Default: 2
  4420|         0|            0|            0|  0.00%|        dim (int): the dimension to reduce. Default: 1
  4421|         0|            0|            0|  0.00%|        eps (float): small value to avoid division by zero. Default: 1e-12
  4422|         0|            0|            0|  0.00%|        out (Tensor, optional): the output tensor. If :attr:`out` is used, this
  4423|         0|            0|            0|  0.00%|                                operation won't be differentiable.
  4424|         0|            0|            0|  0.00%|    """
  4425|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):
  4426|         0|            0|            0|  0.00%|        return handle_torch_function(normalize, (input,), input, p=p, dim=dim, eps=eps, out=out)
  4427|         0|            0|            0|  0.00%|    if out is None:
  4428|         0|            0|            0|  0.00%|        denom = input.norm(p, dim, keepdim=True).clamp_min(eps).expand_as(input)
  4429|         0|            0|            0|  0.00%|        return input / denom
  4430|         0|            0|            0|  0.00%|    else:
  4431|         0|            0|            0|  0.00%|        denom = input.norm(p, dim, keepdim=True).clamp_min_(eps).expand_as(input)
  4432|         0|            0|            0|  0.00%|        return torch.div(input, denom, out=out)
  4433|         0|            0|            0|  0.00%|
  4434|         0|            0|            0|  0.00%|
  4435|         0|            0|            0|  0.00%|def assert_int_or_pair(arg: List[int], arg_name: str, message: str) -> None:
  4436|         0|            0|            0|  0.00%|    assert isinstance(arg, int) or len(arg) == 2, message.format(arg_name)
  4437|         0|            0|            0|  0.00%|
  4438|         0|            0|            0|  0.00%|
  4439|         0|            0|            0|  0.00%|def unfold(
  4440|         0|            0|            0|  0.00%|    input: Tensor, kernel_size: BroadcastingList2[int],
  4441|         0|            0|            0|  0.00%|    dilation: BroadcastingList2[int] = 1,
  4442|         0|            0|            0|  0.00%|    padding: BroadcastingList2[int] = 0,
  4443|         0|            0|            0|  0.00%|    stride: BroadcastingList2[int] = 1
  4444|         0|            0|            0|  0.00%|) -> Tensor:
  4445|         0|            0|            0|  0.00%|    r"""Extracts sliding local blocks from a batched input tensor.
  4446|         0|            0|            0|  0.00%|
  4447|         0|            0|            0|  0.00%|    .. warning::
  4448|         0|            0|            0|  0.00%|        Currently, only 4-D input tensors (batched image-like tensors) are
  4449|         0|            0|            0|  0.00%|        supported.
  4450|         0|            0|            0|  0.00%|
  4451|         0|            0|            0|  0.00%|    .. warning::
  4452|         0|            0|            0|  0.00%|
  4453|         0|            0|            0|  0.00%|        More than one element of the unfolded tensor may refer to a single
  4454|         0|            0|            0|  0.00%|        memory location. As a result, in-place operations (especially ones that
  4455|         0|            0|            0|  0.00%|        are vectorized) may result in incorrect behavior. If you need to write
  4456|         0|            0|            0|  0.00%|        to the tensor, please clone it first.
  4457|         0|            0|            0|  0.00%|
  4458|         0|            0|            0|  0.00%|
  4459|         0|            0|            0|  0.00%|    See :class:`torch.nn.Unfold` for details
  4460|         0|            0|            0|  0.00%|    """
  4461|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):
  4462|         0|            0|            0|  0.00%|        return handle_torch_function(
  4463|         0|            0|            0|  0.00%|            unfold, (input,), input, kernel_size, dilation=dilation, padding=padding, stride=stride
  4464|         0|            0|            0|  0.00%|        )
  4465|         0|            0|            0|  0.00%|    if input.dim() == 4:
  4466|         0|            0|            0|  0.00%|        msg = "{} must be int or 2-tuple for 4D input"
  4467|         0|            0|            0|  0.00%|        assert_int_or_pair(kernel_size, "kernel_size", msg)
  4468|         0|            0|            0|  0.00%|        assert_int_or_pair(dilation, "dilation", msg)
  4469|         0|            0|            0|  0.00%|        assert_int_or_pair(padding, "padding", msg)
  4470|         0|            0|            0|  0.00%|        assert_int_or_pair(stride, "stride", msg)
  4471|         0|            0|            0|  0.00%|
  4472|         0|            0|            0|  0.00%|        return torch._C._nn.im2col(input, _pair(kernel_size), _pair(dilation), _pair(padding), _pair(stride))
  4473|         0|            0|            0|  0.00%|    else:
  4474|         0|            0|            0|  0.00%|        raise NotImplementedError("Input Error: Only 4D input Tensors are supported (got {}D)".format(input.dim()))
  4475|         0|            0|            0|  0.00%|
  4476|         0|            0|            0|  0.00%|
  4477|         0|            0|            0|  0.00%|def fold(
  4478|         0|            0|            0|  0.00%|    input: Tensor, output_size: BroadcastingList2[int],
  4479|         0|            0|            0|  0.00%|    kernel_size: BroadcastingList2[int],
  4480|         0|            0|            0|  0.00%|    dilation: BroadcastingList2[int] = 1,
  4481|         0|            0|            0|  0.00%|    padding: BroadcastingList2[int] = 0,
  4482|         0|            0|            0|  0.00%|    stride: BroadcastingList2[int] = 1
  4483|         0|            0|            0|  0.00%|) -> Tensor:
  4484|         0|            0|            0|  0.00%|    r"""Combines an array of sliding local blocks into a large containing
  4485|         0|            0|            0|  0.00%|    tensor.
  4486|         0|            0|            0|  0.00%|
  4487|         0|            0|            0|  0.00%|    .. warning::
  4488|         0|            0|            0|  0.00%|        Currently, only 3-D output tensors (unfolded batched image-like tensors) are
  4489|         0|            0|            0|  0.00%|        supported.
  4490|         0|            0|            0|  0.00%|
  4491|         0|            0|            0|  0.00%|    See :class:`torch.nn.Fold` for details
  4492|         0|            0|            0|  0.00%|    """
  4493|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):
  4494|         0|            0|            0|  0.00%|        return handle_torch_function(
  4495|         0|            0|            0|  0.00%|            fold, (input,), input, output_size, kernel_size, dilation=dilation, padding=padding, stride=stride
  4496|         0|            0|            0|  0.00%|        )
  4497|         0|            0|            0|  0.00%|    if input.dim() == 3:
  4498|         0|            0|            0|  0.00%|        msg = "{} must be int or 2-tuple for 3D input"
  4499|         0|            0|            0|  0.00%|        assert_int_or_pair(output_size, "output_size", msg)
  4500|         0|            0|            0|  0.00%|        assert_int_or_pair(kernel_size, "kernel_size", msg)
  4501|         0|            0|            0|  0.00%|        assert_int_or_pair(dilation, "dilation", msg)
  4502|         0|            0|            0|  0.00%|        assert_int_or_pair(padding, "padding", msg)
  4503|         0|            0|            0|  0.00%|        assert_int_or_pair(stride, "stride", msg)
  4504|         0|            0|            0|  0.00%|
  4505|         0|            0|            0|  0.00%|        return torch._C._nn.col2im(
  4506|         0|            0|            0|  0.00%|            input, _pair(output_size), _pair(kernel_size), _pair(dilation), _pair(padding), _pair(stride)
  4507|         0|            0|            0|  0.00%|        )
  4508|         0|            0|            0|  0.00%|    else:
  4509|         0|            0|            0|  0.00%|        raise NotImplementedError("Input Error: Only 3D input Tensors are supported (got {}D)".format(input.dim()))
  4510|         0|            0|            0|  0.00%|
  4511|         0|            0|            0|  0.00%|
  4512|         0|            0|            0|  0.00%|def _pad_circular(input: Tensor, padding: List[int]) -> Tensor:
  4513|         0|            0|            0|  0.00%|    """Circularly pads tensor.
  4514|         0|            0|            0|  0.00%|
  4515|         0|            0|            0|  0.00%|    Tensor values at the beginning are used to pad the end, and values at the
  4516|         0|            0|            0|  0.00%|    end are used to pad the beginning. For example, consider a single dimension
  4517|         0|            0|            0|  0.00%|    with values [0, 1, 2, 3]. With circular padding of (1, 1) it would be
  4518|         0|            0|            0|  0.00%|    padded to [3, 0, 1, 2, 3, 0], and with padding (1, 2) it would be padded to
  4519|         0|            0|            0|  0.00%|    [3, 0, 1, 2, 3, 0, 1]. If negative padding is applied then the ends of the
  4520|         0|            0|            0|  0.00%|    tensor get removed. With circular padding of (-1, -1) the previous example
  4521|         0|            0|            0|  0.00%|    would become [1, 2]. Circular padding of (-1, 1) would produce
  4522|         0|            0|            0|  0.00%|    [1, 2, 3, 1].
  4523|         0|            0|            0|  0.00%|
  4524|         0|            0|            0|  0.00%|    The first and second dimensions of the tensor are not padded.
  4525|         0|            0|            0|  0.00%|
  4526|         0|            0|            0|  0.00%|    Args:
  4527|         0|            0|            0|  0.00%|        input: Tensor with shape :math:`(N, C, D[, H, W])`.
  4528|         0|            0|            0|  0.00%|        padding: Tuple containing the number of elements to pad each side of
  4529|         0|            0|            0|  0.00%|            the tensor. The length of padding must be twice the number of
  4530|         0|            0|            0|  0.00%|            paddable dimensions. For example, the length of padding should be 4
  4531|         0|            0|            0|  0.00%|            for a tensor of shape :math:`(N, C, H, W)`, and the length should
  4532|         0|            0|            0|  0.00%|            be 6 for a tensor of shape :math:`(N, C, D, H, W)`.
  4533|         0|            0|            0|  0.00%|
  4534|         0|            0|            0|  0.00%|    Examples::
  4535|         0|            0|            0|  0.00%|
  4536|         0|            0|            0|  0.00%|        >>> x = torch.tensor([[[[0, 1, 2], [3, 4, 5]]]])  # Create tensor
  4537|         0|            0|            0|  0.00%|        >>> # Example 1
  4538|         0|            0|            0|  0.00%|        >>> padding = (1, 1, 1, 1)
  4539|         0|            0|            0|  0.00%|        >>> y = F.pad(x, padding, mode='circular')
  4540|         0|            0|            0|  0.00%|        >>> print(y)
  4541|         0|            0|            0|  0.00%|        tensor([[[[5, 3, 4, 5, 3],
  4542|         0|            0|            0|  0.00%|                  [2, 0, 1, 2, 0],
  4543|         0|            0|            0|  0.00%|                  [5, 3, 4, 5, 3],
  4544|         0|            0|            0|  0.00%|                  [2, 0, 1, 2, 0]]]])
  4545|         0|            0|            0|  0.00%|        >>> print(y.shape)
  4546|         0|            0|            0|  0.00%|        torch.Size([1, 1, 4, 5])
  4547|         0|            0|            0|  0.00%|        >>> # Example 2
  4548|         0|            0|            0|  0.00%|        >>> padding = (1, 1, 2, 2)
  4549|         0|            0|            0|  0.00%|        >>> z = F.pad(x, padding, mode='circular')
  4550|         0|            0|            0|  0.00%|        >>> print(z)
  4551|         0|            0|            0|  0.00%|        tensor([[[[2, 0, 1, 2, 0],
  4552|         0|            0|            0|  0.00%|                  [5, 3, 4, 5, 3],
  4553|         0|            0|            0|  0.00%|                  [2, 0, 1, 2, 0],
  4554|         0|            0|            0|  0.00%|                  [5, 3, 4, 5, 3],
  4555|         0|            0|            0|  0.00%|                  [2, 0, 1, 2, 0],
  4556|         0|            0|            0|  0.00%|                  [5, 3, 4, 5, 3]]]])
  4557|         0|            0|            0|  0.00%|        >>> print(z.shape)
  4558|         0|            0|            0|  0.00%|        torch.Size([1, 1, 6, 5])
  4559|         0|            0|            0|  0.00%|    """
  4560|         0|            0|            0|  0.00%|    in_shape = input.shape
  4561|         0|            0|            0|  0.00%|    paddable_shape = in_shape[2:]
  4562|         0|            0|            0|  0.00%|    ndim = len(paddable_shape)
  4563|         0|            0|            0|  0.00%|
  4564|         0|            0|            0|  0.00%|    for idx, size in enumerate(paddable_shape):
  4565|         0|            0|            0|  0.00%|        # Only supports wrapping around once
  4566|         0|            0|            0|  0.00%|        assert padding[-(idx * 2 + 1)] <= size, "Padding value causes wrapping around more than once."
  4567|         0|            0|            0|  0.00%|        assert padding[-(idx * 2 + 2)] <= size, "Padding value causes wrapping around more than once."
  4568|         0|            0|            0|  0.00%|        # Negative padding should not result in negative sizes
  4569|         0|            0|            0|  0.00%|        assert (
  4570|         0|            0|            0|  0.00%|            padding[-(idx * 2 + 1)] + padding[-(idx * 2 + 2)] + size >= 0
  4571|         0|            0|            0|  0.00%|        ), "Negative padding value is resulting in an empty dimension."
  4572|         0|            0|            0|  0.00%|
  4573|         0|            0|            0|  0.00%|    # Get shape of padded tensor
  4574|         0|            0|            0|  0.00%|    out_shape = in_shape[:2]
  4575|         0|            0|            0|  0.00%|    for idx, size in enumerate(paddable_shape):
  4576|         0|            0|            0|  0.00%|        out_shape += (size + padding[-(idx * 2 + 1)] + padding[-(idx * 2 + 2)],)
  4577|         0|            0|            0|  0.00%|
  4578|         0|            0|            0|  0.00%|    out = torch.empty(out_shape, dtype=input.dtype, layout=input.layout, device=input.device)
  4579|         0|            0|            0|  0.00%|
  4580|         0|            0|            0|  0.00%|    # Put original array in padded array
  4581|         0|            0|            0|  0.00%|    if ndim == 1:
  4582|         0|            0|            0|  0.00%|        out_d0 = max(padding[-2], 0)
  4583|         0|            0|            0|  0.00%|        out_d1 = out_shape[2] - max(padding[-1], 0)
  4584|         0|            0|            0|  0.00%|
  4585|         0|            0|            0|  0.00%|        in_d0 = max(-padding[-2], 0)
  4586|         0|            0|            0|  0.00%|        in_d1 = in_shape[2] - max(-padding[-1], 0)
  4587|         0|            0|            0|  0.00%|
  4588|         0|            0|            0|  0.00%|        out[..., out_d0:out_d1] = input[..., in_d0:in_d1]
  4589|         0|            0|            0|  0.00%|    elif ndim == 2:
  4590|         0|            0|            0|  0.00%|        out_d0 = max(padding[-2], 0)
  4591|         0|            0|            0|  0.00%|        out_d1 = out_shape[2] - max(padding[-1], 0)
  4592|         0|            0|            0|  0.00%|
  4593|         0|            0|            0|  0.00%|        out_h0 = max(padding[-4], 0)
  4594|         0|            0|            0|  0.00%|        out_h1 = out_shape[3] - max(padding[-3], 0)
  4595|         0|            0|            0|  0.00%|
  4596|         0|            0|            0|  0.00%|        in_d0 = max(-padding[-2], 0)
  4597|         0|            0|            0|  0.00%|        in_d1 = in_shape[2] - max(-padding[-1], 0)
  4598|         0|            0|            0|  0.00%|
  4599|         0|            0|            0|  0.00%|        in_h0 = max(-padding[-4], 0)
  4600|         0|            0|            0|  0.00%|        in_h1 = in_shape[3] - max(-padding[-3], 0)
  4601|         0|            0|            0|  0.00%|
  4602|         0|            0|            0|  0.00%|        out[..., out_d0:out_d1, out_h0:out_h1] = input[..., in_d0:in_d1, in_h0:in_h1]
  4603|         0|            0|            0|  0.00%|    elif ndim == 3:
  4604|         0|            0|            0|  0.00%|        out_d0 = max(padding[-2], 0)
  4605|         0|            0|            0|  0.00%|        out_d1 = out_shape[2] - max(padding[-1], 0)
  4606|         0|            0|            0|  0.00%|
  4607|         0|            0|            0|  0.00%|        out_h0 = max(padding[-4], 0)
  4608|         0|            0|            0|  0.00%|        out_h1 = out_shape[3] - max(padding[-3], 0)
  4609|         0|            0|            0|  0.00%|
  4610|         0|            0|            0|  0.00%|        out_w0 = max(padding[-6], 0)
  4611|         0|            0|            0|  0.00%|        out_w1 = out_shape[4] - max(padding[-5], 0)
  4612|         0|            0|            0|  0.00%|
  4613|         0|            0|            0|  0.00%|        in_d0 = max(-padding[-2], 0)
  4614|         0|            0|            0|  0.00%|        in_d1 = in_shape[2] - max(-padding[-1], 0)
  4615|         0|            0|            0|  0.00%|
  4616|         0|            0|            0|  0.00%|        in_h0 = max(-padding[-4], 0)
  4617|         0|            0|            0|  0.00%|        in_h1 = in_shape[3] - max(-padding[-3], 0)
  4618|         0|            0|            0|  0.00%|
  4619|         0|            0|            0|  0.00%|        in_w0 = max(-padding[-6], 0)
  4620|         0|            0|            0|  0.00%|        in_w1 = in_shape[4] - max(-padding[-5], 0)
  4621|         0|            0|            0|  0.00%|
  4622|         0|            0|            0|  0.00%|        out[..., out_d0:out_d1, out_h0:out_h1, out_w0:out_w1] = input[..., in_d0:in_d1, in_h0:in_h1, in_w0:in_w1]
  4623|         0|            0|            0|  0.00%|
  4624|         0|            0|            0|  0.00%|    # The following steps first pad the beginning of the tensor (left side),
  4625|         0|            0|            0|  0.00%|    # and then pad the end of the tensor (right side).
  4626|         0|            0|            0|  0.00%|    # Note: Corners will be written more than once when ndim > 1.
  4627|         0|            0|            0|  0.00%|
  4628|         0|            0|            0|  0.00%|    # Only in cases where padding values are > 0 are when additional copying
  4629|         0|            0|            0|  0.00%|    # is required.
  4630|         0|            0|            0|  0.00%|
  4631|         0|            0|            0|  0.00%|    # Pad first dimension (depth)
  4632|         0|            0|            0|  0.00%|    if padding[-2] > 0:
  4633|         0|            0|            0|  0.00%|        i0 = out_shape[2] - padding[-2] - max(padding[-1], 0)
  4634|         0|            0|            0|  0.00%|        i1 = out_shape[2] - max(padding[-1], 0)
  4635|         0|            0|            0|  0.00%|        o0 = 0
  4636|         0|            0|            0|  0.00%|        o1 = padding[-2]
  4637|         0|            0|            0|  0.00%|        out[:, :, o0:o1] = out[:, :, i0:i1]
  4638|         0|            0|            0|  0.00%|    if padding[-1] > 0:
  4639|         0|            0|            0|  0.00%|        i0 = max(padding[-2], 0)
  4640|         0|            0|            0|  0.00%|        i1 = max(padding[-2], 0) + padding[-1]
  4641|         0|            0|            0|  0.00%|        o0 = out_shape[2] - padding[-1]
  4642|         0|            0|            0|  0.00%|        o1 = out_shape[2]
  4643|         0|            0|            0|  0.00%|        out[:, :, o0:o1] = out[:, :, i0:i1]
  4644|         0|            0|            0|  0.00%|
  4645|         0|            0|            0|  0.00%|    # Pad second dimension (height)
  4646|         0|            0|            0|  0.00%|    if len(padding) > 2:
  4647|         0|            0|            0|  0.00%|        if padding[-4] > 0:
  4648|         0|            0|            0|  0.00%|            i0 = out_shape[3] - padding[-4] - max(padding[-3], 0)
  4649|         0|            0|            0|  0.00%|            i1 = out_shape[3] - max(padding[-3], 0)
  4650|         0|            0|            0|  0.00%|            o0 = 0
  4651|         0|            0|            0|  0.00%|            o1 = padding[-4]
  4652|         0|            0|            0|  0.00%|            out[:, :, :, o0:o1] = out[:, :, :, i0:i1]
  4653|         0|            0|            0|  0.00%|        if padding[-3] > 0:
  4654|         0|            0|            0|  0.00%|            i0 = max(padding[-4], 0)
  4655|         0|            0|            0|  0.00%|            i1 = max(padding[-4], 0) + padding[-3]
  4656|         0|            0|            0|  0.00%|            o0 = out_shape[3] - padding[-3]
  4657|         0|            0|            0|  0.00%|            o1 = out_shape[3]
  4658|         0|            0|            0|  0.00%|            out[:, :, :, o0:o1] = out[:, :, :, i0:i1]
  4659|         0|            0|            0|  0.00%|
  4660|         0|            0|            0|  0.00%|    # Pad third dimension (width)
  4661|         0|            0|            0|  0.00%|    if len(padding) > 4:
  4662|         0|            0|            0|  0.00%|        if padding[-6] > 0:
  4663|         0|            0|            0|  0.00%|            i0 = out_shape[4] - padding[-6] - max(padding[-5], 0)
  4664|         0|            0|            0|  0.00%|            i1 = out_shape[4] - max(padding[-5], 0)
  4665|         0|            0|            0|  0.00%|            o0 = 0
  4666|         0|            0|            0|  0.00%|            o1 = padding[-6]
  4667|         0|            0|            0|  0.00%|            out[:, :, :, :, o0:o1] = out[:, :, :, :, i0:i1]
  4668|         0|            0|            0|  0.00%|        if padding[-5] > 0:
  4669|         0|            0|            0|  0.00%|            i0 = max(padding[-6], 0)
  4670|         0|            0|            0|  0.00%|            i1 = max(padding[-6], 0) + padding[-5]
  4671|         0|            0|            0|  0.00%|            o0 = out_shape[4] - padding[-5]
  4672|         0|            0|            0|  0.00%|            o1 = out_shape[4]
  4673|         0|            0|            0|  0.00%|            out[:, :, :, :, o0:o1] = out[:, :, :, :, i0:i1]
  4674|         0|            0|            0|  0.00%|
  4675|         0|            0|            0|  0.00%|    return out
  4676|         0|            0|            0|  0.00%|
  4677|         0|            0|            0|  0.00%|#
  4678|         0|            0|            0|  0.00%|# multihead attention
  4679|         0|            0|            0|  0.00%|#
  4680|         0|            0|            0|  0.00%|
  4681|         0|            0|            0|  0.00%|def _in_projection_packed(
  4682|         0|            0|            0|  0.00%|    q: Tensor,
  4683|         0|            0|            0|  0.00%|    k: Tensor,
  4684|         0|            0|            0|  0.00%|    v: Tensor,
  4685|         0|            0|            0|  0.00%|    w: Tensor,
  4686|         0|            0|            0|  0.00%|    b: Optional[Tensor] = None,
  4687|         0|            0|            0|  0.00%|) -> List[Tensor]:
  4688|         0|            0|            0|  0.00%|    r"""
  4689|         0|            0|            0|  0.00%|    Performs the in-projection step of the attention operation, using packed weights.
  4690|         0|            0|            0|  0.00%|    Output is a triple containing projection tensors for query, key and value.
  4691|         0|            0|            0|  0.00%|
  4692|         0|            0|            0|  0.00%|    Args:
  4693|         0|            0|            0|  0.00%|        q, k, v: query, key and value tensors to be projected. For self-attention,
  4694|         0|            0|            0|  0.00%|            these are typically the same tensor; for encoder-decoder attention,
  4695|         0|            0|            0|  0.00%|            k and v are typically the same tensor. (We take advantage of these
  4696|         0|            0|            0|  0.00%|            identities for performance if they are present.) Regardless, q, k and v
  4697|         0|            0|            0|  0.00%|            must share a common embedding dimension; otherwise their shapes may vary.
  4698|         0|            0|            0|  0.00%|        w: projection weights for q, k and v, packed into a single tensor. Weights
  4699|         0|            0|            0|  0.00%|            are packed along dimension 0, in q, k, v order.
  4700|         0|            0|            0|  0.00%|        b: optional projection biases for q, k and v, packed into a single tensor
  4701|         0|            0|            0|  0.00%|            in q, k, v order.
  4702|         0|            0|            0|  0.00%|
  4703|         0|            0|            0|  0.00%|    Shape:
  4704|         0|            0|            0|  0.00%|        Inputs:
  4705|         0|            0|            0|  0.00%|        - q: :math:`(..., E)` where E is the embedding dimension
  4706|         0|            0|            0|  0.00%|        - k: :math:`(..., E)` where E is the embedding dimension
  4707|         0|            0|            0|  0.00%|        - v: :math:`(..., E)` where E is the embedding dimension
  4708|         0|            0|            0|  0.00%|        - w: :math:`(E * 3, E)` where E is the embedding dimension
  4709|         0|            0|            0|  0.00%|        - b: :math:`E * 3` where E is the embedding dimension
  4710|         0|            0|            0|  0.00%|
  4711|         0|            0|            0|  0.00%|        Output:
  4712|         0|            0|            0|  0.00%|        - in output list :math:`[q', k', v']`, each output tensor will have the
  4713|         0|            0|            0|  0.00%|            same shape as the corresponding input tensor.
  4714|         0|            0|            0|  0.00%|    """
  4715|         0|            0|            0|  0.00%|    E = q.size(-1)
  4716|         0|            0|            0|  0.00%|    if k is v:
  4717|         0|            0|            0|  0.00%|        if q is k:
  4718|         0|            0|            0|  0.00%|            # self-attention
  4719|         0|            0|            0|  0.00%|            return linear(q, w, b).chunk(3, dim=-1)
  4720|         0|            0|            0|  0.00%|        else:
  4721|         0|            0|            0|  0.00%|            # encoder-decoder attention
  4722|         0|            0|            0|  0.00%|            w_q, w_kv = w.split([E, E * 2])
  4723|         0|            0|            0|  0.00%|            if b is None:
  4724|         0|            0|            0|  0.00%|                b_q = b_kv = None
  4725|         0|            0|            0|  0.00%|            else:
  4726|         0|            0|            0|  0.00%|                b_q, b_kv = b.split([E, E * 2])
  4727|         0|            0|            0|  0.00%|            return (linear(q, w_q, b_q),) + linear(k, w_kv, b_kv).chunk(2, dim=-1)
  4728|         0|            0|            0|  0.00%|    else:
  4729|         0|            0|            0|  0.00%|        w_q, w_k, w_v = w.chunk(3)
  4730|         0|            0|            0|  0.00%|        if b is None:
  4731|         0|            0|            0|  0.00%|            b_q = b_k = b_v = None
  4732|         0|            0|            0|  0.00%|        else:
  4733|         0|            0|            0|  0.00%|            b_q, b_k, b_v = b.chunk(3)
  4734|         0|            0|            0|  0.00%|        return linear(q, w_q, b_q), linear(k, w_k, b_k), linear(v, w_v, b_v)
  4735|         0|            0|            0|  0.00%|
  4736|         0|            0|            0|  0.00%|
  4737|         0|            0|            0|  0.00%|def _in_projection(
  4738|         0|            0|            0|  0.00%|    q: Tensor,
  4739|         0|            0|            0|  0.00%|    k: Tensor,
  4740|         0|            0|            0|  0.00%|    v: Tensor,
  4741|         0|            0|            0|  0.00%|    w_q: Tensor,
  4742|         0|            0|            0|  0.00%|    w_k: Tensor,
  4743|         0|            0|            0|  0.00%|    w_v: Tensor,
  4744|         0|            0|            0|  0.00%|    b_q: Optional[Tensor] = None,
  4745|         0|            0|            0|  0.00%|    b_k: Optional[Tensor] = None,
  4746|         0|            0|            0|  0.00%|    b_v: Optional[Tensor] = None,
  4747|         0|            0|            0|  0.00%|) -> Tuple[Tensor, Tensor, Tensor]:
  4748|         0|            0|            0|  0.00%|    r"""
  4749|         0|            0|            0|  0.00%|    Performs the in-projection step of the attention operation. This is simply
  4750|         0|            0|            0|  0.00%|    a triple of linear projections, with shape constraints on the weights which
  4751|         0|            0|            0|  0.00%|    ensure embedding dimension uniformity in the projected outputs.
  4752|         0|            0|            0|  0.00%|    Output is a triple containing projection tensors for query, key and value.
  4753|         0|            0|            0|  0.00%|
  4754|         0|            0|            0|  0.00%|    Args:
  4755|         0|            0|            0|  0.00%|        q, k, v: query, key and value tensors to be projected.
  4756|         0|            0|            0|  0.00%|        w_q, w_k, w_v: weights for q, k and v, respectively.
  4757|         0|            0|            0|  0.00%|        b_q, b_k, b_v: optional biases for q, k and v, respectively.
  4758|         0|            0|            0|  0.00%|
  4759|         0|            0|            0|  0.00%|    Shape:
  4760|         0|            0|            0|  0.00%|        Inputs:
  4761|         0|            0|            0|  0.00%|        - q: :math:`(Qdims..., Eq)` where Eq is the query embedding dimension and Qdims are any
  4762|         0|            0|            0|  0.00%|            number of leading dimensions.
  4763|         0|            0|            0|  0.00%|        - k: :math:`(Kdims..., Ek)` where Ek is the key embedding dimension and Kdims are any
  4764|         0|            0|            0|  0.00%|            number of leading dimensions.
  4765|         0|            0|            0|  0.00%|        - v: :math:`(Vdims..., Ev)` where Ev is the value embedding dimension and Vdims are any
  4766|         0|            0|            0|  0.00%|            number of leading dimensions.
  4767|         0|            0|            0|  0.00%|        - w_q: :math:`(Eq, Eq)`
  4768|         0|            0|            0|  0.00%|        - w_k: :math:`(Eq, Ek)`
  4769|         0|            0|            0|  0.00%|        - w_v: :math:`(Eq, Ev)`
  4770|         0|            0|            0|  0.00%|        - b_q: :math:`(Eq)`
  4771|         0|            0|            0|  0.00%|        - b_k: :math:`(Eq)`
  4772|         0|            0|            0|  0.00%|        - b_v: :math:`(Eq)`
  4773|         0|            0|            0|  0.00%|
  4774|         0|            0|            0|  0.00%|        Output: in output triple :math:`(q', k', v')`,
  4775|         0|            0|            0|  0.00%|         - q': :math:`[Qdims..., Eq]`
  4776|         0|            0|            0|  0.00%|         - k': :math:`[Kdims..., Eq]`
  4777|         0|            0|            0|  0.00%|         - v': :math:`[Vdims..., Eq]`
  4778|         0|            0|            0|  0.00%|
  4779|         0|            0|            0|  0.00%|    """
  4780|         0|            0|            0|  0.00%|    Eq, Ek, Ev = q.size(-1), k.size(-1), v.size(-1)
  4781|         0|            0|            0|  0.00%|    assert w_q.shape == (Eq, Eq), f"expecting query weights shape of {(Eq, Eq)}, but got {w_q.shape}"
  4782|         0|            0|            0|  0.00%|    assert w_k.shape == (Eq, Ek), f"expecting key weights shape of {(Eq, Ek)}, but got {w_k.shape}"
  4783|         0|            0|            0|  0.00%|    assert w_v.shape == (Eq, Ev), f"expecting value weights shape of {(Eq, Ev)}, but got {w_v.shape}"
  4784|         0|            0|            0|  0.00%|    assert b_q is None or b_q.shape == (Eq,), f"expecting query bias shape of {(Eq,)}, but got {b_q.shape}"
  4785|         0|            0|            0|  0.00%|    assert b_k is None or b_k.shape == (Eq,), f"expecting key bias shape of {(Eq,)}, but got {b_k.shape}"
  4786|         0|            0|            0|  0.00%|    assert b_v is None or b_v.shape == (Eq,), f"expecting value bias shape of {(Eq,)}, but got {b_v.shape}"
  4787|         0|            0|            0|  0.00%|    return linear(q, w_q, b_q), linear(k, w_k, b_k), linear(v, w_v, b_v)
  4788|         0|            0|            0|  0.00%|
  4789|         0|            0|            0|  0.00%|
  4790|         0|            0|            0|  0.00%|def _scaled_dot_product_attention(
  4791|         0|            0|            0|  0.00%|    q: Tensor,
  4792|         0|            0|            0|  0.00%|    k: Tensor,
  4793|         0|            0|            0|  0.00%|    v: Tensor,
  4794|         0|            0|            0|  0.00%|    attn_mask: Optional[Tensor] = None,
  4795|         0|            0|            0|  0.00%|    dropout_p: float = 0.0,
  4796|         0|            0|            0|  0.00%|) -> Tuple[Tensor, Tensor]:
  4797|         0|            0|            0|  0.00%|    r"""
  4798|         0|            0|            0|  0.00%|    Computes scaled dot product attention on query, key and value tensors, using
  4799|         0|            0|            0|  0.00%|    an optional attention mask if passed, and applying dropout if a probability
  4800|         0|            0|            0|  0.00%|    greater than 0.0 is specified.
  4801|         0|            0|            0|  0.00%|    Returns a tensor pair containing attended values and attention weights.
  4802|         0|            0|            0|  0.00%|
  4803|         0|            0|            0|  0.00%|    Args:
  4804|         0|            0|            0|  0.00%|        q, k, v: query, key and value tensors. See Shape section for shape details.
  4805|         0|            0|            0|  0.00%|        attn_mask: optional tensor containing mask values to be added to calculated
  4806|         0|            0|            0|  0.00%|            attention. May be 2D or 3D; see Shape section for details.
  4807|         0|            0|            0|  0.00%|        dropout_p: dropout probability. If greater than 0.0, dropout is applied.
  4808|         0|            0|            0|  0.00%|
  4809|         0|            0|            0|  0.00%|    Shape:
  4810|         0|            0|            0|  0.00%|        - q: :math:`(B, Nt, E)` where B is batch size, Nt is the target sequence length,
  4811|         0|            0|            0|  0.00%|            and E is embedding dimension.
  4812|         0|            0|            0|  0.00%|        - key: :math:`(B, Ns, E)` where B is batch size, Ns is the source sequence length,
  4813|         0|            0|            0|  0.00%|            and E is embedding dimension.
  4814|         0|            0|            0|  0.00%|        - value: :math:`(B, Ns, E)` where B is batch size, Ns is the source sequence length,
  4815|         0|            0|            0|  0.00%|            and E is embedding dimension.
  4816|         0|            0|            0|  0.00%|        - attn_mask: either a 3D tensor of shape :math:`(B, Nt, Ns)` or a 2D tensor of
  4817|         0|            0|            0|  0.00%|            shape :math:`(Nt, Ns)`.
  4818|         0|            0|            0|  0.00%|
  4819|         0|            0|            0|  0.00%|        - Output: attention values have shape :math:`(B, Nt, E)`; attention weights
  4820|         0|            0|            0|  0.00%|            have shape :math:`(B, Nt, Ns)`
  4821|         0|            0|            0|  0.00%|    """
  4822|         0|            0|            0|  0.00%|    B, Nt, E = q.shape
  4823|         0|            0|            0|  0.00%|    q = q / math.sqrt(E)
  4824|         0|            0|            0|  0.00%|    # (B, Nt, E) x (B, E, Ns) -> (B, Nt, Ns)
  4825|         0|            0|            0|  0.00%|    attn = torch.bmm(q, k.transpose(-2, -1))
  4826|         0|            0|            0|  0.00%|    if attn_mask is not None:
  4827|         0|            0|            0|  0.00%|        attn += attn_mask
  4828|         0|            0|            0|  0.00%|    attn = softmax(attn, dim=-1)
  4829|         0|            0|            0|  0.00%|    if dropout_p > 0.0:
  4830|         0|            0|            0|  0.00%|        attn = dropout(attn, p=dropout_p)
  4831|         0|            0|            0|  0.00%|    # (B, Nt, Ns) x (B, Ns, E) -> (B, Nt, E)
  4832|         0|            0|            0|  0.00%|    output = torch.bmm(attn, v)
  4833|         0|            0|            0|  0.00%|    return output, attn
  4834|         0|            0|            0|  0.00%|
  4835|         0|            0|            0|  0.00%|
  4836|         0|            0|            0|  0.00%|def multi_head_attention_forward(
  4837|         0|            0|            0|  0.00%|    query: Tensor,
  4838|         0|            0|            0|  0.00%|    key: Tensor,
  4839|         0|            0|            0|  0.00%|    value: Tensor,
  4840|         0|            0|            0|  0.00%|    embed_dim_to_check: int,
  4841|         0|            0|            0|  0.00%|    num_heads: int,
  4842|         0|            0|            0|  0.00%|    in_proj_weight: Tensor,
  4843|         0|            0|            0|  0.00%|    in_proj_bias: Optional[Tensor],
  4844|         0|            0|            0|  0.00%|    bias_k: Optional[Tensor],
  4845|         0|            0|            0|  0.00%|    bias_v: Optional[Tensor],
  4846|         0|            0|            0|  0.00%|    add_zero_attn: bool,
  4847|         0|            0|            0|  0.00%|    dropout_p: float,
  4848|         0|            0|            0|  0.00%|    out_proj_weight: Tensor,
  4849|         0|            0|            0|  0.00%|    out_proj_bias: Optional[Tensor],
  4850|         0|            0|            0|  0.00%|    training: bool = True,
  4851|         0|            0|            0|  0.00%|    key_padding_mask: Optional[Tensor] = None,
  4852|         0|            0|            0|  0.00%|    need_weights: bool = True,
  4853|         0|            0|            0|  0.00%|    attn_mask: Optional[Tensor] = None,
  4854|         0|            0|            0|  0.00%|    use_separate_proj_weight: bool = False,
  4855|         0|            0|            0|  0.00%|    q_proj_weight: Optional[Tensor] = None,
  4856|         0|            0|            0|  0.00%|    k_proj_weight: Optional[Tensor] = None,
  4857|         0|            0|            0|  0.00%|    v_proj_weight: Optional[Tensor] = None,
  4858|         0|            0|            0|  0.00%|    static_k: Optional[Tensor] = None,
  4859|         0|            0|            0|  0.00%|    static_v: Optional[Tensor] = None,
  4860|         0|            0|            0|  0.00%|) -> Tuple[Tensor, Optional[Tensor]]:
  4861|         0|            0|            0|  0.00%|    r"""
  4862|         0|            0|            0|  0.00%|    Args:
  4863|         0|            0|            0|  0.00%|        query, key, value: map a query and a set of key-value pairs to an output.
  4864|         0|            0|            0|  0.00%|            See "Attention Is All You Need" for more details.
  4865|         0|            0|            0|  0.00%|        embed_dim_to_check: total dimension of the model.
  4866|         0|            0|            0|  0.00%|        num_heads: parallel attention heads.
  4867|         0|            0|            0|  0.00%|        in_proj_weight, in_proj_bias: input projection weight and bias.
  4868|         0|            0|            0|  0.00%|        bias_k, bias_v: bias of the key and value sequences to be added at dim=0.
  4869|         0|            0|            0|  0.00%|        add_zero_attn: add a new batch of zeros to the key and
  4870|         0|            0|            0|  0.00%|                       value sequences at dim=1.
  4871|         0|            0|            0|  0.00%|        dropout_p: probability of an element to be zeroed.
  4872|         0|            0|            0|  0.00%|        out_proj_weight, out_proj_bias: the output projection weight and bias.
  4873|         0|            0|            0|  0.00%|        training: apply dropout if is ``True``.
  4874|         0|            0|            0|  0.00%|        key_padding_mask: if provided, specified padding elements in the key will
  4875|         0|            0|            0|  0.00%|            be ignored by the attention. This is an binary mask. When the value is True,
  4876|         0|            0|            0|  0.00%|            the corresponding value on the attention layer will be filled with -inf.
  4877|         0|            0|            0|  0.00%|        need_weights: output attn_output_weights.
  4878|         0|            0|            0|  0.00%|        attn_mask: 2D or 3D mask that prevents attention to certain positions. A 2D mask will be broadcasted for all
  4879|         0|            0|            0|  0.00%|            the batches while a 3D mask allows to specify a different mask for the entries of each batch.
  4880|         0|            0|            0|  0.00%|        use_separate_proj_weight: the function accept the proj. weights for query, key,
  4881|         0|            0|            0|  0.00%|            and value in different forms. If false, in_proj_weight will be used, which is
  4882|         0|            0|            0|  0.00%|            a combination of q_proj_weight, k_proj_weight, v_proj_weight.
  4883|         0|            0|            0|  0.00%|        q_proj_weight, k_proj_weight, v_proj_weight, in_proj_bias: input projection weight and bias.
  4884|         0|            0|            0|  0.00%|        static_k, static_v: static key and value used for attention operators.
  4885|         0|            0|            0|  0.00%|
  4886|         0|            0|            0|  0.00%|
  4887|         0|            0|            0|  0.00%|    Shape:
  4888|         0|            0|            0|  0.00%|        Inputs:
  4889|         0|            0|            0|  0.00%|        - query: :math:`(L, N, E)` where L is the target sequence length, N is the batch size, E is
  4890|         0|            0|            0|  0.00%|          the embedding dimension.
  4891|         0|            0|            0|  0.00%|        - key: :math:`(S, N, E)`, where S is the source sequence length, N is the batch size, E is
  4892|         0|            0|            0|  0.00%|          the embedding dimension.
  4893|         0|            0|            0|  0.00%|        - value: :math:`(S, N, E)` where S is the source sequence length, N is the batch size, E is
  4894|         0|            0|            0|  0.00%|          the embedding dimension.
  4895|         0|            0|            0|  0.00%|        - key_padding_mask: :math:`(N, S)` where N is the batch size, S is the source sequence length.
  4896|         0|            0|            0|  0.00%|          If a ByteTensor is provided, the non-zero positions will be ignored while the zero positions
  4897|         0|            0|            0|  0.00%|          will be unchanged. If a BoolTensor is provided, the positions with the
  4898|         0|            0|            0|  0.00%|          value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged.
  4899|         0|            0|            0|  0.00%|        - attn_mask: 2D mask :math:`(L, S)` where L is the target sequence length, S is the source sequence length.
  4900|         0|            0|            0|  0.00%|          3D mask :math:`(N*num_heads, L, S)` where N is the batch size, L is the target sequence length,
  4901|         0|            0|            0|  0.00%|          S is the source sequence length. attn_mask ensures that position i is allowed to attend the unmasked
  4902|         0|            0|            0|  0.00%|          positions. If a ByteTensor is provided, the non-zero positions are not allowed to attend
  4903|         0|            0|            0|  0.00%|          while the zero positions will be unchanged. If a BoolTensor is provided, positions with ``True``
  4904|         0|            0|            0|  0.00%|          are not allowed to attend while ``False`` values will be unchanged. If a FloatTensor
  4905|         0|            0|            0|  0.00%|          is provided, it will be added to the attention weight.
  4906|         0|            0|            0|  0.00%|        - static_k: :math:`(N*num_heads, S, E/num_heads)`, where S is the source sequence length,
  4907|         0|            0|            0|  0.00%|          N is the batch size, E is the embedding dimension. E/num_heads is the head dimension.
  4908|         0|            0|            0|  0.00%|        - static_v: :math:`(N*num_heads, S, E/num_heads)`, where S is the source sequence length,
  4909|         0|            0|            0|  0.00%|          N is the batch size, E is the embedding dimension. E/num_heads is the head dimension.
  4910|         0|            0|            0|  0.00%|
  4911|         0|            0|            0|  0.00%|        Outputs:
  4912|         0|            0|            0|  0.00%|        - attn_output: :math:`(L, N, E)` where L is the target sequence length, N is the batch size,
  4913|         0|            0|            0|  0.00%|          E is the embedding dimension.
  4914|         0|            0|            0|  0.00%|        - attn_output_weights: :math:`(N, L, S)` where N is the batch size,
  4915|         0|            0|            0|  0.00%|          L is the target sequence length, S is the source sequence length.
  4916|         0|            0|            0|  0.00%|    """
  4917|         0|            0|            0|  0.00%|    tens_ops = (query, key, value, in_proj_weight, in_proj_bias, bias_k, bias_v, out_proj_weight, out_proj_bias)
  4918|         0|            0|            0|  0.00%|    if has_torch_function(tens_ops):
  4919|         0|            0|            0|  0.00%|        return handle_torch_function(
  4920|         0|            0|            0|  0.00%|            multi_head_attention_forward,
  4921|         0|            0|            0|  0.00%|            tens_ops,
  4922|         0|            0|            0|  0.00%|            query,
  4923|         0|            0|            0|  0.00%|            key,
  4924|         0|            0|            0|  0.00%|            value,
  4925|         0|            0|            0|  0.00%|            embed_dim_to_check,
  4926|         0|            0|            0|  0.00%|            num_heads,
  4927|         0|            0|            0|  0.00%|            in_proj_weight,
  4928|         0|            0|            0|  0.00%|            in_proj_bias,
  4929|         0|            0|            0|  0.00%|            bias_k,
  4930|         0|            0|            0|  0.00%|            bias_v,
  4931|         0|            0|            0|  0.00%|            add_zero_attn,
  4932|         0|            0|            0|  0.00%|            dropout_p,
  4933|         0|            0|            0|  0.00%|            out_proj_weight,
  4934|         0|            0|            0|  0.00%|            out_proj_bias,
  4935|         0|            0|            0|  0.00%|            training=training,
  4936|         0|            0|            0|  0.00%|            key_padding_mask=key_padding_mask,
  4937|         0|            0|            0|  0.00%|            need_weights=need_weights,
  4938|         0|            0|            0|  0.00%|            attn_mask=attn_mask,
  4939|         0|            0|            0|  0.00%|            use_separate_proj_weight=use_separate_proj_weight,
  4940|         0|            0|            0|  0.00%|            q_proj_weight=q_proj_weight,
  4941|         0|            0|            0|  0.00%|            k_proj_weight=k_proj_weight,
  4942|         0|            0|            0|  0.00%|            v_proj_weight=v_proj_weight,
  4943|         0|            0|            0|  0.00%|            static_k=static_k,
  4944|         0|            0|            0|  0.00%|            static_v=static_v,
  4945|         0|            0|            0|  0.00%|        )
  4946|         0|            0|            0|  0.00%|
  4947|         0|            0|            0|  0.00%|    # set up shape vars
  4948|         0|            0|            0|  0.00%|    tgt_len, bsz, embed_dim = query.shape
  4949|         0|            0|            0|  0.00%|    src_len, _, _ = key.shape
  4950|         0|            0|            0|  0.00%|    assert embed_dim == embed_dim_to_check, \
  4951|         0|            0|            0|  0.00%|        f"was expecting embedding dimension of {embed_dim_to_check}, but got {embed_dim}"
  4952|         0|            0|            0|  0.00%|    if isinstance(embed_dim, torch.Tensor):
  4953|         0|            0|            0|  0.00%|        # embed_dim can be a tensor when JIT tracing
  4954|         0|            0|            0|  0.00%|        head_dim = embed_dim.div(num_heads, rounding_mode='trunc')
  4955|         0|            0|            0|  0.00%|    else:
  4956|         0|            0|            0|  0.00%|        head_dim = embed_dim // num_heads
  4957|         0|            0|            0|  0.00%|    assert head_dim * num_heads == embed_dim, f"embed_dim {embed_dim} not divisible by num_heads {num_heads}"
  4958|         0|            0|            0|  0.00%|    if use_separate_proj_weight:
  4959|         0|            0|            0|  0.00%|        # allow MHA to have different embedding dimensions when separate projection weights are used
  4960|         0|            0|            0|  0.00%|        assert key.shape[:2] == value.shape[:2], \
  4961|         0|            0|            0|  0.00%|            f"key's sequence and batch dims {key.shape[:2]} do not match value's {value.shape[:2]}"
  4962|         0|            0|            0|  0.00%|    else:
  4963|         0|            0|            0|  0.00%|        assert key.shape == value.shape, f"key shape {key.shape} does not match value shape {value.shape}"
  4964|         0|            0|            0|  0.00%|
  4965|         0|            0|            0|  0.00%|    #
  4966|         0|            0|            0|  0.00%|    # compute in-projection
  4967|         0|            0|            0|  0.00%|    #
  4968|         0|            0|            0|  0.00%|    if not use_separate_proj_weight:
  4969|         0|            0|            0|  0.00%|        q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)
  4970|         0|            0|            0|  0.00%|    else:
  4971|         0|            0|            0|  0.00%|        assert q_proj_weight is not None, "use_separate_proj_weight is True but q_proj_weight is None"
  4972|         0|            0|            0|  0.00%|        assert k_proj_weight is not None, "use_separate_proj_weight is True but k_proj_weight is None"
  4973|         0|            0|            0|  0.00%|        assert v_proj_weight is not None, "use_separate_proj_weight is True but v_proj_weight is None"
  4974|         0|            0|            0|  0.00%|        if in_proj_bias is None:
  4975|         0|            0|            0|  0.00%|            b_q = b_k = b_v = None
  4976|         0|            0|            0|  0.00%|        else:
  4977|         0|            0|            0|  0.00%|            b_q, b_k, b_v = in_proj_bias.chunk(3)
  4978|         0|            0|            0|  0.00%|        q, k, v = _in_projection(query, key, value, q_proj_weight, k_proj_weight, v_proj_weight, b_q, b_k, b_v)
  4979|         0|            0|            0|  0.00%|
  4980|         0|            0|            0|  0.00%|    # prep attention mask
  4981|         0|            0|            0|  0.00%|    if attn_mask is not None:
  4982|         0|            0|            0|  0.00%|        if attn_mask.dtype == torch.uint8:
  4983|         0|            0|            0|  0.00%|            warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
  4984|         0|            0|            0|  0.00%|            attn_mask = attn_mask.to(torch.bool)
  4985|         0|            0|            0|  0.00%|        else:
  4986|         0|            0|            0|  0.00%|            assert attn_mask.is_floating_point() or attn_mask.dtype == torch.bool, \
  4987|         0|            0|            0|  0.00%|                f"Only float, byte, and bool types are supported for attn_mask, not {attn_mask.dtype}"
  4988|         0|            0|            0|  0.00%|        # ensure attn_mask's dim is 3
  4989|         0|            0|            0|  0.00%|        if attn_mask.dim() == 2:
  4990|         0|            0|            0|  0.00%|            correct_2d_size = (tgt_len, src_len)
  4991|         0|            0|            0|  0.00%|            if attn_mask.shape != correct_2d_size:
  4992|         0|            0|            0|  0.00%|                raise RuntimeError(f"The shape of the 2D attn_mask is {attn_mask.shape}, but should be {correct_2d_size}.")
  4993|         0|            0|            0|  0.00%|            attn_mask = attn_mask.unsqueeze(0)
  4994|         0|            0|            0|  0.00%|        elif attn_mask.dim() == 3:
  4995|         0|            0|            0|  0.00%|            correct_3d_size = (bsz * num_heads, tgt_len, src_len)
  4996|         0|            0|            0|  0.00%|            if attn_mask.shape != correct_3d_size:
  4997|         0|            0|            0|  0.00%|                raise RuntimeError(f"The shape of the 3D attn_mask is {attn_mask.shape}, but should be {correct_3d_size}.")
  4998|         0|            0|            0|  0.00%|        else:
  4999|         0|            0|            0|  0.00%|            raise RuntimeError(f"attn_mask's dimension {attn_mask.dim()} is not supported")
  5000|         0|            0|            0|  0.00%|
  5001|         0|            0|            0|  0.00%|    # prep key padding mask
  5002|         0|            0|            0|  0.00%|    if key_padding_mask is not None and key_padding_mask.dtype == torch.uint8:
  5003|         0|            0|            0|  0.00%|        warnings.warn("Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
  5004|         0|            0|            0|  0.00%|        key_padding_mask = key_padding_mask.to(torch.bool)
  5005|         0|            0|            0|  0.00%|
  5006|         0|            0|            0|  0.00%|    # add bias along batch dimension (currently second)
  5007|         0|            0|            0|  0.00%|    if bias_k is not None and bias_v is not None:
  5008|         0|            0|            0|  0.00%|        assert static_k is None, "bias cannot be added to static key."
  5009|         0|            0|            0|  0.00%|        assert static_v is None, "bias cannot be added to static value."
  5010|         0|            0|            0|  0.00%|        k = torch.cat([k, bias_k.repeat(1, bsz, 1)])
  5011|         0|            0|            0|  0.00%|        v = torch.cat([v, bias_v.repeat(1, bsz, 1)])
  5012|         0|            0|            0|  0.00%|        if attn_mask is not None:
  5013|         0|            0|            0|  0.00%|            attn_mask = pad(attn_mask, (0, 1))
  5014|         0|            0|            0|  0.00%|        if key_padding_mask is not None:
  5015|         0|            0|            0|  0.00%|            key_padding_mask = pad(key_padding_mask, (0, 1))
  5016|         0|            0|            0|  0.00%|    else:
  5017|         0|            0|            0|  0.00%|        assert bias_k is None
  5018|         0|            0|            0|  0.00%|        assert bias_v is None
  5019|         0|            0|            0|  0.00%|
  5020|         0|            0|            0|  0.00%|    #
  5021|         0|            0|            0|  0.00%|    # reshape q, k, v for multihead attention and make em batch first
  5022|         0|            0|            0|  0.00%|    #
  5023|         0|            0|            0|  0.00%|    q = q.contiguous().view(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)
  5024|         0|            0|            0|  0.00%|    if static_k is None:
  5025|         0|            0|            0|  0.00%|        k = k.contiguous().view(-1, bsz * num_heads, head_dim).transpose(0, 1)
  5026|         0|            0|            0|  0.00%|    else:
  5027|         0|            0|            0|  0.00%|        # TODO finish disentangling control flow so we don't do in-projections when statics are passed
  5028|         0|            0|            0|  0.00%|        assert static_k.size(0) == bsz * num_heads, \
  5029|         0|            0|            0|  0.00%|            f"expecting static_k.size(0) of {bsz * num_heads}, but got {static_k.size(0)}"
  5030|         0|            0|            0|  0.00%|        assert static_k.size(2) == head_dim, \
  5031|         0|            0|            0|  0.00%|            f"expecting static_k.size(2) of {head_dim}, but got {static_k.size(2)}"
  5032|         0|            0|            0|  0.00%|        k = static_k
  5033|         0|            0|            0|  0.00%|    if static_v is None:
  5034|         0|            0|            0|  0.00%|        v = v.contiguous().view(-1, bsz * num_heads, head_dim).transpose(0, 1)
  5035|         0|            0|            0|  0.00%|    else:
  5036|         0|            0|            0|  0.00%|        # TODO finish disentangling control flow so we don't do in-projections when statics are passed
  5037|         0|            0|            0|  0.00%|        assert static_v.size(0) == bsz * num_heads, \
  5038|         0|            0|            0|  0.00%|            f"expecting static_v.size(0) of {bsz * num_heads}, but got {static_v.size(0)}"
  5039|         0|            0|            0|  0.00%|        assert static_v.size(2) == head_dim, \
  5040|         0|            0|            0|  0.00%|            f"expecting static_v.size(2) of {head_dim}, but got {static_v.size(2)}"
  5041|         0|            0|            0|  0.00%|        v = static_v
  5042|         0|            0|            0|  0.00%|
  5043|         0|            0|            0|  0.00%|    # add zero attention along batch dimension (now first)
  5044|         0|            0|            0|  0.00%|    if add_zero_attn:
  5045|         0|            0|            0|  0.00%|        zero_attn_shape = (bsz * num_heads, 1, head_dim)
  5046|         0|            0|            0|  0.00%|        k = torch.cat([k, torch.zeros(zero_attn_shape, dtype=k.dtype, device=k.device)], dim=1)
  5047|         0|            0|            0|  0.00%|        v = torch.cat([v, torch.zeros(zero_attn_shape, dtype=v.dtype, device=v.device)], dim=1)
  5048|         0|            0|            0|  0.00%|        if attn_mask is not None:
  5049|         0|            0|            0|  0.00%|            attn_mask = pad(attn_mask, (0, 1))
  5050|         0|            0|            0|  0.00%|        if key_padding_mask is not None:
  5051|         0|            0|            0|  0.00%|            key_padding_mask = pad(key_padding_mask, (0, 1))
  5052|         0|            0|            0|  0.00%|
  5053|         0|            0|            0|  0.00%|    # update source sequence length after adjustments
  5054|         0|            0|            0|  0.00%|    src_len = k.size(1)
  5055|         0|            0|            0|  0.00%|
  5056|         0|            0|            0|  0.00%|    # merge key padding and attention masks
  5057|         0|            0|            0|  0.00%|    if key_padding_mask is not None:
  5058|         0|            0|            0|  0.00%|        assert key_padding_mask.shape == (bsz, src_len), \
  5059|         0|            0|            0|  0.00%|            f"expecting key_padding_mask shape of {(bsz, src_len)}, but got {key_padding_mask.shape}"
  5060|         0|            0|            0|  0.00%|        key_padding_mask = key_padding_mask.view(bsz, 1, 1, src_len).   \
  5061|         0|            0|            0|  0.00%|            expand(-1, num_heads, -1, -1).reshape(bsz * num_heads, 1, src_len)
  5062|         0|            0|            0|  0.00%|        if attn_mask is None:
  5063|         0|            0|            0|  0.00%|            attn_mask = key_padding_mask
  5064|         0|            0|            0|  0.00%|        elif attn_mask.dtype == torch.bool:
  5065|         0|            0|            0|  0.00%|            attn_mask = attn_mask.logical_or(key_padding_mask)
  5066|         0|            0|            0|  0.00%|        else:
  5067|         0|            0|            0|  0.00%|            attn_mask = attn_mask.masked_fill(key_padding_mask, float("-inf"))
  5068|         0|            0|            0|  0.00%|
  5069|         0|            0|            0|  0.00%|    # convert mask to float
  5070|         0|            0|            0|  0.00%|    if attn_mask is not None and attn_mask.dtype == torch.bool:
  5071|         0|            0|            0|  0.00%|        new_attn_mask = torch.zeros_like(attn_mask, dtype=torch.float)
  5072|         0|            0|            0|  0.00%|        new_attn_mask.masked_fill_(attn_mask, float("-inf"))
  5073|         0|            0|            0|  0.00%|        attn_mask = new_attn_mask
  5074|         0|            0|            0|  0.00%|
  5075|         0|            0|            0|  0.00%|    # adjust dropout probability
  5076|         0|            0|            0|  0.00%|    if not training:
  5077|         0|            0|            0|  0.00%|        dropout_p = 0.0
  5078|         0|            0|            0|  0.00%|
  5079|         0|            0|            0|  0.00%|    #
  5080|         0|            0|            0|  0.00%|    # (deep breath) calculate attention and out projection
  5081|         0|            0|            0|  0.00%|    #
  5082|         0|            0|            0|  0.00%|    attn_output, attn_output_weights = _scaled_dot_product_attention(q, k, v, attn_mask, dropout_p)
  5083|         0|            0|            0|  0.00%|    attn_output = attn_output.transpose(0, 1).contiguous().view(tgt_len, bsz, embed_dim)
  5084|         0|            0|            0|  0.00%|    attn_output = linear(attn_output, out_proj_weight, out_proj_bias)
  5085|         0|            0|            0|  0.00%|
  5086|         0|            0|            0|  0.00%|    if need_weights:
  5087|         0|            0|            0|  0.00%|        # average attention weights over heads
  5088|         0|            0|            0|  0.00%|        attn_output_weights = attn_output_weights.view(bsz, num_heads, tgt_len, src_len)
  5089|         0|            0|            0|  0.00%|        return attn_output, attn_output_weights.sum(dim=1) / num_heads
  5090|         0|            0|            0|  0.00%|    else:
  5091|         0|            0|            0|  0.00%|        return attn_output, None
File: /opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py
File duration: 1.60255s (2.99%)
Line #|      Hits|         Time| Time per hit|      %|Source code
------+----------+-------------+-------------+-------+-----------
     1|         0|            0|            0|  0.00%|from collections import OrderedDict, namedtuple
     2|         0|            0|            0|  0.00%|import itertools
     3|         0|            0|            0|  0.00%|import warnings
     4|         0|            0|            0|  0.00%|import functools
     5|         0|            0|            0|  0.00%|
     6|         0|            0|            0|  0.00%|import torch
     7|         0|            0|            0|  0.00%|from ..parameter import Parameter
     8|         0|            0|            0|  0.00%|import torch.utils.hooks as hooks
     9|         0|            0|            0|  0.00%|
    10|         0|            0|            0|  0.00%|from torch import Tensor, device, dtype
    11|         0|            0|            0|  0.00%|from typing import Union, Tuple, Any, Callable, Iterator, Set, Optional, overload, TypeVar, Mapping, Dict, List
    12|         0|            0|            0|  0.00%|from ...utils.hooks import RemovableHandle
    13|         0|            0|            0|  0.00%|
    14|         0|            0|            0|  0.00%|_grad_t = Union[Tuple[Tensor, ...], Tensor]
    15|         0|            0|            0|  0.00%|# See https://mypy.readthedocs.io/en/latest/generics.html#generic-methods-and-generic-self for the use
    16|         0|            0|            0|  0.00%|# of `T` to annotate `self`. Many methods of `Module` return `self` and we want those return values to be
    17|         0|            0|            0|  0.00%|# the type of the subclass, not the looser type of `Module`.
    18|         0|            0|            0|  0.00%|T = TypeVar('T', bound='Module')
    19|         0|            0|            0|  0.00%|
    20|         0|            0|            0|  0.00%|class _IncompatibleKeys(namedtuple('IncompatibleKeys', ['missing_keys', 'unexpected_keys'])):
    21|         0|            0|            0|  0.00%|    def __repr__(self):
    22|         0|            0|            0|  0.00%|        if not self.missing_keys and not self.unexpected_keys:
    23|         0|            0|            0|  0.00%|            return '<All keys matched successfully>'
    24|         0|            0|            0|  0.00%|        return super(_IncompatibleKeys, self).__repr__()
    25|         0|            0|            0|  0.00%|
    26|         0|            0|            0|  0.00%|    __str__ = __repr__
    27|         0|            0|            0|  0.00%|
    28|         0|            0|            0|  0.00%|
    29|         0|            0|            0|  0.00%|def _addindent(s_, numSpaces):
    30|         0|            0|            0|  0.00%|    s = s_.split('\n')
    31|         0|            0|            0|  0.00%|    # don't do anything for single-line stuff
    32|         0|            0|            0|  0.00%|    if len(s) == 1:
    33|         0|            0|            0|  0.00%|        return s_
    34|         0|            0|            0|  0.00%|    first = s.pop(0)
    35|         0|            0|            0|  0.00%|    s = [(numSpaces * ' ') + line for line in s]
    36|         0|            0|            0|  0.00%|    s = '\n'.join(s)
    37|         0|            0|            0|  0.00%|    s = first + '\n' + s
    38|         0|            0|            0|  0.00%|    return s
    39|         0|            0|            0|  0.00%|
    40|         0|            0|            0|  0.00%|
    41|         0|            0|            0|  0.00%|r"""This tracks hooks common to all modules that are executed before/after
    42|         0|            0|            0|  0.00%|calling forward and backward. This is global state used for debugging/profiling
    43|         0|            0|            0|  0.00%|purposes"""
    44|         0|            0|            0|  0.00%|_global_backward_hooks: Dict[int, Callable] = OrderedDict()
    45|         0|            0|            0|  0.00%|_global_is_full_backward_hook: Optional[bool] = None
    46|         0|            0|            0|  0.00%|_global_forward_pre_hooks: Dict[int, Callable] = OrderedDict()
    47|         0|            0|            0|  0.00%|_global_forward_hooks: Dict[int, Callable] = OrderedDict()
    48|         0|            0|            0|  0.00%|
    49|         0|            0|            0|  0.00%|
    50|         0|            0|            0|  0.00%|def register_module_forward_pre_hook(hook: Callable[..., None]) -> RemovableHandle:
    51|         0|            0|            0|  0.00%|    r"""Registers a forward pre-hook common to all modules.
    52|         0|            0|            0|  0.00%|
    53|         0|            0|            0|  0.00%|    .. warning ::
    54|         0|            0|            0|  0.00%|
    55|         0|            0|            0|  0.00%|        This adds global state to the `nn.module` module
    56|         0|            0|            0|  0.00%|        and it is only intended for debugging/profiling purposes.
    57|         0|            0|            0|  0.00%|
    58|         0|            0|            0|  0.00%|    The hook will be called every time before :func:`forward` is invoked.
    59|         0|            0|            0|  0.00%|    It should have the following signature::
    60|         0|            0|            0|  0.00%|
    61|         0|            0|            0|  0.00%|        hook(module, input) -> None or modified input
    62|         0|            0|            0|  0.00%|
    63|         0|            0|            0|  0.00%|    The input contains only the positional arguments given to the module.
    64|         0|            0|            0|  0.00%|    Keyword arguments won't be passed to the hooks and only to the ``forward``.
    65|         0|            0|            0|  0.00%|    The hook can modify the input. User can either return a tuple or a
    66|         0|            0|            0|  0.00%|    single modified value in the hook. We will wrap the value into a tuple
    67|         0|            0|            0|  0.00%|    if a single value is returned(unless that value is already a tuple).
    68|         0|            0|            0|  0.00%|
    69|         0|            0|            0|  0.00%|    This hook has precedence over the specific module hooks registered with
    70|         0|            0|            0|  0.00%|    ``register_forward_pre_hook``.
    71|         0|            0|            0|  0.00%|
    72|         0|            0|            0|  0.00%|    Returns:
    73|         0|            0|            0|  0.00%|        :class:`torch.utils.hooks.RemovableHandle`:
    74|         0|            0|            0|  0.00%|            a handle that can be used to remove the added hook by calling
    75|         0|            0|            0|  0.00%|            ``handle.remove()``
    76|         0|            0|            0|  0.00%|    """
    77|         0|            0|            0|  0.00%|    handle = hooks.RemovableHandle(_global_forward_pre_hooks)
    78|         0|            0|            0|  0.00%|    _global_forward_pre_hooks[handle.id] = hook
    79|         0|            0|            0|  0.00%|    return handle
    80|         0|            0|            0|  0.00%|
    81|         0|            0|            0|  0.00%|
    82|         0|            0|            0|  0.00%|def register_module_forward_hook(hook: Callable[..., None]) -> RemovableHandle:
    83|         0|            0|            0|  0.00%|    r"""Registers a global forward hook for all the modules
    84|         0|            0|            0|  0.00%|
    85|         0|            0|            0|  0.00%|    .. warning ::
    86|         0|            0|            0|  0.00%|
    87|         0|            0|            0|  0.00%|        This adds global state to the `nn.module` module
    88|         0|            0|            0|  0.00%|        and it is only intended for debugging/profiling purposes.
    89|         0|            0|            0|  0.00%|
    90|         0|            0|            0|  0.00%|    The hook will be called every time after :func:`forward` has computed an output.
    91|         0|            0|            0|  0.00%|    It should have the following signature::
    92|         0|            0|            0|  0.00%|
    93|         0|            0|            0|  0.00%|        hook(module, input, output) -> None or modified output
    94|         0|            0|            0|  0.00%|
    95|         0|            0|            0|  0.00%|    The input contains only the positional arguments given to the module.
    96|         0|            0|            0|  0.00%|    Keyword arguments won't be passed to the hooks and only to the ``forward``.
    97|         0|            0|            0|  0.00%|    The hook can modify the output. It can modify the input inplace but
    98|         0|            0|            0|  0.00%|    it will not have effect on forward since this is called after
    99|         0|            0|            0|  0.00%|    :func:`forward` is called.
   100|         0|            0|            0|  0.00%|
   101|         0|            0|            0|  0.00%|    Returns:
   102|         0|            0|            0|  0.00%|        :class:`torch.utils.hooks.RemovableHandle`:
   103|         0|            0|            0|  0.00%|            a handle that can be used to remove the added hook by calling
   104|         0|            0|            0|  0.00%|            ``handle.remove()``
   105|         0|            0|            0|  0.00%|
   106|         0|            0|            0|  0.00%|    This hook will be executed before specific module hooks registered with
   107|         0|            0|            0|  0.00%|    ``register_forward_hook``.
   108|         0|            0|            0|  0.00%|    """
   109|         0|            0|            0|  0.00%|    handle = hooks.RemovableHandle(_global_forward_hooks)
   110|         0|            0|            0|  0.00%|    _global_forward_hooks[handle.id] = hook
   111|         0|            0|            0|  0.00%|    return handle
   112|         0|            0|            0|  0.00%|
   113|         0|            0|            0|  0.00%|def register_module_backward_hook(
   114|         0|            0|            0|  0.00%|    hook: Callable[['Module', _grad_t, _grad_t], Union[None, Tensor]]
   115|         0|            0|            0|  0.00%|) -> RemovableHandle:
   116|         0|            0|            0|  0.00%|    r"""Registers a backward hook common to all the modules.
   117|         0|            0|            0|  0.00%|
   118|         0|            0|            0|  0.00%|    This function is deprecated in favor of :meth:`nn.module.register_module_full_backward_hook`
   119|         0|            0|            0|  0.00%|    and the behavior of this function will change in future versions.
   120|         0|            0|            0|  0.00%|
   121|         0|            0|            0|  0.00%|    Returns:
   122|         0|            0|            0|  0.00%|        :class:`torch.utils.hooks.RemovableHandle`:
   123|         0|            0|            0|  0.00%|            a handle that can be used to remove the added hook by calling
   124|         0|            0|            0|  0.00%|            ``handle.remove()``
   125|         0|            0|            0|  0.00%|
   126|         0|            0|            0|  0.00%|    """
   127|         0|            0|            0|  0.00%|    global _global_is_full_backward_hook
   128|         0|            0|            0|  0.00%|    if _global_is_full_backward_hook is True:
   129|         0|            0|            0|  0.00%|        raise RuntimeError("Cannot use both regular backward hooks and full backward hooks as a "
   130|         0|            0|            0|  0.00%|                           "global Module hook. Please use only one of them.")
   131|         0|            0|            0|  0.00%|
   132|         0|            0|            0|  0.00%|    _global_is_full_backward_hook = False
   133|         0|            0|            0|  0.00%|
   134|         0|            0|            0|  0.00%|    handle = hooks.RemovableHandle(_global_backward_hooks)
   135|         0|            0|            0|  0.00%|    _global_backward_hooks[handle.id] = hook
   136|         0|            0|            0|  0.00%|    return handle
   137|         0|            0|            0|  0.00%|
   138|         0|            0|            0|  0.00%|def register_module_full_backward_hook(
   139|         0|            0|            0|  0.00%|    hook: Callable[['Module', _grad_t, _grad_t], Union[None, Tensor]]
   140|         0|            0|            0|  0.00%|) -> RemovableHandle:
   141|         0|            0|            0|  0.00%|    r"""Registers a backward hook common to all the modules.
   142|         0|            0|            0|  0.00%|
   143|         0|            0|            0|  0.00%|    .. warning ::
   144|         0|            0|            0|  0.00%|        This adds global state to the `nn.module` module
   145|         0|            0|            0|  0.00%|        and it is only intended for debugging/profiling purposes.
   146|         0|            0|            0|  0.00%|
   147|         0|            0|            0|  0.00%|        The current implementation will not have the presented behavior
   148|         0|            0|            0|  0.00%|        for complex :class:`Module` that perform many operations.
   149|         0|            0|            0|  0.00%|        In some failure cases, :attr:`grad_input` and :attr:`grad_output` will only
   150|         0|            0|            0|  0.00%|        contain the gradients for a subset of the inputs and outputs.
   151|         0|            0|            0|  0.00%|        For such :class:`Module`, you should use :func:`torch.Tensor.register_hook`
   152|         0|            0|            0|  0.00%|        directly on a specific input or output to get the required gradients.
   153|         0|            0|            0|  0.00%|
   154|         0|            0|            0|  0.00%|    The hook will be called every time the gradients with respect to module
   155|         0|            0|            0|  0.00%|    inputs are computed. The hook should have the following signature::
   156|         0|            0|            0|  0.00%|
   157|         0|            0|            0|  0.00%|        hook(module, grad_input, grad_output) -> Tensor or None
   158|         0|            0|            0|  0.00%|
   159|         0|            0|            0|  0.00%|    The :attr:`grad_input` and :attr:`grad_output` are tuples. The hook should
   160|         0|            0|            0|  0.00%|    not modify its arguments, but it can optionally return a new gradient with
   161|         0|            0|            0|  0.00%|    respect to the input that will be used in place of :attr:`grad_input` in
   162|         0|            0|            0|  0.00%|    subsequent computations. :attr:`grad_input` will only correspond to the inputs given
   163|         0|            0|            0|  0.00%|    as positional arguments and all kwarg arguments will not appear in the hook. Entries
   164|         0|            0|            0|  0.00%|    in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor
   165|         0|            0|            0|  0.00%|    arguments.
   166|         0|            0|            0|  0.00%|
   167|         0|            0|            0|  0.00%|    Global hooks are called before hooks registered with `register_backward_hook`
   168|         0|            0|            0|  0.00%|
   169|         0|            0|            0|  0.00%|    Returns:
   170|         0|            0|            0|  0.00%|        :class:`torch.utils.hooks.RemovableHandle`:
   171|         0|            0|            0|  0.00%|            a handle that can be used to remove the added hook by calling
   172|         0|            0|            0|  0.00%|            ``handle.remove()``
   173|         0|            0|            0|  0.00%|
   174|         0|            0|            0|  0.00%|    """
   175|         0|            0|            0|  0.00%|    global _global_is_full_backward_hook
   176|         0|            0|            0|  0.00%|    if _global_is_full_backward_hook is False:
   177|         0|            0|            0|  0.00%|        raise RuntimeError("Cannot use both regular backward hooks and full backward hooks as a "
   178|         0|            0|            0|  0.00%|                           "global Module hook. Please use only one of them.")
   179|         0|            0|            0|  0.00%|
   180|         0|            0|            0|  0.00%|    _global_is_full_backward_hook = True
   181|         0|            0|            0|  0.00%|
   182|         0|            0|            0|  0.00%|    handle = hooks.RemovableHandle(_global_backward_hooks)
   183|         0|            0|            0|  0.00%|    _global_backward_hooks[handle.id] = hook
   184|         0|            0|            0|  0.00%|    return handle
   185|         0|            0|            0|  0.00%|
   186|         0|            0|            0|  0.00%|
   187|         0|            0|            0|  0.00%|# Trick mypy into not applying contravariance rules to inputs by defining
   188|         0|            0|            0|  0.00%|# forward as a value, rather than a function.  See also
   189|         0|            0|            0|  0.00%|# https://github.com/python/mypy/issues/8795
   190|         0|            0|            0|  0.00%|def _forward_unimplemented(self, *input: Any) -> None:
   191|         0|            0|            0|  0.00%|    r"""Defines the computation performed at every call.
   192|         0|            0|            0|  0.00%|
   193|         0|            0|            0|  0.00%|    Should be overridden by all subclasses.
   194|         0|            0|            0|  0.00%|
   195|         0|            0|            0|  0.00%|    .. note::
   196|         0|            0|            0|  0.00%|        Although the recipe for forward pass needs to be defined within
   197|         0|            0|            0|  0.00%|        this function, one should call the :class:`Module` instance afterwards
   198|         0|            0|            0|  0.00%|        instead of this since the former takes care of running the
   199|         0|            0|            0|  0.00%|        registered hooks while the latter silently ignores them.
   200|         0|            0|            0|  0.00%|    """
   201|         0|            0|            0|  0.00%|    raise NotImplementedError
   202|         0|            0|            0|  0.00%|
   203|         0|            0|            0|  0.00%|
   204|         0|            0|            0|  0.00%|class Module:
   205|         0|            0|            0|  0.00%|    r"""Base class for all neural network modules.
   206|         0|            0|            0|  0.00%|
   207|         0|            0|            0|  0.00%|    Your models should also subclass this class.
   208|         0|            0|            0|  0.00%|
   209|         0|            0|            0|  0.00%|    Modules can also contain other Modules, allowing to nest them in
   210|         0|            0|            0|  0.00%|    a tree structure. You can assign the submodules as regular attributes::
   211|         0|            0|            0|  0.00%|
   212|         0|            0|            0|  0.00%|        import torch.nn as nn
   213|         0|            0|            0|  0.00%|        import torch.nn.functional as F
   214|         0|            0|            0|  0.00%|
   215|         0|            0|            0|  0.00%|        class Model(nn.Module):
   216|         0|            0|            0|  0.00%|            def __init__(self):
   217|         0|            0|            0|  0.00%|                super(Model, self).__init__()
   218|         0|            0|            0|  0.00%|                self.conv1 = nn.Conv2d(1, 20, 5)
   219|         0|            0|            0|  0.00%|                self.conv2 = nn.Conv2d(20, 20, 5)
   220|         0|            0|            0|  0.00%|
   221|         0|            0|            0|  0.00%|            def forward(self, x):
   222|         0|            0|            0|  0.00%|                x = F.relu(self.conv1(x))
   223|         0|            0|            0|  0.00%|                return F.relu(self.conv2(x))
   224|         0|            0|            0|  0.00%|
   225|         0|            0|            0|  0.00%|    Submodules assigned in this way will be registered, and will have their
   226|         0|            0|            0|  0.00%|    parameters converted too when you call :meth:`to`, etc.
   227|         0|            0|            0|  0.00%|
   228|         0|            0|            0|  0.00%|    :ivar training: Boolean represents whether this module is in training or
   229|         0|            0|            0|  0.00%|                    evaluation mode.
   230|         0|            0|            0|  0.00%|    :vartype training: bool
   231|         0|            0|            0|  0.00%|    """
   232|         0|            0|            0|  0.00%|
   233|         0|            0|            0|  0.00%|    dump_patches: bool = False
   234|         0|            0|            0|  0.00%|
   235|         0|            0|            0|  0.00%|    r"""This allows better BC support for :meth:`load_state_dict`. In
   236|         0|            0|            0|  0.00%|    :meth:`state_dict`, the version number will be saved as in the attribute
   237|         0|            0|            0|  0.00%|    `_metadata` of the returned state dict, and thus pickled. `_metadata` is a
   238|         0|            0|            0|  0.00%|    dictionary with keys that follow the naming convention of state dict. See
   239|         0|            0|            0|  0.00%|    ``_load_from_state_dict`` on how to use this information in loading.
   240|         0|            0|            0|  0.00%|
   241|         0|            0|            0|  0.00%|    If new parameters/buffers are added/removed from a module, this number shall
   242|         0|            0|            0|  0.00%|    be bumped, and the module's `_load_from_state_dict` method can compare the
   243|         0|            0|            0|  0.00%|    version number and do appropriate changes if the state dict is from before
   244|         0|            0|            0|  0.00%|    the change."""
   245|         0|            0|            0|  0.00%|    _version: int = 1
   246|         0|            0|            0|  0.00%|
   247|         0|            0|            0|  0.00%|    training: bool
   248|         0|            0|            0|  0.00%|    _is_full_backward_hook: Optional[bool]
   249|         0|            0|            0|  0.00%|
   250|         0|            0|            0|  0.00%|    def __init__(self):
   251|         0|            0|            0|  0.00%|        """
   252|         0|            0|            0|  0.00%|        Initializes internal Module state, shared by both nn.Module and ScriptModule.
   253|         0|            0|            0|  0.00%|        """
   254|         0|            0|            0|  0.00%|        torch._C._log_api_usage_once("python.nn_module")
   255|         0|            0|            0|  0.00%|
   256|         0|            0|            0|  0.00%|        self.training = True
   257|         0|            0|            0|  0.00%|        self._parameters = OrderedDict()
   258|         0|            0|            0|  0.00%|        self._buffers = OrderedDict()
   259|         0|            0|            0|  0.00%|        self._non_persistent_buffers_set = set()
   260|         0|            0|            0|  0.00%|        self._backward_hooks = OrderedDict()
   261|         0|            0|            0|  0.00%|        self._is_full_backward_hook = None
   262|         0|            0|            0|  0.00%|        self._forward_hooks = OrderedDict()
   263|         0|            0|            0|  0.00%|        self._forward_pre_hooks = OrderedDict()
   264|         0|            0|            0|  0.00%|        self._state_dict_hooks = OrderedDict()
   265|         0|            0|            0|  0.00%|        self._load_state_dict_pre_hooks = OrderedDict()
   266|         0|            0|            0|  0.00%|        self._modules = OrderedDict()
   267|         0|            0|            0|  0.00%|
   268|         0|            0|            0|  0.00%|    forward: Callable[..., Any] = _forward_unimplemented
   269|         0|            0|            0|  0.00%|
   270|         0|            0|            0|  0.00%|    def register_buffer(self, name: str, tensor: Optional[Tensor], persistent: bool = True) -> None:
   271|         0|            0|            0|  0.00%|        r"""Adds a buffer to the module.
   272|         0|            0|            0|  0.00%|
   273|         0|            0|            0|  0.00%|        This is typically used to register a buffer that should not to be
   274|         0|            0|            0|  0.00%|        considered a model parameter. For example, BatchNorm's ``running_mean``
   275|         0|            0|            0|  0.00%|        is not a parameter, but is part of the module's state. Buffers, by
   276|         0|            0|            0|  0.00%|        default, are persistent and will be saved alongside parameters. This
   277|         0|            0|            0|  0.00%|        behavior can be changed by setting :attr:`persistent` to ``False``. The
   278|         0|            0|            0|  0.00%|        only difference between a persistent buffer and a non-persistent buffer
   279|         0|            0|            0|  0.00%|        is that the latter will not be a part of this module's
   280|         0|            0|            0|  0.00%|        :attr:`state_dict`.
   281|         0|            0|            0|  0.00%|
   282|         0|            0|            0|  0.00%|        Buffers can be accessed as attributes using given names.
   283|         0|            0|            0|  0.00%|
   284|         0|            0|            0|  0.00%|        Args:
   285|         0|            0|            0|  0.00%|            name (string): name of the buffer. The buffer can be accessed
   286|         0|            0|            0|  0.00%|                from this module using the given name
   287|         0|            0|            0|  0.00%|            tensor (Tensor): buffer to be registered.
   288|         0|            0|            0|  0.00%|            persistent (bool): whether the buffer is part of this module's
   289|         0|            0|            0|  0.00%|                :attr:`state_dict`.
   290|         0|            0|            0|  0.00%|
   291|         0|            0|            0|  0.00%|        Example::
   292|         0|            0|            0|  0.00%|
   293|         0|            0|            0|  0.00%|            >>> self.register_buffer('running_mean', torch.zeros(num_features))
   294|         0|            0|            0|  0.00%|
   295|         0|            0|            0|  0.00%|        """
   296|         0|            0|            0|  0.00%|        if persistent is False and isinstance(self, torch.jit.ScriptModule):
   297|         0|            0|            0|  0.00%|            raise RuntimeError("ScriptModule does not support non-persistent buffers")
   298|         0|            0|            0|  0.00%|
   299|         0|            0|            0|  0.00%|        if '_buffers' not in self.__dict__:
   300|         0|            0|            0|  0.00%|            raise AttributeError(
   301|         0|            0|            0|  0.00%|                "cannot assign buffer before Module.__init__() call")
   302|         0|            0|            0|  0.00%|        elif not isinstance(name, torch._six.string_classes):
   303|         0|            0|            0|  0.00%|            raise TypeError("buffer name should be a string. "
   304|         0|            0|            0|  0.00%|                            "Got {}".format(torch.typename(name)))
   305|         0|            0|            0|  0.00%|        elif '.' in name:
   306|         0|            0|            0|  0.00%|            raise KeyError("buffer name can't contain \".\"")
   307|         0|            0|            0|  0.00%|        elif name == '':
   308|         0|            0|            0|  0.00%|            raise KeyError("buffer name can't be empty string \"\"")
   309|         0|            0|            0|  0.00%|        elif hasattr(self, name) and name not in self._buffers:
   310|         0|            0|            0|  0.00%|            raise KeyError("attribute '{}' already exists".format(name))
   311|         0|            0|            0|  0.00%|        elif tensor is not None and not isinstance(tensor, torch.Tensor):
   312|         0|            0|            0|  0.00%|            raise TypeError("cannot assign '{}' object to buffer '{}' "
   313|         0|            0|            0|  0.00%|                            "(torch Tensor or None required)"
   314|         0|            0|            0|  0.00%|                            .format(torch.typename(tensor), name))
   315|         0|            0|            0|  0.00%|        else:
   316|         0|            0|            0|  0.00%|            self._buffers[name] = tensor
   317|         0|            0|            0|  0.00%|            if persistent:
   318|         0|            0|            0|  0.00%|                self._non_persistent_buffers_set.discard(name)
   319|         0|            0|            0|  0.00%|            else:
   320|         0|            0|            0|  0.00%|                self._non_persistent_buffers_set.add(name)
   321|         0|            0|            0|  0.00%|
   322|         0|            0|            0|  0.00%|    def register_parameter(self, name: str, param: Optional[Parameter]) -> None:
   323|         0|            0|            0|  0.00%|        r"""Adds a parameter to the module.
   324|         0|            0|            0|  0.00%|
   325|         0|            0|            0|  0.00%|        The parameter can be accessed as an attribute using given name.
   326|         0|            0|            0|  0.00%|
   327|         0|            0|            0|  0.00%|        Args:
   328|         0|            0|            0|  0.00%|            name (string): name of the parameter. The parameter can be accessed
   329|         0|            0|            0|  0.00%|                from this module using the given name
   330|         0|            0|            0|  0.00%|            param (Parameter): parameter to be added to the module.
   331|         0|            0|            0|  0.00%|        """
   332|         0|            0|            0|  0.00%|        if '_parameters' not in self.__dict__:
   333|         0|            0|            0|  0.00%|            raise AttributeError(
   334|         0|            0|            0|  0.00%|                "cannot assign parameter before Module.__init__() call")
   335|         0|            0|            0|  0.00%|
   336|         0|            0|            0|  0.00%|        elif not isinstance(name, torch._six.string_classes):
   337|         0|            0|            0|  0.00%|            raise TypeError("parameter name should be a string. "
   338|         0|            0|            0|  0.00%|                            "Got {}".format(torch.typename(name)))
   339|         0|            0|            0|  0.00%|        elif '.' in name:
   340|         0|            0|            0|  0.00%|            raise KeyError("parameter name can't contain \".\"")
   341|         0|            0|            0|  0.00%|        elif name == '':
   342|         0|            0|            0|  0.00%|            raise KeyError("parameter name can't be empty string \"\"")
   343|         0|            0|            0|  0.00%|        elif hasattr(self, name) and name not in self._parameters:
   344|         0|            0|            0|  0.00%|            raise KeyError("attribute '{}' already exists".format(name))
   345|         0|            0|            0|  0.00%|
   346|         0|            0|            0|  0.00%|        if param is None:
   347|         0|            0|            0|  0.00%|            self._parameters[name] = None
   348|         0|            0|            0|  0.00%|        elif not isinstance(param, Parameter):
   349|         0|            0|            0|  0.00%|            raise TypeError("cannot assign '{}' object to parameter '{}' "
   350|         0|            0|            0|  0.00%|                            "(torch.nn.Parameter or None required)"
   351|         0|            0|            0|  0.00%|                            .format(torch.typename(param), name))
   352|         0|            0|            0|  0.00%|        elif param.grad_fn:
   353|         0|            0|            0|  0.00%|            raise ValueError(
   354|         0|            0|            0|  0.00%|                "Cannot assign non-leaf Tensor to parameter '{0}'. Model "
   355|         0|            0|            0|  0.00%|                "parameters must be created explicitly. To express '{0}' "
   356|         0|            0|            0|  0.00%|                "as a function of another Tensor, compute the value in "
   357|         0|            0|            0|  0.00%|                "the forward() method.".format(name))
   358|         0|            0|            0|  0.00%|        else:
   359|         0|            0|            0|  0.00%|            self._parameters[name] = param
   360|         0|            0|            0|  0.00%|
   361|         0|            0|            0|  0.00%|    def add_module(self, name: str, module: Optional['Module']) -> None:
   362|         0|            0|            0|  0.00%|        r"""Adds a child module to the current module.
   363|         0|            0|            0|  0.00%|
   364|         0|            0|            0|  0.00%|        The module can be accessed as an attribute using the given name.
   365|         0|            0|            0|  0.00%|
   366|         0|            0|            0|  0.00%|        Args:
   367|         0|            0|            0|  0.00%|            name (string): name of the child module. The child module can be
   368|         0|            0|            0|  0.00%|                accessed from this module using the given name
   369|         0|            0|            0|  0.00%|            module (Module): child module to be added to the module.
   370|         0|            0|            0|  0.00%|        """
   371|         0|            0|            0|  0.00%|        if not isinstance(module, Module) and module is not None:
   372|         0|            0|            0|  0.00%|            raise TypeError("{} is not a Module subclass".format(
   373|         0|            0|            0|  0.00%|                torch.typename(module)))
   374|         0|            0|            0|  0.00%|        elif not isinstance(name, torch._six.string_classes):
   375|         0|            0|            0|  0.00%|            raise TypeError("module name should be a string. Got {}".format(
   376|         0|            0|            0|  0.00%|                torch.typename(name)))
   377|         0|            0|            0|  0.00%|        elif hasattr(self, name) and name not in self._modules:
   378|         0|            0|            0|  0.00%|            raise KeyError("attribute '{}' already exists".format(name))
   379|         0|            0|            0|  0.00%|        elif '.' in name:
   380|         0|            0|            0|  0.00%|            raise KeyError("module name can't contain \".\", got: {}".format(name))
   381|         0|            0|            0|  0.00%|        elif name == '':
   382|         0|            0|            0|  0.00%|            raise KeyError("module name can't be empty string \"\"")
   383|         0|            0|            0|  0.00%|        self._modules[name] = module
   384|         0|            0|            0|  0.00%|
   385|         0|            0|            0|  0.00%|    def get_submodule(self, target: str) -> "Module":
   386|         0|            0|            0|  0.00%|        """
   387|         0|            0|            0|  0.00%|        Returns the submodule given by ``target`` if it exists,
   388|         0|            0|            0|  0.00%|        otherwise throws an error.
   389|         0|            0|            0|  0.00%|
   390|         0|            0|            0|  0.00%|        For example, let's say you have an ``nn.Module`` ``A`` that
   391|         0|            0|            0|  0.00%|        looks like this:
   392|         0|            0|            0|  0.00%|
   393|         0|            0|            0|  0.00%|        .. code-block::text
   394|         0|            0|            0|  0.00%|
   395|         0|            0|            0|  0.00%|            A(
   396|         0|            0|            0|  0.00%|                (net_b): Module(
   397|         0|            0|            0|  0.00%|                    (net_c): Module(
   398|         0|            0|            0|  0.00%|                        (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))
   399|         0|            0|            0|  0.00%|                    )
   400|         0|            0|            0|  0.00%|                    (linear): Linear(in_features=100, out_features=200, bias=True)
   401|         0|            0|            0|  0.00%|                )
   402|         0|            0|            0|  0.00%|            )
   403|         0|            0|            0|  0.00%|
   404|         0|            0|            0|  0.00%|        (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested
   405|         0|            0|            0|  0.00%|        submodule ``net_b``, which itself has two submodules ``net_c``
   406|         0|            0|            0|  0.00%|        and ``linear``. ``net_c`` then has a submodule ``conv``.)
   407|         0|            0|            0|  0.00%|
   408|         0|            0|            0|  0.00%|        To check whether or not we have the ``linear`` submodule, we
   409|         0|            0|            0|  0.00%|        would call ``get_submodule("net_b.linear")``. To check whether
   410|         0|            0|            0|  0.00%|        we have the ``conv`` submodule, we would call
   411|         0|            0|            0|  0.00%|        ``get_submodule("net_b.net_c.conv")``.
   412|         0|            0|            0|  0.00%|
   413|         0|            0|            0|  0.00%|        The runtime of ``get_submodule`` is bounded by the degree
   414|         0|            0|            0|  0.00%|        of module nesting in ``target``. A query against
   415|         0|            0|            0|  0.00%|        ``named_modules`` achieves the same result, but it is O(N) in
   416|         0|            0|            0|  0.00%|        the number of transitive modules. So, for a simple check to see
   417|         0|            0|            0|  0.00%|        if some submodule exists, ``get_submodule`` should always be
   418|         0|            0|            0|  0.00%|        used.
   419|         0|            0|            0|  0.00%|
   420|         0|            0|            0|  0.00%|        Args:
   421|         0|            0|            0|  0.00%|            target: The fully-qualified string name of the submodule
   422|         0|            0|            0|  0.00%|                to look for. (See above example for how to specify a
   423|         0|            0|            0|  0.00%|                fully-qualified string.)
   424|         0|            0|            0|  0.00%|
   425|         0|            0|            0|  0.00%|        Returns:
   426|         0|            0|            0|  0.00%|            torch.nn.Module: The submodule referenced by ``target``
   427|         0|            0|            0|  0.00%|
   428|         0|            0|            0|  0.00%|        Raises:
   429|         0|            0|            0|  0.00%|            AttributeError: If the target string references an invalid
   430|         0|            0|            0|  0.00%|                path or resolves to something that is not an
   431|         0|            0|            0|  0.00%|                ``nn.Module``
   432|         0|            0|            0|  0.00%|        """
   433|         0|            0|            0|  0.00%|        if target == "":
   434|         0|            0|            0|  0.00%|            return self
   435|         0|            0|            0|  0.00%|
   436|         0|            0|            0|  0.00%|        atoms: List[str] = target.split(".")
   437|         0|            0|            0|  0.00%|        mod: torch.nn.Module = self
   438|         0|            0|            0|  0.00%|
   439|         0|            0|            0|  0.00%|        for item in atoms:
   440|         0|            0|            0|  0.00%|
   441|         0|            0|            0|  0.00%|            if not hasattr(mod, item):
   442|         0|            0|            0|  0.00%|                raise AttributeError(mod._get_name() + " has no "
   443|         0|            0|            0|  0.00%|                                     "attribute `" + item + "`")
   444|         0|            0|            0|  0.00%|
   445|         0|            0|            0|  0.00%|            mod = getattr(mod, item)
   446|         0|            0|            0|  0.00%|
   447|         0|            0|            0|  0.00%|            if not isinstance(mod, torch.nn.Module):
   448|         0|            0|            0|  0.00%|                raise AttributeError("`" + item + "` is not "
   449|         0|            0|            0|  0.00%|                                     "an nn.Module")
   450|         0|            0|            0|  0.00%|
   451|         0|            0|            0|  0.00%|        return mod
   452|         0|            0|            0|  0.00%|
   453|         0|            0|            0|  0.00%|    def get_parameter(self, target: str) -> "Parameter":
   454|         0|            0|            0|  0.00%|        """
   455|         0|            0|            0|  0.00%|        Returns the parameter given by ``target`` if it exists,
   456|         0|            0|            0|  0.00%|        otherwise throws an error.
   457|         0|            0|            0|  0.00%|
   458|         0|            0|            0|  0.00%|        See the docstring for ``get_submodule`` for a more detailed
   459|         0|            0|            0|  0.00%|        explanation of this method's functionality as well as how to
   460|         0|            0|            0|  0.00%|        correctly specify ``target``.
   461|         0|            0|            0|  0.00%|
   462|         0|            0|            0|  0.00%|        Args:
   463|         0|            0|            0|  0.00%|            target: The fully-qualified string name of the Parameter
   464|         0|            0|            0|  0.00%|                to look for. (See ``get_submodule`` for how to specify a
   465|         0|            0|            0|  0.00%|                fully-qualified string.)
   466|         0|            0|            0|  0.00%|
   467|         0|            0|            0|  0.00%|        Returns:
   468|         0|            0|            0|  0.00%|            torch.nn.Parameter: The Parameter referenced by ``target``
   469|         0|            0|            0|  0.00%|
   470|         0|            0|            0|  0.00%|        Raises:
   471|         0|            0|            0|  0.00%|            AttributeError: If the target string references an invalid
   472|         0|            0|            0|  0.00%|                path or resolves to something that is not an
   473|         0|            0|            0|  0.00%|                ``nn.Parameter``
   474|         0|            0|            0|  0.00%|        """
   475|         0|            0|            0|  0.00%|        module_path, _, param_name = target.rpartition(".")
   476|         0|            0|            0|  0.00%|
   477|         0|            0|            0|  0.00%|        mod: torch.nn.Module = self.get_submodule(module_path)
   478|         0|            0|            0|  0.00%|
   479|         0|            0|            0|  0.00%|        if not hasattr(mod, param_name):
   480|         0|            0|            0|  0.00%|            raise AttributeError(mod._get_name() + " has no attribute `"
   481|         0|            0|            0|  0.00%|                                 + param_name + "`")
   482|         0|            0|            0|  0.00%|
   483|         0|            0|            0|  0.00%|        param: torch.nn.Parameter = getattr(mod, param_name)
   484|         0|            0|            0|  0.00%|
   485|         0|            0|            0|  0.00%|        if not isinstance(param, torch.nn.Parameter):
   486|         0|            0|            0|  0.00%|            raise AttributeError("`" + param_name + "` is not an "
   487|         0|            0|            0|  0.00%|                                 "nn.Parameter")
   488|         0|            0|            0|  0.00%|
   489|         0|            0|            0|  0.00%|        return param
   490|         0|            0|            0|  0.00%|
   491|         0|            0|            0|  0.00%|    def get_buffer(self, target: str) -> "Tensor":
   492|         0|            0|            0|  0.00%|        """
   493|         0|            0|            0|  0.00%|        Returns the buffer given by ``target`` if it exists,
   494|         0|            0|            0|  0.00%|        otherwise throws an error.
   495|         0|            0|            0|  0.00%|
   496|         0|            0|            0|  0.00%|        See the docstring for ``get_submodule`` for a more detailed
   497|         0|            0|            0|  0.00%|        explanation of this method's functionality as well as how to
   498|         0|            0|            0|  0.00%|        correctly specify ``target``.
   499|         0|            0|            0|  0.00%|
   500|         0|            0|            0|  0.00%|        Args:
   501|         0|            0|            0|  0.00%|            target: The fully-qualified string name of the buffer
   502|         0|            0|            0|  0.00%|                to look for. (See ``get_submodule`` for how to specify a
   503|         0|            0|            0|  0.00%|                fully-qualified string.)
   504|         0|            0|            0|  0.00%|
   505|         0|            0|            0|  0.00%|        Returns:
   506|         0|            0|            0|  0.00%|            torch.Tensor: The buffer referenced by ``target``
   507|         0|            0|            0|  0.00%|
   508|         0|            0|            0|  0.00%|        Raises:
   509|         0|            0|            0|  0.00%|            AttributeError: If the target string references an invalid
   510|         0|            0|            0|  0.00%|                path or resolves to something that is not a
   511|         0|            0|            0|  0.00%|                buffer
   512|         0|            0|            0|  0.00%|        """
   513|         0|            0|            0|  0.00%|        module_path, _, buffer_name = target.rpartition(".")
   514|         0|            0|            0|  0.00%|
   515|         0|            0|            0|  0.00%|        mod: torch.nn.Module = self.get_submodule(module_path)
   516|         0|            0|            0|  0.00%|
   517|         0|            0|            0|  0.00%|        if not hasattr(mod, buffer_name):
   518|         0|            0|            0|  0.00%|            raise AttributeError(mod._get_name() + " has no attribute `"
   519|         0|            0|            0|  0.00%|                                 + buffer_name + "`")
   520|         0|            0|            0|  0.00%|
   521|         0|            0|            0|  0.00%|        buffer: torch.Tensor = getattr(mod, buffer_name)
   522|         0|            0|            0|  0.00%|
   523|         0|            0|            0|  0.00%|        if buffer not in mod._buffers.values():
   524|         0|            0|            0|  0.00%|            raise AttributeError("`" + buffer_name + "` is not a buffer")
   525|         0|            0|            0|  0.00%|
   526|         0|            0|            0|  0.00%|        return buffer
   527|         0|            0|            0|  0.00%|
   528|         0|            0|            0|  0.00%|    def _apply(self, fn):
   529|         0|            0|            0|  0.00%|        for module in self.children():
   530|         0|            0|            0|  0.00%|            module._apply(fn)
   531|         0|            0|            0|  0.00%|
   532|         0|            0|            0|  0.00%|        def compute_should_use_set_data(tensor, tensor_applied):
   533|         0|            0|            0|  0.00%|            if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):
   534|         0|            0|            0|  0.00%|                # If the new tensor has compatible tensor type as the existing tensor,
   535|         0|            0|            0|  0.00%|                # the current behavior is to change the tensor in-place using `.data =`,
   536|         0|            0|            0|  0.00%|                # and the future behavior is to overwrite the existing tensor. However,
   537|         0|            0|            0|  0.00%|                # changing the current behavior is a BC-breaking change, and we want it
   538|         0|            0|            0|  0.00%|                # to happen in future releases. So for now we introduce the
   539|         0|            0|            0|  0.00%|                # `torch.__future__.get_overwrite_module_params_on_conversion()`
   540|         0|            0|            0|  0.00%|                # global flag to let the user control whether they want the future
   541|         0|            0|            0|  0.00%|                # behavior of overwriting the existing tensor or not.
   542|         0|            0|            0|  0.00%|                return not torch.__future__.get_overwrite_module_params_on_conversion()
   543|         0|            0|            0|  0.00%|            else:
   544|         0|            0|            0|  0.00%|                return False
   545|         0|            0|            0|  0.00%|
   546|         0|            0|            0|  0.00%|        for key, param in self._parameters.items():
   547|         0|            0|            0|  0.00%|            if param is not None:
   548|         0|            0|            0|  0.00%|                # Tensors stored in modules are graph leaves, and we don't want to
   549|         0|            0|            0|  0.00%|                # track autograd history of `param_applied`, so we have to use
   550|         0|            0|            0|  0.00%|                # `with torch.no_grad():`
   551|         0|            0|            0|  0.00%|                with torch.no_grad():
   552|         0|            0|            0|  0.00%|                    param_applied = fn(param)
   553|         0|            0|            0|  0.00%|                should_use_set_data = compute_should_use_set_data(param, param_applied)
   554|         0|            0|            0|  0.00%|                if should_use_set_data:
   555|         0|            0|            0|  0.00%|                    param.data = param_applied
   556|         0|            0|            0|  0.00%|                else:
   557|         0|            0|            0|  0.00%|                    assert isinstance(param, Parameter)
   558|         0|            0|            0|  0.00%|                    assert param.is_leaf
   559|         0|            0|            0|  0.00%|                    self._parameters[key] = Parameter(param_applied, param.requires_grad)
   560|         0|            0|            0|  0.00%|
   561|         0|            0|            0|  0.00%|                if param.grad is not None:
   562|         0|            0|            0|  0.00%|                    with torch.no_grad():
   563|         0|            0|            0|  0.00%|                        grad_applied = fn(param.grad)
   564|         0|            0|            0|  0.00%|                    should_use_set_data = compute_should_use_set_data(param.grad, grad_applied)
   565|         0|            0|            0|  0.00%|                    if should_use_set_data:
   566|         0|            0|            0|  0.00%|                        param.grad.data = grad_applied
   567|         0|            0|            0|  0.00%|                    else:
   568|         0|            0|            0|  0.00%|                        assert param.grad.is_leaf
   569|         0|            0|            0|  0.00%|                        self._parameters[key].grad = grad_applied.requires_grad_(param.grad.requires_grad)
   570|         0|            0|            0|  0.00%|
   571|         0|            0|            0|  0.00%|        for key, buf in self._buffers.items():
   572|         0|            0|            0|  0.00%|            if buf is not None:
   573|         0|            0|            0|  0.00%|                self._buffers[key] = fn(buf)
   574|         0|            0|            0|  0.00%|
   575|         0|            0|            0|  0.00%|        return self
   576|         0|            0|            0|  0.00%|
   577|         0|            0|            0|  0.00%|    def apply(self: T, fn: Callable[['Module'], None]) -> T:
   578|         0|            0|            0|  0.00%|        r"""Applies ``fn`` recursively to every submodule (as returned by ``.children()``)
   579|         0|            0|            0|  0.00%|        as well as self. Typical use includes initializing the parameters of a model
   580|         0|            0|            0|  0.00%|        (see also :ref:`nn-init-doc`).
   581|         0|            0|            0|  0.00%|
   582|         0|            0|            0|  0.00%|        Args:
   583|         0|            0|            0|  0.00%|            fn (:class:`Module` -> None): function to be applied to each submodule
   584|         0|            0|            0|  0.00%|
   585|         0|            0|            0|  0.00%|        Returns:
   586|         0|            0|            0|  0.00%|            Module: self
   587|         0|            0|            0|  0.00%|
   588|         0|            0|            0|  0.00%|        Example::
   589|         0|            0|            0|  0.00%|
   590|         0|            0|            0|  0.00%|            >>> @torch.no_grad()
   591|         0|            0|            0|  0.00%|            >>> def init_weights(m):
   592|         0|            0|            0|  0.00%|            >>>     print(m)
   593|         0|            0|            0|  0.00%|            >>>     if type(m) == nn.Linear:
   594|         0|            0|            0|  0.00%|            >>>         m.weight.fill_(1.0)
   595|         0|            0|            0|  0.00%|            >>>         print(m.weight)
   596|         0|            0|            0|  0.00%|            >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))
   597|         0|            0|            0|  0.00%|            >>> net.apply(init_weights)
   598|         0|            0|            0|  0.00%|            Linear(in_features=2, out_features=2, bias=True)
   599|         0|            0|            0|  0.00%|            Parameter containing:
   600|         0|            0|            0|  0.00%|            tensor([[ 1.,  1.],
   601|         0|            0|            0|  0.00%|                    [ 1.,  1.]])
   602|         0|            0|            0|  0.00%|            Linear(in_features=2, out_features=2, bias=True)
   603|         0|            0|            0|  0.00%|            Parameter containing:
   604|         0|            0|            0|  0.00%|            tensor([[ 1.,  1.],
   605|         0|            0|            0|  0.00%|                    [ 1.,  1.]])
   606|         0|            0|            0|  0.00%|            Sequential(
   607|         0|            0|            0|  0.00%|              (0): Linear(in_features=2, out_features=2, bias=True)
   608|         0|            0|            0|  0.00%|              (1): Linear(in_features=2, out_features=2, bias=True)
   609|         0|            0|            0|  0.00%|            )
   610|         0|            0|            0|  0.00%|            Sequential(
   611|         0|            0|            0|  0.00%|              (0): Linear(in_features=2, out_features=2, bias=True)
   612|         0|            0|            0|  0.00%|              (1): Linear(in_features=2, out_features=2, bias=True)
   613|         0|            0|            0|  0.00%|            )
   614|         0|            0|            0|  0.00%|        """
   615|         0|            0|            0|  0.00%|        for module in self.children():
   616|         0|            0|            0|  0.00%|            module.apply(fn)
   617|         0|            0|            0|  0.00%|        fn(self)
   618|         0|            0|            0|  0.00%|        return self
   619|         0|            0|            0|  0.00%|
   620|         0|            0|            0|  0.00%|    def cuda(self: T, device: Optional[Union[int, device]] = None) -> T:
   621|         0|            0|            0|  0.00%|        r"""Moves all model parameters and buffers to the GPU.
   622|         0|            0|            0|  0.00%|
   623|         0|            0|            0|  0.00%|        This also makes associated parameters and buffers different objects. So
   624|         0|            0|            0|  0.00%|        it should be called before constructing optimizer if the module will
   625|         0|            0|            0|  0.00%|        live on GPU while being optimized.
   626|         0|            0|            0|  0.00%|
   627|         0|            0|            0|  0.00%|        .. note::
   628|         0|            0|            0|  0.00%|            This method modifies the module in-place.
   629|         0|            0|            0|  0.00%|
   630|         0|            0|            0|  0.00%|        Args:
   631|         0|            0|            0|  0.00%|            device (int, optional): if specified, all parameters will be
   632|         0|            0|            0|  0.00%|                copied to that device
   633|         0|            0|            0|  0.00%|
   634|         0|            0|            0|  0.00%|        Returns:
   635|         0|            0|            0|  0.00%|            Module: self
   636|         0|            0|            0|  0.00%|        """
   637|         0|            0|            0|  0.00%|        return self._apply(lambda t: t.cuda(device))
   638|         0|            0|            0|  0.00%|
   639|         0|            0|            0|  0.00%|    def xpu(self: T, device: Optional[Union[int, device]] = None) -> T:
   640|         0|            0|            0|  0.00%|        r"""Moves all model parameters and buffers to the XPU.
   641|         0|            0|            0|  0.00%|
   642|         0|            0|            0|  0.00%|        This also makes associated parameters and buffers different objects. So
   643|         0|            0|            0|  0.00%|        it should be called before constructing optimizer if the module will
   644|         0|            0|            0|  0.00%|        live on XPU while being optimized.
   645|         0|            0|            0|  0.00%|
   646|         0|            0|            0|  0.00%|        .. note::
   647|         0|            0|            0|  0.00%|            This method modifies the module in-place.
   648|         0|            0|            0|  0.00%|
   649|         0|            0|            0|  0.00%|        Arguments:
   650|         0|            0|            0|  0.00%|            device (int, optional): if specified, all parameters will be
   651|         0|            0|            0|  0.00%|                copied to that device
   652|         0|            0|            0|  0.00%|
   653|         0|            0|            0|  0.00%|        Returns:
   654|         0|            0|            0|  0.00%|            Module: self
   655|         0|            0|            0|  0.00%|        """
   656|         0|            0|            0|  0.00%|        return self._apply(lambda t: t.xpu(device))
   657|         0|            0|            0|  0.00%|
   658|         0|            0|            0|  0.00%|    def cpu(self: T) -> T:
   659|         0|            0|            0|  0.00%|        r"""Moves all model parameters and buffers to the CPU.
   660|         0|            0|            0|  0.00%|
   661|         0|            0|            0|  0.00%|        .. note::
   662|         0|            0|            0|  0.00%|            This method modifies the module in-place.
   663|         0|            0|            0|  0.00%|
   664|         0|            0|            0|  0.00%|        Returns:
   665|         0|            0|            0|  0.00%|            Module: self
   666|         0|            0|            0|  0.00%|        """
   667|         0|            0|            0|  0.00%|        return self._apply(lambda t: t.cpu())
   668|         0|            0|            0|  0.00%|
   669|         0|            0|            0|  0.00%|    def type(self: T, dst_type: Union[dtype, str]) -> T:
   670|         0|            0|            0|  0.00%|        r"""Casts all parameters and buffers to :attr:`dst_type`.
   671|         0|            0|            0|  0.00%|
   672|         0|            0|            0|  0.00%|        .. note::
   673|         0|            0|            0|  0.00%|            This method modifies the module in-place.
   674|         0|            0|            0|  0.00%|
   675|         0|            0|            0|  0.00%|        Args:
   676|         0|            0|            0|  0.00%|            dst_type (type or string): the desired type
   677|         0|            0|            0|  0.00%|
   678|         0|            0|            0|  0.00%|        Returns:
   679|         0|            0|            0|  0.00%|            Module: self
   680|         0|            0|            0|  0.00%|        """
   681|         0|            0|            0|  0.00%|        return self._apply(lambda t: t.type(dst_type))
   682|         0|            0|            0|  0.00%|
   683|         0|            0|            0|  0.00%|    def float(self: T) -> T:
   684|         0|            0|            0|  0.00%|        r"""Casts all floating point parameters and buffers to ``float`` datatype.
   685|         0|            0|            0|  0.00%|
   686|         0|            0|            0|  0.00%|        .. note::
   687|         0|            0|            0|  0.00%|            This method modifies the module in-place.
   688|         0|            0|            0|  0.00%|
   689|         0|            0|            0|  0.00%|        Returns:
   690|         0|            0|            0|  0.00%|            Module: self
   691|         0|            0|            0|  0.00%|        """
   692|         0|            0|            0|  0.00%|        return self._apply(lambda t: t.float() if t.is_floating_point() else t)
   693|         0|            0|            0|  0.00%|
   694|         0|            0|            0|  0.00%|    def double(self: T) -> T:
   695|         0|            0|            0|  0.00%|        r"""Casts all floating point parameters and buffers to ``double`` datatype.
   696|         0|            0|            0|  0.00%|
   697|         0|            0|            0|  0.00%|        .. note::
   698|         0|            0|            0|  0.00%|            This method modifies the module in-place.
   699|         0|            0|            0|  0.00%|
   700|         0|            0|            0|  0.00%|        Returns:
   701|         0|            0|            0|  0.00%|            Module: self
   702|         0|            0|            0|  0.00%|        """
   703|         0|            0|            0|  0.00%|        return self._apply(lambda t: t.double() if t.is_floating_point() else t)
   704|         0|            0|            0|  0.00%|
   705|         0|            0|            0|  0.00%|    def half(self: T) -> T:
   706|         0|            0|            0|  0.00%|        r"""Casts all floating point parameters and buffers to ``half`` datatype.
   707|         0|            0|            0|  0.00%|
   708|         0|            0|            0|  0.00%|        .. note::
   709|         0|            0|            0|  0.00%|            This method modifies the module in-place.
   710|         0|            0|            0|  0.00%|
   711|         0|            0|            0|  0.00%|        Returns:
   712|         0|            0|            0|  0.00%|            Module: self
   713|         0|            0|            0|  0.00%|        """
   714|         0|            0|            0|  0.00%|        return self._apply(lambda t: t.half() if t.is_floating_point() else t)
   715|         0|            0|            0|  0.00%|
   716|         0|            0|            0|  0.00%|    def bfloat16(self: T) -> T:
   717|         0|            0|            0|  0.00%|        r"""Casts all floating point parameters and buffers to ``bfloat16`` datatype.
   718|         0|            0|            0|  0.00%|
   719|         0|            0|            0|  0.00%|        .. note::
   720|         0|            0|            0|  0.00%|            This method modifies the module in-place.
   721|         0|            0|            0|  0.00%|
   722|         0|            0|            0|  0.00%|        Returns:
   723|         0|            0|            0|  0.00%|            Module: self
   724|         0|            0|            0|  0.00%|        """
   725|         0|            0|            0|  0.00%|        return self._apply(lambda t: t.bfloat16() if t.is_floating_point() else t)
   726|         0|            0|            0|  0.00%|
   727|         0|            0|            0|  0.00%|    def to_empty(self: T, *, device: Union[str, device]) -> T:
   728|         0|            0|            0|  0.00%|        r"""Moves the parameters and buffers to the specified device without copying storage.
   729|         0|            0|            0|  0.00%|
   730|         0|            0|            0|  0.00%|        Args:
   731|         0|            0|            0|  0.00%|            device (:class:`torch.device`): The desired device of the parameters
   732|         0|            0|            0|  0.00%|                and buffers in this module.
   733|         0|            0|            0|  0.00%|
   734|         0|            0|            0|  0.00%|        Returns:
   735|         0|            0|            0|  0.00%|            Module: self
   736|         0|            0|            0|  0.00%|        """
   737|         0|            0|            0|  0.00%|        return self._apply(lambda t: torch.empty_like(t, device=device))
   738|         0|            0|            0|  0.00%|
   739|         0|            0|            0|  0.00%|    @overload
   740|         0|            0|            0|  0.00%|    def to(self: T, device: Optional[Union[int, device]] = ..., dtype: Optional[Union[dtype, str]] = ...,
   741|         0|            0|            0|  0.00%|           non_blocking: bool = ...) -> T:
   742|         0|            0|            0|  0.00%|        ...
   743|         0|            0|            0|  0.00%|
   744|         0|            0|            0|  0.00%|    @overload
   745|         0|            0|            0|  0.00%|    def to(self: T, dtype: Union[dtype, str], non_blocking: bool = ...) -> T:
   746|         0|            0|            0|  0.00%|        ...
   747|         0|            0|            0|  0.00%|
   748|         0|            0|            0|  0.00%|    @overload
   749|         0|            0|            0|  0.00%|    def to(self: T, tensor: Tensor, non_blocking: bool = ...) -> T:
   750|         0|            0|            0|  0.00%|        ...
   751|         0|            0|            0|  0.00%|
   752|         0|            0|            0|  0.00%|    def to(self, *args, **kwargs):
   753|         0|            0|            0|  0.00%|        r"""Moves and/or casts the parameters and buffers.
   754|         0|            0|            0|  0.00%|
   755|         0|            0|            0|  0.00%|        This can be called as
   756|         0|            0|            0|  0.00%|
   757|         0|            0|            0|  0.00%|        .. function:: to(device=None, dtype=None, non_blocking=False)
   758|         0|            0|            0|  0.00%|
   759|         0|            0|            0|  0.00%|        .. function:: to(dtype, non_blocking=False)
   760|         0|            0|            0|  0.00%|
   761|         0|            0|            0|  0.00%|        .. function:: to(tensor, non_blocking=False)
   762|         0|            0|            0|  0.00%|
   763|         0|            0|            0|  0.00%|        .. function:: to(memory_format=torch.channels_last)
   764|         0|            0|            0|  0.00%|
   765|         0|            0|            0|  0.00%|        Its signature is similar to :meth:`torch.Tensor.to`, but only accepts
   766|         0|            0|            0|  0.00%|        floating point or complex :attr:`dtype`s. In addition, this method will
   767|         0|            0|            0|  0.00%|        only cast the floating point or complex parameters and buffers to :attr:`dtype`
   768|         0|            0|            0|  0.00%|        (if given). The integral parameters and buffers will be moved
   769|         0|            0|            0|  0.00%|        :attr:`device`, if that is given, but with dtypes unchanged. When
   770|         0|            0|            0|  0.00%|        :attr:`non_blocking` is set, it tries to convert/move asynchronously
   771|         0|            0|            0|  0.00%|        with respect to the host if possible, e.g., moving CPU Tensors with
   772|         0|            0|            0|  0.00%|        pinned memory to CUDA devices.
   773|         0|            0|            0|  0.00%|
   774|         0|            0|            0|  0.00%|        See below for examples.
   775|         0|            0|            0|  0.00%|
   776|         0|            0|            0|  0.00%|        .. note::
   777|         0|            0|            0|  0.00%|            This method modifies the module in-place.
   778|         0|            0|            0|  0.00%|
   779|         0|            0|            0|  0.00%|        Args:
   780|         0|            0|            0|  0.00%|            device (:class:`torch.device`): the desired device of the parameters
   781|         0|            0|            0|  0.00%|                and buffers in this module
   782|         0|            0|            0|  0.00%|            dtype (:class:`torch.dtype`): the desired floating point or complex dtype of
   783|         0|            0|            0|  0.00%|                the parameters and buffers in this module
   784|         0|            0|            0|  0.00%|            tensor (torch.Tensor): Tensor whose dtype and device are the desired
   785|         0|            0|            0|  0.00%|                dtype and device for all parameters and buffers in this module
   786|         0|            0|            0|  0.00%|            memory_format (:class:`torch.memory_format`): the desired memory
   787|         0|            0|            0|  0.00%|                format for 4D parameters and buffers in this module (keyword
   788|         0|            0|            0|  0.00%|                only argument)
   789|         0|            0|            0|  0.00%|
   790|         0|            0|            0|  0.00%|        Returns:
   791|         0|            0|            0|  0.00%|            Module: self
   792|         0|            0|            0|  0.00%|
   793|         0|            0|            0|  0.00%|        Examples::
   794|         0|            0|            0|  0.00%|
   795|         0|            0|            0|  0.00%|            >>> linear = nn.Linear(2, 2)
   796|         0|            0|            0|  0.00%|            >>> linear.weight
   797|         0|            0|            0|  0.00%|            Parameter containing:
   798|         0|            0|            0|  0.00%|            tensor([[ 0.1913, -0.3420],
   799|         0|            0|            0|  0.00%|                    [-0.5113, -0.2325]])
   800|         0|            0|            0|  0.00%|            >>> linear.to(torch.double)
   801|         0|            0|            0|  0.00%|            Linear(in_features=2, out_features=2, bias=True)
   802|         0|            0|            0|  0.00%|            >>> linear.weight
   803|         0|            0|            0|  0.00%|            Parameter containing:
   804|         0|            0|            0|  0.00%|            tensor([[ 0.1913, -0.3420],
   805|         0|            0|            0|  0.00%|                    [-0.5113, -0.2325]], dtype=torch.float64)
   806|         0|            0|            0|  0.00%|            >>> gpu1 = torch.device("cuda:1")
   807|         0|            0|            0|  0.00%|            >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)
   808|         0|            0|            0|  0.00%|            Linear(in_features=2, out_features=2, bias=True)
   809|         0|            0|            0|  0.00%|            >>> linear.weight
   810|         0|            0|            0|  0.00%|            Parameter containing:
   811|         0|            0|            0|  0.00%|            tensor([[ 0.1914, -0.3420],
   812|         0|            0|            0|  0.00%|                    [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')
   813|         0|            0|            0|  0.00%|            >>> cpu = torch.device("cpu")
   814|         0|            0|            0|  0.00%|            >>> linear.to(cpu)
   815|         0|            0|            0|  0.00%|            Linear(in_features=2, out_features=2, bias=True)
   816|         0|            0|            0|  0.00%|            >>> linear.weight
   817|         0|            0|            0|  0.00%|            Parameter containing:
   818|         0|            0|            0|  0.00%|            tensor([[ 0.1914, -0.3420],
   819|         0|            0|            0|  0.00%|                    [-0.5112, -0.2324]], dtype=torch.float16)
   820|         0|            0|            0|  0.00%|
   821|         0|            0|            0|  0.00%|            >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)
   822|         0|            0|            0|  0.00%|            >>> linear.weight
   823|         0|            0|            0|  0.00%|            Parameter containing:
   824|         0|            0|            0|  0.00%|            tensor([[ 0.3741+0.j,  0.2382+0.j],
   825|         0|            0|            0|  0.00%|                    [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)
   826|         0|            0|            0|  0.00%|            >>> linear(torch.ones(3, 2, dtype=torch.cdouble))
   827|         0|            0|            0|  0.00%|            tensor([[0.6122+0.j, 0.1150+0.j],
   828|         0|            0|            0|  0.00%|                    [0.6122+0.j, 0.1150+0.j],
   829|         0|            0|            0|  0.00%|                    [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)
   830|         0|            0|            0|  0.00%|
   831|         0|            0|            0|  0.00%|        """
   832|         0|            0|            0|  0.00%|
   833|         0|            0|            0|  0.00%|        device, dtype, non_blocking, convert_to_format = torch._C._nn._parse_to(*args, **kwargs)
   834|         0|            0|            0|  0.00%|
   835|         0|            0|            0|  0.00%|        if dtype is not None:
   836|         0|            0|            0|  0.00%|            if not (dtype.is_floating_point or dtype.is_complex):
   837|         0|            0|            0|  0.00%|                raise TypeError('nn.Module.to only accepts floating point or complex '
   838|         0|            0|            0|  0.00%|                                'dtypes, but got desired dtype={}'.format(dtype))
   839|         0|            0|            0|  0.00%|            if dtype.is_complex:
   840|         0|            0|            0|  0.00%|                warnings.warn(
   841|         0|            0|            0|  0.00%|                    "Complex modules are a new feature under active development whose design may change, "
   842|         0|            0|            0|  0.00%|                    "and some modules might not work as expected when using complex tensors as parameters or buffers. "
   843|         0|            0|            0|  0.00%|                    "Please file an issue at https://github.com/pytorch/pytorch/issues/new?template=bug-report.md "
   844|         0|            0|            0|  0.00%|                    "if a complex module does not work as expected.")
   845|         0|            0|            0|  0.00%|
   846|         0|            0|            0|  0.00%|        def convert(t):
   847|         0|            0|            0|  0.00%|            if convert_to_format is not None and t.dim() in (4, 5):
   848|         0|            0|            0|  0.00%|                return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
   849|         0|            0|            0|  0.00%|                            non_blocking, memory_format=convert_to_format)
   850|         0|            0|            0|  0.00%|            return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
   851|         0|            0|            0|  0.00%|
   852|         0|            0|            0|  0.00%|        return self._apply(convert)
   853|         0|            0|            0|  0.00%|
   854|         0|            0|            0|  0.00%|    def register_backward_hook(
   855|         0|            0|            0|  0.00%|        self, hook: Callable[['Module', _grad_t, _grad_t], Union[None, Tensor]]
   856|         0|            0|            0|  0.00%|    ) -> RemovableHandle:
   857|         0|            0|            0|  0.00%|        r"""Registers a backward hook on the module.
   858|         0|            0|            0|  0.00%|
   859|         0|            0|            0|  0.00%|        This function is deprecated in favor of :meth:`nn.Module.register_full_backward_hook` and
   860|         0|            0|            0|  0.00%|        the behavior of this function will change in future versions.
   861|         0|            0|            0|  0.00%|
   862|         0|            0|            0|  0.00%|        Returns:
   863|         0|            0|            0|  0.00%|            :class:`torch.utils.hooks.RemovableHandle`:
   864|         0|            0|            0|  0.00%|                a handle that can be used to remove the added hook by calling
   865|         0|            0|            0|  0.00%|                ``handle.remove()``
   866|         0|            0|            0|  0.00%|
   867|         0|            0|            0|  0.00%|        """
   868|         0|            0|            0|  0.00%|        if self._is_full_backward_hook is True:
   869|         0|            0|            0|  0.00%|            raise RuntimeError("Cannot use both regular backward hooks and full backward hooks on a "
   870|         0|            0|            0|  0.00%|                               "single Module. Please use only one of them.")
   871|         0|            0|            0|  0.00%|
   872|         0|            0|            0|  0.00%|        self._is_full_backward_hook = False
   873|         0|            0|            0|  0.00%|
   874|         0|            0|            0|  0.00%|        handle = hooks.RemovableHandle(self._backward_hooks)
   875|         0|            0|            0|  0.00%|        self._backward_hooks[handle.id] = hook
   876|         0|            0|            0|  0.00%|        return handle
   877|         0|            0|            0|  0.00%|
   878|         0|            0|            0|  0.00%|    def register_full_backward_hook(
   879|         0|            0|            0|  0.00%|        self, hook: Callable[['Module', _grad_t, _grad_t], Union[None, Tensor]]
   880|         0|            0|            0|  0.00%|    ) -> RemovableHandle:
   881|         0|            0|            0|  0.00%|        r"""Registers a backward hook on the module.
   882|         0|            0|            0|  0.00%|
   883|         0|            0|            0|  0.00%|        The hook will be called every time the gradients with respect to module
   884|         0|            0|            0|  0.00%|        inputs are computed. The hook should have the following signature::
   885|         0|            0|            0|  0.00%|
   886|         0|            0|            0|  0.00%|            hook(module, grad_input, grad_output) -> tuple(Tensor) or None
   887|         0|            0|            0|  0.00%|
   888|         0|            0|            0|  0.00%|        The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients
   889|         0|            0|            0|  0.00%|        with respect to the inputs and outputs respectively. The hook should
   890|         0|            0|            0|  0.00%|        not modify its arguments, but it can optionally return a new gradient with
   891|         0|            0|            0|  0.00%|        respect to the input that will be used in place of :attr:`grad_input` in
   892|         0|            0|            0|  0.00%|        subsequent computations. :attr:`grad_input` will only correspond to the inputs given
   893|         0|            0|            0|  0.00%|        as positional arguments and all kwarg arguments are ignored. Entries
   894|         0|            0|            0|  0.00%|        in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor
   895|         0|            0|            0|  0.00%|        arguments.
   896|         0|            0|            0|  0.00%|
   897|         0|            0|            0|  0.00%|        .. warning ::
   898|         0|            0|            0|  0.00%|            Modifying inputs or outputs inplace is not allowed when using backward hooks and
   899|         0|            0|            0|  0.00%|            will raise an error.
   900|         0|            0|            0|  0.00%|
   901|         0|            0|            0|  0.00%|        Returns:
   902|         0|            0|            0|  0.00%|            :class:`torch.utils.hooks.RemovableHandle`:
   903|         0|            0|            0|  0.00%|                a handle that can be used to remove the added hook by calling
   904|         0|            0|            0|  0.00%|                ``handle.remove()``
   905|         0|            0|            0|  0.00%|
   906|         0|            0|            0|  0.00%|        """
   907|         0|            0|            0|  0.00%|        if self._is_full_backward_hook is False:
   908|         0|            0|            0|  0.00%|            raise RuntimeError("Cannot use both regular backward hooks and full backward hooks on a "
   909|         0|            0|            0|  0.00%|                               "single Module. Please use only one of them.")
   910|         0|            0|            0|  0.00%|
   911|         0|            0|            0|  0.00%|        self._is_full_backward_hook = True
   912|         0|            0|            0|  0.00%|
   913|         0|            0|            0|  0.00%|        handle = hooks.RemovableHandle(self._backward_hooks)
   914|         0|            0|            0|  0.00%|        self._backward_hooks[handle.id] = hook
   915|         0|            0|            0|  0.00%|        return handle
   916|         0|            0|            0|  0.00%|
   917|         0|            0|            0|  0.00%|    def _get_backward_hooks(self):
   918|         0|            0|            0|  0.00%|        r"""Returns the backward hooks for use in the call function.
   919|         0|            0|            0|  0.00%|        It returns two lists, one with the full backward hooks and one with the non-full
   920|         0|            0|            0|  0.00%|        backward hooks.
   921|         0|            0|            0|  0.00%|        """
   922|         0|            0|            0|  0.00%|        full_backward_hooks: List[Callable] = []
   923|         0|            0|            0|  0.00%|        if (_global_is_full_backward_hook is True):
   924|         0|            0|            0|  0.00%|            full_backward_hooks += _global_backward_hooks.values()
   925|         0|            0|            0|  0.00%|        if (self._is_full_backward_hook is True):
   926|         0|            0|            0|  0.00%|            full_backward_hooks += self._backward_hooks.values()
   927|         0|            0|            0|  0.00%|
   928|         0|            0|            0|  0.00%|        non_full_backward_hooks: List[Callable] = []
   929|         0|            0|            0|  0.00%|        if (_global_is_full_backward_hook is False):
   930|         0|            0|            0|  0.00%|            non_full_backward_hooks += _global_backward_hooks.values()
   931|         0|            0|            0|  0.00%|        if (self._is_full_backward_hook is False):
   932|         0|            0|            0|  0.00%|            non_full_backward_hooks += self._backward_hooks.values()
   933|         0|            0|            0|  0.00%|
   934|         0|            0|            0|  0.00%|        return full_backward_hooks, non_full_backward_hooks
   935|         0|            0|            0|  0.00%|
   936|         0|            0|            0|  0.00%|    def _maybe_warn_non_full_backward_hook(self, inputs, result, grad_fn):
   937|         0|            0|            0|  0.00%|        if not isinstance(result, torch.Tensor):
   938|         0|            0|            0|  0.00%|            if not (isinstance(result, tuple) and all([isinstance(r, torch.Tensor) for r in result])):
   939|         0|            0|            0|  0.00%|                warnings.warn("Using non-full backward hooks on a Module that does not return a "
   940|         0|            0|            0|  0.00%|                              "single Tensor or a tuple of Tensors is deprecated and will be removed "
   941|         0|            0|            0|  0.00%|                              "in future versions. This hook will be missing some of the grad_output. "
   942|         0|            0|            0|  0.00%|                              "Please use register_full_backward_hook to get the documented behavior.")
   943|         0|            0|            0|  0.00%|                return
   944|         0|            0|            0|  0.00%|        else:
   945|         0|            0|            0|  0.00%|            result = (result,)
   946|         0|            0|            0|  0.00%|
   947|         0|            0|            0|  0.00%|        if not isinstance(inputs, torch.Tensor):
   948|         0|            0|            0|  0.00%|            if not (isinstance(inputs, tuple) and all([isinstance(i, torch.Tensor) for i in inputs])):
   949|         0|            0|            0|  0.00%|                warnings.warn("Using non-full backward hooks on a Module that does not take as input a "
   950|         0|            0|            0|  0.00%|                              "single Tensor or a tuple of Tensors is deprecated and will be removed "
   951|         0|            0|            0|  0.00%|                              "in future versions. This hook will be missing some of the grad_input. "
   952|         0|            0|            0|  0.00%|                              "Please use register_full_backward_hook to get the documented behavior.")
   953|         0|            0|            0|  0.00%|                return
   954|         0|            0|            0|  0.00%|        else:
   955|         0|            0|            0|  0.00%|            inputs = (inputs,)
   956|         0|            0|            0|  0.00%|
   957|         0|            0|            0|  0.00%|        # At this point we are sure that inputs and result are tuple of Tensors
   958|         0|            0|            0|  0.00%|        out_grad_fn = {r.grad_fn for r in result if r.grad_fn is not None}
   959|         0|            0|            0|  0.00%|        if len(out_grad_fn) == 0 or (len(out_grad_fn) == 1 and grad_fn not in out_grad_fn):
   960|         0|            0|            0|  0.00%|            warnings.warn("Using a non-full backward hook when outputs are nested in python data structure "
   961|         0|            0|            0|  0.00%|                          "is deprecated and will be removed in future versions. This hook will be missing "
   962|         0|            0|            0|  0.00%|                          "some grad_output.")
   963|         0|            0|            0|  0.00%|        elif len(out_grad_fn) > 1:
   964|         0|            0|            0|  0.00%|            warnings.warn("Using a non-full backward hook when outputs are generated by different autograd Nodes "
   965|         0|            0|            0|  0.00%|                          "is deprecated and will be removed in future versions. This hook will be missing "
   966|         0|            0|            0|  0.00%|                          "some grad_output. Please use register_full_backward_hook to get the documented behavior.")
   967|         0|            0|            0|  0.00%|        else:
   968|         0|            0|            0|  0.00%|            # At this point the grad_ouput part of the hook will most likely be correct
   969|         0|            0|            0|  0.00%|            inputs_grad_fn = {i.grad_fn for i in inputs if i.grad_fn is not None}
   970|         0|            0|            0|  0.00%|
   971|         0|            0|            0|  0.00%|            next_functions = {n[0] for n in grad_fn.next_functions}
   972|         0|            0|            0|  0.00%|
   973|         0|            0|            0|  0.00%|            if inputs_grad_fn != next_functions:
   974|         0|            0|            0|  0.00%|                warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
   975|         0|            0|            0|  0.00%|                              "is deprecated and will be removed in future versions. This hook will be missing "
   976|         0|            0|            0|  0.00%|                              "some grad_input. Please use register_full_backward_hook to get the documented "
   977|         0|            0|            0|  0.00%|                              "behavior.")
   978|         0|            0|            0|  0.00%|
   979|         0|            0|            0|  0.00%|    def register_forward_pre_hook(self, hook: Callable[..., None]) -> RemovableHandle:
   980|         0|            0|            0|  0.00%|        r"""Registers a forward pre-hook on the module.
   981|         0|            0|            0|  0.00%|
   982|         0|            0|            0|  0.00%|        The hook will be called every time before :func:`forward` is invoked.
   983|         0|            0|            0|  0.00%|        It should have the following signature::
   984|         0|            0|            0|  0.00%|
   985|         0|            0|            0|  0.00%|            hook(module, input) -> None or modified input
   986|         0|            0|            0|  0.00%|
   987|         0|            0|            0|  0.00%|        The input contains only the positional arguments given to the module.
   988|         0|            0|            0|  0.00%|        Keyword arguments won't be passed to the hooks and only to the ``forward``.
   989|         0|            0|            0|  0.00%|        The hook can modify the input. User can either return a tuple or a
   990|         0|            0|            0|  0.00%|        single modified value in the hook. We will wrap the value into a tuple
   991|         0|            0|            0|  0.00%|        if a single value is returned(unless that value is already a tuple).
   992|         0|            0|            0|  0.00%|
   993|         0|            0|            0|  0.00%|        Returns:
   994|         0|            0|            0|  0.00%|            :class:`torch.utils.hooks.RemovableHandle`:
   995|         0|            0|            0|  0.00%|                a handle that can be used to remove the added hook by calling
   996|         0|            0|            0|  0.00%|                ``handle.remove()``
   997|         0|            0|            0|  0.00%|        """
   998|         0|            0|            0|  0.00%|        handle = hooks.RemovableHandle(self._forward_pre_hooks)
   999|         0|            0|            0|  0.00%|        self._forward_pre_hooks[handle.id] = hook
  1000|         0|            0|            0|  0.00%|        return handle
  1001|         0|            0|            0|  0.00%|
  1002|         0|            0|            0|  0.00%|    def register_forward_hook(self, hook: Callable[..., None]) -> RemovableHandle:
  1003|         0|            0|            0|  0.00%|        r"""Registers a forward hook on the module.
  1004|         0|            0|            0|  0.00%|
  1005|         0|            0|            0|  0.00%|        The hook will be called every time after :func:`forward` has computed an output.
  1006|         0|            0|            0|  0.00%|        It should have the following signature::
  1007|         0|            0|            0|  0.00%|
  1008|         0|            0|            0|  0.00%|            hook(module, input, output) -> None or modified output
  1009|         0|            0|            0|  0.00%|
  1010|         0|            0|            0|  0.00%|        The input contains only the positional arguments given to the module.
  1011|         0|            0|            0|  0.00%|        Keyword arguments won't be passed to the hooks and only to the ``forward``.
  1012|         0|            0|            0|  0.00%|        The hook can modify the output. It can modify the input inplace but
  1013|         0|            0|            0|  0.00%|        it will not have effect on forward since this is called after
  1014|         0|            0|            0|  0.00%|        :func:`forward` is called.
  1015|         0|            0|            0|  0.00%|
  1016|         0|            0|            0|  0.00%|        Returns:
  1017|         0|            0|            0|  0.00%|            :class:`torch.utils.hooks.RemovableHandle`:
  1018|         0|            0|            0|  0.00%|                a handle that can be used to remove the added hook by calling
  1019|         0|            0|            0|  0.00%|                ``handle.remove()``
  1020|         0|            0|            0|  0.00%|        """
  1021|         0|            0|            0|  0.00%|        handle = hooks.RemovableHandle(self._forward_hooks)
  1022|         0|            0|            0|  0.00%|        self._forward_hooks[handle.id] = hook
  1023|         0|            0|            0|  0.00%|        return handle
  1024|         0|            0|            0|  0.00%|
  1025|         0|            0|            0|  0.00%|    def _slow_forward(self, *input, **kwargs):
  1026|         0|            0|            0|  0.00%|        tracing_state = torch._C._get_tracing_state()
  1027|         0|            0|            0|  0.00%|        if not tracing_state or isinstance(self.forward, torch._C.ScriptMethod):
  1028|         0|            0|            0|  0.00%|            return self.forward(*input, **kwargs)
  1029|         0|            0|            0|  0.00%|        recording_scopes = torch.jit._trace._trace_module_map is not None
  1030|         0|            0|            0|  0.00%|        if recording_scopes:
  1031|         0|            0|            0|  0.00%|            # type ignore was added because at this point one knows that
  1032|         0|            0|            0|  0.00%|            # torch.jit._trace._trace_module_map is not Optional and has type Dict[Any, Any]
  1033|         0|            0|            0|  0.00%|            name = torch.jit._trace._trace_module_map[self] if self in torch.jit._trace._trace_module_map else None  # type: ignore[index, operator] # noqa: B950
  1034|         0|            0|            0|  0.00%|            if name:
  1035|         0|            0|            0|  0.00%|                tracing_state.push_scope(name)
  1036|         0|            0|            0|  0.00%|            else:
  1037|         0|            0|            0|  0.00%|                recording_scopes = False
  1038|         0|            0|            0|  0.00%|        try:
  1039|         0|            0|            0|  0.00%|            result = self.forward(*input, **kwargs)
  1040|         0|            0|            0|  0.00%|        finally:
  1041|         0|            0|            0|  0.00%|            if recording_scopes:
  1042|         0|            0|            0|  0.00%|                tracing_state.pop_scope()
  1043|         0|            0|            0|  0.00%|        return result
  1044|         0|            0|            0|  0.00%|
  1045|      7700|    0.0491402|  6.38185e-06|  0.09%|    def _call_impl(self, *input, **kwargs):
  1046|      7700|    0.0921154|   1.1963e-05|  0.17%|        forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)
  1047|         0|            0|            0|  0.00%|        # If we don't have any hooks, we want to skip the rest of the logic in
  1048|         0|            0|            0|  0.00%|        # this function, and just call forward.
  1049|     23100|     0.137011|  5.93119e-06|  0.26%|        if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
  1050|     15400|    0.0822101|  5.33832e-06|  0.15%|                or _global_forward_hooks or _global_forward_pre_hooks):
  1051|      7700|     0.133991|  1.74015e-05|  0.25%|            return forward_call(*input, **kwargs)
(call)|      2000|      11.5509|   0.00577546| 21.56%|# /opt/conda/lib/python3.8/site-packages/torch/nn/modules/conv.py:442 forward
(call)|      2000|      2.42002|   0.00121001|  4.52%|# /opt/conda/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py:133 forward
(call)|      1700|     0.223283|  0.000131343|  0.42%|# /opt/conda/lib/python3.8/site-packages/torch/nn/modules/activation.py:101 forward
(call)|       100|     0.710612|   0.00710612|  1.33%|# /opt/conda/lib/python3.8/site-packages/torch/nn/modules/pooling.py:161 forward
(call)|       800|       13.779|    0.0172238| 25.72%|# /opt/conda/lib/python3.8/site-packages/torchvision/models/resnet.py:67 forward
(call)|       700|      13.8649|    0.0198071| 25.88%|# /opt/conda/lib/python3.8/site-packages/torch/nn/modules/container.py:137 forward
(call)|       100|    0.0236437|  0.000236437|  0.04%|# /opt/conda/lib/python3.8/site-packages/torch/nn/modules/pooling.py:1131 forward
(call)|       100|    0.0205069|  0.000205069|  0.04%|# /opt/conda/lib/python3.8/site-packages/torch/nn/modules/linear.py:95 forward
(call)|       100|      16.1252|     0.161252| 30.10%|# /opt/conda/lib/python3.8/site-packages/torchvision/models/resnet.py:248 forward
(call)|       100|    0.0251598|  0.000251598|  0.05%|# /opt/conda/lib/python3.8/site-packages/torch/nn/modules/loss.py:1119 forward
  1052|         0|            0|            0|  0.00%|        # Do not call functions when jit is used
  1053|         0|            0|            0|  0.00%|        full_backward_hooks, non_full_backward_hooks = [], []
  1054|         0|            0|            0|  0.00%|        if self._backward_hooks or _global_backward_hooks:
  1055|         0|            0|            0|  0.00%|            full_backward_hooks, non_full_backward_hooks = self._get_backward_hooks()
  1056|         0|            0|            0|  0.00%|        if _global_forward_pre_hooks or self._forward_pre_hooks:
  1057|         0|            0|            0|  0.00%|            for hook in itertools.chain(
  1058|         0|            0|            0|  0.00%|                    _global_forward_pre_hooks.values(),
  1059|         0|            0|            0|  0.00%|                    self._forward_pre_hooks.values()):
  1060|         0|            0|            0|  0.00%|                result = hook(self, input)
  1061|         0|            0|            0|  0.00%|                if result is not None:
  1062|         0|            0|            0|  0.00%|                    if not isinstance(result, tuple):
  1063|         0|            0|            0|  0.00%|                        result = (result,)
  1064|         0|            0|            0|  0.00%|                    input = result
  1065|         0|            0|            0|  0.00%|
  1066|         0|            0|            0|  0.00%|        bw_hook = None
  1067|         0|            0|            0|  0.00%|        if full_backward_hooks:
  1068|         0|            0|            0|  0.00%|            bw_hook = hooks.BackwardHook(self, full_backward_hooks)
  1069|         0|            0|            0|  0.00%|            input = bw_hook.setup_input_hook(input)
  1070|         0|            0|            0|  0.00%|
  1071|         0|            0|            0|  0.00%|        result = forward_call(*input, **kwargs)
  1072|         0|            0|            0|  0.00%|        if _global_forward_hooks or self._forward_hooks:
  1073|         0|            0|            0|  0.00%|            for hook in itertools.chain(
  1074|         0|            0|            0|  0.00%|                    _global_forward_hooks.values(),
  1075|         0|            0|            0|  0.00%|                    self._forward_hooks.values()):
  1076|         0|            0|            0|  0.00%|                hook_result = hook(self, input, result)
  1077|         0|            0|            0|  0.00%|                if hook_result is not None:
  1078|         0|            0|            0|  0.00%|                    result = hook_result
  1079|         0|            0|            0|  0.00%|
  1080|         0|            0|            0|  0.00%|        if bw_hook:
  1081|         0|            0|            0|  0.00%|            result = bw_hook.setup_output_hook(result)
  1082|         0|            0|            0|  0.00%|
  1083|         0|            0|            0|  0.00%|        # Handle the non-full backward hooks
  1084|         0|            0|            0|  0.00%|        if non_full_backward_hooks:
  1085|         0|            0|            0|  0.00%|            var = result
  1086|         0|            0|            0|  0.00%|            while not isinstance(var, torch.Tensor):
  1087|         0|            0|            0|  0.00%|                if isinstance(var, dict):
  1088|         0|            0|            0|  0.00%|                    var = next((v for v in var.values() if isinstance(v, torch.Tensor)))
  1089|         0|            0|            0|  0.00%|                else:
  1090|         0|            0|            0|  0.00%|                    var = var[0]
  1091|         0|            0|            0|  0.00%|            grad_fn = var.grad_fn
  1092|         0|            0|            0|  0.00%|            if grad_fn is not None:
  1093|         0|            0|            0|  0.00%|                for hook in non_full_backward_hooks:
  1094|         0|            0|            0|  0.00%|                    wrapper = functools.partial(hook, self)
  1095|         0|            0|            0|  0.00%|                    functools.update_wrapper(wrapper, hook)
  1096|         0|            0|            0|  0.00%|                    grad_fn.register_hook(wrapper)
  1097|         0|            0|            0|  0.00%|                self._maybe_warn_non_full_backward_hook(input, result, grad_fn)
  1098|         0|            0|            0|  0.00%|
  1099|         0|            0|            0|  0.00%|        return result
  1100|         0|            0|            0|  0.00%|
  1101|         0|            0|            0|  0.00%|    __call__ : Callable[..., Any] = _call_impl
  1102|         0|            0|            0|  0.00%|
  1103|         0|            0|            0|  0.00%|    def __setstate__(self, state):
  1104|         0|            0|            0|  0.00%|        self.__dict__.update(state)
  1105|         0|            0|            0|  0.00%|        # Support loading old checkpoints that don't have the following attrs:
  1106|         0|            0|            0|  0.00%|        if '_forward_pre_hooks' not in self.__dict__:
  1107|         0|            0|            0|  0.00%|            self._forward_pre_hooks = OrderedDict()
  1108|         0|            0|            0|  0.00%|        if '_state_dict_hooks' not in self.__dict__:
  1109|         0|            0|            0|  0.00%|            self._state_dict_hooks = OrderedDict()
  1110|         0|            0|            0|  0.00%|        if '_load_state_dict_pre_hooks' not in self.__dict__:
  1111|         0|            0|            0|  0.00%|            self._load_state_dict_pre_hooks = OrderedDict()
  1112|         0|            0|            0|  0.00%|        if '_non_persistent_buffers_set' not in self.__dict__:
  1113|         0|            0|            0|  0.00%|            self._non_persistent_buffers_set = set()
  1114|         0|            0|            0|  0.00%|        if '_is_full_backward_hook' not in self.__dict__:
  1115|         0|            0|            0|  0.00%|            self._is_full_backward_hook = None
  1116|         0|            0|            0|  0.00%|
  1117|     21920|     0.118047|  5.38536e-06|  0.22%|    def __getattr__(self, name: str) -> Union[Tensor, 'Module']:
  1118|     21920|     0.114979|   5.2454e-06|  0.21%|        if '_parameters' in self.__dict__:
  1119|     21920|      0.10144|  4.62775e-06|  0.19%|            _parameters = self.__dict__['_parameters']
  1120|     21920|     0.096195|  4.38846e-06|  0.18%|            if name in _parameters:
  1121|      8200|    0.0354512|  4.32331e-06|  0.07%|                return _parameters[name]
  1122|     13720|    0.0619068|  4.51216e-06|  0.12%|        if '_buffers' in self.__dict__:
  1123|     13720|    0.0618191|  4.50576e-06|  0.12%|            _buffers = self.__dict__['_buffers']
  1124|     13720|    0.0593483|  4.32568e-06|  0.11%|            if name in _buffers:
  1125|      7320|     0.031599|  4.31681e-06|  0.06%|                return _buffers[name]
  1126|      6400|    0.0284121|  4.43939e-06|  0.05%|        if '_modules' in self.__dict__:
  1127|      6400|    0.0281854|  4.40396e-06|  0.05%|            modules = self.__dict__['_modules']
  1128|      6400|    0.0272233|  4.25365e-06|  0.05%|            if name in modules:
  1129|      6400|    0.0269299|  4.20779e-06|  0.05%|                return modules[name]
  1130|         0|            0|            0|  0.00%|        raise AttributeError("'{}' object has no attribute '{}'".format(
  1131|         0|            0|            0|  0.00%|            type(self).__name__, name))
  1132|         0|            0|            0|  0.00%|
  1133|      1356|    0.0122507|  9.03441e-06|  0.02%|    def __setattr__(self, name: str, value: Union[Tensor, 'Module']) -> None:
  1134|      1356|    0.0098691|   7.2781e-06|  0.02%|        def remove_from(*dicts_or_sets):
  1135|         0|            0|            0|  0.00%|            for d in dicts_or_sets:
  1136|         0|            0|            0|  0.00%|                if name in d:
  1137|         0|            0|            0|  0.00%|                    if isinstance(d, dict):
  1138|         0|            0|            0|  0.00%|                        del d[name]
  1139|         0|            0|            0|  0.00%|                    else:
  1140|         0|            0|            0|  0.00%|                        d.discard(name)
  1141|         0|            0|            0|  0.00%|
  1142|      1356|    0.0113235|  8.35063e-06|  0.02%|        params = self.__dict__.get('_parameters')
  1143|      1356|    0.0110476|   8.1472e-06|  0.02%|        if isinstance(value, Parameter):
  1144|         0|            0|            0|  0.00%|            if params is None:
  1145|         0|            0|            0|  0.00%|                raise AttributeError(
  1146|         0|            0|            0|  0.00%|                    "cannot assign parameters before Module.__init__() call")
  1147|         0|            0|            0|  0.00%|            remove_from(self.__dict__, self._buffers, self._modules, self._non_persistent_buffers_set)
  1148|         0|            0|            0|  0.00%|            self.register_parameter(name, value)
  1149|      1356|    0.0072608|  5.35457e-06|  0.01%|        elif params is not None and name in params:
  1150|         0|            0|            0|  0.00%|            if value is not None:
  1151|         0|            0|            0|  0.00%|                raise TypeError("cannot assign '{}' as parameter '{}' "
  1152|         0|            0|            0|  0.00%|                                "(torch.nn.Parameter or None expected)"
  1153|         0|            0|            0|  0.00%|                                .format(torch.typename(value), name))
  1154|         0|            0|            0|  0.00%|            self.register_parameter(name, value)
  1155|         0|            0|            0|  0.00%|        else:
  1156|      1356|    0.0104887|  7.73507e-06|  0.02%|            modules = self.__dict__.get('_modules')
  1157|      1356|    0.0100739|  7.42913e-06|  0.02%|            if isinstance(value, Module):
  1158|         0|            0|            0|  0.00%|                if modules is None:
  1159|         0|            0|            0|  0.00%|                    raise AttributeError(
  1160|         0|            0|            0|  0.00%|                        "cannot assign module before Module.__init__() call")
  1161|         0|            0|            0|  0.00%|                remove_from(self.__dict__, self._parameters, self._buffers, self._non_persistent_buffers_set)
  1162|         0|            0|            0|  0.00%|                modules[name] = value
  1163|      1356|    0.0072751|  5.36512e-06|  0.01%|            elif modules is not None and name in modules:
  1164|         0|            0|            0|  0.00%|                if value is not None:
  1165|         0|            0|            0|  0.00%|                    raise TypeError("cannot assign '{}' as child module '{}' "
  1166|         0|            0|            0|  0.00%|                                    "(torch.nn.Module or None expected)"
  1167|         0|            0|            0|  0.00%|                                    .format(torch.typename(value), name))
  1168|         0|            0|            0|  0.00%|                modules[name] = value
  1169|         0|            0|            0|  0.00%|            else:
  1170|      1356|   0.00954843|  7.04161e-06|  0.02%|                buffers = self.__dict__.get('_buffers')
  1171|      1356|    0.0070982|  5.23466e-06|  0.01%|                if buffers is not None and name in buffers:
  1172|      1220|   0.00894618|  7.33293e-06|  0.02%|                    if value is not None and not isinstance(value, torch.Tensor):
  1173|         0|            0|            0|  0.00%|                        raise TypeError("cannot assign '{}' as buffer '{}' "
  1174|         0|            0|            0|  0.00%|                                        "(torch.Tensor or None expected)"
  1175|         0|            0|            0|  0.00%|                                        .format(torch.typename(value), name))
  1176|      1220|    0.0107872|  8.84201e-06|  0.02%|                    buffers[name] = value
  1177|         0|            0|            0|  0.00%|                else:
  1178|       136|  0.000723362|  5.31884e-06|  0.00%|                    object.__setattr__(self, name, value)
  1179|         0|            0|            0|  0.00%|
  1180|         0|            0|            0|  0.00%|    def __delattr__(self, name):
  1181|         0|            0|            0|  0.00%|        if name in self._parameters:
  1182|         0|            0|            0|  0.00%|            del self._parameters[name]
  1183|         0|            0|            0|  0.00%|        elif name in self._buffers:
  1184|         0|            0|            0|  0.00%|            del self._buffers[name]
  1185|         0|            0|            0|  0.00%|            self._non_persistent_buffers_set.discard(name)
  1186|         0|            0|            0|  0.00%|        elif name in self._modules:
  1187|         0|            0|            0|  0.00%|            del self._modules[name]
  1188|         0|            0|            0|  0.00%|        else:
  1189|         0|            0|            0|  0.00%|            object.__delattr__(self, name)
  1190|         0|            0|            0|  0.00%|
  1191|         0|            0|            0|  0.00%|    def _register_state_dict_hook(self, hook):
  1192|         0|            0|            0|  0.00%|        r"""These hooks will be called with arguments: `self`, `state_dict`,
  1193|         0|            0|            0|  0.00%|        `prefix`, `local_metadata`, after the `state_dict` of `self` is set.
  1194|         0|            0|            0|  0.00%|        Note that only parameters and buffers of `self` or its children are
  1195|         0|            0|            0|  0.00%|        guaranteed to exist in `state_dict`. The hooks may modify `state_dict`
  1196|         0|            0|            0|  0.00%|        inplace or return a new one.
  1197|         0|            0|            0|  0.00%|        """
  1198|         0|            0|            0|  0.00%|        handle = hooks.RemovableHandle(self._state_dict_hooks)
  1199|         0|            0|            0|  0.00%|        self._state_dict_hooks[handle.id] = hook
  1200|         0|            0|            0|  0.00%|        return handle
  1201|         0|            0|            0|  0.00%|
  1202|       136|  0.000672579|  4.94543e-06|  0.00%|    def _save_to_state_dict(self, destination, prefix, keep_vars):
  1203|         0|            0|            0|  0.00%|        r"""Saves module state to `destination` dictionary, containing a state
  1204|         0|            0|            0|  0.00%|        of the module, but not its descendants. This is called on every
  1205|         0|            0|            0|  0.00%|        submodule in :meth:`~torch.nn.Module.state_dict`.
  1206|         0|            0|            0|  0.00%|
  1207|         0|            0|            0|  0.00%|        In rare cases, subclasses can achieve class-specific behavior by
  1208|         0|            0|            0|  0.00%|        overriding this method with custom logic.
  1209|         0|            0|            0|  0.00%|
  1210|         0|            0|            0|  0.00%|        Args:
  1211|         0|            0|            0|  0.00%|            destination (dict): a dict where state will be stored
  1212|         0|            0|            0|  0.00%|            prefix (str): the prefix for parameters and buffers used in this
  1213|         0|            0|            0|  0.00%|                module
  1214|         0|            0|            0|  0.00%|        """
  1215|       300|   0.00182486|  6.08285e-06|  0.00%|        for name, param in self._parameters.items():
  1216|       164|  0.000680447|  4.14906e-06|  0.00%|            if param is not None:
  1217|       124|   0.00160384|  1.29342e-05|  0.00%|                destination[prefix + name] = param if keep_vars else param.detach()
  1218|       256|   0.00155997|  6.09364e-06|  0.00%|        for name, buf in self._buffers.items():
  1219|       120|  0.000544548|   4.5379e-06|  0.00%|            if buf is not None and name not in self._non_persistent_buffers_set:
  1220|       120|   0.00133681|  1.11401e-05|  0.00%|                destination[prefix + name] = buf if keep_vars else buf.detach()
  1221|         0|            0|            0|  0.00%|
  1222|         0|            0|            0|  0.00%|    # The user can pass an optional arbitrary mappable object to `state_dict`, in which case `state_dict` returns
  1223|         0|            0|            0|  0.00%|    # back that same object. But if they pass nothing, an `OrederedDict` is created and returned.
  1224|         0|            0|            0|  0.00%|    T_destination = TypeVar('T_destination', bound=Mapping[str, Tensor])
  1225|         0|            0|            0|  0.00%|
  1226|         0|            0|            0|  0.00%|    @overload
  1227|         0|            0|            0|  0.00%|    def state_dict(self, destination: T_destination, prefix: str = ..., keep_vars: bool = ...) -> T_destination:
  1228|         0|            0|            0|  0.00%|        ...
  1229|         0|            0|            0|  0.00%|
  1230|         0|            0|            0|  0.00%|    # TODO: Remove string escape once Python-3.6 no longer supported
  1231|         0|            0|            0|  0.00%|    # See https://github.com/python/mypy/issues/6904#issuecomment-496207426
  1232|         0|            0|            0|  0.00%|    @overload
  1233|         0|            0|            0|  0.00%|    def state_dict(self, prefix: str = ..., keep_vars: bool = ...) -> 'OrderedDict[str, Tensor]':
  1234|         0|            0|            0|  0.00%|        ...
  1235|         0|            0|            0|  0.00%|
  1236|       136|  0.000750303|  5.51694e-06|  0.00%|    def state_dict(self, destination=None, prefix='', keep_vars=False):
  1237|         0|            0|            0|  0.00%|        r"""Returns a dictionary containing a whole state of the module.
  1238|         0|            0|            0|  0.00%|
  1239|         0|            0|            0|  0.00%|        Both parameters and persistent buffers (e.g. running averages) are
  1240|         0|            0|            0|  0.00%|        included. Keys are corresponding parameter and buffer names.
  1241|         0|            0|            0|  0.00%|
  1242|         0|            0|            0|  0.00%|        Returns:
  1243|         0|            0|            0|  0.00%|            dict:
  1244|         0|            0|            0|  0.00%|                a dictionary containing a whole state of the module
  1245|         0|            0|            0|  0.00%|
  1246|         0|            0|            0|  0.00%|        Example::
  1247|         0|            0|            0|  0.00%|
  1248|         0|            0|            0|  0.00%|            >>> module.state_dict().keys()
  1249|         0|            0|            0|  0.00%|            ['bias', 'weight']
  1250|         0|            0|            0|  0.00%|
  1251|         0|            0|            0|  0.00%|        """
  1252|       136|  0.000665903|  4.89635e-06|  0.00%|        if destination is None:
  1253|         2|  2.02656e-05|  1.01328e-05|  0.00%|            destination = OrderedDict()
  1254|         2|  1.97887e-05|  9.89437e-06|  0.00%|            destination._metadata = OrderedDict()
  1255|       136|  0.000953674|  7.01231e-06|  0.00%|        destination._metadata[prefix[:-1]] = local_metadata = dict(version=self._version)
  1256|       136|   0.00200844|  1.47679e-05|  0.00%|        self._save_to_state_dict(destination, prefix, keep_vars)
(call)|       136|   0.00822306|  6.04637e-05|  0.02%|# /opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1202 _save_to_state_dict
  1257|       270|   0.00165009|  6.11146e-06|  0.00%|        for name, module in self._modules.items():
  1258|       134|  0.000576973|  4.30577e-06|  0.00%|            if module is not None:
  1259|       134|   0.00200987|   1.4999e-05|  0.00%|                module.state_dict(destination, prefix + name + '.', keep_vars=keep_vars)
(call)|       134|     0.017633|  0.000131589|  0.03%|# /opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1236 state_dict
  1260|       136|  0.000992298|  7.29631e-06|  0.00%|        for hook in self._state_dict_hooks.values():
  1261|         0|            0|            0|  0.00%|            hook_result = hook(self, destination, prefix, local_metadata)
  1262|         0|            0|            0|  0.00%|            if hook_result is not None:
  1263|         0|            0|            0|  0.00%|                destination = hook_result
  1264|       136|  0.000624418|  4.59131e-06|  0.00%|        return destination
  1265|         0|            0|            0|  0.00%|
  1266|         0|            0|            0|  0.00%|    def _register_load_state_dict_pre_hook(self, hook):
  1267|         0|            0|            0|  0.00%|        r"""These hooks will be called with arguments: `state_dict`, `prefix`,
  1268|         0|            0|            0|  0.00%|        `local_metadata`, `strict`, `missing_keys`, `unexpected_keys`,
  1269|         0|            0|            0|  0.00%|        `error_msgs`, before loading `state_dict` into `self`. These arguments
  1270|         0|            0|            0|  0.00%|        are exactly the same as those of `_load_from_state_dict`.
  1271|         0|            0|            0|  0.00%|        """
  1272|         0|            0|            0|  0.00%|        handle = hooks.RemovableHandle(self._load_state_dict_pre_hooks)
  1273|         0|            0|            0|  0.00%|        self._load_state_dict_pre_hooks[handle.id] = hook
  1274|         0|            0|            0|  0.00%|        return handle
  1275|         0|            0|            0|  0.00%|
  1276|        68|  0.000516891|  7.60135e-06|  0.00%|    def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict,
  1277|         0|            0|            0|  0.00%|                              missing_keys, unexpected_keys, error_msgs):
  1278|         0|            0|            0|  0.00%|        r"""Copies parameters and buffers from :attr:`state_dict` into only
  1279|         0|            0|            0|  0.00%|        this module, but not its descendants. This is called on every submodule
  1280|         0|            0|            0|  0.00%|        in :meth:`~torch.nn.Module.load_state_dict`. Metadata saved for this
  1281|         0|            0|            0|  0.00%|        module in input :attr:`state_dict` is provided as :attr:`local_metadata`.
  1282|         0|            0|            0|  0.00%|        For state dicts without metadata, :attr:`local_metadata` is empty.
  1283|         0|            0|            0|  0.00%|        Subclasses can achieve class-specific backward compatible loading using
  1284|         0|            0|            0|  0.00%|        the version number at `local_metadata.get("version", None)`.
  1285|         0|            0|            0|  0.00%|
  1286|         0|            0|            0|  0.00%|        .. note::
  1287|         0|            0|            0|  0.00%|            :attr:`state_dict` is not the same object as the input
  1288|         0|            0|            0|  0.00%|            :attr:`state_dict` to :meth:`~torch.nn.Module.load_state_dict`. So
  1289|         0|            0|            0|  0.00%|            it can be modified.
  1290|         0|            0|            0|  0.00%|
  1291|         0|            0|            0|  0.00%|        Args:
  1292|         0|            0|            0|  0.00%|            state_dict (dict): a dict containing parameters and
  1293|         0|            0|            0|  0.00%|                persistent buffers.
  1294|         0|            0|            0|  0.00%|            prefix (str): the prefix for parameters and buffers used in this
  1295|         0|            0|            0|  0.00%|                module
  1296|         0|            0|            0|  0.00%|            local_metadata (dict): a dict containing the metadata for this module.
  1297|         0|            0|            0|  0.00%|                See
  1298|         0|            0|            0|  0.00%|            strict (bool): whether to strictly enforce that the keys in
  1299|         0|            0|            0|  0.00%|                :attr:`state_dict` with :attr:`prefix` match the names of
  1300|         0|            0|            0|  0.00%|                parameters and buffers in this module
  1301|         0|            0|            0|  0.00%|            missing_keys (list of str): if ``strict=True``, add missing keys to
  1302|         0|            0|            0|  0.00%|                this list
  1303|         0|            0|            0|  0.00%|            unexpected_keys (list of str): if ``strict=True``, add unexpected
  1304|         0|            0|            0|  0.00%|                keys to this list
  1305|         0|            0|            0|  0.00%|            error_msgs (list of str): error messages should be added to this
  1306|         0|            0|            0|  0.00%|                list, and will be reported together in
  1307|         0|            0|            0|  0.00%|                :meth:`~torch.nn.Module.load_state_dict`
  1308|         0|            0|            0|  0.00%|        """
  1309|        68|  0.000699043|    1.028e-05|  0.00%|        for hook in self._load_state_dict_pre_hooks.values():
  1310|         0|            0|            0|  0.00%|            hook(state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)
  1311|         0|            0|            0|  0.00%|
  1312|       264|   0.00228715|  8.66344e-06|  0.00%|        persistent_buffers = {k: v for k, v in self._buffers.items() if k not in self._non_persistent_buffers_set}
(call)|        68|  0.000952482|  1.40071e-05|  0.00%|# /opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1312 <dictcomp>
  1313|        68|  0.000781298|  1.14897e-05|  0.00%|        local_name_params = itertools.chain(self._parameters.items(), persistent_buffers.items())
  1314|       346|   0.00239778|  6.92999e-06|  0.00%|        local_state = {k: v for k, v in local_name_params if v is not None}
(call)|        68|   0.00128579|  1.89087e-05|  0.00%|# /opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1314 <dictcomp>
  1315|         0|            0|            0|  0.00%|
  1316|       190|   0.00127435|  6.70709e-06|  0.00%|        for name, param in local_state.items():
  1317|       122|  0.000713587|  5.84907e-06|  0.00%|            key = prefix + name
  1318|       122|  0.000705004|  5.77872e-06|  0.00%|            if key in state_dict:
  1319|       122|  0.000658035|  5.39373e-06|  0.00%|                input_param = state_dict[key]
  1320|         0|            0|            0|  0.00%|                # This is used to avoid copying uninitialized parameters into
  1321|         0|            0|            0|  0.00%|                # non-lazy modules, since they dont have the hook to do the checks
  1322|         0|            0|            0|  0.00%|                # in such case, it will error when accessing the .shape attribute.
  1323|       122|   0.00190544|  1.56184e-05|  0.00%|                is_param_lazy = torch.nn.parameter.is_lazy(param)
(call)|       122|   0.00141168|  1.15711e-05|  0.00%|# /opt/conda/lib/python3.8/site-packages/torch/nn/parameter.py:129 is_lazy
  1324|         0|            0|            0|  0.00%|                # Backward compatibility: loading 1-dim tensor from 0.3.* to version 0.4+
  1325|       122|   0.00123191|  1.00976e-05|  0.00%|                if not is_param_lazy and len(param.shape) == 0 and len(input_param.shape) == 1:
  1326|         0|            0|            0|  0.00%|                    input_param = input_param[0]
  1327|         0|            0|            0|  0.00%|
  1328|       122|  0.000829458|  6.79884e-06|  0.00%|                if not is_param_lazy and input_param.shape != param.shape:
  1329|         0|            0|            0|  0.00%|                    # local shape should match the one in checkpoint
  1330|         0|            0|            0|  0.00%|                    error_msgs.append('size mismatch for {}: copying a param with shape {} from checkpoint, '
  1331|         0|            0|            0|  0.00%|                                      'the shape in current model is {}.'
  1332|         0|            0|            0|  0.00%|                                      .format(key, input_param.shape, param.shape))
  1333|         0|            0|            0|  0.00%|                    continue
  1334|       122|  0.000666618|  5.46408e-06|  0.00%|                try:
  1335|       122|   0.00308156|  2.52587e-05|  0.01%|                    with torch.no_grad():
(call)|       122|    0.0042603|  3.49205e-05|  0.01%|# /opt/conda/lib/python3.8/site-packages/torch/autograd/grad_mode.py:119 __init__
(call)|       122|   0.00496745|  4.07168e-05|  0.01%|# /opt/conda/lib/python3.8/site-packages/torch/autograd/grad_mode.py:124 __enter__
  1336|       122|    0.0186882|  0.000153182|  0.03%|                        param.copy_(input_param)
(call)|       122|   0.00477791|  3.91632e-05|  0.01%|# /opt/conda/lib/python3.8/site-packages/torch/autograd/grad_mode.py:128 __exit__
  1337|         0|            0|            0|  0.00%|                except Exception as ex:
  1338|         0|            0|            0|  0.00%|                    error_msgs.append('While copying the parameter named "{}", '
  1339|         0|            0|            0|  0.00%|                                      'whose dimensions in the model are {} and '
  1340|         0|            0|            0|  0.00%|                                      'whose dimensions in the checkpoint are {}, '
  1341|         0|            0|            0|  0.00%|                                      'an exception occurred : {}.'
  1342|         0|            0|            0|  0.00%|                                      .format(key, param.size(), input_param.size(), ex.args))
  1343|         0|            0|            0|  0.00%|            elif strict:
  1344|         0|            0|            0|  0.00%|                missing_keys.append(key)
  1345|         0|            0|            0|  0.00%|
  1346|        68|   0.00040102|  5.89735e-06|  0.00%|        if strict:
  1347|      8364|    0.0467315|  5.58722e-06|  0.09%|            for key in state_dict.keys():
  1348|      8296|     0.061806|  7.45009e-06|  0.12%|                if key.startswith(prefix):
  1349|       490|   0.00396347|  8.08872e-06|  0.01%|                    input_name = key[len(prefix):]
  1350|       490|   0.00404334|  8.25172e-06|  0.01%|                    input_name = input_name.split('.', 1)[0]  # get the name of param/buffer/child
  1351|       490|   0.00310421|  6.33512e-06|  0.01%|                    if input_name not in self._modules and input_name not in local_state:
  1352|         0|            0|            0|  0.00%|                        unexpected_keys.append(key)
  1353|         0|            0|            0|  0.00%|
  1354|         1|  3.67165e-05|  3.67165e-05|  0.00%|    def load_state_dict(self, state_dict: 'OrderedDict[str, Tensor]',
  1355|         0|            0|            0|  0.00%|                        strict: bool = True):
  1356|         0|            0|            0|  0.00%|        r"""Copies parameters and buffers from :attr:`state_dict` into
  1357|         0|            0|            0|  0.00%|        this module and its descendants. If :attr:`strict` is ``True``, then
  1358|         0|            0|            0|  0.00%|        the keys of :attr:`state_dict` must exactly match the keys returned
  1359|         0|            0|            0|  0.00%|        by this module's :meth:`~torch.nn.Module.state_dict` function.
  1360|         0|            0|            0|  0.00%|
  1361|         0|            0|            0|  0.00%|        Args:
  1362|         0|            0|            0|  0.00%|            state_dict (dict): a dict containing parameters and
  1363|         0|            0|            0|  0.00%|                persistent buffers.
  1364|         0|            0|            0|  0.00%|            strict (bool, optional): whether to strictly enforce that the keys
  1365|         0|            0|            0|  0.00%|                in :attr:`state_dict` match the keys returned by this module's
  1366|         0|            0|            0|  0.00%|                :meth:`~torch.nn.Module.state_dict` function. Default: ``True``
  1367|         0|            0|            0|  0.00%|
  1368|         0|            0|            0|  0.00%|        Returns:
  1369|         0|            0|            0|  0.00%|            ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:
  1370|         0|            0|            0|  0.00%|                * **missing_keys** is a list of str containing the missing keys
  1371|         0|            0|            0|  0.00%|                * **unexpected_keys** is a list of str containing the unexpected keys
  1372|         0|            0|            0|  0.00%|        """
  1373|         1|  1.71661e-05|  1.71661e-05|  0.00%|        missing_keys: List[str] = []
  1374|         1|   1.4782e-05|   1.4782e-05|  0.00%|        unexpected_keys: List[str] = []
  1375|         1|  1.38283e-05|  1.38283e-05|  0.00%|        error_msgs: List[str] = []
  1376|         0|            0|            0|  0.00%|
  1377|         0|            0|            0|  0.00%|        # copy state_dict so _load_from_state_dict can modify it
  1378|         1|  1.74046e-05|  1.74046e-05|  0.00%|        metadata = getattr(state_dict, '_metadata', None)
  1379|         1|   9.5129e-05|   9.5129e-05|  0.00%|        state_dict = state_dict.copy()
  1380|         1|  2.00272e-05|  2.00272e-05|  0.00%|        if metadata is not None:
  1381|         0|            0|            0|  0.00%|            # mypy isn't aware that "_metadata" exists in state_dict
  1382|         1|  1.40667e-05|  1.40667e-05|  0.00%|            state_dict._metadata = metadata  # type: ignore[attr-defined]
  1383|         0|            0|            0|  0.00%|
  1384|        69|  0.000424623|  6.15396e-06|  0.00%|        def load(module, prefix=''):
  1385|        68|   0.00057888|  8.51295e-06|  0.00%|            local_metadata = {} if metadata is None else metadata.get(prefix[:-1], {})
  1386|       136|   0.00162935|  1.19805e-05|  0.00%|            module._load_from_state_dict(
(call)|        48|     0.111487|   0.00232265|  0.21%|# /opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1276 _load_from_state_dict
(call)|        20|    0.0618911|   0.00309455|  0.12%|# /opt/conda/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py:87 _load_from_state_dict
  1387|        68|  0.000313282|  4.60709e-06|  0.00%|                state_dict, prefix, local_metadata, True, missing_keys, unexpected_keys, error_msgs)
  1388|       135|  0.000896692|  6.64217e-06|  0.00%|            for name, child in module._modules.items():
  1389|        67|  0.000294209|  4.39117e-06|  0.00%|                if child is not None:
  1390|        67|   0.00105739|  1.57819e-05|  0.00%|                    load(child, prefix + name + '.')
(call)|        67|     0.173636|   0.00259159|  0.32%|# /opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1384 load
  1391|         0|            0|            0|  0.00%|
  1392|         1|  3.67165e-05|  3.67165e-05|  0.00%|        load(self)
(call)|         1|     0.178547|     0.178547|  0.33%|# /opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1384 load
  1393|         1|  1.43051e-05|  1.43051e-05|  0.00%|        del load
  1394|         0|            0|            0|  0.00%|
  1395|         1|  8.10623e-06|  8.10623e-06|  0.00%|        if strict:
  1396|         1|   1.0252e-05|   1.0252e-05|  0.00%|            if len(unexpected_keys) > 0:
  1397|         0|            0|            0|  0.00%|                error_msgs.insert(
  1398|         0|            0|            0|  0.00%|                    0, 'Unexpected key(s) in state_dict: {}. '.format(
  1399|         0|            0|            0|  0.00%|                        ', '.join('"{}"'.format(k) for k in unexpected_keys)))
  1400|         1|  9.53674e-06|  9.53674e-06|  0.00%|            if len(missing_keys) > 0:
  1401|         0|            0|            0|  0.00%|                error_msgs.insert(
  1402|         0|            0|            0|  0.00%|                    0, 'Missing key(s) in state_dict: {}. '.format(
  1403|         0|            0|            0|  0.00%|                        ', '.join('"{}"'.format(k) for k in missing_keys)))
  1404|         0|            0|            0|  0.00%|
  1405|         1|  9.53674e-06|  9.53674e-06|  0.00%|        if len(error_msgs) > 0:
  1406|         0|            0|            0|  0.00%|            raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
  1407|         0|            0|            0|  0.00%|                               self.__class__.__name__, "\n\t".join(error_msgs)))
  1408|         1|  5.05447e-05|  5.05447e-05|  0.00%|        return _IncompatibleKeys(missing_keys, unexpected_keys)
(call)|         1|  3.24249e-05|  3.24249e-05|  0.00%|# <string>_0:1 __new__
  1409|         0|            0|            0|  0.00%|
  1410|         0|            0|            0|  0.00%|    def _named_members(self, get_members_fn, prefix='', recurse=True):
  1411|         0|            0|            0|  0.00%|        r"""Helper method for yielding various names + members of modules."""
  1412|         0|            0|            0|  0.00%|        memo = set()
  1413|         0|            0|            0|  0.00%|        modules = self.named_modules(prefix=prefix) if recurse else [(prefix, self)]
  1414|         0|            0|            0|  0.00%|        for module_prefix, module in modules:
  1415|         0|            0|            0|  0.00%|            members = get_members_fn(module)
  1416|         0|            0|            0|  0.00%|            for k, v in members:
  1417|         0|            0|            0|  0.00%|                if v is None or v in memo:
  1418|         0|            0|            0|  0.00%|                    continue
  1419|         0|            0|            0|  0.00%|                memo.add(v)
  1420|         0|            0|            0|  0.00%|                name = module_prefix + ('.' if module_prefix else '') + k
  1421|         0|            0|            0|  0.00%|                yield name, v
  1422|         0|            0|            0|  0.00%|
  1423|         0|            0|            0|  0.00%|    def parameters(self, recurse: bool = True) -> Iterator[Parameter]:
  1424|         0|            0|            0|  0.00%|        r"""Returns an iterator over module parameters.
  1425|         0|            0|            0|  0.00%|
  1426|         0|            0|            0|  0.00%|        This is typically passed to an optimizer.
  1427|         0|            0|            0|  0.00%|
  1428|         0|            0|            0|  0.00%|        Args:
  1429|         0|            0|            0|  0.00%|            recurse (bool): if True, then yields parameters of this module
  1430|         0|            0|            0|  0.00%|                and all submodules. Otherwise, yields only parameters that
  1431|         0|            0|            0|  0.00%|                are direct members of this module.
  1432|         0|            0|            0|  0.00%|
  1433|         0|            0|            0|  0.00%|        Yields:
  1434|         0|            0|            0|  0.00%|            Parameter: module parameter
  1435|         0|            0|            0|  0.00%|
  1436|         0|            0|            0|  0.00%|        Example::
  1437|         0|            0|            0|  0.00%|
  1438|         0|            0|            0|  0.00%|            >>> for param in model.parameters():
  1439|         0|            0|            0|  0.00%|            >>>     print(type(param), param.size())
  1440|         0|            0|            0|  0.00%|            <class 'torch.Tensor'> (20L,)
  1441|         0|            0|            0|  0.00%|            <class 'torch.Tensor'> (20L, 1L, 5L, 5L)
  1442|         0|            0|            0|  0.00%|
  1443|         0|            0|            0|  0.00%|        """
  1444|         0|            0|            0|  0.00%|        for name, param in self.named_parameters(recurse=recurse):
  1445|         0|            0|            0|  0.00%|            yield param
  1446|         0|            0|            0|  0.00%|
  1447|         0|            0|            0|  0.00%|    def named_parameters(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, Parameter]]:
  1448|         0|            0|            0|  0.00%|        r"""Returns an iterator over module parameters, yielding both the
  1449|         0|            0|            0|  0.00%|        name of the parameter as well as the parameter itself.
  1450|         0|            0|            0|  0.00%|
  1451|         0|            0|            0|  0.00%|        Args:
  1452|         0|            0|            0|  0.00%|            prefix (str): prefix to prepend to all parameter names.
  1453|         0|            0|            0|  0.00%|            recurse (bool): if True, then yields parameters of this module
  1454|         0|            0|            0|  0.00%|                and all submodules. Otherwise, yields only parameters that
  1455|         0|            0|            0|  0.00%|                are direct members of this module.
  1456|         0|            0|            0|  0.00%|
  1457|         0|            0|            0|  0.00%|        Yields:
  1458|         0|            0|            0|  0.00%|            (string, Parameter): Tuple containing the name and parameter
  1459|         0|            0|            0|  0.00%|
  1460|         0|            0|            0|  0.00%|        Example::
  1461|         0|            0|            0|  0.00%|
  1462|         0|            0|            0|  0.00%|            >>> for name, param in self.named_parameters():
  1463|         0|            0|            0|  0.00%|            >>>    if name in ['bias']:
  1464|         0|            0|            0|  0.00%|            >>>        print(param.size())
  1465|         0|            0|            0|  0.00%|
  1466|         0|            0|            0|  0.00%|        """
  1467|         0|            0|            0|  0.00%|        gen = self._named_members(
  1468|         0|            0|            0|  0.00%|            lambda module: module._parameters.items(),
  1469|         0|            0|            0|  0.00%|            prefix=prefix, recurse=recurse)
  1470|         0|            0|            0|  0.00%|        for elem in gen:
  1471|         0|            0|            0|  0.00%|            yield elem
  1472|         0|            0|            0|  0.00%|
  1473|         0|            0|            0|  0.00%|    def buffers(self, recurse: bool = True) -> Iterator[Tensor]:
  1474|         0|            0|            0|  0.00%|        r"""Returns an iterator over module buffers.
  1475|         0|            0|            0|  0.00%|
  1476|         0|            0|            0|  0.00%|        Args:
  1477|         0|            0|            0|  0.00%|            recurse (bool): if True, then yields buffers of this module
  1478|         0|            0|            0|  0.00%|                and all submodules. Otherwise, yields only buffers that
  1479|         0|            0|            0|  0.00%|                are direct members of this module.
  1480|         0|            0|            0|  0.00%|
  1481|         0|            0|            0|  0.00%|        Yields:
  1482|         0|            0|            0|  0.00%|            torch.Tensor: module buffer
  1483|         0|            0|            0|  0.00%|
  1484|         0|            0|            0|  0.00%|        Example::
  1485|         0|            0|            0|  0.00%|
  1486|         0|            0|            0|  0.00%|            >>> for buf in model.buffers():
  1487|         0|            0|            0|  0.00%|            >>>     print(type(buf), buf.size())
  1488|         0|            0|            0|  0.00%|            <class 'torch.Tensor'> (20L,)
  1489|         0|            0|            0|  0.00%|            <class 'torch.Tensor'> (20L, 1L, 5L, 5L)
  1490|         0|            0|            0|  0.00%|
  1491|         0|            0|            0|  0.00%|        """
  1492|         0|            0|            0|  0.00%|        for _, buf in self.named_buffers(recurse=recurse):
  1493|         0|            0|            0|  0.00%|            yield buf
  1494|         0|            0|            0|  0.00%|
  1495|         0|            0|            0|  0.00%|    def named_buffers(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, Tensor]]:
  1496|         0|            0|            0|  0.00%|        r"""Returns an iterator over module buffers, yielding both the
  1497|         0|            0|            0|  0.00%|        name of the buffer as well as the buffer itself.
  1498|         0|            0|            0|  0.00%|
  1499|         0|            0|            0|  0.00%|        Args:
  1500|         0|            0|            0|  0.00%|            prefix (str): prefix to prepend to all buffer names.
  1501|         0|            0|            0|  0.00%|            recurse (bool): if True, then yields buffers of this module
  1502|         0|            0|            0|  0.00%|                and all submodules. Otherwise, yields only buffers that
  1503|         0|            0|            0|  0.00%|                are direct members of this module.
  1504|         0|            0|            0|  0.00%|
  1505|         0|            0|            0|  0.00%|        Yields:
  1506|         0|            0|            0|  0.00%|            (string, torch.Tensor): Tuple containing the name and buffer
  1507|         0|            0|            0|  0.00%|
  1508|         0|            0|            0|  0.00%|        Example::
  1509|         0|            0|            0|  0.00%|
  1510|         0|            0|            0|  0.00%|            >>> for name, buf in self.named_buffers():
  1511|         0|            0|            0|  0.00%|            >>>    if name in ['running_var']:
  1512|         0|            0|            0|  0.00%|            >>>        print(buf.size())
  1513|         0|            0|            0|  0.00%|
  1514|         0|            0|            0|  0.00%|        """
  1515|         0|            0|            0|  0.00%|        gen = self._named_members(
  1516|         0|            0|            0|  0.00%|            lambda module: module._buffers.items(),
  1517|         0|            0|            0|  0.00%|            prefix=prefix, recurse=recurse)
  1518|         0|            0|            0|  0.00%|        for elem in gen:
  1519|         0|            0|            0|  0.00%|            yield elem
  1520|         0|            0|            0|  0.00%|
  1521|       136|   0.00063324|  4.65617e-06|  0.00%|    def children(self) -> Iterator['Module']:
  1522|         0|            0|            0|  0.00%|        r"""Returns an iterator over immediate children modules.
  1523|         0|            0|            0|  0.00%|
  1524|         0|            0|            0|  0.00%|        Yields:
  1525|         0|            0|            0|  0.00%|            Module: a child module
  1526|         0|            0|            0|  0.00%|        """
  1527|       270|   0.00349188|  1.29329e-05|  0.01%|        for name, module in self.named_children():
(call)|       270|   0.00508165|  1.88209e-05|  0.01%|# /opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1530 named_children
  1528|       268|   0.00103736|  3.87074e-06|  0.00%|            yield module
  1529|         0|            0|            0|  0.00%|
  1530|       136|   0.00062108|  4.56677e-06|  0.00%|    def named_children(self) -> Iterator[Tuple[str, 'Module']]:
  1531|         0|            0|            0|  0.00%|        r"""Returns an iterator over immediate children modules, yielding both
  1532|         0|            0|            0|  0.00%|        the name of the module as well as the module itself.
  1533|         0|            0|            0|  0.00%|
  1534|         0|            0|            0|  0.00%|        Yields:
  1535|         0|            0|            0|  0.00%|            (string, Module): Tuple containing a name and child module
  1536|         0|            0|            0|  0.00%|
  1537|         0|            0|            0|  0.00%|        Example::
  1538|         0|            0|            0|  0.00%|
  1539|         0|            0|            0|  0.00%|            >>> for name, module in model.named_children():
  1540|         0|            0|            0|  0.00%|            >>>     if name in ['conv4', 'conv5']:
  1541|         0|            0|            0|  0.00%|            >>>         print(module)
  1542|         0|            0|            0|  0.00%|
  1543|         0|            0|            0|  0.00%|        """
  1544|       136|  0.000629187|  4.62637e-06|  0.00%|        memo = set()
  1545|       270|   0.00149226|   5.5269e-06|  0.00%|        for name, module in self._modules.items():
  1546|       134|  0.000531435|  3.96593e-06|  0.00%|            if module is not None and module not in memo:
  1547|       134|    0.0007658|  5.71493e-06|  0.00%|                memo.add(module)
  1548|       268|   0.00104189|  3.88765e-06|  0.00%|                yield name, module
  1549|         0|            0|            0|  0.00%|
  1550|         0|            0|            0|  0.00%|    def modules(self) -> Iterator['Module']:
  1551|         0|            0|            0|  0.00%|        r"""Returns an iterator over all modules in the network.
  1552|         0|            0|            0|  0.00%|
  1553|         0|            0|            0|  0.00%|        Yields:
  1554|         0|            0|            0|  0.00%|            Module: a module in the network
  1555|         0|            0|            0|  0.00%|
  1556|         0|            0|            0|  0.00%|        Note:
  1557|         0|            0|            0|  0.00%|            Duplicate modules are returned only once. In the following
  1558|         0|            0|            0|  0.00%|            example, ``l`` will be returned only once.
  1559|         0|            0|            0|  0.00%|
  1560|         0|            0|            0|  0.00%|        Example::
  1561|         0|            0|            0|  0.00%|
  1562|         0|            0|            0|  0.00%|            >>> l = nn.Linear(2, 2)
  1563|         0|            0|            0|  0.00%|            >>> net = nn.Sequential(l, l)
  1564|         0|            0|            0|  0.00%|            >>> for idx, m in enumerate(net.modules()):
  1565|         0|            0|            0|  0.00%|                    print(idx, '->', m)
  1566|         0|            0|            0|  0.00%|
  1567|         0|            0|            0|  0.00%|            0 -> Sequential(
  1568|         0|            0|            0|  0.00%|              (0): Linear(in_features=2, out_features=2, bias=True)
  1569|         0|            0|            0|  0.00%|              (1): Linear(in_features=2, out_features=2, bias=True)
  1570|         0|            0|            0|  0.00%|            )
  1571|         0|            0|            0|  0.00%|            1 -> Linear(in_features=2, out_features=2, bias=True)
  1572|         0|            0|            0|  0.00%|
  1573|         0|            0|            0|  0.00%|        """
  1574|         0|            0|            0|  0.00%|        for _, module in self.named_modules():
  1575|         0|            0|            0|  0.00%|            yield module
  1576|         0|            0|            0|  0.00%|
  1577|         0|            0|            0|  0.00%|    def named_modules(self, memo: Optional[Set['Module']] = None, prefix: str = '', remove_duplicate: bool = True):
  1578|         0|            0|            0|  0.00%|        r"""Returns an iterator over all modules in the network, yielding
  1579|         0|            0|            0|  0.00%|        both the name of the module as well as the module itself.
  1580|         0|            0|            0|  0.00%|
  1581|         0|            0|            0|  0.00%|        Args:
  1582|         0|            0|            0|  0.00%|            memo: a memo to store the set of modules already added to the result
  1583|         0|            0|            0|  0.00%|            prefix: a prefix that will be added to the name of the module
  1584|         0|            0|            0|  0.00%|            remove_duplicate: whether to remove the duplicated module instances in the result
  1585|         0|            0|            0|  0.00%|            or not
  1586|         0|            0|            0|  0.00%|
  1587|         0|            0|            0|  0.00%|        Yields:
  1588|         0|            0|            0|  0.00%|            (string, Module): Tuple of name and module
  1589|         0|            0|            0|  0.00%|
  1590|         0|            0|            0|  0.00%|        Note:
  1591|         0|            0|            0|  0.00%|            Duplicate modules are returned only once. In the following
  1592|         0|            0|            0|  0.00%|            example, ``l`` will be returned only once.
  1593|         0|            0|            0|  0.00%|
  1594|         0|            0|            0|  0.00%|        Example::
  1595|         0|            0|            0|  0.00%|
  1596|         0|            0|            0|  0.00%|            >>> l = nn.Linear(2, 2)
  1597|         0|            0|            0|  0.00%|            >>> net = nn.Sequential(l, l)
  1598|         0|            0|            0|  0.00%|            >>> for idx, m in enumerate(net.named_modules()):
  1599|         0|            0|            0|  0.00%|                    print(idx, '->', m)
  1600|         0|            0|            0|  0.00%|
  1601|         0|            0|            0|  0.00%|            0 -> ('', Sequential(
  1602|         0|            0|            0|  0.00%|              (0): Linear(in_features=2, out_features=2, bias=True)
  1603|         0|            0|            0|  0.00%|              (1): Linear(in_features=2, out_features=2, bias=True)
  1604|         0|            0|            0|  0.00%|            ))
  1605|         0|            0|            0|  0.00%|            1 -> ('0', Linear(in_features=2, out_features=2, bias=True))
  1606|         0|            0|            0|  0.00%|
  1607|         0|            0|            0|  0.00%|        """
  1608|         0|            0|            0|  0.00%|
  1609|         0|            0|            0|  0.00%|        if memo is None:
  1610|         0|            0|            0|  0.00%|            memo = set()
  1611|         0|            0|            0|  0.00%|        if self not in memo:
  1612|         0|            0|            0|  0.00%|            if remove_duplicate:
  1613|         0|            0|            0|  0.00%|                memo.add(self)
  1614|         0|            0|            0|  0.00%|            yield prefix, self
  1615|         0|            0|            0|  0.00%|            for name, module in self._modules.items():
  1616|         0|            0|            0|  0.00%|                if module is None:
  1617|         0|            0|            0|  0.00%|                    continue
  1618|         0|            0|            0|  0.00%|                submodule_prefix = prefix + ('.' if prefix else '') + name
  1619|         0|            0|            0|  0.00%|                for m in module.named_modules(memo, submodule_prefix, remove_duplicate):
  1620|         0|            0|            0|  0.00%|                    yield m
  1621|         0|            0|            0|  0.00%|
  1622|       136|  0.000609398|  4.48087e-06|  0.00%|    def train(self: T, mode: bool = True) -> T:
  1623|         0|            0|            0|  0.00%|        r"""Sets the module in training mode.
  1624|         0|            0|            0|  0.00%|
  1625|         0|            0|            0|  0.00%|        This has any effect only on certain modules. See documentations of
  1626|         0|            0|            0|  0.00%|        particular modules for details of their behaviors in training/evaluation
  1627|         0|            0|            0|  0.00%|        mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,
  1628|         0|            0|            0|  0.00%|        etc.
  1629|         0|            0|            0|  0.00%|
  1630|         0|            0|            0|  0.00%|        Args:
  1631|         0|            0|            0|  0.00%|            mode (bool): whether to set training mode (``True``) or evaluation
  1632|         0|            0|            0|  0.00%|                         mode (``False``). Default: ``True``.
  1633|         0|            0|            0|  0.00%|
  1634|         0|            0|            0|  0.00%|        Returns:
  1635|         0|            0|            0|  0.00%|            Module: self
  1636|         0|            0|            0|  0.00%|        """
  1637|       136|  0.000849247|  6.24446e-06|  0.00%|        if not isinstance(mode, bool):
  1638|         0|            0|            0|  0.00%|            raise ValueError("training mode is expected to be boolean")
  1639|       136|   0.00194693|  1.43156e-05|  0.00%|        self.training = mode
(call)|       136|   0.00865722|   6.3656e-05|  0.02%|# /opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1133 __setattr__
  1640|       270|   0.00336981|  1.24808e-05|  0.01%|        for module in self.children():
(call)|       270|    0.0102441|  3.79412e-05|  0.02%|# /opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1521 children
  1641|       134|    0.0016973|  1.26664e-05|  0.00%|            module.train(mode)
(call)|       134|     0.026123|  0.000194948|  0.05%|# /opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1622 train
  1642|       136|  0.000546932|  4.02156e-06|  0.00%|        return self
  1643|         0|            0|            0|  0.00%|
  1644|         1|  1.43051e-05|  1.43051e-05|  0.00%|    def eval(self: T) -> T:
  1645|         0|            0|            0|  0.00%|        r"""Sets the module in evaluation mode.
  1646|         0|            0|            0|  0.00%|
  1647|         0|            0|            0|  0.00%|        This has any effect only on certain modules. See documentations of
  1648|         0|            0|            0|  0.00%|        particular modules for details of their behaviors in training/evaluation
  1649|         0|            0|            0|  0.00%|        mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,
  1650|         0|            0|            0|  0.00%|        etc.
  1651|         0|            0|            0|  0.00%|
  1652|         0|            0|            0|  0.00%|        This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.
  1653|         0|            0|            0|  0.00%|
  1654|         0|            0|            0|  0.00%|        See :ref:`locally-disable-grad-doc` for a comparison between
  1655|         0|            0|            0|  0.00%|        `.eval()` and several similar mechanisms that may be confused with it.
  1656|         0|            0|            0|  0.00%|
  1657|         0|            0|            0|  0.00%|        Returns:
  1658|         0|            0|            0|  0.00%|            Module: self
  1659|         0|            0|            0|  0.00%|        """
  1660|         1|  2.81334e-05|  2.81334e-05|  0.00%|        return self.train(False)
(call)|         1|    0.0140588|    0.0140588|  0.03%|# /opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1622 train
  1661|         0|            0|            0|  0.00%|
  1662|         0|            0|            0|  0.00%|    def requires_grad_(self: T, requires_grad: bool = True) -> T:
  1663|         0|            0|            0|  0.00%|        r"""Change if autograd should record operations on parameters in this
  1664|         0|            0|            0|  0.00%|        module.
  1665|         0|            0|            0|  0.00%|
  1666|         0|            0|            0|  0.00%|        This method sets the parameters' :attr:`requires_grad` attributes
  1667|         0|            0|            0|  0.00%|        in-place.
  1668|         0|            0|            0|  0.00%|
  1669|         0|            0|            0|  0.00%|        This method is helpful for freezing part of the module for finetuning
  1670|         0|            0|            0|  0.00%|        or training parts of a model individually (e.g., GAN training).
  1671|         0|            0|            0|  0.00%|
  1672|         0|            0|            0|  0.00%|        See :ref:`locally-disable-grad-doc` for a comparison between
  1673|         0|            0|            0|  0.00%|        `.requires_grad_()` and several similar mechanisms that may be confused with it.
  1674|         0|            0|            0|  0.00%|
  1675|         0|            0|            0|  0.00%|        Args:
  1676|         0|            0|            0|  0.00%|            requires_grad (bool): whether autograd should record operations on
  1677|         0|            0|            0|  0.00%|                                  parameters in this module. Default: ``True``.
  1678|         0|            0|            0|  0.00%|
  1679|         0|            0|            0|  0.00%|        Returns:
  1680|         0|            0|            0|  0.00%|            Module: self
  1681|         0|            0|            0|  0.00%|        """
  1682|         0|            0|            0|  0.00%|        for p in self.parameters():
  1683|         0|            0|            0|  0.00%|            p.requires_grad_(requires_grad)
  1684|         0|            0|            0|  0.00%|        return self
  1685|         0|            0|            0|  0.00%|
  1686|         0|            0|            0|  0.00%|    def zero_grad(self, set_to_none: bool = False) -> None:
  1687|         0|            0|            0|  0.00%|        r"""Sets gradients of all model parameters to zero. See similar function
  1688|         0|            0|            0|  0.00%|        under :class:`torch.optim.Optimizer` for more context.
  1689|         0|            0|            0|  0.00%|
  1690|         0|            0|            0|  0.00%|        Args:
  1691|         0|            0|            0|  0.00%|            set_to_none (bool): instead of setting to zero, set the grads to None.
  1692|         0|            0|            0|  0.00%|                See :meth:`torch.optim.Optimizer.zero_grad` for details.
  1693|         0|            0|            0|  0.00%|        """
  1694|         0|            0|            0|  0.00%|        if getattr(self, '_is_replica', False):
  1695|         0|            0|            0|  0.00%|            warnings.warn(
  1696|         0|            0|            0|  0.00%|                "Calling .zero_grad() from a module created with nn.DataParallel() has no effect. "
  1697|         0|            0|            0|  0.00%|                "The parameters are copied (in a differentiable manner) from the original module. "
  1698|         0|            0|            0|  0.00%|                "This means they are not leaf nodes in autograd and so don't accumulate gradients. "
  1699|         0|            0|            0|  0.00%|                "If you need gradients in your forward method, consider using autograd.grad instead.")
  1700|         0|            0|            0|  0.00%|
  1701|         0|            0|            0|  0.00%|        for p in self.parameters():
  1702|         0|            0|            0|  0.00%|            if p.grad is not None:
  1703|         0|            0|            0|  0.00%|                if set_to_none:
  1704|         0|            0|            0|  0.00%|                    p.grad = None
  1705|         0|            0|            0|  0.00%|                else:
  1706|         0|            0|            0|  0.00%|                    if p.grad.grad_fn is not None:
  1707|         0|            0|            0|  0.00%|                        p.grad.detach_()
  1708|         0|            0|            0|  0.00%|                    else:
  1709|         0|            0|            0|  0.00%|                        p.grad.requires_grad_(False)
  1710|         0|            0|            0|  0.00%|                    p.grad.zero_()
  1711|         0|            0|            0|  0.00%|
  1712|         0|            0|            0|  0.00%|    def share_memory(self: T) -> T:
  1713|         0|            0|            0|  0.00%|        r"""See :meth:`torch.Tensor.share_memory_`"""
  1714|         0|            0|            0|  0.00%|        return self._apply(lambda t: t.share_memory_())
  1715|         0|            0|            0|  0.00%|
  1716|         0|            0|            0|  0.00%|    def _get_name(self):
  1717|         0|            0|            0|  0.00%|        return self.__class__.__name__
  1718|         0|            0|            0|  0.00%|
  1719|         0|            0|            0|  0.00%|    def extra_repr(self) -> str:
  1720|         0|            0|            0|  0.00%|        r"""Set the extra representation of the module
  1721|         0|            0|            0|  0.00%|
  1722|         0|            0|            0|  0.00%|        To print customized extra information, you should re-implement
  1723|         0|            0|            0|  0.00%|        this method in your own modules. Both single-line and multi-line
  1724|         0|            0|            0|  0.00%|        strings are acceptable.
  1725|         0|            0|            0|  0.00%|        """
  1726|         0|            0|            0|  0.00%|        return ''
  1727|         0|            0|            0|  0.00%|
  1728|         0|            0|            0|  0.00%|    def __repr__(self):
  1729|         0|            0|            0|  0.00%|        # We treat the extra repr like the sub-module, one item per line
  1730|         0|            0|            0|  0.00%|        extra_lines = []
  1731|         0|            0|            0|  0.00%|        extra_repr = self.extra_repr()
  1732|         0|            0|            0|  0.00%|        # empty string will be split into list ['']
  1733|         0|            0|            0|  0.00%|        if extra_repr:
  1734|         0|            0|            0|  0.00%|            extra_lines = extra_repr.split('\n')
  1735|         0|            0|            0|  0.00%|        child_lines = []
  1736|         0|            0|            0|  0.00%|        for key, module in self._modules.items():
  1737|         0|            0|            0|  0.00%|            mod_str = repr(module)
  1738|         0|            0|            0|  0.00%|            mod_str = _addindent(mod_str, 2)
  1739|         0|            0|            0|  0.00%|            child_lines.append('(' + key + '): ' + mod_str)
  1740|         0|            0|            0|  0.00%|        lines = extra_lines + child_lines
  1741|         0|            0|            0|  0.00%|
  1742|         0|            0|            0|  0.00%|        main_str = self._get_name() + '('
  1743|         0|            0|            0|  0.00%|        if lines:
  1744|         0|            0|            0|  0.00%|            # simple one-liner info, which most builtin Modules will use
  1745|         0|            0|            0|  0.00%|            if len(extra_lines) == 1 and not child_lines:
  1746|         0|            0|            0|  0.00%|                main_str += extra_lines[0]
  1747|         0|            0|            0|  0.00%|            else:
  1748|         0|            0|            0|  0.00%|                main_str += '\n  ' + '\n  '.join(lines) + '\n'
  1749|         0|            0|            0|  0.00%|
  1750|         0|            0|            0|  0.00%|        main_str += ')'
  1751|         0|            0|            0|  0.00%|        return main_str
  1752|         0|            0|            0|  0.00%|
  1753|         0|            0|            0|  0.00%|    def __dir__(self):
  1754|         0|            0|            0|  0.00%|        module_attrs = dir(self.__class__)
  1755|         0|            0|            0|  0.00%|        attrs = list(self.__dict__.keys())
  1756|         0|            0|            0|  0.00%|        parameters = list(self._parameters.keys())
  1757|         0|            0|            0|  0.00%|        modules = list(self._modules.keys())
  1758|         0|            0|            0|  0.00%|        buffers = list(self._buffers.keys())
  1759|         0|            0|            0|  0.00%|        keys = module_attrs + attrs + parameters + modules + buffers
  1760|         0|            0|            0|  0.00%|
  1761|         0|            0|            0|  0.00%|        # Eliminate attrs that are not legal Python variable names
  1762|         0|            0|            0|  0.00%|        keys = [key for key in keys if not key[0].isdigit()]
  1763|         0|            0|            0|  0.00%|
  1764|         0|            0|            0|  0.00%|        return sorted(keys)
  1765|         0|            0|            0|  0.00%|
  1766|         0|            0|            0|  0.00%|    def _replicate_for_data_parallel(self):
  1767|         0|            0|            0|  0.00%|        replica = self.__new__(type(self))
  1768|         0|            0|            0|  0.00%|        replica.__dict__ = self.__dict__.copy()
  1769|         0|            0|            0|  0.00%|
  1770|         0|            0|            0|  0.00%|        # replicas do not have parameters themselves, the replicas reference the original
  1771|         0|            0|            0|  0.00%|        # module.
  1772|         0|            0|            0|  0.00%|        replica._parameters = OrderedDict()
  1773|         0|            0|            0|  0.00%|        replica._buffers = replica._buffers.copy()
  1774|         0|            0|            0|  0.00%|        replica._modules = replica._modules.copy()
  1775|         0|            0|            0|  0.00%|        replica._is_replica = True
  1776|         0|            0|            0|  0.00%|
  1777|         0|            0|            0|  0.00%|        return replica
File: /opt/conda/lib/python3.8/multiprocessing/popen_fork.py
File duration: 1.01159s (1.89%)
Line #|      Hits|         Time| Time per hit|      %|Source code
------+----------+-------------+-------------+-------+-----------
     1|         0|            0|            0|  0.00%|import os
     2|         0|            0|            0|  0.00%|import signal
     3|         0|            0|            0|  0.00%|
     4|         0|            0|            0|  0.00%|from . import util
     5|         0|            0|            0|  0.00%|
     6|         0|            0|            0|  0.00%|__all__ = ['Popen']
     7|         0|            0|            0|  0.00%|
     8|         0|            0|            0|  0.00%|#
     9|         0|            0|            0|  0.00%|# Start child process using fork
    10|         0|            0|            0|  0.00%|#
    11|         0|            0|            0|  0.00%|
    12|         0|            0|            0|  0.00%|class Popen(object):
    13|         0|            0|            0|  0.00%|    method = 'fork'
    14|         0|            0|            0|  0.00%|
    15|         8|  4.36306e-05|  5.45382e-06|  0.00%|    def __init__(self, process_obj):
    16|         8|   0.00016427|  2.05338e-05|  0.00%|        util._flush_std_streams()
(call)|         8|  0.000432491|  5.40614e-05|  0.00%|# /opt/conda/lib/python3.8/multiprocessing/util.py:433 _flush_std_streams
    17|         8|  3.91006e-05|  4.88758e-06|  0.00%|        self.returncode = None
    18|         8|  3.43323e-05|  4.29153e-06|  0.00%|        self.finalizer = None
    19|         8|  0.000535727|  6.69658e-05|  0.00%|        self._launch(process_obj)
(call)|         8|      1.01478|     0.126847|  1.89%|# /opt/conda/lib/python3.8/multiprocessing/popen_fork.py:66 _launch
    20|         0|            0|            0|  0.00%|
    21|         0|            0|            0|  0.00%|    def duplicate_for_child(self, fd):
    22|         0|            0|            0|  0.00%|        return fd
    23|         0|            0|            0|  0.00%|
    24|        28|  0.000172138|  6.14779e-06|  0.00%|    def poll(self, flag=os.WNOHANG):
    25|        28|  0.000140667|  5.02382e-06|  0.00%|        if self.returncode is None:
    26|        20|  8.60691e-05|  4.30346e-06|  0.00%|            try:
    27|        20|  0.000384092|  1.92046e-05|  0.00%|                pid, sts = os.waitpid(self.pid, flag)
    28|         0|            0|            0|  0.00%|            except OSError as e:
    29|         0|            0|            0|  0.00%|                # Child process not yet created. See #1731717
    30|         0|            0|            0|  0.00%|                # e.errno == errno.ECHILD == 10
    31|         0|            0|            0|  0.00%|                return None
    32|        20|  0.000115871|  5.79357e-06|  0.00%|            if pid == self.pid:
    33|         8|  6.05583e-05|  7.56979e-06|  0.00%|                if os.WIFSIGNALED(sts):
    34|         0|            0|            0|  0.00%|                    self.returncode = -os.WTERMSIG(sts)
    35|         0|            0|            0|  0.00%|                else:
    36|         8|  6.62804e-05|  8.28505e-06|  0.00%|                    assert os.WIFEXITED(sts), "Status is {:n}".format(sts)
    37|         8|  5.98431e-05|  7.48038e-06|  0.00%|                    self.returncode = os.WEXITSTATUS(sts)
    38|        28|  0.000123024|  4.39371e-06|  0.00%|        return self.returncode
    39|         0|            0|            0|  0.00%|
    40|         8|  4.43459e-05|  5.54323e-06|  0.00%|    def wait(self, timeout=None):
    41|         8|  4.26769e-05|  5.33462e-06|  0.00%|        if self.returncode is None:
    42|         8|  3.52859e-05|  4.41074e-06|  0.00%|            if timeout is not None:
    43|         8|  9.36985e-05|  1.17123e-05|  0.00%|                from multiprocessing.connection import wait
    44|         8|  0.000157356|  1.96695e-05|  0.00%|                if not wait([self.sentinel], timeout):
(call)|         8|     0.125103|    0.0156379|  0.23%|# /opt/conda/lib/python3.8/multiprocessing/connection.py:917 wait
    45|         0|            0|            0|  0.00%|                    return None
    46|         0|            0|            0|  0.00%|            # This shouldn't block if wait() returned successfully.
    47|         8|  0.000172615|  2.15769e-05|  0.00%|            return self.poll(os.WNOHANG if timeout == 0.0 else 0)
(call)|         8|  0.000634432|   7.9304e-05|  0.00%|# /opt/conda/lib/python3.8/multiprocessing/popen_fork.py:24 poll
    48|         0|            0|            0|  0.00%|        return self.returncode
    49|         0|            0|            0|  0.00%|
    50|         0|            0|            0|  0.00%|    def _send_signal(self, sig):
    51|         0|            0|            0|  0.00%|        if self.returncode is None:
    52|         0|            0|            0|  0.00%|            try:
    53|         0|            0|            0|  0.00%|                os.kill(self.pid, sig)
    54|         0|            0|            0|  0.00%|            except ProcessLookupError:
    55|         0|            0|            0|  0.00%|                pass
    56|         0|            0|            0|  0.00%|            except OSError:
    57|         0|            0|            0|  0.00%|                if self.wait(timeout=0.1) is None:
    58|         0|            0|            0|  0.00%|                    raise
    59|         0|            0|            0|  0.00%|
    60|         0|            0|            0|  0.00%|    def terminate(self):
    61|         0|            0|            0|  0.00%|        self._send_signal(signal.SIGTERM)
    62|         0|            0|            0|  0.00%|
    63|         0|            0|            0|  0.00%|    def kill(self):
    64|         0|            0|            0|  0.00%|        self._send_signal(signal.SIGKILL)
    65|         0|            0|            0|  0.00%|
    66|         8|  4.81606e-05|  6.02007e-06|  0.00%|    def _launch(self, process_obj):
    67|         8|  4.17233e-05|  5.21541e-06|  0.00%|        code = 1
    68|         8|  0.000130415|  1.63019e-05|  0.00%|        parent_r, child_w = os.pipe()
    69|         8|  9.87053e-05|  1.23382e-05|  0.00%|        child_r, parent_w = os.pipe()
    70|         8|      1.00676|     0.125845|  1.88%|        self.pid = os.fork()
(call)|         8|  0.000182152|   2.2769e-05|  0.00%|# /opt/conda/lib/python3.8/logging/__init__.py:214 _acquireLock
(call)|         8|   0.00357509|  0.000446886|  0.01%|# /opt/conda/lib/python3.8/logging/__init__.py:223 _releaseLock
    71|         8|  0.000101089|  1.26362e-05|  0.00%|        if self.pid == 0:
    72|         0|            0|            0|  0.00%|            try:
    73|         0|            0|            0|  0.00%|                os.close(parent_r)
    74|         0|            0|            0|  0.00%|                os.close(parent_w)
    75|         0|            0|            0|  0.00%|                code = process_obj._bootstrap(parent_sentinel=child_r)
    76|         0|            0|            0|  0.00%|            finally:
    77|         0|            0|            0|  0.00%|                os._exit(code)
    78|         0|            0|            0|  0.00%|        else:
    79|         8|  0.000393629|  4.92036e-05|  0.00%|            os.close(child_w)
    80|         8|   0.00020504|    2.563e-05|  0.00%|            os.close(child_r)
    81|        16|   0.00106883|  6.68019e-05|  0.00%|            self.finalizer = util.Finalize(self, util.close_fds,
(call)|         8|   0.00199914|  0.000249892|  0.00%|# /opt/conda/lib/python3.8/multiprocessing/util.py:186 __init__
    82|         8|  5.10216e-05|   6.3777e-06|  0.00%|                                           (parent_r, parent_w,))
    83|         8|  0.000123024|   1.5378e-05|  0.00%|            self.sentinel = parent_r
    84|         0|            0|            0|  0.00%|
    85|         0|            0|            0|  0.00%|    def close(self):
    86|         0|            0|            0|  0.00%|        if self.finalizer is not None:
    87|         0|            0|            0|  0.00%|            self.finalizer()
File: /opt/conda/lib/python3.8/site-packages/torch/_tensor.py
File duration: 0.860083s (1.61%)
Line #|      Hits|         Time| Time per hit|      %|Source code
------+----------+-------------+-------------+-------+-----------
     1|         0|            0|            0|  0.00%|from collections import OrderedDict
     2|         0|            0|            0|  0.00%|import functools
     3|         0|            0|            0|  0.00%|from numbers import Number
     4|         0|            0|            0|  0.00%|from typing import Any, Dict, Optional, Tuple, Union
     5|         0|            0|            0|  0.00%|import warnings
     6|         0|            0|            0|  0.00%|import weakref
     7|         0|            0|            0|  0.00%|
     8|         0|            0|            0|  0.00%|import torch
     9|         0|            0|            0|  0.00%|import torch._C as _C
    10|         0|            0|            0|  0.00%|from torch._namedtensor_internals import (
    11|         0|            0|            0|  0.00%|    update_names, check_serializing_named_tensor, resolve_ellipsis,
    12|         0|            0|            0|  0.00%|    unzip_namedshape, single_ellipsis_index, is_ellipsis)
    13|         0|            0|            0|  0.00%|from torch.overrides import (
    14|         0|            0|            0|  0.00%|    has_torch_function, has_torch_function_unary, has_torch_function_variadic,
    15|         0|            0|            0|  0.00%|    handle_torch_function)
    16|         0|            0|            0|  0.00%|import torch.utils.hooks as hooks
    17|         0|            0|            0|  0.00%|
    18|         0|            0|            0|  0.00%|
    19|         0|            0|            0|  0.00%|def _wrap_type_error_to_not_implemented(f):
    20|         0|            0|            0|  0.00%|    # functools.wraps doesn't work well with methods in python 2
    21|         0|            0|            0|  0.00%|    method_assignments = ('__name__', '__doc__')
    22|         0|            0|            0|  0.00%|    assigned = functools.WRAPPER_ASSIGNMENTS
    23|         0|            0|            0|  0.00%|
    24|         0|            0|            0|  0.00%|    @functools.wraps(f, assigned=assigned)
    25|         0|            0|            0|  0.00%|    def wrapped(*args, **kwargs):
    26|         0|            0|            0|  0.00%|        if has_torch_function(args):
    27|         0|            0|            0|  0.00%|            return handle_torch_function(wrapped, args, *args, **kwargs)
    28|         0|            0|            0|  0.00%|        try:
    29|         0|            0|            0|  0.00%|            return f(*args, **kwargs)
    30|         0|            0|            0|  0.00%|        except TypeError:
    31|         0|            0|            0|  0.00%|            return NotImplemented
    32|         0|            0|            0|  0.00%|    return wrapped
    33|         0|            0|            0|  0.00%|
    34|         0|            0|            0|  0.00%|def _rebuild_from_type(func, type, args, dict):
    35|         0|            0|            0|  0.00%|    if type is Tensor:
    36|         0|            0|            0|  0.00%|        return func(*args)
    37|         0|            0|            0|  0.00%|
    38|         0|            0|            0|  0.00%|    ret = func(*args).as_subclass(type)
    39|         0|            0|            0|  0.00%|    ret.__dict__ = dict
    40|         0|            0|            0|  0.00%|    return ret
    41|         0|            0|            0|  0.00%|
    42|         0|            0|            0|  0.00%|
    43|         0|            0|            0|  0.00%|# NB: If you subclass Tensor, and want to share the subclassed class
    44|         0|            0|            0|  0.00%|# across processes, you must also update torch/multiprocessing/reductions.py
    45|         0|            0|            0|  0.00%|# to define a ForkingPickler serialization mode for the class.
    46|         0|            0|            0|  0.00%|#
    47|         0|            0|            0|  0.00%|# NB: If you add a new method to Tensor, you must update
    48|         0|            0|            0|  0.00%|# torch/__init__.py.in to add a type annotation for your method;
    49|         0|            0|            0|  0.00%|# otherwise, it will not show up in autocomplete.
    50|         0|            0|            0|  0.00%|class Tensor(torch._C._TensorBase):
    51|       244|   0.00129151|  5.29309e-06|  0.00%|    def __deepcopy__(self, memo):
    52|       244|   0.00167322|  6.85747e-06|  0.00%|        if has_torch_function_unary(self):
    53|         0|            0|            0|  0.00%|            return handle_torch_function(Tensor.__deepcopy__, (self,), self, memo)
    54|       244|   0.00146341|   5.9976e-06|  0.00%|        if not self.is_leaf:
    55|         0|            0|            0|  0.00%|            raise RuntimeError("Only Tensors created explicitly by the user "
    56|         0|            0|            0|  0.00%|                               "(graph leaves) support the deepcopy protocol at the moment")
    57|       244|   0.00165248|  6.77246e-06|  0.00%|        if id(self) in memo:
    58|         0|            0|            0|  0.00%|            return memo[id(self)]
    59|       244|   0.00577426|   2.3665e-05|  0.01%|        with torch.no_grad():
(call)|       244|   0.00848937|  3.47925e-05|  0.02%|# /opt/conda/lib/python3.8/site-packages/torch/autograd/grad_mode.py:119 __init__
(call)|       244|    0.0099678|  4.08517e-05|  0.02%|# /opt/conda/lib/python3.8/site-packages/torch/autograd/grad_mode.py:124 __enter__
    60|         0|            0|            0|  0.00%|            # TODO: skipping storage copy is wrong for meta, as meta
    61|         0|            0|            0|  0.00%|            # does accurate alias tracking; however, the code below
    62|         0|            0|            0|  0.00%|            # doesn't work because of
    63|         0|            0|            0|  0.00%|            # https://github.com/pytorch/pytorch/issues/47442
    64|       244|   0.00336123|  1.37755e-05|  0.01%|            if self.is_sparse or self.device.type in ['xla', 'mlc', 'meta']:
    65|         0|            0|            0|  0.00%|                new_tensor = self.clone()
    66|         0|            0|            0|  0.00%|            else:
    67|       244|   0.00477314|   1.9562e-05|  0.01%|                new_storage = self.storage().__deepcopy__(memo)
(call)|       244|     0.141763|  0.000580998|  0.26%|# /opt/conda/lib/python3.8/site-packages/torch/storage.py:44 __deepcopy__
    68|       244|   0.00133848|  5.48558e-06|  0.00%|                if self.is_quantized:
    69|         0|            0|            0|  0.00%|                    # quantizer_params can be different type based on torch attribute
    70|         0|            0|            0|  0.00%|                    quantizer_params: Union[Tuple[torch.qscheme, float, int], Tuple[torch.qscheme, Tensor, Tensor, int]]
    71|         0|            0|            0|  0.00%|                    if self.qscheme() == torch.per_tensor_affine:
    72|         0|            0|            0|  0.00%|                        quantizer_params = self.qscheme(), self.q_scale(), self.q_zero_point()
    73|         0|            0|            0|  0.00%|                    elif self.qscheme() in (torch.per_channel_affine, torch.per_channel_affine_float_qparams):
    74|         0|            0|            0|  0.00%|                        quantizer_params = self.qscheme(), \
    75|         0|            0|            0|  0.00%|                            self.q_per_channel_scales(), \
    76|         0|            0|            0|  0.00%|                            self.q_per_channel_zero_points(), \
    77|         0|            0|            0|  0.00%|                            self.q_per_channel_axis()
    78|         0|            0|            0|  0.00%|                    else:
    79|         0|            0|            0|  0.00%|                        raise RuntimeError(f"Unsupported qscheme {self.qscheme()} in deepcopy")
    80|         0|            0|            0|  0.00%|                    new_tensor = torch._utils._rebuild_qtensor(
    81|         0|            0|            0|  0.00%|                        new_storage,
    82|         0|            0|            0|  0.00%|                        self.storage_offset(),
    83|         0|            0|            0|  0.00%|                        self.size(),
    84|         0|            0|            0|  0.00%|                        self.stride(),
    85|         0|            0|            0|  0.00%|                        quantizer_params,
    86|         0|            0|            0|  0.00%|                        self.requires_grad,
    87|         0|            0|            0|  0.00%|                        self._backward_hooks)
    88|         0|            0|            0|  0.00%|                else:
    89|       244|   0.00326729|  1.33905e-05|  0.01%|                    new_tensor = self.new()
    90|       244|   0.00633931|  2.59808e-05|  0.01%|                    new_tensor.set_(new_storage, self.storage_offset(), self.size(), self.stride())
    91|       244|   0.00184345|  7.55513e-06|  0.00%|                    new_tensor.requires_grad = self.requires_grad
    92|       244|   0.00347996|  1.42621e-05|  0.01%|            if self.grad is not None:
(call)|       244|   0.00492072|  2.01669e-05|  0.01%|# /opt/conda/lib/python3.8/site-packages/torch/_tensor.py:967 grad
    93|         0|            0|            0|  0.00%|                new_tensor.grad = self.grad.__deepcopy__(memo)
    94|       244|   0.00168157|  6.89166e-06|  0.00%|            memo[id(self)] = new_tensor
    95|       244|   0.00338268|  1.38635e-05|  0.01%|            return new_tensor
(call)|       244|   0.00870657|  3.56827e-05|  0.02%|# /opt/conda/lib/python3.8/site-packages/torch/autograd/grad_mode.py:128 __exit__
    96|         0|            0|            0|  0.00%|
    97|         0|            0|            0|  0.00%|    def __reduce_ex__(self, proto):
    98|         0|            0|            0|  0.00%|        if type(self) is Tensor:
    99|         0|            0|            0|  0.00%|            return self._reduce_ex_internal(proto)
   100|         0|            0|            0|  0.00%|        relevant_args = (self,)
   101|         0|            0|            0|  0.00%|        from torch.overrides import has_torch_function, handle_torch_function
   102|         0|            0|            0|  0.00%|        if type(self) is not Tensor and has_torch_function(relevant_args):
   103|         0|            0|            0|  0.00%|            return handle_torch_function(Tensor.__reduce_ex__, relevant_args, self, proto)
   104|         0|            0|            0|  0.00%|        func, args = self._reduce_ex_internal(proto)
   105|         0|            0|            0|  0.00%|        return (_rebuild_from_type, (func, type(self), args, self.__dict__))
   106|         0|            0|            0|  0.00%|
   107|         0|            0|            0|  0.00%|    def _reduce_ex_internal(self, proto):
   108|         0|            0|            0|  0.00%|        if has_torch_function_unary(self):
   109|         0|            0|            0|  0.00%|            return handle_torch_function(Tensor.__reduce_ex__, (self,), self, proto)
   110|         0|            0|            0|  0.00%|        check_serializing_named_tensor(self)
   111|         0|            0|            0|  0.00%|        # See Note [Don't serialize hooks]
   112|         0|            0|            0|  0.00%|        torch.utils.hooks.warn_if_has_hooks(self)
   113|         0|            0|            0|  0.00%|        backward_hooks: Dict[Any, Any] = OrderedDict()
   114|         0|            0|            0|  0.00%|        # Note: Numpy array is chosen to be the rebuild component for XLA Tensor.
   115|         0|            0|            0|  0.00%|        # We considered a few options:
   116|         0|            0|            0|  0.00%|        # 1. CPU tensor can't be used here.
   117|         0|            0|            0|  0.00%|        #    Otherwise in torch.load CPU storage is reconstructed with randomly
   118|         0|            0|            0|  0.00%|        #    initialized data, moved onto XLA device, and then storage is updated
   119|         0|            0|            0|  0.00%|        #    to the serialized content. This works perfectly for CPU/CUDA but not XLA.
   120|         0|            0|            0|  0.00%|        #    XLA tensor is disconnected with storage so it doesn't get the update.
   121|         0|            0|            0|  0.00%|        # 2. Python list is not a good fit due to performance reason.
   122|         0|            0|            0|  0.00%|        #    `tolist()` converts every single element in the tensor into python objects
   123|         0|            0|            0|  0.00%|        #    and serialize them one by one.
   124|         0|            0|            0|  0.00%|        if self.device.type == 'xla':
   125|         0|            0|            0|  0.00%|            arg_xla = (self.cpu().numpy(),
   126|         0|            0|            0|  0.00%|                       self.dtype,
   127|         0|            0|            0|  0.00%|                       str(self.device),
   128|         0|            0|            0|  0.00%|                       self.requires_grad)
   129|         0|            0|            0|  0.00%|            return (torch._utils._rebuild_xla_tensor, arg_xla)
   130|         0|            0|            0|  0.00%|        if self.device.type == 'mlc':
   131|         0|            0|            0|  0.00%|            arg_mlc = (self.cpu().numpy(),
   132|         0|            0|            0|  0.00%|                       self.dtype,
   133|         0|            0|            0|  0.00%|                       str(self.device),
   134|         0|            0|            0|  0.00%|                       self.requires_grad)
   135|         0|            0|            0|  0.00%|            return (torch._utils._rebuild_mlc_tensor, arg_mlc)
   136|         0|            0|            0|  0.00%|        if self.is_quantized:
   137|         0|            0|            0|  0.00%|            # quantizer_params can be different type based on torch attribute
   138|         0|            0|            0|  0.00%|            quantizer_params: Union[Tuple[torch.qscheme, float, int], Tuple[Any, Tensor, Tensor, int]]
   139|         0|            0|            0|  0.00%|            if self.qscheme() == torch.per_tensor_affine:
   140|         0|            0|            0|  0.00%|                quantizer_params = (torch.per_tensor_affine,
   141|         0|            0|            0|  0.00%|                                    self.q_scale(),
   142|         0|            0|            0|  0.00%|                                    self.q_zero_point())
   143|         0|            0|            0|  0.00%|            elif self.qscheme() in (torch.per_channel_affine, torch.per_channel_affine_float_qparams):
   144|         0|            0|            0|  0.00%|                # convert scales and zero points to tuple to avoid recursive calls
   145|         0|            0|            0|  0.00%|                # when/if we get multi-axis quantized tensors in the future, the shape
   146|         0|            0|            0|  0.00%|                # is recoverable from the main tensor shape
   147|         0|            0|            0|  0.00%|                quantizer_params = (torch.per_channel_affine,
   148|         0|            0|            0|  0.00%|                                    self.q_per_channel_scales(),
   149|         0|            0|            0|  0.00%|                                    self.q_per_channel_zero_points(),
   150|         0|            0|            0|  0.00%|                                    self.q_per_channel_axis())
   151|         0|            0|            0|  0.00%|            else:
   152|         0|            0|            0|  0.00%|                raise RuntimeError(f"Serialization is not supported for tensors of type {self.qscheme()}")
   153|         0|            0|            0|  0.00%|            args_qtensor = (self.storage(),
   154|         0|            0|            0|  0.00%|                            self.storage_offset(),
   155|         0|            0|            0|  0.00%|                            tuple(self.size()),
   156|         0|            0|            0|  0.00%|                            self.stride(),
   157|         0|            0|            0|  0.00%|                            quantizer_params,
   158|         0|            0|            0|  0.00%|                            self.requires_grad,
   159|         0|            0|            0|  0.00%|                            backward_hooks)
   160|         0|            0|            0|  0.00%|            return (torch._utils._rebuild_qtensor, args_qtensor)
   161|         0|            0|            0|  0.00%|        elif self.is_sparse:
   162|         0|            0|            0|  0.00%|            if self.layout == torch.sparse_coo:
   163|         0|            0|            0|  0.00%|                args_sparse = (self.layout,
   164|         0|            0|            0|  0.00%|                               (self._indices(),
   165|         0|            0|            0|  0.00%|                                self._values(),
   166|         0|            0|            0|  0.00%|                                self.size()))
   167|         0|            0|            0|  0.00%|            else:
   168|         0|            0|            0|  0.00%|                raise NotImplementedError(
   169|         0|            0|            0|  0.00%|                    'sparse tensor __reduce_ex__ for layout `%s`' % (self.layout))
   170|         0|            0|            0|  0.00%|            return (torch._utils._rebuild_sparse_tensor, args_sparse)
   171|         0|            0|            0|  0.00%|        else:
   172|         0|            0|            0|  0.00%|            args = (self.storage(),
   173|         0|            0|            0|  0.00%|                    self.storage_offset(),
   174|         0|            0|            0|  0.00%|                    tuple(self.size()),
   175|         0|            0|            0|  0.00%|                    self.stride(),
   176|         0|            0|            0|  0.00%|                    self.requires_grad,
   177|         0|            0|            0|  0.00%|                    backward_hooks)  # previously was self._backward_hooks
   178|         0|            0|            0|  0.00%|            return (torch._utils._rebuild_tensor_v2, args)
   179|         0|            0|            0|  0.00%|
   180|         0|            0|            0|  0.00%|    def __setstate__(self, state):
   181|         0|            0|            0|  0.00%|        if has_torch_function_unary(self):
   182|         0|            0|            0|  0.00%|            return handle_torch_function(Tensor.__setstate__, (self,), self, state)
   183|         0|            0|            0|  0.00%|        # Warning: this method is NOT called when you torch.load() a tensor;
   184|         0|            0|            0|  0.00%|        # that is managed by _rebuild_tensor_v2
   185|         0|            0|            0|  0.00%|        if not self.is_leaf:
   186|         0|            0|            0|  0.00%|            raise RuntimeError('__setstate__ can be only called on leaf Tensors')
   187|         0|            0|            0|  0.00%|        if len(state) == 4:
   188|         0|            0|            0|  0.00%|            # legacy serialization of Tensor
   189|         0|            0|            0|  0.00%|            self.set_(*state)
   190|         0|            0|            0|  0.00%|            return
   191|         0|            0|            0|  0.00%|        elif len(state) == 5:
   192|         0|            0|            0|  0.00%|            # legacy serialization of Variable
   193|         0|            0|            0|  0.00%|            self.data = state[0]
   194|         0|            0|            0|  0.00%|            state = (state[3], state[4], state[2])
   195|         0|            0|            0|  0.00%|        # The setting of _backward_hooks is expected to be a no-op.
   196|         0|            0|            0|  0.00%|        # See Note [Don't serialize hooks]
   197|         0|            0|            0|  0.00%|        self.requires_grad, _, self._backward_hooks = state
   198|         0|            0|            0|  0.00%|
   199|         0|            0|            0|  0.00%|    def __repr__(self):
   200|         0|            0|            0|  0.00%|        if has_torch_function_unary(self):
   201|         0|            0|            0|  0.00%|            return handle_torch_function(Tensor.__repr__, (self,), self)
   202|         0|            0|            0|  0.00%|        # All strings are unicode in Python 3.
   203|         0|            0|            0|  0.00%|        return torch._tensor_str._str(self)
   204|         0|            0|            0|  0.00%|
   205|        61|  0.000510216|  8.36419e-06|  0.00%|    def backward(self, gradient=None, retain_graph=None, create_graph=False, inputs=None):
   206|         0|            0|            0|  0.00%|        r"""Computes the gradient of current tensor w.r.t. graph leaves.
   207|         0|            0|            0|  0.00%|
   208|         0|            0|            0|  0.00%|        The graph is differentiated using the chain rule. If the tensor is
   209|         0|            0|            0|  0.00%|        non-scalar (i.e. its data has more than one element) and requires
   210|         0|            0|            0|  0.00%|        gradient, the function additionally requires specifying ``gradient``.
   211|         0|            0|            0|  0.00%|        It should be a tensor of matching type and location, that contains
   212|         0|            0|            0|  0.00%|        the gradient of the differentiated function w.r.t. ``self``.
   213|         0|            0|            0|  0.00%|
   214|         0|            0|            0|  0.00%|        This function accumulates gradients in the leaves - you might need to zero
   215|         0|            0|            0|  0.00%|        ``.grad`` attributes or set them to ``None`` before calling it.
   216|         0|            0|            0|  0.00%|        See :ref:`Default gradient layouts<default-grad-layouts>`
   217|         0|            0|            0|  0.00%|        for details on the memory layout of accumulated gradients.
   218|         0|            0|            0|  0.00%|
   219|         0|            0|            0|  0.00%|        .. note::
   220|         0|            0|            0|  0.00%|
   221|         0|            0|            0|  0.00%|            If you run any forward ops, create ``gradient``, and/or call ``backward``
   222|         0|            0|            0|  0.00%|            in a user-specified CUDA stream context, see
   223|         0|            0|            0|  0.00%|            :ref:`Stream semantics of backward passes<bwd-cuda-stream-semantics>`.
   224|         0|            0|            0|  0.00%|
   225|         0|            0|            0|  0.00%|        Args:
   226|         0|            0|            0|  0.00%|            gradient (Tensor or None): Gradient w.r.t. the
   227|         0|            0|            0|  0.00%|                tensor. If it is a tensor, it will be automatically converted
   228|         0|            0|            0|  0.00%|                to a Tensor that does not require grad unless ``create_graph`` is True.
   229|         0|            0|            0|  0.00%|                None values can be specified for scalar Tensors or ones that
   230|         0|            0|            0|  0.00%|                don't require grad. If a None value would be acceptable then
   231|         0|            0|            0|  0.00%|                this argument is optional.
   232|         0|            0|            0|  0.00%|            retain_graph (bool, optional): If ``False``, the graph used to compute
   233|         0|            0|            0|  0.00%|                the grads will be freed. Note that in nearly all cases setting
   234|         0|            0|            0|  0.00%|                this option to True is not needed and often can be worked around
   235|         0|            0|            0|  0.00%|                in a much more efficient way. Defaults to the value of
   236|         0|            0|            0|  0.00%|                ``create_graph``.
   237|         0|            0|            0|  0.00%|            create_graph (bool, optional): If ``True``, graph of the derivative will
   238|         0|            0|            0|  0.00%|                be constructed, allowing to compute higher order derivative
   239|         0|            0|            0|  0.00%|                products. Defaults to ``False``.
   240|         0|            0|            0|  0.00%|            inputs (sequence of Tensor): Inputs w.r.t. which the gradient will be
   241|         0|            0|            0|  0.00%|                accumulated into ``.grad``. All other Tensors will be ignored. If not
   242|         0|            0|            0|  0.00%|                provided, the gradient is accumulated into all the leaf Tensors that were
   243|         0|            0|            0|  0.00%|                used to compute the attr::tensors. All the provided inputs must be leaf
   244|         0|            0|            0|  0.00%|                Tensors.
   245|         0|            0|            0|  0.00%|        """
   246|        61|  0.000504494|  8.27039e-06|  0.00%|        if has_torch_function_unary(self):
   247|         0|            0|            0|  0.00%|            return handle_torch_function(
   248|         0|            0|            0|  0.00%|                Tensor.backward,
   249|         0|            0|            0|  0.00%|                (self,),
   250|         0|            0|            0|  0.00%|                self,
   251|         0|            0|            0|  0.00%|                gradient=gradient,
   252|         0|            0|            0|  0.00%|                retain_graph=retain_graph,
   253|         0|            0|            0|  0.00%|                create_graph=create_graph,
   254|         0|            0|            0|  0.00%|                inputs=inputs)
   255|        61|   0.00414896|  6.80157e-05|  0.01%|        torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
(call)|        61|      31.3149|     0.513359| 58.45%|# /opt/conda/lib/python3.8/site-packages/torch/autograd/__init__.py:68 backward
   256|         0|            0|            0|  0.00%|
   257|         0|            0|            0|  0.00%|    def register_hook(self, hook):
   258|         0|            0|            0|  0.00%|        r"""Registers a backward hook.
   259|         0|            0|            0|  0.00%|
   260|         0|            0|            0|  0.00%|        The hook will be called every time a gradient with respect to the
   261|         0|            0|            0|  0.00%|        Tensor is computed. The hook should have the following signature::
   262|         0|            0|            0|  0.00%|
   263|         0|            0|            0|  0.00%|            hook(grad) -> Tensor or None
   264|         0|            0|            0|  0.00%|
   265|         0|            0|            0|  0.00%|
   266|         0|            0|            0|  0.00%|        The hook should not modify its argument, but it can optionally return
   267|         0|            0|            0|  0.00%|        a new gradient which will be used in place of :attr:`grad`.
   268|         0|            0|            0|  0.00%|
   269|         0|            0|            0|  0.00%|        This function returns a handle with a method ``handle.remove()``
   270|         0|            0|            0|  0.00%|        that removes the hook from the module.
   271|         0|            0|            0|  0.00%|
   272|         0|            0|            0|  0.00%|        Example::
   273|         0|            0|            0|  0.00%|
   274|         0|            0|            0|  0.00%|            >>> v = torch.tensor([0., 0., 0.], requires_grad=True)
   275|         0|            0|            0|  0.00%|            >>> h = v.register_hook(lambda grad: grad * 2)  # double the gradient
   276|         0|            0|            0|  0.00%|            >>> v.backward(torch.tensor([1., 2., 3.]))
   277|         0|            0|            0|  0.00%|            >>> v.grad
   278|         0|            0|            0|  0.00%|
   279|         0|            0|            0|  0.00%|             2
   280|         0|            0|            0|  0.00%|             4
   281|         0|            0|            0|  0.00%|             6
   282|         0|            0|            0|  0.00%|            [torch.FloatTensor of size (3,)]
   283|         0|            0|            0|  0.00%|
   284|         0|            0|            0|  0.00%|            >>> h.remove()  # removes the hook
   285|         0|            0|            0|  0.00%|        """
   286|         0|            0|            0|  0.00%|        if has_torch_function_unary(self):
   287|         0|            0|            0|  0.00%|            return handle_torch_function(Tensor.register_hook, (self,), self, hook)
   288|         0|            0|            0|  0.00%|        if not self.requires_grad:
   289|         0|            0|            0|  0.00%|            raise RuntimeError("cannot register a hook on a tensor that "
   290|         0|            0|            0|  0.00%|                               "doesn't require gradient")
   291|         0|            0|            0|  0.00%|        if self._backward_hooks is None:
   292|         0|            0|            0|  0.00%|            self._backward_hooks = OrderedDict()
   293|         0|            0|            0|  0.00%|            if self.grad_fn is not None:
   294|         0|            0|            0|  0.00%|                self.grad_fn._register_hook_dict(self)
   295|         0|            0|            0|  0.00%|        handle = hooks.RemovableHandle(self._backward_hooks)
   296|         0|            0|            0|  0.00%|        self._backward_hooks[handle.id] = hook
   297|         0|            0|            0|  0.00%|        return handle
   298|         0|            0|            0|  0.00%|
   299|         0|            0|            0|  0.00%|    def reinforce(self, reward):
   300|         0|            0|            0|  0.00%|        def trim(str):
   301|         0|            0|            0|  0.00%|            return '\n'.join([line.strip() for line in str.split('\n')])
   302|         0|            0|            0|  0.00%|
   303|         0|            0|            0|  0.00%|        raise RuntimeError(trim(r"""reinforce() was removed.
   304|         0|            0|            0|  0.00%|            Use torch.distributions instead.
   305|         0|            0|            0|  0.00%|            See https://pytorch.org/docs/master/distributions.html
   306|         0|            0|            0|  0.00%|
   307|         0|            0|            0|  0.00%|            Instead of:
   308|         0|            0|            0|  0.00%|
   309|         0|            0|            0|  0.00%|            probs = policy_network(state)
   310|         0|            0|            0|  0.00%|            action = probs.multinomial()
   311|         0|            0|            0|  0.00%|            next_state, reward = env.step(action)
   312|         0|            0|            0|  0.00%|            action.reinforce(reward)
   313|         0|            0|            0|  0.00%|            action.backward()
   314|         0|            0|            0|  0.00%|
   315|         0|            0|            0|  0.00%|            Use:
   316|         0|            0|            0|  0.00%|
   317|         0|            0|            0|  0.00%|            probs = policy_network(state)
   318|         0|            0|            0|  0.00%|            # NOTE: categorical is equivalent to what used to be called multinomial
   319|         0|            0|            0|  0.00%|            m = torch.distributions.Categorical(probs)
   320|         0|            0|            0|  0.00%|            action = m.sample()
   321|         0|            0|            0|  0.00%|            next_state, reward = env.step(action)
   322|         0|            0|            0|  0.00%|            loss = -m.log_prob(action) * reward
   323|         0|            0|            0|  0.00%|            loss.backward()
   324|         0|            0|            0|  0.00%|        """))
   325|         0|            0|            0|  0.00%|
   326|         0|            0|            0|  0.00%|    detach = _C._add_docstr(_C._TensorBase.detach, r"""
   327|         0|            0|            0|  0.00%|    Returns a new Tensor, detached from the current graph.
   328|         0|            0|            0|  0.00%|
   329|         0|            0|            0|  0.00%|    The result will never require gradient.
   330|         0|            0|            0|  0.00%|
   331|         0|            0|            0|  0.00%|    This method also affects forward mode AD gradients and the result will never
   332|         0|            0|            0|  0.00%|    have forward mode AD gradients.
   333|         0|            0|            0|  0.00%|
   334|         0|            0|            0|  0.00%|    .. note::
   335|         0|            0|            0|  0.00%|
   336|         0|            0|            0|  0.00%|      Returned Tensor shares the same storage with the original one.
   337|         0|            0|            0|  0.00%|      In-place modifications on either of them will be seen, and may trigger
   338|         0|            0|            0|  0.00%|      errors in correctness checks.
   339|         0|            0|            0|  0.00%|      IMPORTANT NOTE: Previously, in-place size / stride / storage changes
   340|         0|            0|            0|  0.00%|      (such as `resize_` / `resize_as_` / `set_` / `transpose_`) to the returned tensor
   341|         0|            0|            0|  0.00%|      also update the original tensor. Now, these in-place changes will not update the
   342|         0|            0|            0|  0.00%|      original tensor anymore, and will instead trigger an error.
   343|         0|            0|            0|  0.00%|      For sparse tensors:
   344|         0|            0|            0|  0.00%|      In-place indices / values changes (such as `zero_` / `copy_` / `add_`) to the
   345|         0|            0|            0|  0.00%|      returned tensor will not update the original tensor anymore, and will instead
   346|         0|            0|            0|  0.00%|      trigger an error.
   347|         0|            0|            0|  0.00%|    """)
   348|         0|            0|            0|  0.00%|
   349|         0|            0|            0|  0.00%|    detach_ = _C._add_docstr(_C._TensorBase.detach_, r"""
   350|         0|            0|            0|  0.00%|    Detaches the Tensor from the graph that created it, making it a leaf.
   351|         0|            0|            0|  0.00%|    Views cannot be detached in-place.
   352|         0|            0|            0|  0.00%|
   353|         0|            0|            0|  0.00%|    This method also affects forward mode AD gradients and the result will never
   354|         0|            0|            0|  0.00%|    have forward mode AD gradients.
   355|         0|            0|            0|  0.00%|    """)
   356|         0|            0|            0|  0.00%|
   357|         0|            0|            0|  0.00%|    def retain_grad(self):
   358|         0|            0|            0|  0.00%|        r"""Enables .grad attribute for non-leaf Tensors."""
   359|         0|            0|            0|  0.00%|        if has_torch_function_unary(self):
   360|         0|            0|            0|  0.00%|            return handle_torch_function(Tensor.retain_grad, (self,), self)
   361|         0|            0|            0|  0.00%|        if not self.requires_grad:
   362|         0|            0|            0|  0.00%|            raise RuntimeError("can't retain_grad on Tensor that has requires_grad=False")
   363|         0|            0|            0|  0.00%|        if self.is_leaf:  # no-op for leaves
   364|         0|            0|            0|  0.00%|            return
   365|         0|            0|            0|  0.00%|        if hasattr(self, 'retains_grad'):
   366|         0|            0|            0|  0.00%|            return
   367|         0|            0|            0|  0.00%|        weak_self = weakref.ref(self)
   368|         0|            0|            0|  0.00%|
   369|         0|            0|            0|  0.00%|        def retain_grad_hook(grad):
   370|         0|            0|            0|  0.00%|            var = weak_self()
   371|         0|            0|            0|  0.00%|            if var is None:
   372|         0|            0|            0|  0.00%|                return
   373|         0|            0|            0|  0.00%|            if var._grad is None:
   374|         0|            0|            0|  0.00%|                if grad.is_sparse:
   375|         0|            0|            0|  0.00%|                    var._grad = grad.clone()
   376|         0|            0|            0|  0.00%|                else:
   377|         0|            0|            0|  0.00%|                    var._grad = grad.clone(memory_format=torch.contiguous_format)
   378|         0|            0|            0|  0.00%|            else:
   379|         0|            0|            0|  0.00%|                var._grad = var._grad + grad
   380|         0|            0|            0|  0.00%|
   381|         0|            0|            0|  0.00%|        self.register_hook(retain_grad_hook)
   382|         0|            0|            0|  0.00%|        self.retains_grad = True
   383|         0|            0|            0|  0.00%|
   384|         0|            0|            0|  0.00%|    def is_shared(self):
   385|         0|            0|            0|  0.00%|        r"""Checks if tensor is in shared memory.
   386|         0|            0|            0|  0.00%|
   387|         0|            0|            0|  0.00%|        This is always ``True`` for CUDA tensors.
   388|         0|            0|            0|  0.00%|        """
   389|         0|            0|            0|  0.00%|        if has_torch_function_unary(self):
   390|         0|            0|            0|  0.00%|            return handle_torch_function(Tensor.is_shared, (self,), self)
   391|         0|            0|            0|  0.00%|        return self.storage().is_shared()
   392|         0|            0|            0|  0.00%|
   393|         0|            0|            0|  0.00%|    def share_memory_(self):
   394|         0|            0|            0|  0.00%|        r"""Moves the underlying storage to shared memory.
   395|         0|            0|            0|  0.00%|
   396|         0|            0|            0|  0.00%|        This is a no-op if the underlying storage is already in shared memory
   397|         0|            0|            0|  0.00%|        and for CUDA tensors. Tensors in shared memory cannot be resized.
   398|         0|            0|            0|  0.00%|        """
   399|         0|            0|            0|  0.00%|        if has_torch_function_unary(self):
   400|         0|            0|            0|  0.00%|            return handle_torch_function(Tensor.share_memory_, (self,), self)
   401|         0|            0|            0|  0.00%|        self.storage().share_memory_()
   402|         0|            0|            0|  0.00%|        return self
   403|         0|            0|            0|  0.00%|
   404|         0|            0|            0|  0.00%|    def __reversed__(self):
   405|         0|            0|            0|  0.00%|        r"""Reverses the tensor along dimension 0."""
   406|         0|            0|            0|  0.00%|        if has_torch_function_unary(self):
   407|         0|            0|            0|  0.00%|            return handle_torch_function(Tensor.__reversed__, (self,), self)
   408|         0|            0|            0|  0.00%|        if self.dim() == 0:
   409|         0|            0|            0|  0.00%|            return self
   410|         0|            0|            0|  0.00%|        else:
   411|         0|            0|            0|  0.00%|            return self.flip(0)
   412|         0|            0|            0|  0.00%|
   413|         0|            0|            0|  0.00%|    def norm(self, p="fro", dim=None, keepdim=False, dtype=None):
   414|         0|            0|            0|  0.00%|        r"""See :func:`torch.norm`"""
   415|         0|            0|            0|  0.00%|        if has_torch_function_unary(self):
   416|         0|            0|            0|  0.00%|            return handle_torch_function(Tensor.norm, (self,), self, p=p, dim=dim, keepdim=keepdim, dtype=dtype)
   417|         0|            0|            0|  0.00%|        return torch.norm(self, p, dim, keepdim, dtype=dtype)
   418|         0|            0|            0|  0.00%|
   419|         0|            0|            0|  0.00%|    def lu(self, pivot=True, get_infos=False):
   420|         0|            0|            0|  0.00%|        r"""See :func:`torch.lu`"""
   421|         0|            0|            0|  0.00%|        # If get_infos is True, then we don't need to check for errors and vice versa
   422|         0|            0|            0|  0.00%|        if has_torch_function_unary(self):
   423|         0|            0|            0|  0.00%|            return handle_torch_function(Tensor.lu, (self,), self, pivot=pivot, get_infos=get_infos)
   424|         0|            0|            0|  0.00%|
   425|         0|            0|            0|  0.00%|        if not torch._jit_internal.is_scripting():
   426|         0|            0|            0|  0.00%|            if self.requires_grad:
   427|         0|            0|            0|  0.00%|                if not (self.size(-2) == self.size(-1) and (self.dtype.is_floating_point) or self.is_complex):
   428|         0|            0|            0|  0.00%|                    raise ValueError(
   429|         0|            0|            0|  0.00%|                        'lu.backward works only with batches of squared full-rank matrices'
   430|         0|            0|            0|  0.00%|                        ' of floating or complex types.'
   431|         0|            0|            0|  0.00%|                    )
   432|         0|            0|            0|  0.00%|
   433|         0|            0|            0|  0.00%|                from torch._autograd_functions import _LU
   434|         0|            0|            0|  0.00%|                LU, pivots, infos = _LU.apply(self, pivot, get_infos)
   435|         0|            0|            0|  0.00%|                if get_infos:
   436|         0|            0|            0|  0.00%|                    return LU, pivots, infos
   437|         0|            0|            0|  0.00%|                else:
   438|         0|            0|            0|  0.00%|                    return LU, pivots
   439|         0|            0|            0|  0.00%|        else:
   440|         0|            0|            0|  0.00%|            if self.requires_grad:
   441|         0|            0|            0|  0.00%|                raise RuntimeError(
   442|         0|            0|            0|  0.00%|                    'Script and require gradients is not supported at the moment.'
   443|         0|            0|            0|  0.00%|                    'If you just want to do the forward, use .detach()'
   444|         0|            0|            0|  0.00%|                    'on the input before calling the function.'
   445|         0|            0|            0|  0.00%|                )
   446|         0|            0|            0|  0.00%|
   447|         0|            0|            0|  0.00%|        LU, pivots, infos = torch._lu_with_info(self, pivot=pivot, check_errors=(not get_infos))
   448|         0|            0|            0|  0.00%|        if get_infos:
   449|         0|            0|            0|  0.00%|            return LU, pivots, infos
   450|         0|            0|            0|  0.00%|        else:
   451|         0|            0|            0|  0.00%|            return LU, pivots
   452|         0|            0|            0|  0.00%|
   453|         0|            0|            0|  0.00%|    def stft(self, n_fft: int, hop_length: Optional[int] = None,
   454|         0|            0|            0|  0.00%|             win_length: Optional[int] = None, window: 'Optional[Tensor]' = None,
   455|         0|            0|            0|  0.00%|             center: bool = True, pad_mode: str = 'reflect', normalized: bool = False,
   456|         0|            0|            0|  0.00%|             onesided: Optional[bool] = None, return_complex: Optional[bool] = None):
   457|         0|            0|            0|  0.00%|        r"""See :func:`torch.stft`
   458|         0|            0|            0|  0.00%|
   459|         0|            0|            0|  0.00%|        .. warning::
   460|         0|            0|            0|  0.00%|          This function changed signature at version 0.4.1. Calling with
   461|         0|            0|            0|  0.00%|          the previous signature may cause error or return incorrect result.
   462|         0|            0|            0|  0.00%|        """
   463|         0|            0|            0|  0.00%|        if has_torch_function_unary(self):
   464|         0|            0|            0|  0.00%|            return handle_torch_function(
   465|         0|            0|            0|  0.00%|                Tensor.stft, (self,), self, n_fft, hop_length=hop_length,
   466|         0|            0|            0|  0.00%|                win_length=win_length, window=window, center=center, pad_mode=pad_mode, normalized=normalized,
   467|         0|            0|            0|  0.00%|                onesided=onesided, return_complex=return_complex
   468|         0|            0|            0|  0.00%|            )
   469|         0|            0|            0|  0.00%|        return torch.stft(self, n_fft, hop_length, win_length, window, center,
   470|         0|            0|            0|  0.00%|                          pad_mode, normalized, onesided, return_complex=return_complex)
   471|         0|            0|            0|  0.00%|
   472|         0|            0|            0|  0.00%|    def istft(self, n_fft: int, hop_length: Optional[int] = None,
   473|         0|            0|            0|  0.00%|              win_length: Optional[int] = None, window: 'Optional[Tensor]' = None,
   474|         0|            0|            0|  0.00%|              center: bool = True, normalized: bool = False,
   475|         0|            0|            0|  0.00%|              onesided: Optional[bool] = None, length: Optional[int] = None,
   476|         0|            0|            0|  0.00%|              return_complex: bool = False):
   477|         0|            0|            0|  0.00%|        r"""See :func:`torch.istft`"""
   478|         0|            0|            0|  0.00%|        if has_torch_function_unary(self):
   479|         0|            0|            0|  0.00%|            return handle_torch_function(
   480|         0|            0|            0|  0.00%|                Tensor.istft, (self,), self, n_fft, hop_length=hop_length, win_length=win_length,
   481|         0|            0|            0|  0.00%|                window=window, center=center, normalized=normalized, onesided=onesided, length=length,
   482|         0|            0|            0|  0.00%|                return_complex=return_complex
   483|         0|            0|            0|  0.00%|            )
   484|         0|            0|            0|  0.00%|        return torch.istft(self, n_fft, hop_length, win_length, window, center,
   485|         0|            0|            0|  0.00%|                           normalized, onesided, length, return_complex=return_complex)
   486|         0|            0|            0|  0.00%|
   487|         0|            0|            0|  0.00%|    def resize(self, *sizes):
   488|         0|            0|            0|  0.00%|        if has_torch_function_unary(self):
   489|         0|            0|            0|  0.00%|            return handle_torch_function(Tensor.resize, (self,), self, *sizes)
   490|         0|            0|            0|  0.00%|        warnings.warn("non-inplace resize is deprecated")
   491|         0|            0|            0|  0.00%|        from torch.autograd._functions import Resize
   492|         0|            0|            0|  0.00%|        return Resize.apply(self, sizes)
   493|         0|            0|            0|  0.00%|
   494|         0|            0|            0|  0.00%|    def resize_as(self, tensor):
   495|         0|            0|            0|  0.00%|        if has_torch_function_variadic(self, tensor):
   496|         0|            0|            0|  0.00%|            return handle_torch_function(Tensor.resize_as, (self, tensor), self, tensor)
   497|         0|            0|            0|  0.00%|        warnings.warn("non-inplace resize_as is deprecated")
   498|         0|            0|            0|  0.00%|        from torch.autograd._functions import Resize
   499|         0|            0|            0|  0.00%|        return Resize.apply(self, tensor.size())
   500|         0|            0|            0|  0.00%|
   501|         0|            0|            0|  0.00%|    def split(self, split_size, dim=0):
   502|         0|            0|            0|  0.00%|        r"""See :func:`torch.split`
   503|         0|            0|            0|  0.00%|        """
   504|         0|            0|            0|  0.00%|        if has_torch_function_unary(self):
   505|         0|            0|            0|  0.00%|            return handle_torch_function(Tensor.split, (self,), self, split_size, dim=dim)
   506|         0|            0|            0|  0.00%|        if isinstance(split_size, int):
   507|         0|            0|            0|  0.00%|            return super(Tensor, self).split(split_size, dim)
   508|         0|            0|            0|  0.00%|        elif isinstance(split_size, Tensor):
   509|         0|            0|            0|  0.00%|            try:
   510|         0|            0|            0|  0.00%|                split_size = int(split_size)
   511|         0|            0|            0|  0.00%|                return super(Tensor, self).split(split_size, dim)
   512|         0|            0|            0|  0.00%|            except ValueError:
   513|         0|            0|            0|  0.00%|                return super(Tensor, self).split_with_sizes(split_size, dim)
   514|         0|            0|            0|  0.00%|        else:
   515|         0|            0|            0|  0.00%|            return super(Tensor, self).split_with_sizes(split_size, dim)
   516|         0|            0|            0|  0.00%|
   517|         0|            0|            0|  0.00%|    def unique(self, sorted=True, return_inverse=False, return_counts=False, dim=None):
   518|         0|            0|            0|  0.00%|        r"""Returns the unique elements of the input tensor.
   519|         0|            0|            0|  0.00%|
   520|         0|            0|            0|  0.00%|        See :func:`torch.unique`
   521|         0|            0|            0|  0.00%|        """
   522|         0|            0|            0|  0.00%|        if has_torch_function_unary(self):
   523|         0|            0|            0|  0.00%|            return handle_torch_function(
   524|         0|            0|            0|  0.00%|                Tensor.unique, (self,), self, sorted=sorted, return_inverse=return_inverse,
   525|         0|            0|            0|  0.00%|                return_counts=return_counts, dim=dim
   526|         0|            0|            0|  0.00%|            )
   527|         0|            0|            0|  0.00%|        return torch.unique(self, sorted=sorted, return_inverse=return_inverse, return_counts=return_counts, dim=dim)
   528|         0|            0|            0|  0.00%|
   529|         0|            0|            0|  0.00%|    def unique_consecutive(self, return_inverse=False, return_counts=False, dim=None):
   530|         0|            0|            0|  0.00%|        r"""Eliminates all but the first element from every consecutive group of equivalent elements.
   531|         0|            0|            0|  0.00%|
   532|         0|            0|            0|  0.00%|        See :func:`torch.unique_consecutive`
   533|         0|            0|            0|  0.00%|        """
   534|         0|            0|            0|  0.00%|        if has_torch_function_unary(self):
   535|         0|            0|            0|  0.00%|            return handle_torch_function(
   536|         0|            0|            0|  0.00%|                Tensor.unique_consecutive, (self,), self, return_inverse=return_inverse,
   537|         0|            0|            0|  0.00%|                return_counts=return_counts, dim=dim
   538|         0|            0|            0|  0.00%|            )
   539|         0|            0|            0|  0.00%|        return torch.unique_consecutive(self, return_inverse=return_inverse, return_counts=return_counts, dim=dim)
   540|         0|            0|            0|  0.00%|
   541|         0|            0|            0|  0.00%|    def __rsub__(self, other):
   542|         0|            0|            0|  0.00%|        if has_torch_function_variadic(self, other):
   543|         0|            0|            0|  0.00%|            return handle_torch_function(Tensor.__rsub__, (self, other), self, other)
   544|         0|            0|            0|  0.00%|        return _C._VariableFunctions.rsub(self, other)
   545|         0|            0|            0|  0.00%|
   546|         0|            0|            0|  0.00%|    def __rdiv__(self, other):
   547|         0|            0|            0|  0.00%|        if has_torch_function_variadic(self, other):
   548|         0|            0|            0|  0.00%|            return handle_torch_function(Tensor.__rdiv__, (self, other), self, other)
   549|         0|            0|            0|  0.00%|        return self.reciprocal() * other
   550|         0|            0|            0|  0.00%|
   551|         0|            0|            0|  0.00%|    __rtruediv__ = __rdiv__
   552|         0|            0|            0|  0.00%|    __itruediv__ = _C._TensorBase.__idiv__
   553|         0|            0|            0|  0.00%|
   554|         0|            0|            0|  0.00%|    __pow__ = _C._TensorBase.pow
   555|         0|            0|            0|  0.00%|
   556|         3|  3.29018e-05|  1.09673e-05|  0.00%|    def __format__(self, format_spec):
   557|         3|  2.98023e-05|  9.93411e-06|  0.00%|        if has_torch_function_unary(self):
   558|         0|            0|            0|  0.00%|            return handle_torch_function(Tensor.__format__, (self,), self, format_spec)
   559|         3|  3.02792e-05|  1.00931e-05|  0.00%|        if self.dim() == 0:
   560|         3|  0.000111818|  3.72728e-05|  0.00%|            return self.item().__format__(format_spec)
   561|         0|            0|            0|  0.00%|        return object.__format__(self, format_spec)
   562|         0|            0|            0|  0.00%|
   563|         0|            0|            0|  0.00%|    def __ipow__(self, other):  # type: ignore[misc]
   564|         0|            0|            0|  0.00%|        if has_torch_function_variadic(self, other):
   565|         0|            0|            0|  0.00%|            return handle_torch_function(Tensor.__ipow__, (self, other), self, other)
   566|         0|            0|            0|  0.00%|        return NotImplemented
   567|         0|            0|            0|  0.00%|
   568|         0|            0|            0|  0.00%|    @_wrap_type_error_to_not_implemented
   569|         0|            0|            0|  0.00%|    def __rpow__(self, other):
   570|         0|            0|            0|  0.00%|        dtype = torch.result_type(other, self)
   571|         0|            0|            0|  0.00%|        return torch.tensor(other, dtype=dtype, device=self.device) ** self
   572|         0|            0|            0|  0.00%|
   573|         0|            0|            0|  0.00%|    @_wrap_type_error_to_not_implemented
   574|         0|            0|            0|  0.00%|    def __floordiv__(self, other):
   575|         0|            0|            0|  0.00%|        return torch.floor_divide(self, other)
   576|         0|            0|            0|  0.00%|
   577|         0|            0|            0|  0.00%|    @_wrap_type_error_to_not_implemented
   578|         0|            0|            0|  0.00%|    def __rfloordiv__(self, other):
   579|         0|            0|            0|  0.00%|        return torch.floor_divide(other, self)
   580|         0|            0|            0|  0.00%|
   581|         0|            0|            0|  0.00%|    __pos__ = _C._TensorBase.positive
   582|         0|            0|            0|  0.00%|    __neg__ = _C._TensorBase.neg
   583|         0|            0|            0|  0.00%|    __abs__ = _C._TensorBase.abs
   584|         0|            0|            0|  0.00%|
   585|         0|            0|            0|  0.00%|    def __len__(self):
   586|         0|            0|            0|  0.00%|        if has_torch_function_unary(self):
   587|         0|            0|            0|  0.00%|            return handle_torch_function(Tensor.__len__, (self,), self)
   588|         0|            0|            0|  0.00%|        if self.dim() == 0:
   589|         0|            0|            0|  0.00%|            raise TypeError("len() of a 0-d tensor")
   590|         0|            0|            0|  0.00%|        if torch._C._get_tracing_state():
   591|         0|            0|            0|  0.00%|            warnings.warn('Using len to get tensor shape might cause the trace to be incorrect. '
   592|         0|            0|            0|  0.00%|                          'Recommended usage would be tensor.shape[0]. '
   593|         0|            0|            0|  0.00%|                          'Passing a tensor of different shape might lead to errors or silently give '
   594|         0|            0|            0|  0.00%|                          'incorrect results.', category=torch.jit.TracerWarning, stacklevel=2)
   595|         0|            0|            0|  0.00%|        return self.shape[0]
   596|         0|            0|            0|  0.00%|
   597|         0|            0|            0|  0.00%|    def __iter__(self):
   598|         0|            0|            0|  0.00%|        # NB: we use 'imap' and not 'map' here, so that in Python 2 we get a
   599|         0|            0|            0|  0.00%|        # generator and don't eagerly perform all the indexes.  This could
   600|         0|            0|            0|  0.00%|        # save us work, and also helps keep trace ordering deterministic
   601|         0|            0|            0|  0.00%|        # (e.g., if you zip(*hiddens), the eager map will force all the
   602|         0|            0|            0|  0.00%|        # indexes of hiddens[0] before hiddens[1], while the generator
   603|         0|            0|            0|  0.00%|        # map will interleave them.)
   604|         0|            0|            0|  0.00%|        # NB: We have intentionally skipped __torch_function__ dispatch here.
   605|         0|            0|            0|  0.00%|        # See gh-54457
   606|         0|            0|            0|  0.00%|        if self.dim() == 0:
   607|         0|            0|            0|  0.00%|            raise TypeError('iteration over a 0-d tensor')
   608|         0|            0|            0|  0.00%|        if torch._C._get_tracing_state():
   609|         0|            0|            0|  0.00%|            warnings.warn('Iterating over a tensor might cause the trace to be incorrect. '
   610|         0|            0|            0|  0.00%|                          'Passing a tensor of different shape won\'t change the number of '
   611|         0|            0|            0|  0.00%|                          'iterations executed (and might lead to errors or silently give '
   612|         0|            0|            0|  0.00%|                          'incorrect results).', category=torch.jit.TracerWarning, stacklevel=2)
   613|         0|            0|            0|  0.00%|        return iter(self.unbind(0))
   614|         0|            0|            0|  0.00%|
   615|      7626|    0.0318851|  4.18111e-06|  0.06%|    def __hash__(self):
   616|      7626|    0.0423541|  5.55391e-06|  0.08%|        if has_torch_function_unary(self):
   617|         0|            0|            0|  0.00%|            return handle_torch_function(Tensor.__hash__, (self,), self)
   618|      7626|    0.0406289|  5.32768e-06|  0.08%|        return id(self)
   619|         0|            0|            0|  0.00%|
   620|         0|            0|            0|  0.00%|    def __dir__(self):
   621|         0|            0|            0|  0.00%|        if has_torch_function_unary(self):
   622|         0|            0|            0|  0.00%|            return handle_torch_function(Tensor.__dir__, (self,), self)
   623|         0|            0|            0|  0.00%|        if self.is_quantized:
   624|         0|            0|            0|  0.00%|            warnings.warn('Only a small subset of methods are supported for quantized tensors.')
   625|         0|            0|            0|  0.00%|        tensor_methods = dir(self.__class__)
   626|         0|            0|            0|  0.00%|        tensor_methods.remove('volatile')  # deprecated
   627|         0|            0|            0|  0.00%|        attrs = list(self.__dict__.keys())
   628|         0|            0|            0|  0.00%|        keys = tensor_methods + attrs
   629|         0|            0|            0|  0.00%|
   630|         0|            0|            0|  0.00%|        # property only available dense, cuda tensors
   631|         0|            0|            0|  0.00%|        if (not self.is_cuda) or self.is_sparse:
   632|         0|            0|            0|  0.00%|            keys.remove("__cuda_array_interface__")
   633|         0|            0|            0|  0.00%|
   634|         0|            0|            0|  0.00%|        return sorted(keys)
   635|         0|            0|            0|  0.00%|
   636|         0|            0|            0|  0.00%|    # Numpy array interface, to support `numpy.asarray(tensor) -> ndarray`
   637|         0|            0|            0|  0.00%|    __array_priority__ = 1000    # prefer Tensor ops over numpy ones
   638|         0|            0|            0|  0.00%|
   639|         0|            0|            0|  0.00%|    def __array__(self, dtype=None):
   640|         0|            0|            0|  0.00%|        if has_torch_function_unary(self):
   641|         0|            0|            0|  0.00%|            return handle_torch_function(Tensor.__array__, (self,), self, dtype=dtype)
   642|         0|            0|            0|  0.00%|        if dtype is None:
   643|         0|            0|            0|  0.00%|            return self.numpy()
   644|         0|            0|            0|  0.00%|        else:
   645|         0|            0|            0|  0.00%|            return self.numpy().astype(dtype, copy=False)
   646|         0|            0|            0|  0.00%|
   647|         0|            0|            0|  0.00%|    # Wrap Numpy array again in a suitable tensor when done, to support e.g.
   648|         0|            0|            0|  0.00%|    # `numpy.sin(tensor) -> tensor` or `numpy.greater(tensor, 0) -> ByteTensor`
   649|         0|            0|            0|  0.00%|    def __array_wrap__(self, array):
   650|         0|            0|            0|  0.00%|        if has_torch_function_unary(self):
   651|         0|            0|            0|  0.00%|            return handle_torch_function(Tensor.__array_wrap__, (self,), self, array=array)
   652|         0|            0|            0|  0.00%|        if array.dtype == bool:
   653|         0|            0|            0|  0.00%|            # Workaround, torch has no built-in bool tensor
   654|         0|            0|            0|  0.00%|            array = array.astype('uint8')
   655|         0|            0|            0|  0.00%|        return torch.from_numpy(array)
   656|         0|            0|            0|  0.00%|
   657|         0|            0|            0|  0.00%|    def __contains__(self, element):
   658|         0|            0|            0|  0.00%|        r"""Check if `element` is present in tensor
   659|         0|            0|            0|  0.00%|
   660|         0|            0|            0|  0.00%|        Args:
   661|         0|            0|            0|  0.00%|            element (Tensor or scalar): element to be checked
   662|         0|            0|            0|  0.00%|                for presence in current tensor"
   663|         0|            0|            0|  0.00%|        """
   664|         0|            0|            0|  0.00%|        if has_torch_function_unary(self):
   665|         0|            0|            0|  0.00%|            return handle_torch_function(Tensor.__contains__, (self,), self, element)
   666|         0|            0|            0|  0.00%|        if isinstance(element, (torch.Tensor, Number)):
   667|         0|            0|            0|  0.00%|            # type hint doesn't understand the __contains__ result array
   668|         0|            0|            0|  0.00%|            return (element == self).any().item()  # type: ignore[union-attr]
   669|         0|            0|            0|  0.00%|
   670|         0|            0|            0|  0.00%|        raise RuntimeError(
   671|         0|            0|            0|  0.00%|            "Tensor.__contains__ only supports Tensor or scalar, but you passed in a %s." %
   672|         0|            0|            0|  0.00%|            type(element)
   673|         0|            0|            0|  0.00%|        )
   674|         0|            0|            0|  0.00%|
   675|         0|            0|            0|  0.00%|    @property
   676|         0|            0|            0|  0.00%|    def __cuda_array_interface__(self):
   677|         0|            0|            0|  0.00%|        """Array view description for cuda tensors.
   678|         0|            0|            0|  0.00%|
   679|         0|            0|            0|  0.00%|        See:
   680|         0|            0|            0|  0.00%|        https://numba.pydata.org/numba-doc/latest/cuda/cuda_array_interface.html
   681|         0|            0|            0|  0.00%|        """
   682|         0|            0|            0|  0.00%|        if has_torch_function_unary(self):
   683|         0|            0|            0|  0.00%|            # TODO mypy doesn't support @property, see: https://github.com/python/mypy/issues/6185
   684|         0|            0|            0|  0.00%|            return handle_torch_function(Tensor.__cuda_array_interface__.__get__, (self,), self)  # type: ignore[attr-defined]
   685|         0|            0|            0|  0.00%|
   686|         0|            0|            0|  0.00%|        # raise AttributeError for unsupported tensors, so that
   687|         0|            0|            0|  0.00%|        # hasattr(cpu_tensor, "__cuda_array_interface__") is False.
   688|         0|            0|            0|  0.00%|        if not self.is_cuda:
   689|         0|            0|            0|  0.00%|            raise AttributeError(
   690|         0|            0|            0|  0.00%|                "Can't get __cuda_array_interface__ on non-CUDA tensor type: %s "
   691|         0|            0|            0|  0.00%|                "If CUDA data is required use tensor.cuda() to copy tensor to device memory." %
   692|         0|            0|            0|  0.00%|                self.type()
   693|         0|            0|            0|  0.00%|            )
   694|         0|            0|            0|  0.00%|
   695|         0|            0|            0|  0.00%|        if self.is_sparse:
   696|         0|            0|            0|  0.00%|            raise AttributeError(
   697|         0|            0|            0|  0.00%|                "Can't get __cuda_array_interface__ on sparse type: %s "
   698|         0|            0|            0|  0.00%|                "Use Tensor.to_dense() to convert to a dense tensor first." %
   699|         0|            0|            0|  0.00%|                self.type()
   700|         0|            0|            0|  0.00%|            )
   701|         0|            0|            0|  0.00%|
   702|         0|            0|            0|  0.00%|        # RuntimeError, matching tensor.__array__() behavior.
   703|         0|            0|            0|  0.00%|        if self.requires_grad:
   704|         0|            0|            0|  0.00%|            raise RuntimeError(
   705|         0|            0|            0|  0.00%|                "Can't get __cuda_array_interface__ on Variable that requires grad. "
   706|         0|            0|            0|  0.00%|                "If gradients aren't required, use var.detach() to get Variable that doesn't require grad."
   707|         0|            0|            0|  0.00%|            )
   708|         0|            0|            0|  0.00%|
   709|         0|            0|            0|  0.00%|        # CUDA devices are little-endian and tensors are stored in native byte
   710|         0|            0|            0|  0.00%|        # order. 1-byte entries are endian-agnostic.
   711|         0|            0|            0|  0.00%|        typestr = {
   712|         0|            0|            0|  0.00%|            torch.complex64: "<c8",
   713|         0|            0|            0|  0.00%|            torch.complex128: "<c16",
   714|         0|            0|            0|  0.00%|            torch.float16: "<f2",
   715|         0|            0|            0|  0.00%|            torch.float32: "<f4",
   716|         0|            0|            0|  0.00%|            torch.float64: "<f8",
   717|         0|            0|            0|  0.00%|            torch.uint8: "|u1",
   718|         0|            0|            0|  0.00%|            torch.int8: "|i1",
   719|         0|            0|            0|  0.00%|            torch.int16: "<i2",
   720|         0|            0|            0|  0.00%|            torch.int32: "<i4",
   721|         0|            0|            0|  0.00%|            torch.int64: "<i8",
   722|         0|            0|            0|  0.00%|        }[self.dtype]
   723|         0|            0|            0|  0.00%|
   724|         0|            0|            0|  0.00%|        itemsize = self.storage().element_size()
   725|         0|            0|            0|  0.00%|
   726|         0|            0|            0|  0.00%|        shape = tuple(self.shape)
   727|         0|            0|            0|  0.00%|        if self.is_contiguous():
   728|         0|            0|            0|  0.00%|            # __cuda_array_interface__ v2 requires the strides to be omitted
   729|         0|            0|            0|  0.00%|            # (either not set or set to None) for C-contiguous arrays.
   730|         0|            0|            0|  0.00%|            strides = None
   731|         0|            0|            0|  0.00%|        else:
   732|         0|            0|            0|  0.00%|            strides = tuple(s * itemsize for s in self.stride())
   733|         0|            0|            0|  0.00%|        data_ptr = self.data_ptr() if self.numel() > 0 else 0
   734|         0|            0|            0|  0.00%|        data = (data_ptr, False)  # read-only is false
   735|         0|            0|            0|  0.00%|
   736|         0|            0|            0|  0.00%|        return dict(typestr=typestr, shape=shape, strides=strides, data=data, version=2)
   737|         0|            0|            0|  0.00%|
   738|         0|            0|            0|  0.00%|    def refine_names(self, *names):
   739|         0|            0|            0|  0.00%|        r"""Refines the dimension names of :attr:`self` according to :attr:`names`.
   740|         0|            0|            0|  0.00%|
   741|         0|            0|            0|  0.00%|        Refining is a special case of renaming that "lifts" unnamed dimensions.
   742|         0|            0|            0|  0.00%|        A ``None`` dim can be refined to have any name; a named dim can only be
   743|         0|            0|            0|  0.00%|        refined to have the same name.
   744|         0|            0|            0|  0.00%|
   745|         0|            0|            0|  0.00%|        Because named tensors can coexist with unnamed tensors, refining names
   746|         0|            0|            0|  0.00%|        gives a nice way to write named-tensor-aware code that works with both
   747|         0|            0|            0|  0.00%|        named and unnamed tensors.
   748|         0|            0|            0|  0.00%|
   749|         0|            0|            0|  0.00%|        :attr:`names` may contain up to one Ellipsis (``...``).
   750|         0|            0|            0|  0.00%|        The Ellipsis is expanded greedily; it is expanded in-place to fill
   751|         0|            0|            0|  0.00%|        :attr:`names` to the same length as ``self.dim()`` using names from the
   752|         0|            0|            0|  0.00%|        corresponding indices of ``self.names``.
   753|         0|            0|            0|  0.00%|
   754|         0|            0|            0|  0.00%|        Python 2 does not support Ellipsis but one may use a string literal
   755|         0|            0|            0|  0.00%|        instead (``'...'``).
   756|         0|            0|            0|  0.00%|
   757|         0|            0|            0|  0.00%|        Args:
   758|         0|            0|            0|  0.00%|            names (iterable of str): The desired names of the output tensor. May
   759|         0|            0|            0|  0.00%|                contain up to one Ellipsis.
   760|         0|            0|            0|  0.00%|
   761|         0|            0|            0|  0.00%|        Examples::
   762|         0|            0|            0|  0.00%|
   763|         0|            0|            0|  0.00%|            >>> imgs = torch.randn(32, 3, 128, 128)
   764|         0|            0|            0|  0.00%|            >>> named_imgs = imgs.refine_names('N', 'C', 'H', 'W')
   765|         0|            0|            0|  0.00%|            >>> named_imgs.names
   766|         0|            0|            0|  0.00%|            ('N', 'C', 'H', 'W')
   767|         0|            0|            0|  0.00%|
   768|         0|            0|            0|  0.00%|            >>> tensor = torch.randn(2, 3, 5, 7, 11)
   769|         0|            0|            0|  0.00%|            >>> tensor = tensor.refine_names('A', ..., 'B', 'C')
   770|         0|            0|            0|  0.00%|            >>> tensor.names
   771|         0|            0|            0|  0.00%|            ('A', None, None, 'B', 'C')
   772|         0|            0|            0|  0.00%|
   773|         0|            0|            0|  0.00%|        .. warning::
   774|         0|            0|            0|  0.00%|            The named tensor API is experimental and subject to change.
   775|         0|            0|            0|  0.00%|
   776|         0|            0|            0|  0.00%|        """
   777|         0|            0|            0|  0.00%|        if has_torch_function_unary(self):
   778|         0|            0|            0|  0.00%|            return handle_torch_function(Tensor.refine_names, (self,), self, *names)
   779|         0|            0|            0|  0.00%|        names = resolve_ellipsis(names, self.names, 'refine_names')
   780|         0|            0|            0|  0.00%|        return super(Tensor, self).refine_names(names)
   781|         0|            0|            0|  0.00%|
   782|         0|            0|            0|  0.00%|    def align_to(self, *names):
   783|         0|            0|            0|  0.00%|        r"""Permutes the dimensions of the :attr:`self` tensor to match the order
   784|         0|            0|            0|  0.00%|        specified in :attr:`names`, adding size-one dims for any new names.
   785|         0|            0|            0|  0.00%|
   786|         0|            0|            0|  0.00%|        All of the dims of :attr:`self` must be named in order to use this method.
   787|         0|            0|            0|  0.00%|        The resulting tensor is a view on the original tensor.
   788|         0|            0|            0|  0.00%|
   789|         0|            0|            0|  0.00%|        All dimension names of :attr:`self` must be present in :attr:`names`.
   790|         0|            0|            0|  0.00%|        :attr:`names` may contain additional names that are not in ``self.names``;
   791|         0|            0|            0|  0.00%|        the output tensor has a size-one dimension for each of those new names.
   792|         0|            0|            0|  0.00%|
   793|         0|            0|            0|  0.00%|        :attr:`names` may contain up to one Ellipsis (``...``).
   794|         0|            0|            0|  0.00%|        The Ellipsis is expanded to be equal to all dimension names of :attr:`self`
   795|         0|            0|            0|  0.00%|        that are not mentioned in :attr:`names`, in the order that they appear
   796|         0|            0|            0|  0.00%|        in :attr:`self`.
   797|         0|            0|            0|  0.00%|
   798|         0|            0|            0|  0.00%|        Python 2 does not support Ellipsis but one may use a string literal
   799|         0|            0|            0|  0.00%|        instead (``'...'``).
   800|         0|            0|            0|  0.00%|
   801|         0|            0|            0|  0.00%|        Args:
   802|         0|            0|            0|  0.00%|            names (iterable of str): The desired dimension ordering of the
   803|         0|            0|            0|  0.00%|                output tensor. May contain up to one Ellipsis that is expanded
   804|         0|            0|            0|  0.00%|                to all unmentioned dim names of :attr:`self`.
   805|         0|            0|            0|  0.00%|
   806|         0|            0|            0|  0.00%|        Examples::
   807|         0|            0|            0|  0.00%|
   808|         0|            0|            0|  0.00%|            >>> tensor = torch.randn(2, 2, 2, 2, 2, 2)
   809|         0|            0|            0|  0.00%|            >>> named_tensor = tensor.refine_names('A', 'B', 'C', 'D', 'E', 'F')
   810|         0|            0|            0|  0.00%|
   811|         0|            0|            0|  0.00%|            # Move the F and E dims to the front while keeping the rest in order
   812|         0|            0|            0|  0.00%|            >>> named_tensor.align_to('F', 'E', ...)
   813|         0|            0|            0|  0.00%|
   814|         0|            0|            0|  0.00%|        .. warning::
   815|         0|            0|            0|  0.00%|            The named tensor API is experimental and subject to change.
   816|         0|            0|            0|  0.00%|
   817|         0|            0|            0|  0.00%|        """
   818|         0|            0|            0|  0.00%|        if has_torch_function_unary(self):
   819|         0|            0|            0|  0.00%|            return handle_torch_function(Tensor.align_to, (self,), self, *names)
   820|         0|            0|            0|  0.00%|        ellipsis_idx = single_ellipsis_index(names, 'align_to')
   821|         0|            0|            0|  0.00%|        if ellipsis_idx is None:
   822|         0|            0|            0|  0.00%|            return super(Tensor, self).align_to(names)
   823|         0|            0|            0|  0.00%|        return super(Tensor, self).align_to(
   824|         0|            0|            0|  0.00%|            [name for name in names if not is_ellipsis(name)],
   825|         0|            0|            0|  0.00%|            ellipsis_idx)
   826|         0|            0|            0|  0.00%|
   827|         0|            0|            0|  0.00%|    def unflatten(self, dim, sizes):
   828|         0|            0|            0|  0.00%|        r"""Expands the dimension :attr:`dim` of the :attr:`self` tensor over multiple dimensions
   829|         0|            0|            0|  0.00%|        of sizes given by :attr:`sizes`.
   830|         0|            0|            0|  0.00%|
   831|         0|            0|            0|  0.00%|        * :attr:`sizes` is the new shape of the unflattened dimension and it can be a `Tuple[int]` as well
   832|         0|            0|            0|  0.00%|          as `torch.Size` if :attr:`self` is a `Tensor`, or `namedshape` (Tuple[(name: str, size: int)])
   833|         0|            0|            0|  0.00%|          if :attr:`self` is a `NamedTensor`. The total number of elements in sizes must match the number
   834|         0|            0|            0|  0.00%|          of elements in the original dim being unflattened.
   835|         0|            0|            0|  0.00%|
   836|         0|            0|            0|  0.00%|        Args:
   837|         0|            0|            0|  0.00%|            dim (Union[int, str]): Dimension to unflatten
   838|         0|            0|            0|  0.00%|            sizes (Union[Tuple[int] or torch.Size, Tuple[Tuple[str, int]]]): New shape of the unflattened dimension
   839|         0|            0|            0|  0.00%|
   840|         0|            0|            0|  0.00%|        Examples:
   841|         0|            0|            0|  0.00%|            >>> torch.randn(3, 4, 1).unflatten(1, (2, 2)).shape
   842|         0|            0|            0|  0.00%|            torch.Size([3, 2, 2, 1])
   843|         0|            0|            0|  0.00%|            >>> torch.randn(3, 4, 1).unflatten(1, (-1, 2)).shape # the size -1 is inferred from the size of dimension 1
   844|         0|            0|            0|  0.00%|            torch.Size([3, 2, 2, 1])
   845|         0|            0|            0|  0.00%|            >>> torch.randn(2, 4, names=('A', 'B')).unflatten('B', (('B1', 2), ('B2', 2)))
   846|         0|            0|            0|  0.00%|            tensor([[[-1.1772,  0.0180],
   847|         0|            0|            0|  0.00%|                    [ 0.2412,  0.1431]],
   848|         0|            0|            0|  0.00%|                    [[-1.1819, -0.8899],
   849|         0|            0|            0|  0.00%|                    [ 1.5813,  0.2274]]], names=('A', 'B1', 'B2'))
   850|         0|            0|            0|  0.00%|            >>> torch.randn(2, names=('A',)).unflatten('A', (('B1', -1), ('B2', 1)))
   851|         0|            0|            0|  0.00%|            tensor([[-0.8591],
   852|         0|            0|            0|  0.00%|                    [ 0.3100]], names=('B1', 'B2'))
   853|         0|            0|            0|  0.00%|
   854|         0|            0|            0|  0.00%|        .. warning::
   855|         0|            0|            0|  0.00%|            The named tensor API is experimental and subject to change.
   856|         0|            0|            0|  0.00%|
   857|         0|            0|            0|  0.00%|        """
   858|         0|            0|            0|  0.00%|        if has_torch_function_unary(self):
   859|         0|            0|            0|  0.00%|            return handle_torch_function(Tensor.unflatten, (self,), self, dim, sizes)
   860|         0|            0|            0|  0.00%|
   861|         0|            0|            0|  0.00%|        if not sizes:
   862|         0|            0|            0|  0.00%|            raise RuntimeError("unflatten: sizes must be non-empty")
   863|         0|            0|            0|  0.00%|
   864|         0|            0|            0|  0.00%|        names = None
   865|         0|            0|            0|  0.00%|        if isinstance(sizes, OrderedDict) or (isinstance(sizes, (tuple, list)) and isinstance(sizes[0], (tuple, list))):
   866|         0|            0|            0|  0.00%|            names, sizes = unzip_namedshape(sizes)
   867|         0|            0|            0|  0.00%|        return super(Tensor, self).unflatten(dim, sizes, names)
   868|         0|            0|            0|  0.00%|
   869|         0|            0|            0|  0.00%|
   870|         0|            0|            0|  0.00%|    def rename_(self, *names, **rename_map):
   871|         0|            0|            0|  0.00%|        """In-place version of :meth:`~Tensor.rename`."""
   872|         0|            0|            0|  0.00%|
   873|         0|            0|            0|  0.00%|        if has_torch_function_unary(self):
   874|         0|            0|            0|  0.00%|            return handle_torch_function(Tensor.rename_, (self,), self, *names, **rename_map)
   875|         0|            0|            0|  0.00%|
   876|         0|            0|            0|  0.00%|        # Note [rename_ / rename API]
   877|         0|            0|            0|  0.00%|        # The Python API for these is different from the C++ API. In Python:
   878|         0|            0|            0|  0.00%|        # 1) tensor.rename(*names) takes a vararglist of names
   879|         0|            0|            0|  0.00%|        # 2) tensor.rename(**rename_map) takes a map of names to rename.
   880|         0|            0|            0|  0.00%|        # C++ is static, making it difficult to implement similar behavior.
   881|         0|            0|            0|  0.00%|        return update_names(self, names, rename_map, inplace=True)
   882|         0|            0|            0|  0.00%|
   883|         0|            0|            0|  0.00%|    def rename(self, *names, **rename_map):
   884|         0|            0|            0|  0.00%|        """Renames dimension names of :attr:`self`.
   885|         0|            0|            0|  0.00%|
   886|         0|            0|            0|  0.00%|        There are two main usages:
   887|         0|            0|            0|  0.00%|
   888|         0|            0|            0|  0.00%|        ``self.rename(**rename_map)`` returns a view on tensor that has dims
   889|         0|            0|            0|  0.00%|        renamed as specified in the mapping :attr:`rename_map`.
   890|         0|            0|            0|  0.00%|
   891|         0|            0|            0|  0.00%|        ``self.rename(*names)`` returns a view on tensor, renaming all
   892|         0|            0|            0|  0.00%|        dimensions positionally using :attr:`names`.
   893|         0|            0|            0|  0.00%|        Use ``self.rename(None)`` to drop names on a tensor.
   894|         0|            0|            0|  0.00%|
   895|         0|            0|            0|  0.00%|        One cannot specify both positional args :attr:`names` and keyword args
   896|         0|            0|            0|  0.00%|        :attr:`rename_map`.
   897|         0|            0|            0|  0.00%|
   898|         0|            0|            0|  0.00%|        Examples::
   899|         0|            0|            0|  0.00%|
   900|         0|            0|            0|  0.00%|            >>> imgs = torch.rand(2, 3, 5, 7, names=('N', 'C', 'H', 'W'))
   901|         0|            0|            0|  0.00%|            >>> renamed_imgs = imgs.rename(N='batch', C='channels')
   902|         0|            0|            0|  0.00%|            >>> renamed_imgs.names
   903|         0|            0|            0|  0.00%|            ('batch', 'channels', 'H', 'W')
   904|         0|            0|            0|  0.00%|
   905|         0|            0|            0|  0.00%|            >>> renamed_imgs = imgs.rename(None)
   906|         0|            0|            0|  0.00%|            >>> renamed_imgs.names
   907|         0|            0|            0|  0.00%|            (None,)
   908|         0|            0|            0|  0.00%|
   909|         0|            0|            0|  0.00%|            >>> renamed_imgs = imgs.rename('batch', 'channel', 'height', 'width')
   910|         0|            0|            0|  0.00%|            >>> renamed_imgs.names
   911|         0|            0|            0|  0.00%|            ('batch', 'channel', 'height', 'width')
   912|         0|            0|            0|  0.00%|
   913|         0|            0|            0|  0.00%|        .. warning::
   914|         0|            0|            0|  0.00%|            The named tensor API is experimental and subject to change.
   915|         0|            0|            0|  0.00%|
   916|         0|            0|            0|  0.00%|        """
   917|         0|            0|            0|  0.00%|        if has_torch_function_unary(self):
   918|         0|            0|            0|  0.00%|            return handle_torch_function(Tensor.rename, (self,), self, *names, **rename_map)
   919|         0|            0|            0|  0.00%|
   920|         0|            0|            0|  0.00%|        # See Note [rename_ / rename API]
   921|         0|            0|            0|  0.00%|        return update_names(self, names, rename_map, inplace=False)
   922|         0|            0|            0|  0.00%|
   923|         0|            0|            0|  0.00%|    def _to_sparse_csr(self):
   924|         0|            0|            0|  0.00%|        """ Convert a tensor to compressed row storage format. Only works with 2D tensors.
   925|         0|            0|            0|  0.00%|
   926|         0|            0|            0|  0.00%|        Examples::
   927|         0|            0|            0|  0.00%|
   928|         0|            0|            0|  0.00%|            >>> dense = torch.randn(5, 5)
   929|         0|            0|            0|  0.00%|            >>> sparse = dense._to_sparse_csr()
   930|         0|            0|            0|  0.00%|            >>> sparse._nnz()
   931|         0|            0|            0|  0.00%|            25
   932|         0|            0|            0|  0.00%|
   933|         0|            0|            0|  0.00%|        """
   934|         0|            0|            0|  0.00%|        shape = self.size()
   935|         0|            0|            0|  0.00%|        fill_value = 0
   936|         0|            0|            0|  0.00%|        if len(shape) != 2:
   937|         0|            0|            0|  0.00%|            raise RuntimeError("Only 2D tensors can be converted to the CSR format but got shape: ", shape)
   938|         0|            0|            0|  0.00%|
   939|         0|            0|            0|  0.00%|        if self.is_sparse:
   940|         0|            0|            0|  0.00%|            coalesced_self = self.coalesce()
   941|         0|            0|            0|  0.00%|            row_indices = coalesced_self.indices()[0]
   942|         0|            0|            0|  0.00%|            ro = [0]
   943|         0|            0|            0|  0.00%|            i = 0
   944|         0|            0|            0|  0.00%|            for irow in range(self.shape[0]):
   945|         0|            0|            0|  0.00%|                while i < row_indices.size()[0] and row_indices[i] == irow:
   946|         0|            0|            0|  0.00%|                    i += 1
   947|         0|            0|            0|  0.00%|                ro.append(i)
   948|         0|            0|            0|  0.00%|
   949|         0|            0|            0|  0.00%|            return torch._sparse_csr_tensor(torch.tensor(ro, dtype=row_indices.dtype),
   950|         0|            0|            0|  0.00%|                                            coalesced_self.indices()[1], coalesced_self.values(),
   951|         0|            0|            0|  0.00%|                                            size=coalesced_self.shape, dtype=coalesced_self.dtype)
   952|         0|            0|            0|  0.00%|        elif self.is_sparse_csr:
   953|         0|            0|            0|  0.00%|            return self
   954|         0|            0|            0|  0.00%|        else:
   955|         0|            0|            0|  0.00%|            return self.to_sparse()._to_sparse_csr()
   956|         0|            0|            0|  0.00%|
   957|         0|            0|            0|  0.00%|    def _update_names(self, names, inplace):
   958|         0|            0|            0|  0.00%|        if has_torch_function_unary(self):
   959|         0|            0|            0|  0.00%|            return handle_torch_function(Tensor._update_names, (self,), self, names, inplace)
   960|         0|            0|            0|  0.00%|
   961|         0|            0|            0|  0.00%|        # See Note [rename_ / rename API]
   962|         0|            0|            0|  0.00%|        if inplace:
   963|         0|            0|            0|  0.00%|            return super(Tensor, self).rename_(names)
   964|         0|            0|            0|  0.00%|        else:
   965|         0|            0|            0|  0.00%|            return super(Tensor, self).rename(names)
   966|         0|            0|            0|  0.00%|
   967|     32422|     0.140947|  4.34725e-06|  0.26%|    @property
   968|         0|            0|            0|  0.00%|    def grad(self):
   969|         0|            0|            0|  0.00%|        """
   970|         0|            0|            0|  0.00%|        This attribute is ``None`` by default and becomes a Tensor the first time a call to
   971|         0|            0|            0|  0.00%|        :func:`backward` computes gradients for ``self``.
   972|         0|            0|            0|  0.00%|        The attribute will then contain the gradients computed and future calls to
   973|         0|            0|            0|  0.00%|        :func:`backward` will accumulate (add) gradients into it.
   974|         0|            0|            0|  0.00%|        """
   975|     32422|     0.185432|  5.71932e-06|  0.35%|        if has_torch_function_unary(self):
   976|         0|            0|            0|  0.00%|            # TODO mypy doesn't support @property, see: https://github.com/python/mypy/issues/6185
   977|         0|            0|            0|  0.00%|            return handle_torch_function(Tensor.grad.__get__, (self,), self)  # type: ignore[attr-defined]
   978|         0|            0|            0|  0.00%|
   979|     32422|     0.218625|  6.74309e-06|  0.41%|        if self.requires_grad and not hasattr(self, "retains_grad") and not self.is_leaf and self._grad is None:
   980|         0|            0|            0|  0.00%|            warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "
   981|         0|            0|            0|  0.00%|                          "attribute won't be populated during autograd.backward(). If you indeed want the gradient "
   982|         0|            0|            0|  0.00%|                          "for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the "
   983|         0|            0|            0|  0.00%|                          "non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See "
   984|         0|            0|            0|  0.00%|                          "github.com/pytorch/pytorch/pull/30531 for more information.", stacklevel=2)
   985|     32422|     0.153522|  4.73511e-06|  0.29%|        return self._grad
   986|         0|            0|            0|  0.00%|
   987|         0|            0|            0|  0.00%|    @grad.setter
   988|         0|            0|            0|  0.00%|    def grad(self, new_grad):
   989|         0|            0|            0|  0.00%|        if has_torch_function_unary(self):
   990|         0|            0|            0|  0.00%|            # TODO mypy doesn't support @property, see: https://github.com/python/mypy/issues/6185
   991|         0|            0|            0|  0.00%|            return handle_torch_function(Tensor.grad.__set__, (self,), self, new_grad)  # type: ignore[attr-defined]
   992|         0|            0|            0|  0.00%|        self._grad = new_grad
   993|         0|            0|            0|  0.00%|
   994|         0|            0|            0|  0.00%|    @grad.deleter
   995|         0|            0|            0|  0.00%|    def grad(self):
   996|         0|            0|            0|  0.00%|        if has_torch_function_unary(self):
   997|         0|            0|            0|  0.00%|            # TODO mypy doesn't support @property, see: https://github.com/python/mypy/issues/6185
   998|         0|            0|            0|  0.00%|            return handle_torch_function(Tensor.grad.__delete__, (self,), self)  # type: ignore[attr-defined]
   999|         0|            0|            0|  0.00%|        del self._grad
  1000|         0|            0|            0|  0.00%|
  1001|         0|            0|            0|  0.00%|    @classmethod
  1002|         0|            0|            0|  0.00%|    def __torch_function__(cls, func, types, args=(), kwargs=None):
  1003|         0|            0|            0|  0.00%|        """
  1004|         0|            0|            0|  0.00%|        This __torch_function__ implementation wraps subclasses such that
  1005|         0|            0|            0|  0.00%|        methods called on subclasses return a subclass instance instead of
  1006|         0|            0|            0|  0.00%|        a ``torch.Tensor`` instance.
  1007|         0|            0|            0|  0.00%|
  1008|         0|            0|            0|  0.00%|        One corollary to this is that you need coverage for torch.Tensor
  1009|         0|            0|            0|  0.00%|        methods if implementing __torch_function__ for subclasses.
  1010|         0|            0|            0|  0.00%|
  1011|         0|            0|            0|  0.00%|        We recommend always calling ``super().__torch_function__`` as the base
  1012|         0|            0|            0|  0.00%|        case when doing the above.
  1013|         0|            0|            0|  0.00%|
  1014|         0|            0|            0|  0.00%|        While not mandatory, we recommend making `__torch_function__` a classmethod.
  1015|         0|            0|            0|  0.00%|        """
  1016|         0|            0|            0|  0.00%|        if kwargs is None:
  1017|         0|            0|            0|  0.00%|            kwargs = {}
  1018|         0|            0|            0|  0.00%|
  1019|         0|            0|            0|  0.00%|        if not all(issubclass(cls, t) for t in types):
  1020|         0|            0|            0|  0.00%|            return NotImplemented
  1021|         0|            0|            0|  0.00%|
  1022|         0|            0|            0|  0.00%|        with _C.DisableTorchFunction():
  1023|         0|            0|            0|  0.00%|            ret = func(*args, **kwargs)
  1024|         0|            0|            0|  0.00%|            return _convert(ret, cls)
  1025|         0|            0|            0|  0.00%|
  1026|         0|            0|            0|  0.00%|    __module__ = 'torch'
  1027|         0|            0|            0|  0.00%|
  1028|         0|            0|            0|  0.00%|def _convert(ret, cls):
  1029|         0|            0|            0|  0.00%|    if cls is Tensor:
  1030|         0|            0|            0|  0.00%|        return ret
  1031|         0|            0|            0|  0.00%|
  1032|         0|            0|            0|  0.00%|    if isinstance(ret, Tensor):
  1033|         0|            0|            0|  0.00%|        ret = ret.as_subclass(cls)
  1034|         0|            0|            0|  0.00%|
  1035|         0|            0|            0|  0.00%|    if isinstance(ret, (tuple, list)):
  1036|         0|            0|            0|  0.00%|        # Also handles things like namedtuples
  1037|         0|            0|            0|  0.00%|        ret = type(ret)(_convert(r, cls) for r in ret)
  1038|         0|            0|            0|  0.00%|
  1039|         0|            0|            0|  0.00%|    return ret
File: /opt/conda/lib/python3.8/site-packages/torch/optim/_functional.py
File duration: 0.806853s (1.51%)
Line #|      Hits|         Time| Time per hit|      %|Source code
------+----------+-------------+-------------+-------+-----------
     1|         0|            0|            0|  0.00%|r"""Functional interface"""
     2|         0|            0|            0|  0.00%|import math
     3|         0|            0|            0|  0.00%|import torch
     4|         0|            0|            0|  0.00%|from torch import Tensor
     5|         0|            0|            0|  0.00%|from typing import List, Optional
     6|         0|            0|            0|  0.00%|
     7|         0|            0|            0|  0.00%|# TODO: use foreach API in optim._functional to do all the computation
     8|         0|            0|            0|  0.00%|
     9|         0|            0|            0|  0.00%|def _make_sparse(grad, grad_indices, values):
    10|         0|            0|            0|  0.00%|    size = grad.size()
    11|         0|            0|            0|  0.00%|    if grad_indices.numel() == 0 or values.numel() == 0:
    12|         0|            0|            0|  0.00%|        return torch.empty_like(grad)
    13|         0|            0|            0|  0.00%|    return torch.sparse_coo_tensor(grad_indices, values, size)
    14|         0|            0|            0|  0.00%|
    15|         0|            0|            0|  0.00%|
    16|         0|            0|            0|  0.00%|def adagrad(params: List[Tensor],
    17|         0|            0|            0|  0.00%|            grads: List[Tensor],
    18|         0|            0|            0|  0.00%|            state_sums: List[Tensor],
    19|         0|            0|            0|  0.00%|            state_steps: List[int],
    20|         0|            0|            0|  0.00%|            *,
    21|         0|            0|            0|  0.00%|            lr: float,
    22|         0|            0|            0|  0.00%|            weight_decay: float,
    23|         0|            0|            0|  0.00%|            lr_decay: float,
    24|         0|            0|            0|  0.00%|            eps: float):
    25|         0|            0|            0|  0.00%|    r"""Functional API that performs Adagrad algorithm computation.
    26|         0|            0|            0|  0.00%|
    27|         0|            0|            0|  0.00%|    See :class:`~torch.optim.Adagrad` for details.
    28|         0|            0|            0|  0.00%|    """
    29|         0|            0|            0|  0.00%|
    30|         0|            0|            0|  0.00%|    for (param, grad, state_sum, step) in zip(params, grads, state_sums, state_steps):
    31|         0|            0|            0|  0.00%|        if weight_decay != 0:
    32|         0|            0|            0|  0.00%|            if grad.is_sparse:
    33|         0|            0|            0|  0.00%|                raise RuntimeError("weight_decay option is not compatible with sparse gradients")
    34|         0|            0|            0|  0.00%|            grad = grad.add(param, alpha=weight_decay)
    35|         0|            0|            0|  0.00%|
    36|         0|            0|            0|  0.00%|        clr = lr / (1 + (step - 1) * lr_decay)
    37|         0|            0|            0|  0.00%|
    38|         0|            0|            0|  0.00%|        if grad.is_sparse:
    39|         0|            0|            0|  0.00%|            grad = grad.coalesce()  # the update is non-linear so indices must be unique
    40|         0|            0|            0|  0.00%|            grad_indices = grad._indices()
    41|         0|            0|            0|  0.00%|            grad_values = grad._values()
    42|         0|            0|            0|  0.00%|            size = grad.size()
    43|         0|            0|            0|  0.00%|
    44|         0|            0|            0|  0.00%|            state_sum.add_(_make_sparse(grad, grad_indices, grad_values.pow(2)))
    45|         0|            0|            0|  0.00%|            std = state_sum.sparse_mask(grad)
    46|         0|            0|            0|  0.00%|            std_values = std._values().sqrt_().add_(eps)
    47|         0|            0|            0|  0.00%|            param.add_(_make_sparse(grad, grad_indices, grad_values / std_values), alpha=-clr)
    48|         0|            0|            0|  0.00%|        else:
    49|         0|            0|            0|  0.00%|            state_sum.addcmul_(grad, grad, value=1)
    50|         0|            0|            0|  0.00%|            std = state_sum.sqrt().add_(eps)
    51|         0|            0|            0|  0.00%|            param.addcdiv_(grad, std, value=-clr)
    52|         0|            0|            0|  0.00%|
    53|         0|            0|            0|  0.00%|
    54|         0|            0|            0|  0.00%|def adam(params: List[Tensor],
    55|         0|            0|            0|  0.00%|         grads: List[Tensor],
    56|         0|            0|            0|  0.00%|         exp_avgs: List[Tensor],
    57|         0|            0|            0|  0.00%|         exp_avg_sqs: List[Tensor],
    58|         0|            0|            0|  0.00%|         max_exp_avg_sqs: List[Tensor],
    59|         0|            0|            0|  0.00%|         state_steps: List[int],
    60|         0|            0|            0|  0.00%|         *,
    61|         0|            0|            0|  0.00%|         amsgrad: bool,
    62|         0|            0|            0|  0.00%|         beta1: float,
    63|         0|            0|            0|  0.00%|         beta2: float,
    64|         0|            0|            0|  0.00%|         lr: float,
    65|         0|            0|            0|  0.00%|         weight_decay: float,
    66|         0|            0|            0|  0.00%|         eps: float):
    67|         0|            0|            0|  0.00%|    r"""Functional API that performs Adam algorithm computation.
    68|         0|            0|            0|  0.00%|
    69|         0|            0|            0|  0.00%|    See :class:`~torch.optim.Adam` for details.
    70|         0|            0|            0|  0.00%|    """
    71|         0|            0|            0|  0.00%|
    72|         0|            0|            0|  0.00%|    for i, param in enumerate(params):
    73|         0|            0|            0|  0.00%|
    74|         0|            0|            0|  0.00%|        grad = grads[i]
    75|         0|            0|            0|  0.00%|        exp_avg = exp_avgs[i]
    76|         0|            0|            0|  0.00%|        exp_avg_sq = exp_avg_sqs[i]
    77|         0|            0|            0|  0.00%|        step = state_steps[i]
    78|         0|            0|            0|  0.00%|
    79|         0|            0|            0|  0.00%|        bias_correction1 = 1 - beta1 ** step
    80|         0|            0|            0|  0.00%|        bias_correction2 = 1 - beta2 ** step
    81|         0|            0|            0|  0.00%|
    82|         0|            0|            0|  0.00%|        if weight_decay != 0:
    83|         0|            0|            0|  0.00%|            grad = grad.add(param, alpha=weight_decay)
    84|         0|            0|            0|  0.00%|
    85|         0|            0|            0|  0.00%|        # Decay the first and second moment running average coefficient
    86|         0|            0|            0|  0.00%|        exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)
    87|         0|            0|            0|  0.00%|        exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)
    88|         0|            0|            0|  0.00%|        if amsgrad:
    89|         0|            0|            0|  0.00%|            # Maintains the maximum of all 2nd moment running avg. till now
    90|         0|            0|            0|  0.00%|            torch.maximum(max_exp_avg_sqs[i], exp_avg_sq, out=max_exp_avg_sqs[i])
    91|         0|            0|            0|  0.00%|            # Use the max. for normalizing running avg. of gradient
    92|         0|            0|            0|  0.00%|            denom = (max_exp_avg_sqs[i].sqrt() / math.sqrt(bias_correction2)).add_(eps)
    93|         0|            0|            0|  0.00%|        else:
    94|         0|            0|            0|  0.00%|            denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(eps)
    95|         0|            0|            0|  0.00%|
    96|         0|            0|            0|  0.00%|        step_size = lr / bias_correction1
    97|         0|            0|            0|  0.00%|
    98|         0|            0|            0|  0.00%|        param.addcdiv_(exp_avg, denom, value=-step_size)
    99|         0|            0|            0|  0.00%|
   100|         0|            0|            0|  0.00%|
   101|         0|            0|            0|  0.00%|def adamw(params: List[Tensor],
   102|         0|            0|            0|  0.00%|          grads: List[Tensor],
   103|         0|            0|            0|  0.00%|          exp_avgs: List[Tensor],
   104|         0|            0|            0|  0.00%|          exp_avg_sqs: List[Tensor],
   105|         0|            0|            0|  0.00%|          max_exp_avg_sqs: List[Tensor],
   106|         0|            0|            0|  0.00%|          state_steps: List[int],
   107|         0|            0|            0|  0.00%|          *,
   108|         0|            0|            0|  0.00%|          amsgrad: bool,
   109|         0|            0|            0|  0.00%|          beta1: float,
   110|         0|            0|            0|  0.00%|          beta2: float,
   111|         0|            0|            0|  0.00%|          lr: float,
   112|         0|            0|            0|  0.00%|          weight_decay: float,
   113|         0|            0|            0|  0.00%|          eps: float):
   114|         0|            0|            0|  0.00%|    r"""Functional API that performs AdamW algorithm computation.
   115|         0|            0|            0|  0.00%|
   116|         0|            0|            0|  0.00%|    See :class:`~torch.optim.AdamW` for details.
   117|         0|            0|            0|  0.00%|    """
   118|         0|            0|            0|  0.00%|    for i, param in enumerate(params):
   119|         0|            0|            0|  0.00%|        grad = grads[i]
   120|         0|            0|            0|  0.00%|        exp_avg = exp_avgs[i]
   121|         0|            0|            0|  0.00%|        exp_avg_sq = exp_avg_sqs[i]
   122|         0|            0|            0|  0.00%|        step = state_steps[i]
   123|         0|            0|            0|  0.00%|
   124|         0|            0|            0|  0.00%|        # Perform stepweight decay
   125|         0|            0|            0|  0.00%|        param.mul_(1 - lr * weight_decay)
   126|         0|            0|            0|  0.00%|
   127|         0|            0|            0|  0.00%|        bias_correction1 = 1 - beta1 ** step
   128|         0|            0|            0|  0.00%|        bias_correction2 = 1 - beta2 ** step
   129|         0|            0|            0|  0.00%|
   130|         0|            0|            0|  0.00%|        # Decay the first and second moment running average coefficient
   131|         0|            0|            0|  0.00%|        exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)
   132|         0|            0|            0|  0.00%|        exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)
   133|         0|            0|            0|  0.00%|        if amsgrad:
   134|         0|            0|            0|  0.00%|            # Maintains the maximum of all 2nd moment running avg. till now
   135|         0|            0|            0|  0.00%|            torch.maximum(max_exp_avg_sqs[i], exp_avg_sq, out=max_exp_avg_sqs[i])
   136|         0|            0|            0|  0.00%|            # Use the max. for normalizing running avg. of gradient
   137|         0|            0|            0|  0.00%|            denom = (max_exp_avg_sqs[i].sqrt() / math.sqrt(bias_correction2)).add_(eps)
   138|         0|            0|            0|  0.00%|        else:
   139|         0|            0|            0|  0.00%|            denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(eps)
   140|         0|            0|            0|  0.00%|
   141|         0|            0|            0|  0.00%|        step_size = lr / bias_correction1
   142|         0|            0|            0|  0.00%|
   143|         0|            0|            0|  0.00%|        param.addcdiv_(exp_avg, denom, value=-step_size)
   144|         0|            0|            0|  0.00%|
   145|         0|            0|            0|  0.00%|
   146|        61|  0.000486135|  7.96943e-06|  0.00%|def sgd(params: List[Tensor],
   147|         0|            0|            0|  0.00%|        d_p_list: List[Tensor],
   148|         0|            0|            0|  0.00%|        momentum_buffer_list: List[Optional[Tensor]],
   149|         0|            0|            0|  0.00%|        *,
   150|         0|            0|            0|  0.00%|        weight_decay: float,
   151|         0|            0|            0|  0.00%|        momentum: float,
   152|         0|            0|            0|  0.00%|        lr: float,
   153|         0|            0|            0|  0.00%|        dampening: float,
   154|         0|            0|            0|  0.00%|        nesterov: bool):
   155|         0|            0|            0|  0.00%|    r"""Functional API that performs SGD algorithm computation.
   156|         0|            0|            0|  0.00%|
   157|         0|            0|            0|  0.00%|    See :class:`~torch.optim.SGD` for details.
   158|         0|            0|            0|  0.00%|    """
   159|         0|            0|            0|  0.00%|
   160|      3843|    0.0272458|  7.08971e-06|  0.05%|    for i, param in enumerate(params):
   161|         0|            0|            0|  0.00%|
   162|      3782|    0.0170343|  4.50404e-06|  0.03%|        d_p = d_p_list[i]
   163|      3782|    0.0161614|  4.27325e-06|  0.03%|        if weight_decay != 0:
   164|         0|            0|            0|  0.00%|            d_p = d_p.add(param, alpha=weight_decay)
   165|         0|            0|            0|  0.00%|
   166|      3782|    0.0172288|  4.55548e-06|  0.03%|        if momentum != 0:
   167|      3782|    0.0166187|  4.39416e-06|  0.03%|            buf = momentum_buffer_list[i]
   168|         0|            0|            0|  0.00%|
   169|      3782|    0.0159092|  4.20656e-06|  0.03%|            if buf is None:
   170|        62|   0.00533915|  8.61153e-05|  0.01%|                buf = torch.clone(d_p).detach()
   171|        62|  0.000458956|  7.40251e-06|  0.00%|                momentum_buffer_list[i] = buf
   172|         0|            0|            0|  0.00%|            else:
   173|      3720|     0.398279|  0.000107064|  0.74%|                buf.mul_(momentum).add_(d_p, alpha=1 - dampening)
   174|         0|            0|            0|  0.00%|
   175|      3782|    0.0307646|  8.13447e-06|  0.06%|            if nesterov:
   176|         0|            0|            0|  0.00%|                d_p = d_p.add(buf, alpha=momentum)
   177|         0|            0|            0|  0.00%|            else:
   178|      3782|    0.0166132|  4.39271e-06|  0.03%|                d_p = buf
   179|         0|            0|            0|  0.00%|
   180|      3782|     0.244714|  6.47049e-05|  0.46%|        param.add_(d_p, alpha=-lr)
   181|         0|            0|            0|  0.00%|
   182|         0|            0|            0|  0.00%|
   183|         0|            0|            0|  0.00%|def adadelta(params: List[Tensor],
   184|         0|            0|            0|  0.00%|             grads: List[Tensor],
   185|         0|            0|            0|  0.00%|             square_avgs: List[Tensor],
   186|         0|            0|            0|  0.00%|             acc_deltas: List[Tensor],
   187|         0|            0|            0|  0.00%|             *,
   188|         0|            0|            0|  0.00%|             lr: float,
   189|         0|            0|            0|  0.00%|             rho: float,
   190|         0|            0|            0|  0.00%|             eps: float,
   191|         0|            0|            0|  0.00%|             weight_decay: float):
   192|         0|            0|            0|  0.00%|    r"""Functional API that performs Adadelta algorithm computation.
   193|         0|            0|            0|  0.00%|
   194|         0|            0|            0|  0.00%|    See :class:`~torch.optim.Adadelta` for details.
   195|         0|            0|            0|  0.00%|    """
   196|         0|            0|            0|  0.00%|
   197|         0|            0|            0|  0.00%|    for (param, grad, square_avg, acc_delta) in zip(params, grads, square_avgs, acc_deltas):
   198|         0|            0|            0|  0.00%|        if weight_decay != 0:
   199|         0|            0|            0|  0.00%|            grad = grad.add(param, alpha=weight_decay)
   200|         0|            0|            0|  0.00%|
   201|         0|            0|            0|  0.00%|        square_avg.mul_(rho).addcmul_(grad, grad, value=1 - rho)
   202|         0|            0|            0|  0.00%|        std = square_avg.add(eps).sqrt_()
   203|         0|            0|            0|  0.00%|        delta = acc_delta.add(eps).sqrt_().div_(std).mul_(grad)
   204|         0|            0|            0|  0.00%|        param.add_(delta, alpha=-lr)
   205|         0|            0|            0|  0.00%|        acc_delta.mul_(rho).addcmul_(delta, delta, value=1 - rho)
   206|         0|            0|            0|  0.00%|
   207|         0|            0|            0|  0.00%|
   208|         0|            0|            0|  0.00%|def rmsprop(params: List[Tensor],
   209|         0|            0|            0|  0.00%|            grads: List[Tensor],
   210|         0|            0|            0|  0.00%|            square_avgs: List[Tensor],
   211|         0|            0|            0|  0.00%|            grad_avgs: List[Tensor],
   212|         0|            0|            0|  0.00%|            momentum_buffer_list: List[Tensor],
   213|         0|            0|            0|  0.00%|            *,
   214|         0|            0|            0|  0.00%|            lr: float,
   215|         0|            0|            0|  0.00%|            alpha: float,
   216|         0|            0|            0|  0.00%|            eps: float,
   217|         0|            0|            0|  0.00%|            weight_decay: float,
   218|         0|            0|            0|  0.00%|            momentum: float,
   219|         0|            0|            0|  0.00%|            centered: bool):
   220|         0|            0|            0|  0.00%|    r"""Functional API that performs rmsprop algorithm computation.
   221|         0|            0|            0|  0.00%|
   222|         0|            0|            0|  0.00%|    See :class:`~torch.optim.RMSProp` for details.
   223|         0|            0|            0|  0.00%|    """
   224|         0|            0|            0|  0.00%|
   225|         0|            0|            0|  0.00%|    for i, param in enumerate(params):
   226|         0|            0|            0|  0.00%|        grad = grads[i]
   227|         0|            0|            0|  0.00%|        square_avg = square_avgs[i]
   228|         0|            0|            0|  0.00%|
   229|         0|            0|            0|  0.00%|        if weight_decay != 0:
   230|         0|            0|            0|  0.00%|            grad = grad.add(param, alpha=weight_decay)
   231|         0|            0|            0|  0.00%|
   232|         0|            0|            0|  0.00%|        square_avg.mul_(alpha).addcmul_(grad, grad, value=1 - alpha)
   233|         0|            0|            0|  0.00%|
   234|         0|            0|            0|  0.00%|        if centered:
   235|         0|            0|            0|  0.00%|            grad_avg = grad_avgs[i]
   236|         0|            0|            0|  0.00%|            grad_avg.mul_(alpha).add_(grad, alpha=1 - alpha)
   237|         0|            0|            0|  0.00%|            avg = square_avg.addcmul(grad_avg, grad_avg, value=-1).sqrt_().add_(eps)
   238|         0|            0|            0|  0.00%|        else:
   239|         0|            0|            0|  0.00%|            avg = square_avg.sqrt().add_(eps)
   240|         0|            0|            0|  0.00%|
   241|         0|            0|            0|  0.00%|        if momentum > 0:
   242|         0|            0|            0|  0.00%|            buf = momentum_buffer_list[i]
   243|         0|            0|            0|  0.00%|            buf.mul_(momentum).addcdiv_(grad, avg)
   244|         0|            0|            0|  0.00%|            param.add_(buf, alpha=-lr)
   245|         0|            0|            0|  0.00%|        else:
   246|         0|            0|            0|  0.00%|            param.addcdiv_(grad, avg, value=-lr)
   247|         0|            0|            0|  0.00%|
   248|         0|            0|            0|  0.00%|
   249|         0|            0|            0|  0.00%|def rprop(params: List[Tensor],
   250|         0|            0|            0|  0.00%|          grads: List[Tensor],
   251|         0|            0|            0|  0.00%|          prevs: List[Tensor],
   252|         0|            0|            0|  0.00%|          step_sizes: List[Tensor],
   253|         0|            0|            0|  0.00%|          *,
   254|         0|            0|            0|  0.00%|          step_size_min: float,
   255|         0|            0|            0|  0.00%|          step_size_max: float,
   256|         0|            0|            0|  0.00%|          etaminus: float,
   257|         0|            0|            0|  0.00%|          etaplus: float):
   258|         0|            0|            0|  0.00%|    r"""Functional API that performs rprop algorithm computation.
   259|         0|            0|            0|  0.00%|
   260|         0|            0|            0|  0.00%|    See :class:`~torch.optim.Rprop` for details.
   261|         0|            0|            0|  0.00%|    """
   262|         0|            0|            0|  0.00%|
   263|         0|            0|            0|  0.00%|    for i, param in enumerate(params):
   264|         0|            0|            0|  0.00%|        grad = grads[i]
   265|         0|            0|            0|  0.00%|        prev = prevs[i]
   266|         0|            0|            0|  0.00%|        step_size = step_sizes[i]
   267|         0|            0|            0|  0.00%|
   268|         0|            0|            0|  0.00%|        sign = grad.mul(prev).sign()
   269|         0|            0|            0|  0.00%|        sign[sign.gt(0)] = etaplus
   270|         0|            0|            0|  0.00%|        sign[sign.lt(0)] = etaminus
   271|         0|            0|            0|  0.00%|        sign[sign.eq(0)] = 1
   272|         0|            0|            0|  0.00%|
   273|         0|            0|            0|  0.00%|        # update stepsizes with step size updates
   274|         0|            0|            0|  0.00%|        step_size.mul_(sign).clamp_(step_size_min, step_size_max)
   275|         0|            0|            0|  0.00%|
   276|         0|            0|            0|  0.00%|        # for dir<0, dfdx=0
   277|         0|            0|            0|  0.00%|        # for dir>=0 dfdx=dfdx
   278|         0|            0|            0|  0.00%|        grad = grad.clone(memory_format=torch.preserve_format)
   279|         0|            0|            0|  0.00%|        grad[sign.eq(etaminus)] = 0
   280|         0|            0|            0|  0.00%|
   281|         0|            0|            0|  0.00%|        # update parameters
   282|         0|            0|            0|  0.00%|        param.addcmul_(grad.sign(), step_size, value=-1)
   283|         0|            0|            0|  0.00%|
   284|         0|            0|            0|  0.00%|        prev.copy_(grad)
   285|         0|            0|            0|  0.00%|
   286|         0|            0|            0|  0.00%|
   287|         0|            0|            0|  0.00%|def adamax(params: List[Tensor],
   288|         0|            0|            0|  0.00%|           grads: List[Tensor],
   289|         0|            0|            0|  0.00%|           exp_avgs: List[Tensor],
   290|         0|            0|            0|  0.00%|           exp_infs: List[Tensor],
   291|         0|            0|            0|  0.00%|           state_steps: List[int],
   292|         0|            0|            0|  0.00%|           *,
   293|         0|            0|            0|  0.00%|           eps: float,
   294|         0|            0|            0|  0.00%|           beta1: float,
   295|         0|            0|            0|  0.00%|           beta2: float,
   296|         0|            0|            0|  0.00%|           lr: float,
   297|         0|            0|            0|  0.00%|           weight_decay: float):
   298|         0|            0|            0|  0.00%|    r"""Functional API that performs adamax algorithm computation.
   299|         0|            0|            0|  0.00%|
   300|         0|            0|            0|  0.00%|    See :class:`~torch.optim.Adamax` for details.
   301|         0|            0|            0|  0.00%|    """
   302|         0|            0|            0|  0.00%|
   303|         0|            0|            0|  0.00%|    for i, param in enumerate(params):
   304|         0|            0|            0|  0.00%|        grad = grads[i]
   305|         0|            0|            0|  0.00%|        exp_avg = exp_avgs[i]
   306|         0|            0|            0|  0.00%|        exp_inf = exp_infs[i]
   307|         0|            0|            0|  0.00%|        step = state_steps[i]
   308|         0|            0|            0|  0.00%|
   309|         0|            0|            0|  0.00%|        if weight_decay != 0:
   310|         0|            0|            0|  0.00%|            grad = grad.add(param, alpha=weight_decay)
   311|         0|            0|            0|  0.00%|
   312|         0|            0|            0|  0.00%|        # Update biased first moment estimate.
   313|         0|            0|            0|  0.00%|        exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)
   314|         0|            0|            0|  0.00%|        # Update the exponentially weighted infinity norm.
   315|         0|            0|            0|  0.00%|        norm_buf = torch.cat([
   316|         0|            0|            0|  0.00%|            exp_inf.mul_(beta2).unsqueeze(0),
   317|         0|            0|            0|  0.00%|            grad.abs().add_(eps).unsqueeze_(0)
   318|         0|            0|            0|  0.00%|        ], 0)
   319|         0|            0|            0|  0.00%|        torch.amax(norm_buf, 0, keepdim=False, out=exp_inf)
   320|         0|            0|            0|  0.00%|
   321|         0|            0|            0|  0.00%|        bias_correction = 1 - beta1 ** step
   322|         0|            0|            0|  0.00%|        clr = lr / bias_correction
   323|         0|            0|            0|  0.00%|
   324|         0|            0|            0|  0.00%|        param.addcdiv_(exp_avg, exp_inf, value=-clr)
File: /opt/conda/lib/python3.8/site-packages/torch/optim/optimizer.py
File duration: 0.710222s (1.33%)
Line #|      Hits|         Time| Time per hit|      %|Source code
------+----------+-------------+-------------+-------+-----------
     1|         0|            0|            0|  0.00%|from collections import defaultdict, abc as container_abcs
     2|         0|            0|            0|  0.00%|
     3|         0|            0|            0|  0.00%|import torch
     4|         0|            0|            0|  0.00%|from copy import deepcopy
     5|         0|            0|            0|  0.00%|from itertools import chain
     6|         0|            0|            0|  0.00%|import warnings
     7|         0|            0|            0|  0.00%|import functools
     8|         0|            0|            0|  0.00%|
     9|         0|            0|            0|  0.00%|
    10|         0|            0|            0|  0.00%|class _RequiredParameter(object):
    11|         0|            0|            0|  0.00%|    """Singleton class representing a required parameter for an Optimizer."""
    12|         0|            0|            0|  0.00%|    def __repr__(self):
    13|         0|            0|            0|  0.00%|        return "<required parameter>"
    14|         0|            0|            0|  0.00%|
    15|         0|            0|            0|  0.00%|required = _RequiredParameter()
    16|         0|            0|            0|  0.00%|
    17|         0|            0|            0|  0.00%|
    18|         0|            0|            0|  0.00%|class Optimizer(object):
    19|         0|            0|            0|  0.00%|    r"""Base class for all optimizers.
    20|         0|            0|            0|  0.00%|
    21|         0|            0|            0|  0.00%|    .. warning::
    22|         0|            0|            0|  0.00%|        Parameters need to be specified as collections that have a deterministic
    23|         0|            0|            0|  0.00%|        ordering that is consistent between runs. Examples of objects that don't
    24|         0|            0|            0|  0.00%|        satisfy those properties are sets and iterators over values of dictionaries.
    25|         0|            0|            0|  0.00%|
    26|         0|            0|            0|  0.00%|    Args:
    27|         0|            0|            0|  0.00%|        params (iterable): an iterable of :class:`torch.Tensor` s or
    28|         0|            0|            0|  0.00%|            :class:`dict` s. Specifies what Tensors should be optimized.
    29|         0|            0|            0|  0.00%|        defaults: (dict): a dict containing default values of optimization
    30|         0|            0|            0|  0.00%|            options (used when a parameter group doesn't specify them).
    31|         0|            0|            0|  0.00%|    """
    32|         0|            0|            0|  0.00%|
    33|         0|            0|            0|  0.00%|    def __init__(self, params, defaults):
    34|         0|            0|            0|  0.00%|        torch._C._log_api_usage_once("python.optimizer")
    35|         0|            0|            0|  0.00%|        self.defaults = defaults
    36|         0|            0|            0|  0.00%|
    37|         0|            0|            0|  0.00%|        self._hook_for_profile()
    38|         0|            0|            0|  0.00%|
    39|         0|            0|            0|  0.00%|        if isinstance(params, torch.Tensor):
    40|         0|            0|            0|  0.00%|            raise TypeError("params argument given to the optimizer should be "
    41|         0|            0|            0|  0.00%|                            "an iterable of Tensors or dicts, but got " +
    42|         0|            0|            0|  0.00%|                            torch.typename(params))
    43|         0|            0|            0|  0.00%|
    44|         0|            0|            0|  0.00%|        self.state = defaultdict(dict)
    45|         0|            0|            0|  0.00%|        self.param_groups = []
    46|         0|            0|            0|  0.00%|
    47|         0|            0|            0|  0.00%|        param_groups = list(params)
    48|         0|            0|            0|  0.00%|        if len(param_groups) == 0:
    49|         0|            0|            0|  0.00%|            raise ValueError("optimizer got an empty parameter list")
    50|         0|            0|            0|  0.00%|        if not isinstance(param_groups[0], dict):
    51|         0|            0|            0|  0.00%|            param_groups = [{'params': param_groups}]
    52|         0|            0|            0|  0.00%|
    53|         0|            0|            0|  0.00%|        for param_group in param_groups:
    54|         0|            0|            0|  0.00%|            self.add_param_group(param_group)
    55|         0|            0|            0|  0.00%|
    56|         0|            0|            0|  0.00%|    def __getstate__(self):
    57|         0|            0|            0|  0.00%|        return {
    58|         0|            0|            0|  0.00%|            'defaults': self.defaults,
    59|         0|            0|            0|  0.00%|            'state': self.state,
    60|         0|            0|            0|  0.00%|            'param_groups': self.param_groups,
    61|         0|            0|            0|  0.00%|        }
    62|         0|            0|            0|  0.00%|
    63|         0|            0|            0|  0.00%|    def __setstate__(self, state):
    64|         0|            0|            0|  0.00%|        self.__dict__.update(state)
    65|         0|            0|            0|  0.00%|        self._hook_for_profile()  # To support multiprocessing pickle/unpickle.
    66|         0|            0|            0|  0.00%|
    67|         0|            0|            0|  0.00%|    def __repr__(self):
    68|         0|            0|            0|  0.00%|        format_string = self.__class__.__name__ + ' ('
    69|         0|            0|            0|  0.00%|        for i, group in enumerate(self.param_groups):
    70|         0|            0|            0|  0.00%|            format_string += '\n'
    71|         0|            0|            0|  0.00%|            format_string += 'Parameter Group {0}\n'.format(i)
    72|         0|            0|            0|  0.00%|            for key in sorted(group.keys()):
    73|         0|            0|            0|  0.00%|                if key != 'params':
    74|         0|            0|            0|  0.00%|                    format_string += '    {0}: {1}\n'.format(key, group[key])
    75|         0|            0|            0|  0.00%|        format_string += ')'
    76|         0|            0|            0|  0.00%|        return format_string
    77|         0|            0|            0|  0.00%|
    78|         0|            0|            0|  0.00%|    def _hook_for_profile(self):
    79|         0|            0|            0|  0.00%|        self._zero_grad_profile_name = "Optimizer.zero_grad#{}.zero_grad".format(self.__class__.__name__)
    80|         0|            0|            0|  0.00%|
    81|         0|            0|            0|  0.00%|        def profile_hook_step(func):
    82|         0|            0|            0|  0.00%|
    83|        61|  0.000439405|  7.20337e-06|  0.00%|            @functools.wraps(func)
    84|         0|            0|            0|  0.00%|            def wrapper(*args, **kwargs):
    85|        61|  0.000671625|  1.10102e-05|  0.00%|                obj, *_ = args
    86|        61|   0.00114608|  1.87882e-05|  0.00%|                profile_name = "Optimizer.step#{}.step".format(obj.__class__.__name__)
    87|        61|   0.00243521|  3.99214e-05|  0.00%|                with torch.autograd.profiler.record_function(profile_name):
(call)|        61|   0.00333405|  5.46565e-05|  0.01%|# /opt/conda/lib/python3.8/site-packages/torch/autograd/profiler.py:605 __init__
(call)|        61|   0.00454092|  7.44413e-05|  0.01%|# /opt/conda/lib/python3.8/site-packages/torch/autograd/profiler.py:613 __enter__
    88|        61|   0.00244999|  4.01638e-05|  0.00%|                    return func(*args, **kwargs)
(call)|        61|      1.43772|    0.0235692|  2.68%|# /opt/conda/lib/python3.8/site-packages/torch/autograd/grad_mode.py:25 decorate_context
(call)|        61|   0.00334859|  5.48949e-05|  0.01%|# /opt/conda/lib/python3.8/site-packages/torch/autograd/profiler.py:617 __exit__
    89|         0|            0|            0|  0.00%|            return wrapper
    90|         0|            0|            0|  0.00%|
    91|         0|            0|            0|  0.00%|        hooked = getattr(self.__class__.step, "hooked", None)
    92|         0|            0|            0|  0.00%|        if not hooked:
    93|         0|            0|            0|  0.00%|            self.__class__.step = profile_hook_step(self.__class__.step)
    94|         0|            0|            0|  0.00%|            self.__class__.step.hooked = True
    95|         0|            0|            0|  0.00%|
    96|         0|            0|            0|  0.00%|    def state_dict(self):
    97|         0|            0|            0|  0.00%|        r"""Returns the state of the optimizer as a :class:`dict`.
    98|         0|            0|            0|  0.00%|
    99|         0|            0|            0|  0.00%|        It contains two entries:
   100|         0|            0|            0|  0.00%|
   101|         0|            0|            0|  0.00%|        * state - a dict holding current optimization state. Its content
   102|         0|            0|            0|  0.00%|            differs between optimizer classes.
   103|         0|            0|            0|  0.00%|        * param_groups - a dict containing all parameter groups
   104|         0|            0|            0|  0.00%|        """
   105|         0|            0|            0|  0.00%|        # Save order indices instead of Tensors
   106|         0|            0|            0|  0.00%|        param_mappings = {}
   107|         0|            0|            0|  0.00%|        start_index = 0
   108|         0|            0|            0|  0.00%|
   109|         0|            0|            0|  0.00%|        def pack_group(group):
   110|         0|            0|            0|  0.00%|            nonlocal start_index
   111|         0|            0|            0|  0.00%|            packed = {k: v for k, v in group.items() if k != 'params'}
   112|         0|            0|            0|  0.00%|            param_mappings.update({id(p): i for i, p in enumerate(group['params'], start_index)
   113|         0|            0|            0|  0.00%|                                   if id(p) not in param_mappings})
   114|         0|            0|            0|  0.00%|            packed['params'] = [param_mappings[id(p)] for p in group['params']]
   115|         0|            0|            0|  0.00%|            start_index += len(packed['params'])
   116|         0|            0|            0|  0.00%|            return packed
   117|         0|            0|            0|  0.00%|        param_groups = [pack_group(g) for g in self.param_groups]
   118|         0|            0|            0|  0.00%|        # Remap state to use order indices as keys
   119|         0|            0|            0|  0.00%|        packed_state = {(param_mappings[id(k)] if isinstance(k, torch.Tensor) else k): v
   120|         0|            0|            0|  0.00%|                        for k, v in self.state.items()}
   121|         0|            0|            0|  0.00%|        return {
   122|         0|            0|            0|  0.00%|            'state': packed_state,
   123|         0|            0|            0|  0.00%|            'param_groups': param_groups,
   124|         0|            0|            0|  0.00%|        }
   125|         0|            0|            0|  0.00%|
   126|         0|            0|            0|  0.00%|    def load_state_dict(self, state_dict):
   127|         0|            0|            0|  0.00%|        r"""Loads the optimizer state.
   128|         0|            0|            0|  0.00%|
   129|         0|            0|            0|  0.00%|        Args:
   130|         0|            0|            0|  0.00%|            state_dict (dict): optimizer state. Should be an object returned
   131|         0|            0|            0|  0.00%|                from a call to :meth:`state_dict`.
   132|         0|            0|            0|  0.00%|        """
   133|         0|            0|            0|  0.00%|        # deepcopy, to be consistent with module API
   134|         0|            0|            0|  0.00%|        state_dict = deepcopy(state_dict)
   135|         0|            0|            0|  0.00%|        # Validate the state_dict
   136|         0|            0|            0|  0.00%|        groups = self.param_groups
   137|         0|            0|            0|  0.00%|        saved_groups = state_dict['param_groups']
   138|         0|            0|            0|  0.00%|
   139|         0|            0|            0|  0.00%|        if len(groups) != len(saved_groups):
   140|         0|            0|            0|  0.00%|            raise ValueError("loaded state dict has a different number of "
   141|         0|            0|            0|  0.00%|                             "parameter groups")
   142|         0|            0|            0|  0.00%|        param_lens = (len(g['params']) for g in groups)
   143|         0|            0|            0|  0.00%|        saved_lens = (len(g['params']) for g in saved_groups)
   144|         0|            0|            0|  0.00%|        if any(p_len != s_len for p_len, s_len in zip(param_lens, saved_lens)):
   145|         0|            0|            0|  0.00%|            raise ValueError("loaded state dict contains a parameter group "
   146|         0|            0|            0|  0.00%|                             "that doesn't match the size of optimizer's group")
   147|         0|            0|            0|  0.00%|
   148|         0|            0|            0|  0.00%|        # Update the state
   149|         0|            0|            0|  0.00%|        id_map = {old_id: p for old_id, p in
   150|         0|            0|            0|  0.00%|                  zip(chain.from_iterable((g['params'] for g in saved_groups)),
   151|         0|            0|            0|  0.00%|                      chain.from_iterable((g['params'] for g in groups)))}
   152|         0|            0|            0|  0.00%|
   153|         0|            0|            0|  0.00%|        def cast(param, value):
   154|         0|            0|            0|  0.00%|            r"""Make a deep copy of value, casting all tensors to device of param."""
   155|         0|            0|            0|  0.00%|            if isinstance(value, torch.Tensor):
   156|         0|            0|            0|  0.00%|                # Floating-point types are a bit special here. They are the only ones
   157|         0|            0|            0|  0.00%|                # that are assumed to always match the type of params.
   158|         0|            0|            0|  0.00%|                if param.is_floating_point():
   159|         0|            0|            0|  0.00%|                    value = value.to(param.dtype)
   160|         0|            0|            0|  0.00%|                value = value.to(param.device)
   161|         0|            0|            0|  0.00%|                return value
   162|         0|            0|            0|  0.00%|            elif isinstance(value, dict):
   163|         0|            0|            0|  0.00%|                return {k: cast(param, v) for k, v in value.items()}
   164|         0|            0|            0|  0.00%|            elif isinstance(value, container_abcs.Iterable):
   165|         0|            0|            0|  0.00%|                return type(value)(cast(param, v) for v in value)
   166|         0|            0|            0|  0.00%|            else:
   167|         0|            0|            0|  0.00%|                return value
   168|         0|            0|            0|  0.00%|
   169|         0|            0|            0|  0.00%|        # Copy state assigned to params (and cast tensors to appropriate types).
   170|         0|            0|            0|  0.00%|        # State that is not assigned to params is copied as is (needed for
   171|         0|            0|            0|  0.00%|        # backward compatibility).
   172|         0|            0|            0|  0.00%|        state = defaultdict(dict)
   173|         0|            0|            0|  0.00%|        for k, v in state_dict['state'].items():
   174|         0|            0|            0|  0.00%|            if k in id_map:
   175|         0|            0|            0|  0.00%|                param = id_map[k]
   176|         0|            0|            0|  0.00%|                state[param] = cast(param, v)
   177|         0|            0|            0|  0.00%|            else:
   178|         0|            0|            0|  0.00%|                state[k] = v
   179|         0|            0|            0|  0.00%|
   180|         0|            0|            0|  0.00%|        # Update parameter groups, setting their 'params' value
   181|         0|            0|            0|  0.00%|        def update_group(group, new_group):
   182|         0|            0|            0|  0.00%|            new_group['params'] = group['params']
   183|         0|            0|            0|  0.00%|            return new_group
   184|         0|            0|            0|  0.00%|        param_groups = [
   185|         0|            0|            0|  0.00%|            update_group(g, ng) for g, ng in zip(groups, saved_groups)]
   186|         0|            0|            0|  0.00%|        self.__setstate__({'state': state, 'param_groups': param_groups})
   187|         0|            0|            0|  0.00%|
   188|       100|  0.000827789|  8.27789e-06|  0.00%|    def zero_grad(self, set_to_none: bool = False):
   189|         0|            0|            0|  0.00%|        r"""Sets the gradients of all optimized :class:`torch.Tensor` s to zero.
   190|         0|            0|            0|  0.00%|
   191|         0|            0|            0|  0.00%|        Args:
   192|         0|            0|            0|  0.00%|            set_to_none (bool): instead of setting to zero, set the grads to None.
   193|         0|            0|            0|  0.00%|                This will in general have lower memory footprint, and can modestly improve performance.
   194|         0|            0|            0|  0.00%|                However, it changes certain behaviors. For example:
   195|         0|            0|            0|  0.00%|                1. When the user tries to access a gradient and perform manual ops on it,
   196|         0|            0|            0|  0.00%|                a None attribute or a Tensor full of 0s will behave differently.
   197|         0|            0|            0|  0.00%|                2. If the user requests ``zero_grad(set_to_none=True)`` followed by a backward pass, ``.grad``\ s
   198|         0|            0|            0|  0.00%|                are guaranteed to be None for params that did not receive a gradient.
   199|         0|            0|            0|  0.00%|                3. ``torch.optim`` optimizers have a different behavior if the gradient is 0 or None
   200|         0|            0|            0|  0.00%|                (in one case it does the step with a gradient of 0 and in the other it skips
   201|         0|            0|            0|  0.00%|                the step altogether).
   202|         0|            0|            0|  0.00%|        """
   203|       100|   0.00086832|   8.6832e-06|  0.00%|        if not hasattr(self, "_zero_grad_profile_name"):
   204|         0|            0|            0|  0.00%|            self._hook_for_profile()
   205|       100|   0.00328708|  3.28708e-05|  0.01%|        with torch.autograd.profiler.record_function(self._zero_grad_profile_name):
(call)|       100|   0.00940967|  9.40967e-05|  0.02%|# /opt/conda/lib/python3.8/site-packages/torch/autograd/profiler.py:605 __init__
(call)|       100|   0.00567389|  5.67389e-05|  0.01%|# /opt/conda/lib/python3.8/site-packages/torch/autograd/profiler.py:613 __enter__
   206|       200|   0.00295639|   1.4782e-05|  0.01%|            for group in self.param_groups:
(call)|       100|   0.00512218|  5.12218e-05|  0.01%|# /opt/conda/lib/python3.8/site-packages/torch/autograd/profiler.py:617 __exit__
   207|      6300|    0.0348842|  5.53718e-06|  0.07%|                for p in group['params']:
   208|      6200|    0.0840509|  1.35566e-05|  0.16%|                    if p.grad is not None:
(call)|      6200|     0.142952|  2.30568e-05|  0.27%|# /opt/conda/lib/python3.8/site-packages/torch/_tensor.py:967 grad
   209|      6138|    0.0247366|  4.03008e-06|  0.05%|                        if set_to_none:
   210|         0|            0|            0|  0.00%|                            p.grad = None
   211|         0|            0|            0|  0.00%|                        else:
   212|      6138|    0.0823357|  1.34141e-05|  0.15%|                            if p.grad.grad_fn is not None:
(call)|      6138|     0.129938|  2.11695e-05|  0.24%|# /opt/conda/lib/python3.8/site-packages/torch/_tensor.py:967 grad
   213|         0|            0|            0|  0.00%|                                p.grad.detach_()
   214|         0|            0|            0|  0.00%|                            else:
   215|      6138|    0.0998244|  1.62633e-05|  0.19%|                                p.grad.requires_grad_(False)
(call)|      6138|     0.129516|  2.11007e-05|  0.24%|# /opt/conda/lib/python3.8/site-packages/torch/_tensor.py:967 grad
   216|      6138|     0.369308|  6.01675e-05|  0.69%|                            p.grad.zero_()
(call)|      6138|     0.130461|  2.12547e-05|  0.24%|# /opt/conda/lib/python3.8/site-packages/torch/_tensor.py:967 grad
   217|         0|            0|            0|  0.00%|
   218|         0|            0|            0|  0.00%|    def step(self, closure):
   219|         0|            0|            0|  0.00%|        r"""Performs a single optimization step (parameter update).
   220|         0|            0|            0|  0.00%|
   221|         0|            0|            0|  0.00%|        Args:
   222|         0|            0|            0|  0.00%|            closure (callable): A closure that reevaluates the model and
   223|         0|            0|            0|  0.00%|                returns the loss. Optional for most optimizers.
   224|         0|            0|            0|  0.00%|
   225|         0|            0|            0|  0.00%|        .. note::
   226|         0|            0|            0|  0.00%|            Unless otherwise specified, this function should not modify the
   227|         0|            0|            0|  0.00%|            ``.grad`` field of the parameters.
   228|         0|            0|            0|  0.00%|        """
   229|         0|            0|            0|  0.00%|        raise NotImplementedError
   230|         0|            0|            0|  0.00%|
   231|         0|            0|            0|  0.00%|    def add_param_group(self, param_group):
   232|         0|            0|            0|  0.00%|        r"""Add a param group to the :class:`Optimizer` s `param_groups`.
   233|         0|            0|            0|  0.00%|
   234|         0|            0|            0|  0.00%|        This can be useful when fine tuning a pre-trained network as frozen layers can be made
   235|         0|            0|            0|  0.00%|        trainable and added to the :class:`Optimizer` as training progresses.
   236|         0|            0|            0|  0.00%|
   237|         0|            0|            0|  0.00%|        Args:
   238|         0|            0|            0|  0.00%|            param_group (dict): Specifies what Tensors should be optimized along with group
   239|         0|            0|            0|  0.00%|            specific optimization options.
   240|         0|            0|            0|  0.00%|        """
   241|         0|            0|            0|  0.00%|        assert isinstance(param_group, dict), "param group must be a dict"
   242|         0|            0|            0|  0.00%|
   243|         0|            0|            0|  0.00%|        params = param_group['params']
   244|         0|            0|            0|  0.00%|        if isinstance(params, torch.Tensor):
   245|         0|            0|            0|  0.00%|            param_group['params'] = [params]
   246|         0|            0|            0|  0.00%|        elif isinstance(params, set):
   247|         0|            0|            0|  0.00%|            raise TypeError('optimizer parameters need to be organized in ordered collections, but '
   248|         0|            0|            0|  0.00%|                            'the ordering of tensors in sets will change between runs. Please use a list instead.')
   249|         0|            0|            0|  0.00%|        else:
   250|         0|            0|            0|  0.00%|            param_group['params'] = list(params)
   251|         0|            0|            0|  0.00%|
   252|         0|            0|            0|  0.00%|        for param in param_group['params']:
   253|         0|            0|            0|  0.00%|            if not isinstance(param, torch.Tensor):
   254|         0|            0|            0|  0.00%|                raise TypeError("optimizer can only optimize Tensors, "
   255|         0|            0|            0|  0.00%|                                "but one of the params is " + torch.typename(param))
   256|         0|            0|            0|  0.00%|            if not param.is_leaf:
   257|         0|            0|            0|  0.00%|                raise ValueError("can't optimize a non-leaf Tensor")
   258|         0|            0|            0|  0.00%|
   259|         0|            0|            0|  0.00%|        for name, default in self.defaults.items():
   260|         0|            0|            0|  0.00%|            if default is required and name not in param_group:
   261|         0|            0|            0|  0.00%|                raise ValueError("parameter group didn't specify a value of required optimization parameter " +
   262|         0|            0|            0|  0.00%|                                 name)
   263|         0|            0|            0|  0.00%|            else:
   264|         0|            0|            0|  0.00%|                param_group.setdefault(name, default)
   265|         0|            0|            0|  0.00%|
   266|         0|            0|            0|  0.00%|        params = param_group['params']
   267|         0|            0|            0|  0.00%|        if len(params) != len(set(params)):
   268|         0|            0|            0|  0.00%|            warnings.warn("optimizer contains a parameter group with duplicate parameters; "
   269|         0|            0|            0|  0.00%|                          "in future, this will cause an error; "
   270|         0|            0|            0|  0.00%|                          "see github.com/pytorch/pytorch/issues/40967 for more information", stacklevel=3)
   271|         0|            0|            0|  0.00%|
   272|         0|            0|            0|  0.00%|        param_set = set()
   273|         0|            0|            0|  0.00%|        for group in self.param_groups:
   274|         0|            0|            0|  0.00%|            param_set.update(set(group['params']))
   275|         0|            0|            0|  0.00%|
   276|         0|            0|            0|  0.00%|        if not param_set.isdisjoint(set(param_group['params'])):
   277|         0|            0|            0|  0.00%|            raise ValueError("some parameters appear in more than one parameter group")
   278|         0|            0|            0|  0.00%|
   279|         0|            0|            0|  0.00%|        self.param_groups.append(param_group)
File: /opt/conda/lib/python3.8/multiprocessing/connection.py
File duration: 0.495972s (0.93%)
Line #|      Hits|         Time| Time per hit|      %|Source code
------+----------+-------------+-------------+-------+-----------
     1|         0|            0|            0|  0.00%|#
     2|         0|            0|            0|  0.00%|# A higher level module for using sockets (or Windows named pipes)
     3|         0|            0|            0|  0.00%|#
     4|         0|            0|            0|  0.00%|# multiprocessing/connection.py
     5|         0|            0|            0|  0.00%|#
     6|         0|            0|            0|  0.00%|# Copyright (c) 2006-2008, R Oudkerk
     7|         0|            0|            0|  0.00%|# Licensed to PSF under a Contributor Agreement.
     8|         0|            0|            0|  0.00%|#
     9|         0|            0|            0|  0.00%|
    10|         0|            0|            0|  0.00%|__all__ = [ 'Client', 'Listener', 'Pipe', 'wait' ]
    11|         0|            0|            0|  0.00%|
    12|         0|            0|            0|  0.00%|import io
    13|         0|            0|            0|  0.00%|import os
    14|         0|            0|            0|  0.00%|import sys
    15|         0|            0|            0|  0.00%|import socket
    16|         0|            0|            0|  0.00%|import struct
    17|         0|            0|            0|  0.00%|import time
    18|         0|            0|            0|  0.00%|import tempfile
    19|         0|            0|            0|  0.00%|import itertools
    20|         0|            0|            0|  0.00%|
    21|         0|            0|            0|  0.00%|import _multiprocessing
    22|         0|            0|            0|  0.00%|
    23|         0|            0|            0|  0.00%|from . import util
    24|         0|            0|            0|  0.00%|
    25|         0|            0|            0|  0.00%|from . import AuthenticationError, BufferTooShort
    26|         0|            0|            0|  0.00%|from .context import reduction
    27|         0|            0|            0|  0.00%|_ForkingPickler = reduction.ForkingPickler
    28|         0|            0|            0|  0.00%|
    29|         0|            0|            0|  0.00%|try:
    30|         0|            0|            0|  0.00%|    import _winapi
    31|         0|            0|            0|  0.00%|    from _winapi import WAIT_OBJECT_0, WAIT_ABANDONED_0, WAIT_TIMEOUT, INFINITE
    32|         0|            0|            0|  0.00%|except ImportError:
    33|         0|            0|            0|  0.00%|    if sys.platform == 'win32':
    34|         0|            0|            0|  0.00%|        raise
    35|         0|            0|            0|  0.00%|    _winapi = None
    36|         0|            0|            0|  0.00%|
    37|         0|            0|            0|  0.00%|#
    38|         0|            0|            0|  0.00%|#
    39|         0|            0|            0|  0.00%|#
    40|         0|            0|            0|  0.00%|
    41|         0|            0|            0|  0.00%|BUFSIZE = 8192
    42|         0|            0|            0|  0.00%|# A very generous timeout when it comes to local connections...
    43|         0|            0|            0|  0.00%|CONNECTION_TIMEOUT = 20.
    44|         0|            0|            0|  0.00%|
    45|         0|            0|            0|  0.00%|_mmap_counter = itertools.count()
    46|         0|            0|            0|  0.00%|
    47|         0|            0|            0|  0.00%|default_family = 'AF_INET'
    48|         0|            0|            0|  0.00%|families = ['AF_INET']
    49|         0|            0|            0|  0.00%|
    50|         0|            0|            0|  0.00%|if hasattr(socket, 'AF_UNIX'):
    51|         0|            0|            0|  0.00%|    default_family = 'AF_UNIX'
    52|         0|            0|            0|  0.00%|    families += ['AF_UNIX']
    53|         0|            0|            0|  0.00%|
    54|         0|            0|            0|  0.00%|if sys.platform == 'win32':
    55|         0|            0|            0|  0.00%|    default_family = 'AF_PIPE'
    56|         0|            0|            0|  0.00%|    families += ['AF_PIPE']
    57|         0|            0|            0|  0.00%|
    58|         0|            0|            0|  0.00%|
    59|         0|            0|            0|  0.00%|def _init_timeout(timeout=CONNECTION_TIMEOUT):
    60|         0|            0|            0|  0.00%|    return time.monotonic() + timeout
    61|         0|            0|            0|  0.00%|
    62|         0|            0|            0|  0.00%|def _check_timeout(t):
    63|         0|            0|            0|  0.00%|    return time.monotonic() > t
    64|         0|            0|            0|  0.00%|
    65|         0|            0|            0|  0.00%|#
    66|         0|            0|            0|  0.00%|#
    67|         0|            0|            0|  0.00%|#
    68|         0|            0|            0|  0.00%|
    69|         0|            0|            0|  0.00%|def arbitrary_address(family):
    70|         0|            0|            0|  0.00%|    '''
    71|         0|            0|            0|  0.00%|    Return an arbitrary free address for the given family
    72|         0|            0|            0|  0.00%|    '''
    73|         0|            0|            0|  0.00%|    if family == 'AF_INET':
    74|         0|            0|            0|  0.00%|        return ('localhost', 0)
    75|         0|            0|            0|  0.00%|    elif family == 'AF_UNIX':
    76|         0|            0|            0|  0.00%|        return tempfile.mktemp(prefix='listener-', dir=util.get_temp_dir())
    77|         0|            0|            0|  0.00%|    elif family == 'AF_PIPE':
    78|         0|            0|            0|  0.00%|        return tempfile.mktemp(prefix=r'\\.\pipe\pyc-%d-%d-' %
    79|         0|            0|            0|  0.00%|                               (os.getpid(), next(_mmap_counter)), dir="")
    80|         0|            0|            0|  0.00%|    else:
    81|         0|            0|            0|  0.00%|        raise ValueError('unrecognized family')
    82|         0|            0|            0|  0.00%|
    83|       200|  0.000937223|  4.68612e-06|  0.00%|def _validate_family(family):
    84|         0|            0|            0|  0.00%|    '''
    85|         0|            0|            0|  0.00%|    Checks if the family is valid for the current environment.
    86|         0|            0|            0|  0.00%|    '''
    87|       200|  0.000988483|  4.94242e-06|  0.00%|    if sys.platform != 'win32' and family == 'AF_PIPE':
    88|         0|            0|            0|  0.00%|        raise ValueError('Family %s is not recognized.' % family)
    89|         0|            0|            0|  0.00%|
    90|       200|   0.00085187|  4.25935e-06|  0.00%|    if sys.platform == 'win32' and family == 'AF_UNIX':
    91|         0|            0|            0|  0.00%|        # double check
    92|         0|            0|            0|  0.00%|        if not hasattr(socket, family):
    93|         0|            0|            0|  0.00%|            raise ValueError('Family %s is not recognized.' % family)
    94|         0|            0|            0|  0.00%|
    95|       400|   0.00185847|  4.64618e-06|  0.00%|def address_type(address):
    96|         0|            0|            0|  0.00%|    '''
    97|         0|            0|            0|  0.00%|    Return the types of the address
    98|         0|            0|            0|  0.00%|
    99|         0|            0|            0|  0.00%|    This can be 'AF_INET', 'AF_UNIX', or 'AF_PIPE'
   100|         0|            0|            0|  0.00%|    '''
   101|       400|   0.00187159|  4.67896e-06|  0.00%|    if type(address) == tuple:
   102|         0|            0|            0|  0.00%|        return 'AF_INET'
   103|       400|   0.00289774|  7.24435e-06|  0.01%|    elif type(address) is str and address.startswith('\\\\'):
   104|         0|            0|            0|  0.00%|        return 'AF_PIPE'
   105|       400|   0.00169253|  4.23133e-06|  0.00%|    elif type(address) is str or util.is_abstract_socket_namespace(address):
   106|       400|   0.00153661|  3.84152e-06|  0.00%|        return 'AF_UNIX'
   107|         0|            0|            0|  0.00%|    else:
   108|         0|            0|            0|  0.00%|        raise ValueError('address type of %r unrecognized' % address)
   109|         0|            0|            0|  0.00%|
   110|         0|            0|            0|  0.00%|#
   111|         0|            0|            0|  0.00%|# Connection classes
   112|         0|            0|            0|  0.00%|#
   113|         0|            0|            0|  0.00%|
   114|         0|            0|            0|  0.00%|class _ConnectionBase:
   115|         0|            0|            0|  0.00%|    _handle = None
   116|         0|            0|            0|  0.00%|
   117|       220|   0.00115347|  5.24304e-06|  0.00%|    def __init__(self, handle, readable=True, writable=True):
   118|       220|   0.00132656|  6.02982e-06|  0.00%|        handle = handle.__index__()
   119|       220|  0.000923634|  4.19833e-06|  0.00%|        if handle < 0:
   120|         0|            0|            0|  0.00%|            raise ValueError("invalid handle")
   121|       220|  0.000867844|  3.94474e-06|  0.00%|        if not readable and not writable:
   122|         0|            0|            0|  0.00%|            raise ValueError(
   123|         0|            0|            0|  0.00%|                "at least one of `readable` and `writable` must be True")
   124|       220|   0.00111175|  5.05339e-06|  0.00%|        self._handle = handle
   125|       220|  0.000952721|  4.33055e-06|  0.00%|        self._readable = readable
   126|       220|   0.00088954|  4.04336e-06|  0.00%|        self._writable = writable
   127|         0|            0|            0|  0.00%|
   128|         0|            0|            0|  0.00%|    # XXX should we use util.Finalize instead of a __del__?
   129|         0|            0|            0|  0.00%|
   130|       215|   0.00103021|  4.79166e-06|  0.00%|    def __del__(self):
   131|       215|  0.000994444|  4.62532e-06|  0.00%|        if self._handle is not None:
   132|         4|   5.8651e-05|  1.46627e-05|  0.00%|            self._close()
(call)|         4|  0.000101566|  2.53916e-05|  0.00%|# /opt/conda/lib/python3.8/multiprocessing/connection.py:360 _close
   133|         0|            0|            0|  0.00%|
   134|      1900|   0.00792885|  4.17308e-06|  0.01%|    def _check_closed(self):
   135|      1900|   0.00772905|  4.06792e-06|  0.01%|        if self._handle is None:
   136|         0|            0|            0|  0.00%|            raise OSError("handle is closed")
   137|         0|            0|            0|  0.00%|
   138|       800|   0.00328779|  4.10974e-06|  0.01%|    def _check_readable(self):
   139|       800|   0.00328445|  4.10557e-06|  0.01%|        if not self._readable:
   140|         0|            0|            0|  0.00%|            raise OSError("connection is write-only")
   141|         0|            0|            0|  0.00%|
   142|       800|    0.0033896|    4.237e-06|  0.01%|    def _check_writable(self):
   143|       800|    0.0031929|  3.99113e-06|  0.01%|        if not self._writable:
   144|         0|            0|            0|  0.00%|            raise OSError("connection is read-only")
   145|         0|            0|            0|  0.00%|
   146|         0|            0|            0|  0.00%|    def _bad_message_length(self):
   147|         0|            0|            0|  0.00%|        if self._writable:
   148|         0|            0|            0|  0.00%|            self._readable = False
   149|         0|            0|            0|  0.00%|        else:
   150|         0|            0|            0|  0.00%|            self.close()
   151|         0|            0|            0|  0.00%|        raise OSError("bad message length")
   152|         0|            0|            0|  0.00%|
   153|         0|            0|            0|  0.00%|    @property
   154|         0|            0|            0|  0.00%|    def closed(self):
   155|         0|            0|            0|  0.00%|        """True if the connection is closed"""
   156|         0|            0|            0|  0.00%|        return self._handle is None
   157|         0|            0|            0|  0.00%|
   158|         0|            0|            0|  0.00%|    @property
   159|         0|            0|            0|  0.00%|    def readable(self):
   160|         0|            0|            0|  0.00%|        """True if the connection is readable"""
   161|         0|            0|            0|  0.00%|        return self._readable
   162|         0|            0|            0|  0.00%|
   163|         0|            0|            0|  0.00%|    @property
   164|         0|            0|            0|  0.00%|    def writable(self):
   165|         0|            0|            0|  0.00%|        """True if the connection is writable"""
   166|         0|            0|            0|  0.00%|        return self._writable
   167|         0|            0|            0|  0.00%|
   168|       300|   0.00138879|  4.62929e-06|  0.00%|    def fileno(self):
   169|         0|            0|            0|  0.00%|        """File descriptor or handle of the connection"""
   170|       300|   0.00359297|  1.19766e-05|  0.01%|        self._check_closed()
(call)|       300|   0.00241923|  8.06411e-06|  0.00%|# /opt/conda/lib/python3.8/multiprocessing/connection.py:134 _check_closed
   171|       300|   0.00120902|  4.03007e-06|  0.00%|        return self._handle
   172|         0|            0|            0|  0.00%|
   173|       208|  0.000929117|  4.46691e-06|  0.00%|    def close(self):
   174|         0|            0|            0|  0.00%|        """Close the connection"""
   175|       208|  0.000847101|   4.0726e-06|  0.00%|        if self._handle is not None:
   176|       208|  0.000772238|  3.71268e-06|  0.00%|            try:
   177|       208|    0.0030067|  1.44553e-05|  0.01%|                self._close()
(call)|       208|   0.00472927|  2.27369e-05|  0.01%|# /opt/conda/lib/python3.8/multiprocessing/connection.py:360 _close
   178|         0|            0|            0|  0.00%|            finally:
   179|       208|  0.000934601|  4.49327e-06|  0.00%|                self._handle = None
   180|         0|            0|            0|  0.00%|
   181|       600|   0.00324869|  5.41449e-06|  0.01%|    def send_bytes(self, buf, offset=0, size=None):
   182|         0|            0|            0|  0.00%|        """Send the bytes data from a bytes-like object"""
   183|       600|    0.0078063|  1.30105e-05|  0.01%|        self._check_closed()
(call)|       600|    0.0049274|  8.21233e-06|  0.01%|# /opt/conda/lib/python3.8/multiprocessing/connection.py:134 _check_closed
   184|       600|   0.00758648|  1.26441e-05|  0.01%|        self._check_writable()
(call)|       600|   0.00497746|  8.29577e-06|  0.01%|# /opt/conda/lib/python3.8/multiprocessing/connection.py:142 _check_writable
   185|       600|   0.00353622|  5.89371e-06|  0.01%|        m = memoryview(buf)
   186|         0|            0|            0|  0.00%|        # HACK for byte-indexing of non-bytewise buffers (e.g. array.array)
   187|       600|   0.00290656|  4.84427e-06|  0.01%|        if m.itemsize > 1:
   188|         0|            0|            0|  0.00%|            m = memoryview(bytes(m))
   189|       600|   0.00389457|  6.49095e-06|  0.01%|        n = len(m)
   190|       600|   0.00254965|  4.24941e-06|  0.00%|        if offset < 0:
   191|         0|            0|            0|  0.00%|            raise ValueError("offset is negative")
   192|       600|   0.00240731|  4.01219e-06|  0.00%|        if n < offset:
   193|         0|            0|            0|  0.00%|            raise ValueError("buffer length < offset")
   194|       600|   0.00237727|  3.96212e-06|  0.00%|        if size is None:
   195|       600|   0.00240397|  4.00662e-06|  0.00%|            size = n - offset
   196|         0|            0|            0|  0.00%|        elif size < 0:
   197|         0|            0|            0|  0.00%|            raise ValueError("size is negative")
   198|         0|            0|            0|  0.00%|        elif offset + size > n:
   199|         0|            0|            0|  0.00%|            raise ValueError("buffer length < offset + size")
   200|       600|   0.00907183|  1.51197e-05|  0.02%|        self._send_bytes(m[offset:offset + size])
(call)|       600|     0.051574|  8.59567e-05|  0.10%|# /opt/conda/lib/python3.8/multiprocessing/connection.py:390 _send_bytes
   201|         0|            0|            0|  0.00%|
   202|       200|  0.000980854|  4.90427e-06|  0.00%|    def send(self, obj):
   203|         0|            0|            0|  0.00%|        """Send a (picklable) object"""
   204|       200|   0.00241685|  1.20842e-05|  0.00%|        self._check_closed()
(call)|       200|    0.0016048|  8.02398e-06|  0.00%|# /opt/conda/lib/python3.8/multiprocessing/connection.py:134 _check_closed
   205|       200|   0.00231075|  1.15538e-05|  0.00%|        self._check_writable()
(call)|       200|   0.00160503|  8.02517e-06|  0.00%|# /opt/conda/lib/python3.8/multiprocessing/connection.py:142 _check_writable
   206|       200|   0.00493717|  2.46859e-05|  0.01%|        self._send_bytes(_ForkingPickler.dumps(obj))
(call)|       200|    0.0172858|  8.64291e-05|  0.03%|# /opt/conda/lib/python3.8/multiprocessing/reduction.py:48 dumps
(call)|       200|    0.0169389|  8.46946e-05|  0.03%|# /opt/conda/lib/python3.8/multiprocessing/connection.py:390 _send_bytes
   207|         0|            0|            0|  0.00%|
   208|       700|   0.00354338|  5.06197e-06|  0.01%|    def recv_bytes(self, maxlength=None):
   209|         0|            0|            0|  0.00%|        """
   210|         0|            0|            0|  0.00%|        Receive bytes data as a bytes object.
   211|         0|            0|            0|  0.00%|        """
   212|       700|   0.00863004|  1.23286e-05|  0.02%|        self._check_closed()
(call)|       700|   0.00567341|  8.10487e-06|  0.01%|# /opt/conda/lib/python3.8/multiprocessing/connection.py:134 _check_closed
   213|       700|   0.00826836|  1.18119e-05|  0.02%|        self._check_readable()
(call)|       700|   0.00561309|   8.0187e-06|  0.01%|# /opt/conda/lib/python3.8/multiprocessing/connection.py:138 _check_readable
   214|       700|   0.00294852|  4.21218e-06|  0.01%|        if maxlength is not None and maxlength < 0:
   215|         0|            0|            0|  0.00%|            raise ValueError("negative maxlength")
   216|       700|   0.00916648|   1.3095e-05|  0.02%|        buf = self._recv_bytes(maxlength)
(call)|       700|     0.141005|  0.000201436|  0.26%|# /opt/conda/lib/python3.8/multiprocessing/connection.py:413 _recv_bytes
   217|       700|   0.00281286|  4.01837e-06|  0.01%|        if buf is None:
   218|         0|            0|            0|  0.00%|            self._bad_message_length()
   219|       700|   0.00410581|  5.86544e-06|  0.01%|        return buf.getvalue()
   220|         0|            0|            0|  0.00%|
   221|         0|            0|            0|  0.00%|    def recv_bytes_into(self, buf, offset=0):
   222|         0|            0|            0|  0.00%|        """
   223|         0|            0|            0|  0.00%|        Receive bytes data into a writeable bytes-like object.
   224|         0|            0|            0|  0.00%|        Return the number of bytes read.
   225|         0|            0|            0|  0.00%|        """
   226|         0|            0|            0|  0.00%|        self._check_closed()
   227|         0|            0|            0|  0.00%|        self._check_readable()
   228|         0|            0|            0|  0.00%|        with memoryview(buf) as m:
   229|         0|            0|            0|  0.00%|            # Get bytesize of arbitrary buffer
   230|         0|            0|            0|  0.00%|            itemsize = m.itemsize
   231|         0|            0|            0|  0.00%|            bytesize = itemsize * len(m)
   232|         0|            0|            0|  0.00%|            if offset < 0:
   233|         0|            0|            0|  0.00%|                raise ValueError("negative offset")
   234|         0|            0|            0|  0.00%|            elif offset > bytesize:
   235|         0|            0|            0|  0.00%|                raise ValueError("offset too large")
   236|         0|            0|            0|  0.00%|            result = self._recv_bytes()
   237|         0|            0|            0|  0.00%|            size = result.tell()
   238|         0|            0|            0|  0.00%|            if bytesize < offset + size:
   239|         0|            0|            0|  0.00%|                raise BufferTooShort(result.getvalue())
   240|         0|            0|            0|  0.00%|            # Message can fit in dest
   241|         0|            0|            0|  0.00%|            result.seek(0)
   242|         0|            0|            0|  0.00%|            result.readinto(m[offset // itemsize :
   243|         0|            0|            0|  0.00%|                              (offset + size) // itemsize])
   244|         0|            0|            0|  0.00%|            return size
   245|         0|            0|            0|  0.00%|
   246|         0|            0|            0|  0.00%|    def recv(self):
   247|         0|            0|            0|  0.00%|        """Receive a (picklable) object"""
   248|         0|            0|            0|  0.00%|        self._check_closed()
   249|         0|            0|            0|  0.00%|        self._check_readable()
   250|         0|            0|            0|  0.00%|        buf = self._recv_bytes()
   251|         0|            0|            0|  0.00%|        return _ForkingPickler.loads(buf.getbuffer())
   252|         0|            0|            0|  0.00%|
   253|       100|  0.000643969|  6.43969e-06|  0.00%|    def poll(self, timeout=0.0):
   254|         0|            0|            0|  0.00%|        """Whether there is any input available to be read"""
   255|       100|   0.00159216|  1.59216e-05|  0.00%|        self._check_closed()
(call)|       100|   0.00103307|  1.03307e-05|  0.00%|# /opt/conda/lib/python3.8/multiprocessing/connection.py:134 _check_closed
   256|       100|   0.00127935|  1.27935e-05|  0.00%|        self._check_readable()
(call)|       100|  0.000959158|  9.59158e-06|  0.00%|# /opt/conda/lib/python3.8/multiprocessing/connection.py:138 _check_readable
   257|       100|   0.00144339|  1.44339e-05|  0.00%|        return self._poll(timeout)
(call)|       100|     0.361938|   0.00361938|  0.68%|# /opt/conda/lib/python3.8/multiprocessing/connection.py:423 _poll
   258|         0|            0|            0|  0.00%|
   259|       200|  0.000919104|  4.59552e-06|  0.00%|    def __enter__(self):
   260|       200|   0.00084734|   4.2367e-06|  0.00%|        return self
   261|         0|            0|            0|  0.00%|
   262|       200|  0.000972986|  4.86493e-06|  0.00%|    def __exit__(self, exc_type, exc_value, exc_tb):
   263|       200|   0.00263953|  1.31977e-05|  0.00%|        self.close()
(call)|       200|    0.0107901|  5.39505e-05|  0.02%|# /opt/conda/lib/python3.8/multiprocessing/connection.py:173 close
   264|         0|            0|            0|  0.00%|
   265|         0|            0|            0|  0.00%|
   266|         0|            0|            0|  0.00%|if _winapi:
   267|         0|            0|            0|  0.00%|
   268|         0|            0|            0|  0.00%|    class PipeConnection(_ConnectionBase):
   269|         0|            0|            0|  0.00%|        """
   270|         0|            0|            0|  0.00%|        Connection class based on a Windows named pipe.
   271|         0|            0|            0|  0.00%|        Overlapped I/O is used, so the handles must have been created
   272|         0|            0|            0|  0.00%|        with FILE_FLAG_OVERLAPPED.
   273|         0|            0|            0|  0.00%|        """
   274|         0|            0|            0|  0.00%|        _got_empty_message = False
   275|         0|            0|            0|  0.00%|
   276|         0|            0|            0|  0.00%|        def _close(self, _CloseHandle=_winapi.CloseHandle):
   277|         0|            0|            0|  0.00%|            _CloseHandle(self._handle)
   278|         0|            0|            0|  0.00%|
   279|         0|            0|            0|  0.00%|        def _send_bytes(self, buf):
   280|         0|            0|            0|  0.00%|            ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
   281|         0|            0|            0|  0.00%|            try:
   282|         0|            0|            0|  0.00%|                if err == _winapi.ERROR_IO_PENDING:
   283|         0|            0|            0|  0.00%|                    waitres = _winapi.WaitForMultipleObjects(
   284|         0|            0|            0|  0.00%|                        [ov.event], False, INFINITE)
   285|         0|            0|            0|  0.00%|                    assert waitres == WAIT_OBJECT_0
   286|         0|            0|            0|  0.00%|            except:
   287|         0|            0|            0|  0.00%|                ov.cancel()
   288|         0|            0|            0|  0.00%|                raise
   289|         0|            0|            0|  0.00%|            finally:
   290|         0|            0|            0|  0.00%|                nwritten, err = ov.GetOverlappedResult(True)
   291|         0|            0|            0|  0.00%|            assert err == 0
   292|         0|            0|            0|  0.00%|            assert nwritten == len(buf)
   293|         0|            0|            0|  0.00%|
   294|         0|            0|            0|  0.00%|        def _recv_bytes(self, maxsize=None):
   295|         0|            0|            0|  0.00%|            if self._got_empty_message:
   296|         0|            0|            0|  0.00%|                self._got_empty_message = False
   297|         0|            0|            0|  0.00%|                return io.BytesIO()
   298|         0|            0|            0|  0.00%|            else:
   299|         0|            0|            0|  0.00%|                bsize = 128 if maxsize is None else min(maxsize, 128)
   300|         0|            0|            0|  0.00%|                try:
   301|         0|            0|            0|  0.00%|                    ov, err = _winapi.ReadFile(self._handle, bsize,
   302|         0|            0|            0|  0.00%|                                                overlapped=True)
   303|         0|            0|            0|  0.00%|                    try:
   304|         0|            0|            0|  0.00%|                        if err == _winapi.ERROR_IO_PENDING:
   305|         0|            0|            0|  0.00%|                            waitres = _winapi.WaitForMultipleObjects(
   306|         0|            0|            0|  0.00%|                                [ov.event], False, INFINITE)
   307|         0|            0|            0|  0.00%|                            assert waitres == WAIT_OBJECT_0
   308|         0|            0|            0|  0.00%|                    except:
   309|         0|            0|            0|  0.00%|                        ov.cancel()
   310|         0|            0|            0|  0.00%|                        raise
   311|         0|            0|            0|  0.00%|                    finally:
   312|         0|            0|            0|  0.00%|                        nread, err = ov.GetOverlappedResult(True)
   313|         0|            0|            0|  0.00%|                        if err == 0:
   314|         0|            0|            0|  0.00%|                            f = io.BytesIO()
   315|         0|            0|            0|  0.00%|                            f.write(ov.getbuffer())
   316|         0|            0|            0|  0.00%|                            return f
   317|         0|            0|            0|  0.00%|                        elif err == _winapi.ERROR_MORE_DATA:
   318|         0|            0|            0|  0.00%|                            return self._get_more_data(ov, maxsize)
   319|         0|            0|            0|  0.00%|                except OSError as e:
   320|         0|            0|            0|  0.00%|                    if e.winerror == _winapi.ERROR_BROKEN_PIPE:
   321|         0|            0|            0|  0.00%|                        raise EOFError
   322|         0|            0|            0|  0.00%|                    else:
   323|         0|            0|            0|  0.00%|                        raise
   324|         0|            0|            0|  0.00%|            raise RuntimeError("shouldn't get here; expected KeyboardInterrupt")
   325|         0|            0|            0|  0.00%|
   326|         0|            0|            0|  0.00%|        def _poll(self, timeout):
   327|         0|            0|            0|  0.00%|            if (self._got_empty_message or
   328|         0|            0|            0|  0.00%|                        _winapi.PeekNamedPipe(self._handle)[0] != 0):
   329|         0|            0|            0|  0.00%|                return True
   330|         0|            0|            0|  0.00%|            return bool(wait([self], timeout))
   331|         0|            0|            0|  0.00%|
   332|         0|            0|            0|  0.00%|        def _get_more_data(self, ov, maxsize):
   333|         0|            0|            0|  0.00%|            buf = ov.getbuffer()
   334|         0|            0|            0|  0.00%|            f = io.BytesIO()
   335|         0|            0|            0|  0.00%|            f.write(buf)
   336|         0|            0|            0|  0.00%|            left = _winapi.PeekNamedPipe(self._handle)[1]
   337|         0|            0|            0|  0.00%|            assert left > 0
   338|         0|            0|            0|  0.00%|            if maxsize is not None and len(buf) + left > maxsize:
   339|         0|            0|            0|  0.00%|                self._bad_message_length()
   340|         0|            0|            0|  0.00%|            ov, err = _winapi.ReadFile(self._handle, left, overlapped=True)
   341|         0|            0|            0|  0.00%|            rbytes, err = ov.GetOverlappedResult(True)
   342|         0|            0|            0|  0.00%|            assert err == 0
   343|         0|            0|            0|  0.00%|            assert rbytes == left
   344|         0|            0|            0|  0.00%|            f.write(ov.getbuffer())
   345|         0|            0|            0|  0.00%|            return f
   346|         0|            0|            0|  0.00%|
   347|         0|            0|            0|  0.00%|
   348|         0|            0|            0|  0.00%|class Connection(_ConnectionBase):
   349|         0|            0|            0|  0.00%|    """
   350|         0|            0|            0|  0.00%|    Connection class based on an arbitrary file descriptor (Unix only), or
   351|         0|            0|            0|  0.00%|    a socket handle (Windows).
   352|         0|            0|            0|  0.00%|    """
   353|         0|            0|            0|  0.00%|
   354|         0|            0|            0|  0.00%|    if _winapi:
   355|         0|            0|            0|  0.00%|        def _close(self, _close=_multiprocessing.closesocket):
   356|         0|            0|            0|  0.00%|            _close(self._handle)
   357|         0|            0|            0|  0.00%|        _write = _multiprocessing.send
   358|         0|            0|            0|  0.00%|        _read = _multiprocessing.recv
   359|         0|            0|            0|  0.00%|    else:
   360|       212|  0.000895023|  4.22181e-06|  0.00%|        def _close(self, _close=os.close):
   361|       212|   0.00393581|  1.85652e-05|  0.01%|            _close(self._handle)
   362|         0|            0|            0|  0.00%|        _write = os.write
   363|         0|            0|            0|  0.00%|        _read = os.read
   364|         0|            0|            0|  0.00%|
   365|       800|   0.00384545|  4.80682e-06|  0.01%|    def _send(self, buf, write=_write):
   366|       800|   0.00511765|  6.39707e-06|  0.01%|        remaining = len(buf)
   367|         0|            0|            0|  0.00%|        while True:
   368|       800|    0.0143721|  1.79651e-05|  0.03%|            n = write(self._handle, buf)
   369|       800|   0.00462055|  5.77569e-06|  0.01%|            remaining -= n
   370|       800|   0.00325608|   4.0701e-06|  0.01%|            if remaining == 0:
   371|       800|   0.00318432|   3.9804e-06|  0.01%|                break
   372|         0|            0|            0|  0.00%|            buf = buf[n:]
   373|         0|            0|            0|  0.00%|
   374|      1400|   0.00712395|  5.08853e-06|  0.01%|    def _recv(self, size, read=_read):
   375|      1400|   0.00805664|  5.75474e-06|  0.02%|        buf = io.BytesIO()
   376|      1400|   0.00663996|  4.74283e-06|  0.01%|        handle = self._handle
   377|      1400|   0.00636768|  4.54835e-06|  0.01%|        remaining = size
   378|      2800|    0.0116115|  4.14695e-06|  0.02%|        while remaining > 0:
   379|      1400|    0.0284405|  2.03146e-05|  0.05%|            chunk = read(handle, remaining)
   380|      1400|    0.0102553|  7.32524e-06|  0.02%|            n = len(chunk)
   381|      1400|   0.00606656|  4.33326e-06|  0.01%|            if n == 0:
   382|         0|            0|            0|  0.00%|                if remaining == size:
   383|         0|            0|            0|  0.00%|                    raise EOFError
   384|         0|            0|            0|  0.00%|                else:
   385|         0|            0|            0|  0.00%|                    raise OSError("got end of file during message")
   386|      1400|   0.00913095|  6.52211e-06|  0.02%|            buf.write(chunk)
   387|      1400|   0.00617862|   4.4133e-06|  0.01%|            remaining -= n
   388|      1400|   0.00543833|  3.88452e-06|  0.01%|        return buf
   389|         0|            0|            0|  0.00%|
   390|       800|   0.00424409|  5.30511e-06|  0.01%|    def _send_bytes(self, buf):
   391|       800|   0.00540257|  6.75321e-06|  0.01%|        n = len(buf)
   392|       800|   0.00357795|  4.47243e-06|  0.01%|        if n > 0x7fffffff:
   393|         0|            0|            0|  0.00%|            pre_header = struct.pack("!i", -1)
   394|         0|            0|            0|  0.00%|            header = struct.pack("!Q", n)
   395|         0|            0|            0|  0.00%|            self._send(pre_header)
   396|         0|            0|            0|  0.00%|            self._send(header)
   397|         0|            0|            0|  0.00%|            self._send(buf)
   398|         0|            0|            0|  0.00%|        else:
   399|         0|            0|            0|  0.00%|            # For wire compatibility with 3.7 and lower
   400|       800|   0.00566244|  7.07805e-06|  0.01%|            header = struct.pack("!i", n)
   401|       800|    0.0035758|  4.46975e-06|  0.01%|            if n > 16384:
   402|         0|            0|            0|  0.00%|                # The payload is large so Nagle's algorithm won't be triggered
   403|         0|            0|            0|  0.00%|                # and we'd better avoid the cost of concatenation.
   404|         0|            0|            0|  0.00%|                self._send(header)
   405|         0|            0|            0|  0.00%|                self._send(buf)
   406|         0|            0|            0|  0.00%|            else:
   407|         0|            0|            0|  0.00%|                # Issue #20540: concatenate before sending, to avoid delays due
   408|         0|            0|            0|  0.00%|                # to Nagle's algorithm on a TCP socket.
   409|         0|            0|            0|  0.00%|                # Also note we want to avoid sending a 0-length buffer separately,
   410|         0|            0|            0|  0.00%|                # to avoid "broken pipe" errors if the other end closed the pipe.
   411|       800|    0.0116539|  1.45674e-05|  0.02%|                self._send(header + buf)
(call)|       800|    0.0343962|  4.29952e-05|  0.06%|# /opt/conda/lib/python3.8/multiprocessing/connection.py:365 _send
   412|         0|            0|            0|  0.00%|
   413|       700|   0.00332069|  4.74385e-06|  0.01%|    def _recv_bytes(self, maxsize=None):
   414|       700|    0.0100882|  1.44117e-05|  0.02%|        buf = self._recv(4)
(call)|       700|    0.0562732|  8.03903e-05|  0.11%|# /opt/conda/lib/python3.8/multiprocessing/connection.py:374 _recv
   415|       700|   0.00686216|  9.80309e-06|  0.01%|        size, = struct.unpack("!i", buf.getvalue())
   416|       700|   0.00303936|  4.34194e-06|  0.01%|        if size == -1:
   417|         0|            0|            0|  0.00%|            buf = self._recv(8)
   418|         0|            0|            0|  0.00%|            size, = struct.unpack("!Q", buf.getvalue())
   419|       700|   0.00284338|  4.06197e-06|  0.01%|        if maxsize is not None and size > maxsize:
   420|         0|            0|            0|  0.00%|            return None
   421|       700|   0.00954151|  1.36307e-05|  0.02%|        return self._recv(size)
(call)|       700|    0.0490367|  7.00525e-05|  0.09%|# /opt/conda/lib/python3.8/multiprocessing/connection.py:374 _recv
   422|         0|            0|            0|  0.00%|
   423|       100|  0.000545025|  5.45025e-06|  0.00%|    def _poll(self, timeout):
   424|       100|   0.00180435|  1.80435e-05|  0.00%|        r = wait([self], timeout)
(call)|       100|     0.359014|   0.00359014|  0.67%|# /opt/conda/lib/python3.8/multiprocessing/connection.py:917 wait
   425|       100|   0.00057435|   5.7435e-06|  0.00%|        return bool(r)
   426|         0|            0|            0|  0.00%|
   427|         0|            0|            0|  0.00%|
   428|         0|            0|            0|  0.00%|#
   429|         0|            0|            0|  0.00%|# Public functions
   430|         0|            0|            0|  0.00%|#
   431|         0|            0|            0|  0.00%|
   432|         0|            0|            0|  0.00%|class Listener(object):
   433|         0|            0|            0|  0.00%|    '''
   434|         0|            0|            0|  0.00%|    Returns a listener object.
   435|         0|            0|            0|  0.00%|
   436|         0|            0|            0|  0.00%|    This is a wrapper for a bound socket which is 'listening' for
   437|         0|            0|            0|  0.00%|    connections, or for a Windows named pipe.
   438|         0|            0|            0|  0.00%|    '''
   439|         0|            0|            0|  0.00%|    def __init__(self, address=None, family=None, backlog=1, authkey=None):
   440|         0|            0|            0|  0.00%|        family = family or (address and address_type(address)) \
   441|         0|            0|            0|  0.00%|                 or default_family
   442|         0|            0|            0|  0.00%|        address = address or arbitrary_address(family)
   443|         0|            0|            0|  0.00%|
   444|         0|            0|            0|  0.00%|        _validate_family(family)
   445|         0|            0|            0|  0.00%|        if family == 'AF_PIPE':
   446|         0|            0|            0|  0.00%|            self._listener = PipeListener(address, backlog)
   447|         0|            0|            0|  0.00%|        else:
   448|         0|            0|            0|  0.00%|            self._listener = SocketListener(address, family, backlog)
   449|         0|            0|            0|  0.00%|
   450|         0|            0|            0|  0.00%|        if authkey is not None and not isinstance(authkey, bytes):
   451|         0|            0|            0|  0.00%|            raise TypeError('authkey should be a byte string')
   452|         0|            0|            0|  0.00%|
   453|         0|            0|            0|  0.00%|        self._authkey = authkey
   454|         0|            0|            0|  0.00%|
   455|         0|            0|            0|  0.00%|    def accept(self):
   456|         0|            0|            0|  0.00%|        '''
   457|         0|            0|            0|  0.00%|        Accept a connection on the bound socket or named pipe of `self`.
   458|         0|            0|            0|  0.00%|
   459|         0|            0|            0|  0.00%|        Returns a `Connection` object.
   460|         0|            0|            0|  0.00%|        '''
   461|         0|            0|            0|  0.00%|        if self._listener is None:
   462|         0|            0|            0|  0.00%|            raise OSError('listener is closed')
   463|         0|            0|            0|  0.00%|        c = self._listener.accept()
   464|         0|            0|            0|  0.00%|        if self._authkey:
   465|         0|            0|            0|  0.00%|            deliver_challenge(c, self._authkey)
   466|         0|            0|            0|  0.00%|            answer_challenge(c, self._authkey)
   467|         0|            0|            0|  0.00%|        return c
   468|         0|            0|            0|  0.00%|
   469|         0|            0|            0|  0.00%|    def close(self):
   470|         0|            0|            0|  0.00%|        '''
   471|         0|            0|            0|  0.00%|        Close the bound socket or named pipe of `self`.
   472|         0|            0|            0|  0.00%|        '''
   473|         0|            0|            0|  0.00%|        listener = self._listener
   474|         0|            0|            0|  0.00%|        if listener is not None:
   475|         0|            0|            0|  0.00%|            self._listener = None
   476|         0|            0|            0|  0.00%|            listener.close()
   477|         0|            0|            0|  0.00%|
   478|         0|            0|            0|  0.00%|    @property
   479|         0|            0|            0|  0.00%|    def address(self):
   480|         0|            0|            0|  0.00%|        return self._listener._address
   481|         0|            0|            0|  0.00%|
   482|         0|            0|            0|  0.00%|    @property
   483|         0|            0|            0|  0.00%|    def last_accepted(self):
   484|         0|            0|            0|  0.00%|        return self._listener._last_accepted
   485|         0|            0|            0|  0.00%|
   486|         0|            0|            0|  0.00%|    def __enter__(self):
   487|         0|            0|            0|  0.00%|        return self
   488|         0|            0|            0|  0.00%|
   489|         0|            0|            0|  0.00%|    def __exit__(self, exc_type, exc_value, exc_tb):
   490|         0|            0|            0|  0.00%|        self.close()
   491|         0|            0|            0|  0.00%|
   492|         0|            0|            0|  0.00%|
   493|       200|   0.00109243|  5.46217e-06|  0.00%|def Client(address, family=None, authkey=None):
   494|         0|            0|            0|  0.00%|    '''
   495|         0|            0|            0|  0.00%|    Returns a connection to the address of a `Listener`
   496|         0|            0|            0|  0.00%|    '''
   497|       200|   0.00286055|  1.43027e-05|  0.01%|    family = family or address_type(address)
(call)|       200|   0.00530124|  2.65062e-05|  0.01%|# /opt/conda/lib/python3.8/multiprocessing/connection.py:95 address_type
   498|       200|   0.00265169|  1.32585e-05|  0.00%|    _validate_family(family)
(call)|       200|   0.00277758|  1.38879e-05|  0.01%|# /opt/conda/lib/python3.8/multiprocessing/connection.py:83 _validate_family
   499|       200|  0.000916243|  4.58121e-06|  0.00%|    if family == 'AF_PIPE':
   500|         0|            0|            0|  0.00%|        c = PipeClient(address)
   501|         0|            0|            0|  0.00%|    else:
   502|       200|   0.00297952|  1.48976e-05|  0.01%|        c = SocketClient(address)
(call)|       200|    0.0575938|  0.000287969|  0.11%|# /opt/conda/lib/python3.8/multiprocessing/connection.py:623 SocketClient
   503|         0|            0|            0|  0.00%|
   504|       200|   0.00140142|  7.00712e-06|  0.00%|    if authkey is not None and not isinstance(authkey, bytes):
   505|         0|            0|            0|  0.00%|        raise TypeError('authkey should be a byte string')
   506|         0|            0|            0|  0.00%|
   507|       200|  0.000846148|  4.23074e-06|  0.00%|    if authkey is not None:
   508|       200|   0.00304341|  1.52171e-05|  0.01%|        answer_challenge(c, authkey)
(call)|       200|     0.236393|   0.00118196|  0.44%|# /opt/conda/lib/python3.8/multiprocessing/connection.py:747 answer_challenge
   509|       200|    0.0030303|  1.51515e-05|  0.01%|        deliver_challenge(c, authkey)
(call)|       200|     0.202865|   0.00101432|  0.38%|# /opt/conda/lib/python3.8/multiprocessing/connection.py:732 deliver_challenge
   510|         0|            0|            0|  0.00%|
   511|       200|  0.000888348|  4.44174e-06|  0.00%|    return c
   512|         0|            0|            0|  0.00%|
   513|         0|            0|            0|  0.00%|
   514|         0|            0|            0|  0.00%|if sys.platform != 'win32':
   515|         0|            0|            0|  0.00%|
   516|        10|  0.000115633|  1.15633e-05|  0.00%|    def Pipe(duplex=True):
   517|         0|            0|            0|  0.00%|        '''
   518|         0|            0|            0|  0.00%|        Returns pair of connection objects at either end of a pipe
   519|         0|            0|            0|  0.00%|        '''
   520|        10|  9.08375e-05|  9.08375e-06|  0.00%|        if duplex:
   521|         0|            0|            0|  0.00%|            s1, s2 = socket.socketpair()
   522|         0|            0|            0|  0.00%|            s1.setblocking(True)
   523|         0|            0|            0|  0.00%|            s2.setblocking(True)
   524|         0|            0|            0|  0.00%|            c1 = Connection(s1.detach())
   525|         0|            0|            0|  0.00%|            c2 = Connection(s2.detach())
   526|         0|            0|            0|  0.00%|        else:
   527|        10|     0.000211|     2.11e-05|  0.00%|            fd1, fd2 = os.pipe()
   528|        10|  0.000506878|  5.06878e-05|  0.00%|            c1 = Connection(fd1, writable=False)
(call)|        10|  0.000656366|  6.56366e-05|  0.00%|# /opt/conda/lib/python3.8/multiprocessing/connection.py:117 __init__
   529|        10|  0.000212908|  2.12908e-05|  0.00%|            c2 = Connection(fd2, readable=False)
(call)|        10|  0.000278473|  2.78473e-05|  0.00%|# /opt/conda/lib/python3.8/multiprocessing/connection.py:117 __init__
   530|         0|            0|            0|  0.00%|
   531|        10|  6.36578e-05|  6.36578e-06|  0.00%|        return c1, c2
   532|         0|            0|            0|  0.00%|
   533|         0|            0|            0|  0.00%|else:
   534|         0|            0|            0|  0.00%|
   535|         0|            0|            0|  0.00%|    def Pipe(duplex=True):
   536|         0|            0|            0|  0.00%|        '''
   537|         0|            0|            0|  0.00%|        Returns pair of connection objects at either end of a pipe
   538|         0|            0|            0|  0.00%|        '''
   539|         0|            0|            0|  0.00%|        address = arbitrary_address('AF_PIPE')
   540|         0|            0|            0|  0.00%|        if duplex:
   541|         0|            0|            0|  0.00%|            openmode = _winapi.PIPE_ACCESS_DUPLEX
   542|         0|            0|            0|  0.00%|            access = _winapi.GENERIC_READ | _winapi.GENERIC_WRITE
   543|         0|            0|            0|  0.00%|            obsize, ibsize = BUFSIZE, BUFSIZE
   544|         0|            0|            0|  0.00%|        else:
   545|         0|            0|            0|  0.00%|            openmode = _winapi.PIPE_ACCESS_INBOUND
   546|         0|            0|            0|  0.00%|            access = _winapi.GENERIC_WRITE
   547|         0|            0|            0|  0.00%|            obsize, ibsize = 0, BUFSIZE
   548|         0|            0|            0|  0.00%|
   549|         0|            0|            0|  0.00%|        h1 = _winapi.CreateNamedPipe(
   550|         0|            0|            0|  0.00%|            address, openmode | _winapi.FILE_FLAG_OVERLAPPED |
   551|         0|            0|            0|  0.00%|            _winapi.FILE_FLAG_FIRST_PIPE_INSTANCE,
   552|         0|            0|            0|  0.00%|            _winapi.PIPE_TYPE_MESSAGE | _winapi.PIPE_READMODE_MESSAGE |
   553|         0|            0|            0|  0.00%|            _winapi.PIPE_WAIT,
   554|         0|            0|            0|  0.00%|            1, obsize, ibsize, _winapi.NMPWAIT_WAIT_FOREVER,
   555|         0|            0|            0|  0.00%|            # default security descriptor: the handle cannot be inherited
   556|         0|            0|            0|  0.00%|            _winapi.NULL
   557|         0|            0|            0|  0.00%|            )
   558|         0|            0|            0|  0.00%|        h2 = _winapi.CreateFile(
   559|         0|            0|            0|  0.00%|            address, access, 0, _winapi.NULL, _winapi.OPEN_EXISTING,
   560|         0|            0|            0|  0.00%|            _winapi.FILE_FLAG_OVERLAPPED, _winapi.NULL
   561|         0|            0|            0|  0.00%|            )
   562|         0|            0|            0|  0.00%|        _winapi.SetNamedPipeHandleState(
   563|         0|            0|            0|  0.00%|            h2, _winapi.PIPE_READMODE_MESSAGE, None, None
   564|         0|            0|            0|  0.00%|            )
   565|         0|            0|            0|  0.00%|
   566|         0|            0|            0|  0.00%|        overlapped = _winapi.ConnectNamedPipe(h1, overlapped=True)
   567|         0|            0|            0|  0.00%|        _, err = overlapped.GetOverlappedResult(True)
   568|         0|            0|            0|  0.00%|        assert err == 0
   569|         0|            0|            0|  0.00%|
   570|         0|            0|            0|  0.00%|        c1 = PipeConnection(h1, writable=duplex)
   571|         0|            0|            0|  0.00%|        c2 = PipeConnection(h2, readable=duplex)
   572|         0|            0|            0|  0.00%|
   573|         0|            0|            0|  0.00%|        return c1, c2
   574|         0|            0|            0|  0.00%|
   575|         0|            0|            0|  0.00%|#
   576|         0|            0|            0|  0.00%|# Definitions for connections based on sockets
   577|         0|            0|            0|  0.00%|#
   578|         0|            0|            0|  0.00%|
   579|         0|            0|            0|  0.00%|class SocketListener(object):
   580|         0|            0|            0|  0.00%|    '''
   581|         0|            0|            0|  0.00%|    Representation of a socket which is bound to an address and listening
   582|         0|            0|            0|  0.00%|    '''
   583|         0|            0|            0|  0.00%|    def __init__(self, address, family, backlog=1):
   584|         0|            0|            0|  0.00%|        self._socket = socket.socket(getattr(socket, family))
   585|         0|            0|            0|  0.00%|        try:
   586|         0|            0|            0|  0.00%|            # SO_REUSEADDR has different semantics on Windows (issue #2550).
   587|         0|            0|            0|  0.00%|            if os.name == 'posix':
   588|         0|            0|            0|  0.00%|                self._socket.setsockopt(socket.SOL_SOCKET,
   589|         0|            0|            0|  0.00%|                                        socket.SO_REUSEADDR, 1)
   590|         0|            0|            0|  0.00%|            self._socket.setblocking(True)
   591|         0|            0|            0|  0.00%|            self._socket.bind(address)
   592|         0|            0|            0|  0.00%|            self._socket.listen(backlog)
   593|         0|            0|            0|  0.00%|            self._address = self._socket.getsockname()
   594|         0|            0|            0|  0.00%|        except OSError:
   595|         0|            0|            0|  0.00%|            self._socket.close()
   596|         0|            0|            0|  0.00%|            raise
   597|         0|            0|            0|  0.00%|        self._family = family
   598|         0|            0|            0|  0.00%|        self._last_accepted = None
   599|         0|            0|            0|  0.00%|
   600|         0|            0|            0|  0.00%|        if family == 'AF_UNIX' and not util.is_abstract_socket_namespace(address):
   601|         0|            0|            0|  0.00%|            # Linux abstract socket namespaces do not need to be explicitly unlinked
   602|         0|            0|            0|  0.00%|            self._unlink = util.Finalize(
   603|         0|            0|            0|  0.00%|                self, os.unlink, args=(address,), exitpriority=0
   604|         0|            0|            0|  0.00%|                )
   605|         0|            0|            0|  0.00%|        else:
   606|         0|            0|            0|  0.00%|            self._unlink = None
   607|         0|            0|            0|  0.00%|
   608|         0|            0|            0|  0.00%|    def accept(self):
   609|         0|            0|            0|  0.00%|        s, self._last_accepted = self._socket.accept()
   610|         0|            0|            0|  0.00%|        s.setblocking(True)
   611|         0|            0|            0|  0.00%|        return Connection(s.detach())
   612|         0|            0|            0|  0.00%|
   613|         0|            0|            0|  0.00%|    def close(self):
   614|         0|            0|            0|  0.00%|        try:
   615|         0|            0|            0|  0.00%|            self._socket.close()
   616|         0|            0|            0|  0.00%|        finally:
   617|         0|            0|            0|  0.00%|            unlink = self._unlink
   618|         0|            0|            0|  0.00%|            if unlink is not None:
   619|         0|            0|            0|  0.00%|                self._unlink = None
   620|         0|            0|            0|  0.00%|                unlink()
   621|         0|            0|            0|  0.00%|
   622|         0|            0|            0|  0.00%|
   623|       200|   0.00102639|  5.13196e-06|  0.00%|def SocketClient(address):
   624|         0|            0|            0|  0.00%|    '''
   625|         0|            0|            0|  0.00%|    Return a connection object connected to the socket given by `address`
   626|         0|            0|            0|  0.00%|    '''
   627|       200|   0.00251389|  1.25694e-05|  0.00%|    family = address_type(address)
(call)|       200|    0.0045557|  2.27785e-05|  0.01%|# /opt/conda/lib/python3.8/multiprocessing/connection.py:95 address_type
   628|       200|     0.005687|   2.8435e-05|  0.01%|    with socket.socket( getattr(socket, family) ) as s:
(call)|       200|    0.0124192|  6.20961e-05|  0.02%|# /opt/conda/lib/python3.8/socket.py:219 __init__
(call)|       200|   0.00174117|  8.70585e-06|  0.00%|# /opt/conda/lib/python3.8/socket.py:235 __enter__
   629|       200|   0.00170994|  8.54969e-06|  0.00%|        s.setblocking(True)
   630|       200|   0.00874472|  4.37236e-05|  0.02%|        s.connect(address)
   631|       200|   0.00735497|  3.67749e-05|  0.01%|        return Connection(s.detach())
(call)|       200|   0.00375962|  1.87981e-05|  0.01%|# /opt/conda/lib/python3.8/socket.py:502 detach
(call)|       200|   0.00629067|  3.14534e-05|  0.01%|# /opt/conda/lib/python3.8/multiprocessing/connection.py:117 __init__
(call)|       200|   0.00179052|  8.95262e-06|  0.00%|# /opt/conda/lib/python3.8/socket.py:238 __exit__
   632|         0|            0|            0|  0.00%|
   633|         0|            0|            0|  0.00%|#
   634|         0|            0|            0|  0.00%|# Definitions for connections based on named pipes
   635|         0|            0|            0|  0.00%|#
   636|         0|            0|            0|  0.00%|
   637|         0|            0|            0|  0.00%|if sys.platform == 'win32':
   638|         0|            0|            0|  0.00%|
   639|         0|            0|            0|  0.00%|    class PipeListener(object):
   640|         0|            0|            0|  0.00%|        '''
   641|         0|            0|            0|  0.00%|        Representation of a named pipe
   642|         0|            0|            0|  0.00%|        '''
   643|         0|            0|            0|  0.00%|        def __init__(self, address, backlog=None):
   644|         0|            0|            0|  0.00%|            self._address = address
   645|         0|            0|            0|  0.00%|            self._handle_queue = [self._new_handle(first=True)]
   646|         0|            0|            0|  0.00%|
   647|         0|            0|            0|  0.00%|            self._last_accepted = None
   648|         0|            0|            0|  0.00%|            util.sub_debug('listener created with address=%r', self._address)
   649|         0|            0|            0|  0.00%|            self.close = util.Finalize(
   650|         0|            0|            0|  0.00%|                self, PipeListener._finalize_pipe_listener,
   651|         0|            0|            0|  0.00%|                args=(self._handle_queue, self._address), exitpriority=0
   652|         0|            0|            0|  0.00%|                )
   653|         0|            0|            0|  0.00%|
   654|         0|            0|            0|  0.00%|        def _new_handle(self, first=False):
   655|         0|            0|            0|  0.00%|            flags = _winapi.PIPE_ACCESS_DUPLEX | _winapi.FILE_FLAG_OVERLAPPED
   656|         0|            0|            0|  0.00%|            if first:
   657|         0|            0|            0|  0.00%|                flags |= _winapi.FILE_FLAG_FIRST_PIPE_INSTANCE
   658|         0|            0|            0|  0.00%|            return _winapi.CreateNamedPipe(
   659|         0|            0|            0|  0.00%|                self._address, flags,
   660|         0|            0|            0|  0.00%|                _winapi.PIPE_TYPE_MESSAGE | _winapi.PIPE_READMODE_MESSAGE |
   661|         0|            0|            0|  0.00%|                _winapi.PIPE_WAIT,
   662|         0|            0|            0|  0.00%|                _winapi.PIPE_UNLIMITED_INSTANCES, BUFSIZE, BUFSIZE,
   663|         0|            0|            0|  0.00%|                _winapi.NMPWAIT_WAIT_FOREVER, _winapi.NULL
   664|         0|            0|            0|  0.00%|                )
   665|         0|            0|            0|  0.00%|
   666|         0|            0|            0|  0.00%|        def accept(self):
   667|         0|            0|            0|  0.00%|            self._handle_queue.append(self._new_handle())
   668|         0|            0|            0|  0.00%|            handle = self._handle_queue.pop(0)
   669|         0|            0|            0|  0.00%|            try:
   670|         0|            0|            0|  0.00%|                ov = _winapi.ConnectNamedPipe(handle, overlapped=True)
   671|         0|            0|            0|  0.00%|            except OSError as e:
   672|         0|            0|            0|  0.00%|                if e.winerror != _winapi.ERROR_NO_DATA:
   673|         0|            0|            0|  0.00%|                    raise
   674|         0|            0|            0|  0.00%|                # ERROR_NO_DATA can occur if a client has already connected,
   675|         0|            0|            0|  0.00%|                # written data and then disconnected -- see Issue 14725.
   676|         0|            0|            0|  0.00%|            else:
   677|         0|            0|            0|  0.00%|                try:
   678|         0|            0|            0|  0.00%|                    res = _winapi.WaitForMultipleObjects(
   679|         0|            0|            0|  0.00%|                        [ov.event], False, INFINITE)
   680|         0|            0|            0|  0.00%|                except:
   681|         0|            0|            0|  0.00%|                    ov.cancel()
   682|         0|            0|            0|  0.00%|                    _winapi.CloseHandle(handle)
   683|         0|            0|            0|  0.00%|                    raise
   684|         0|            0|            0|  0.00%|                finally:
   685|         0|            0|            0|  0.00%|                    _, err = ov.GetOverlappedResult(True)
   686|         0|            0|            0|  0.00%|                    assert err == 0
   687|         0|            0|            0|  0.00%|            return PipeConnection(handle)
   688|         0|            0|            0|  0.00%|
   689|         0|            0|            0|  0.00%|        @staticmethod
   690|         0|            0|            0|  0.00%|        def _finalize_pipe_listener(queue, address):
   691|         0|            0|            0|  0.00%|            util.sub_debug('closing listener with address=%r', address)
   692|         0|            0|            0|  0.00%|            for handle in queue:
   693|         0|            0|            0|  0.00%|                _winapi.CloseHandle(handle)
   694|         0|            0|            0|  0.00%|
   695|         0|            0|            0|  0.00%|    def PipeClient(address):
   696|         0|            0|            0|  0.00%|        '''
   697|         0|            0|            0|  0.00%|        Return a connection object connected to the pipe given by `address`
   698|         0|            0|            0|  0.00%|        '''
   699|         0|            0|            0|  0.00%|        t = _init_timeout()
   700|         0|            0|            0|  0.00%|        while 1:
   701|         0|            0|            0|  0.00%|            try:
   702|         0|            0|            0|  0.00%|                _winapi.WaitNamedPipe(address, 1000)
   703|         0|            0|            0|  0.00%|                h = _winapi.CreateFile(
   704|         0|            0|            0|  0.00%|                    address, _winapi.GENERIC_READ | _winapi.GENERIC_WRITE,
   705|         0|            0|            0|  0.00%|                    0, _winapi.NULL, _winapi.OPEN_EXISTING,
   706|         0|            0|            0|  0.00%|                    _winapi.FILE_FLAG_OVERLAPPED, _winapi.NULL
   707|         0|            0|            0|  0.00%|                    )
   708|         0|            0|            0|  0.00%|            except OSError as e:
   709|         0|            0|            0|  0.00%|                if e.winerror not in (_winapi.ERROR_SEM_TIMEOUT,
   710|         0|            0|            0|  0.00%|                                      _winapi.ERROR_PIPE_BUSY) or _check_timeout(t):
   711|         0|            0|            0|  0.00%|                    raise
   712|         0|            0|            0|  0.00%|            else:
   713|         0|            0|            0|  0.00%|                break
   714|         0|            0|            0|  0.00%|        else:
   715|         0|            0|            0|  0.00%|            raise
   716|         0|            0|            0|  0.00%|
   717|         0|            0|            0|  0.00%|        _winapi.SetNamedPipeHandleState(
   718|         0|            0|            0|  0.00%|            h, _winapi.PIPE_READMODE_MESSAGE, None, None
   719|         0|            0|            0|  0.00%|            )
   720|         0|            0|            0|  0.00%|        return PipeConnection(h)
   721|         0|            0|            0|  0.00%|
   722|         0|            0|            0|  0.00%|#
   723|         0|            0|            0|  0.00%|# Authentication stuff
   724|         0|            0|            0|  0.00%|#
   725|         0|            0|            0|  0.00%|
   726|         0|            0|            0|  0.00%|MESSAGE_LENGTH = 20
   727|         0|            0|            0|  0.00%|
   728|         0|            0|            0|  0.00%|CHALLENGE = b'#CHALLENGE#'
   729|         0|            0|            0|  0.00%|WELCOME = b'#WELCOME#'
   730|         0|            0|            0|  0.00%|FAILURE = b'#FAILURE#'
   731|         0|            0|            0|  0.00%|
   732|       200|    0.0011797|  5.89848e-06|  0.00%|def deliver_challenge(connection, authkey):
   733|       200|   0.00121236|  6.06179e-06|  0.00%|    import hmac
   734|       200|   0.00145555|  7.27773e-06|  0.00%|    if not isinstance(authkey, bytes):
   735|         0|            0|            0|  0.00%|        raise ValueError(
   736|         0|            0|            0|  0.00%|            "Authkey must be bytes, not {0!s}".format(type(authkey)))
   737|       200|   0.00292921|  1.46461e-05|  0.01%|    message = os.urandom(MESSAGE_LENGTH)
   738|       200|   0.00323582|  1.61791e-05|  0.01%|    connection.send_bytes(CHALLENGE + message)
(call)|       200|    0.0354753|  0.000177376|  0.07%|# /opt/conda/lib/python3.8/multiprocessing/connection.py:181 send_bytes
   739|       200|   0.00500178|  2.50089e-05|  0.01%|    digest = hmac.new(authkey, message, 'md5').digest()
(call)|       200|    0.0494139|   0.00024707|  0.09%|# /opt/conda/lib/python3.8/hmac.py:136 new
(call)|       200|   0.00928712|  4.64356e-05|  0.02%|# /opt/conda/lib/python3.8/hmac.py:120 digest
   740|       200|   0.00288081|  1.44041e-05|  0.01%|    response = connection.recv_bytes(256)        # reject large message
(call)|       200|    0.0514328|  0.000257164|  0.10%|# /opt/conda/lib/python3.8/multiprocessing/connection.py:208 recv_bytes
   741|       200|  0.000987053|  4.93526e-06|  0.00%|    if response == digest:
   742|       200|   0.00294614|  1.47307e-05|  0.01%|        connection.send_bytes(WELCOME)
(call)|       200|    0.0354271|  0.000177135|  0.07%|# /opt/conda/lib/python3.8/multiprocessing/connection.py:181 send_bytes
   743|         0|            0|            0|  0.00%|    else:
   744|         0|            0|            0|  0.00%|        connection.send_bytes(FAILURE)
   745|         0|            0|            0|  0.00%|        raise AuthenticationError('digest received was wrong')
   746|         0|            0|            0|  0.00%|
   747|       200|   0.00123119|  6.15597e-06|  0.00%|def answer_challenge(connection, authkey):
   748|       200|   0.00134587|  6.72936e-06|  0.00%|    import hmac
   749|       200|   0.00138235|  6.91175e-06|  0.00%|    if not isinstance(authkey, bytes):
   750|         0|            0|            0|  0.00%|        raise ValueError(
   751|         0|            0|            0|  0.00%|            "Authkey must be bytes, not {0!s}".format(type(authkey)))
   752|       200|     0.002882|    1.441e-05|  0.01%|    message = connection.recv_bytes(256)         # reject large message
(call)|       200|    0.0604022|  0.000302011|  0.11%|# /opt/conda/lib/python3.8/multiprocessing/connection.py:208 recv_bytes
   753|       200|   0.00166225|  8.31127e-06|  0.00%|    assert message[:len(CHALLENGE)] == CHALLENGE, 'message = %r' % message
   754|       200|   0.00143504|  7.17521e-06|  0.00%|    message = message[len(CHALLENGE):]
   755|       200|   0.00525355|  2.62678e-05|  0.01%|    digest = hmac.new(authkey, message, 'md5').digest()
(call)|       200|    0.0533407|  0.000266703|  0.10%|# /opt/conda/lib/python3.8/hmac.py:136 new
(call)|       200|    0.0100183|  5.00917e-05|  0.02%|# /opt/conda/lib/python3.8/hmac.py:120 digest
   756|       200|   0.00323606|  1.61803e-05|  0.01%|    connection.send_bytes(digest)
(call)|       200|    0.0383654|  0.000191827|  0.07%|# /opt/conda/lib/python3.8/multiprocessing/connection.py:181 send_bytes
   757|       200|   0.00293469|  1.46735e-05|  0.01%|    response = connection.recv_bytes(256)        # reject large message
(call)|       200|    0.0518723|  0.000259361|  0.10%|# /opt/conda/lib/python3.8/multiprocessing/connection.py:208 recv_bytes
   758|       200|   0.00103116|   5.1558e-06|  0.00%|    if response != WELCOME:
   759|         0|            0|            0|  0.00%|        raise AuthenticationError('digest sent was rejected')
   760|         0|            0|            0|  0.00%|
   761|         0|            0|            0|  0.00%|#
   762|         0|            0|            0|  0.00%|# Support for using xmlrpclib for serialization
   763|         0|            0|            0|  0.00%|#
   764|         0|            0|            0|  0.00%|
   765|         0|            0|            0|  0.00%|class ConnectionWrapper(object):
   766|         0|            0|            0|  0.00%|    def __init__(self, conn, dumps, loads):
   767|         0|            0|            0|  0.00%|        self._conn = conn
   768|         0|            0|            0|  0.00%|        self._dumps = dumps
   769|         0|            0|            0|  0.00%|        self._loads = loads
   770|         0|            0|            0|  0.00%|        for attr in ('fileno', 'close', 'poll', 'recv_bytes', 'send_bytes'):
   771|         0|            0|            0|  0.00%|            obj = getattr(conn, attr)
   772|         0|            0|            0|  0.00%|            setattr(self, attr, obj)
   773|         0|            0|            0|  0.00%|    def send(self, obj):
   774|         0|            0|            0|  0.00%|        s = self._dumps(obj)
   775|         0|            0|            0|  0.00%|        self._conn.send_bytes(s)
   776|         0|            0|            0|  0.00%|    def recv(self):
   777|         0|            0|            0|  0.00%|        s = self._conn.recv_bytes()
   778|         0|            0|            0|  0.00%|        return self._loads(s)
   779|         0|            0|            0|  0.00%|
   780|         0|            0|            0|  0.00%|def _xml_dumps(obj):
   781|         0|            0|            0|  0.00%|    return xmlrpclib.dumps((obj,), None, None, None, 1).encode('utf-8')
   782|         0|            0|            0|  0.00%|
   783|         0|            0|            0|  0.00%|def _xml_loads(s):
   784|         0|            0|            0|  0.00%|    (obj,), method = xmlrpclib.loads(s.decode('utf-8'))
   785|         0|            0|            0|  0.00%|    return obj
   786|         0|            0|            0|  0.00%|
   787|         0|            0|            0|  0.00%|class XmlListener(Listener):
   788|         0|            0|            0|  0.00%|    def accept(self):
   789|         0|            0|            0|  0.00%|        global xmlrpclib
   790|         0|            0|            0|  0.00%|        import xmlrpc.client as xmlrpclib
   791|         0|            0|            0|  0.00%|        obj = Listener.accept(self)
   792|         0|            0|            0|  0.00%|        return ConnectionWrapper(obj, _xml_dumps, _xml_loads)
   793|         0|            0|            0|  0.00%|
   794|         0|            0|            0|  0.00%|def XmlClient(*args, **kwds):
   795|         0|            0|            0|  0.00%|    global xmlrpclib
   796|         0|            0|            0|  0.00%|    import xmlrpc.client as xmlrpclib
   797|         0|            0|            0|  0.00%|    return ConnectionWrapper(Client(*args, **kwds), _xml_dumps, _xml_loads)
   798|         0|            0|            0|  0.00%|
   799|         0|            0|            0|  0.00%|#
   800|         0|            0|            0|  0.00%|# Wait
   801|         0|            0|            0|  0.00%|#
   802|         0|            0|            0|  0.00%|
   803|         0|            0|            0|  0.00%|if sys.platform == 'win32':
   804|         0|            0|            0|  0.00%|
   805|         0|            0|            0|  0.00%|    def _exhaustive_wait(handles, timeout):
   806|         0|            0|            0|  0.00%|        # Return ALL handles which are currently signalled.  (Only
   807|         0|            0|            0|  0.00%|        # returning the first signalled might create starvation issues.)
   808|         0|            0|            0|  0.00%|        L = list(handles)
   809|         0|            0|            0|  0.00%|        ready = []
   810|         0|            0|            0|  0.00%|        while L:
   811|         0|            0|            0|  0.00%|            res = _winapi.WaitForMultipleObjects(L, False, timeout)
   812|         0|            0|            0|  0.00%|            if res == WAIT_TIMEOUT:
   813|         0|            0|            0|  0.00%|                break
   814|         0|            0|            0|  0.00%|            elif WAIT_OBJECT_0 <= res < WAIT_OBJECT_0 + len(L):
   815|         0|            0|            0|  0.00%|                res -= WAIT_OBJECT_0
   816|         0|            0|            0|  0.00%|            elif WAIT_ABANDONED_0 <= res < WAIT_ABANDONED_0 + len(L):
   817|         0|            0|            0|  0.00%|                res -= WAIT_ABANDONED_0
   818|         0|            0|            0|  0.00%|            else:
   819|         0|            0|            0|  0.00%|                raise RuntimeError('Should not get here')
   820|         0|            0|            0|  0.00%|            ready.append(L[res])
   821|         0|            0|            0|  0.00%|            L = L[res+1:]
   822|         0|            0|            0|  0.00%|            timeout = 0
   823|         0|            0|            0|  0.00%|        return ready
   824|         0|            0|            0|  0.00%|
   825|         0|            0|            0|  0.00%|    _ready_errors = {_winapi.ERROR_BROKEN_PIPE, _winapi.ERROR_NETNAME_DELETED}
   826|         0|            0|            0|  0.00%|
   827|         0|            0|            0|  0.00%|    def wait(object_list, timeout=None):
   828|         0|            0|            0|  0.00%|        '''
   829|         0|            0|            0|  0.00%|        Wait till an object in object_list is ready/readable.
   830|         0|            0|            0|  0.00%|
   831|         0|            0|            0|  0.00%|        Returns list of those objects in object_list which are ready/readable.
   832|         0|            0|            0|  0.00%|        '''
   833|         0|            0|            0|  0.00%|        if timeout is None:
   834|         0|            0|            0|  0.00%|            timeout = INFINITE
   835|         0|            0|            0|  0.00%|        elif timeout < 0:
   836|         0|            0|            0|  0.00%|            timeout = 0
   837|         0|            0|            0|  0.00%|        else:
   838|         0|            0|            0|  0.00%|            timeout = int(timeout * 1000 + 0.5)
   839|         0|            0|            0|  0.00%|
   840|         0|            0|            0|  0.00%|        object_list = list(object_list)
   841|         0|            0|            0|  0.00%|        waithandle_to_obj = {}
   842|         0|            0|            0|  0.00%|        ov_list = []
   843|         0|            0|            0|  0.00%|        ready_objects = set()
   844|         0|            0|            0|  0.00%|        ready_handles = set()
   845|         0|            0|            0|  0.00%|
   846|         0|            0|            0|  0.00%|        try:
   847|         0|            0|            0|  0.00%|            for o in object_list:
   848|         0|            0|            0|  0.00%|                try:
   849|         0|            0|            0|  0.00%|                    fileno = getattr(o, 'fileno')
   850|         0|            0|            0|  0.00%|                except AttributeError:
   851|         0|            0|            0|  0.00%|                    waithandle_to_obj[o.__index__()] = o
   852|         0|            0|            0|  0.00%|                else:
   853|         0|            0|            0|  0.00%|                    # start an overlapped read of length zero
   854|         0|            0|            0|  0.00%|                    try:
   855|         0|            0|            0|  0.00%|                        ov, err = _winapi.ReadFile(fileno(), 0, True)
   856|         0|            0|            0|  0.00%|                    except OSError as e:
   857|         0|            0|            0|  0.00%|                        ov, err = None, e.winerror
   858|         0|            0|            0|  0.00%|                        if err not in _ready_errors:
   859|         0|            0|            0|  0.00%|                            raise
   860|         0|            0|            0|  0.00%|                    if err == _winapi.ERROR_IO_PENDING:
   861|         0|            0|            0|  0.00%|                        ov_list.append(ov)
   862|         0|            0|            0|  0.00%|                        waithandle_to_obj[ov.event] = o
   863|         0|            0|            0|  0.00%|                    else:
   864|         0|            0|            0|  0.00%|                        # If o.fileno() is an overlapped pipe handle and
   865|         0|            0|            0|  0.00%|                        # err == 0 then there is a zero length message
   866|         0|            0|            0|  0.00%|                        # in the pipe, but it HAS NOT been consumed...
   867|         0|            0|            0|  0.00%|                        if ov and sys.getwindowsversion()[:2] >= (6, 2):
   868|         0|            0|            0|  0.00%|                            # ... except on Windows 8 and later, where
   869|         0|            0|            0|  0.00%|                            # the message HAS been consumed.
   870|         0|            0|            0|  0.00%|                            try:
   871|         0|            0|            0|  0.00%|                                _, err = ov.GetOverlappedResult(False)
   872|         0|            0|            0|  0.00%|                            except OSError as e:
   873|         0|            0|            0|  0.00%|                                err = e.winerror
   874|         0|            0|            0|  0.00%|                            if not err and hasattr(o, '_got_empty_message'):
   875|         0|            0|            0|  0.00%|                                o._got_empty_message = True
   876|         0|            0|            0|  0.00%|                        ready_objects.add(o)
   877|         0|            0|            0|  0.00%|                        timeout = 0
   878|         0|            0|            0|  0.00%|
   879|         0|            0|            0|  0.00%|            ready_handles = _exhaustive_wait(waithandle_to_obj.keys(), timeout)
   880|         0|            0|            0|  0.00%|        finally:
   881|         0|            0|            0|  0.00%|            # request that overlapped reads stop
   882|         0|            0|            0|  0.00%|            for ov in ov_list:
   883|         0|            0|            0|  0.00%|                ov.cancel()
   884|         0|            0|            0|  0.00%|
   885|         0|            0|            0|  0.00%|            # wait for all overlapped reads to stop
   886|         0|            0|            0|  0.00%|            for ov in ov_list:
   887|         0|            0|            0|  0.00%|                try:
   888|         0|            0|            0|  0.00%|                    _, err = ov.GetOverlappedResult(True)
   889|         0|            0|            0|  0.00%|                except OSError as e:
   890|         0|            0|            0|  0.00%|                    err = e.winerror
   891|         0|            0|            0|  0.00%|                    if err not in _ready_errors:
   892|         0|            0|            0|  0.00%|                        raise
   893|         0|            0|            0|  0.00%|                if err != _winapi.ERROR_OPERATION_ABORTED:
   894|         0|            0|            0|  0.00%|                    o = waithandle_to_obj[ov.event]
   895|         0|            0|            0|  0.00%|                    ready_objects.add(o)
   896|         0|            0|            0|  0.00%|                    if err == 0:
   897|         0|            0|            0|  0.00%|                        # If o.fileno() is an overlapped pipe handle then
   898|         0|            0|            0|  0.00%|                        # a zero length message HAS been consumed.
   899|         0|            0|            0|  0.00%|                        if hasattr(o, '_got_empty_message'):
   900|         0|            0|            0|  0.00%|                            o._got_empty_message = True
   901|         0|            0|            0|  0.00%|
   902|         0|            0|            0|  0.00%|        ready_objects.update(waithandle_to_obj[h] for h in ready_handles)
   903|         0|            0|            0|  0.00%|        return [o for o in object_list if o in ready_objects]
   904|         0|            0|            0|  0.00%|
   905|         0|            0|            0|  0.00%|else:
   906|         0|            0|            0|  0.00%|
   907|         0|            0|            0|  0.00%|    import selectors
   908|         0|            0|            0|  0.00%|
   909|         0|            0|            0|  0.00%|    # poll/select have the advantage of not requiring any extra file
   910|         0|            0|            0|  0.00%|    # descriptor, contrarily to epoll/kqueue (also, they require a single
   911|         0|            0|            0|  0.00%|    # syscall).
   912|         0|            0|            0|  0.00%|    if hasattr(selectors, 'PollSelector'):
   913|         0|            0|            0|  0.00%|        _WaitSelector = selectors.PollSelector
   914|         0|            0|            0|  0.00%|    else:
   915|         0|            0|            0|  0.00%|        _WaitSelector = selectors.SelectSelector
   916|         0|            0|            0|  0.00%|
   917|       108|  0.000728607|  6.74636e-06|  0.00%|    def wait(object_list, timeout=None):
   918|         0|            0|            0|  0.00%|        '''
   919|         0|            0|            0|  0.00%|        Wait till an object in object_list is ready/readable.
   920|         0|            0|            0|  0.00%|
   921|         0|            0|            0|  0.00%|        Returns list of those objects in object_list which are ready/readable.
   922|         0|            0|            0|  0.00%|        '''
   923|       108|   0.00332046|   3.0745e-05|  0.01%|        with _WaitSelector() as selector:
(call)|       108|   0.00785995|  7.27773e-05|  0.01%|# /opt/conda/lib/python3.8/selectors.py:347 __init__
(call)|       108|  0.000977993|  9.05549e-06|  0.00%|# /opt/conda/lib/python3.8/selectors.py:199 __enter__
   924|       216|   0.00114441|  5.29819e-06|  0.00%|            for obj in object_list:
   925|       108|   0.00180554|   1.6718e-05|  0.00%|                selector.register(obj, selectors.EVENT_READ)
(call)|       108|    0.0244725|  0.000226597|  0.05%|# /opt/conda/lib/python3.8/selectors.py:351 register
   926|         0|            0|            0|  0.00%|
   927|       108|  0.000464916|  4.30478e-06|  0.00%|            if timeout is not None:
   928|       108|  0.000871181|   8.0665e-06|  0.00%|                deadline = time.monotonic() + timeout
   929|         0|            0|            0|  0.00%|
   930|         0|            0|            0|  0.00%|            while True:
   931|       108|   0.00177693|  1.64531e-05|  0.00%|                ready = selector.select(timeout)
(call)|       108|     0.432065|    0.0040006|  0.81%|# /opt/conda/lib/python3.8/selectors.py:402 select
   932|       108|  0.000536203|  4.96485e-06|  0.00%|                if ready:
   933|       432|   0.00419617|  9.71335e-06|  0.01%|                    return [key.fileobj for (key, events) in ready]
(call)|       108|   0.00158238|  1.46517e-05|  0.00%|# /opt/conda/lib/python3.8/multiprocessing/connection.py:933 <listcomp>
(call)|       108|   0.00389743|  3.60873e-05|  0.01%|# /opt/conda/lib/python3.8/selectors.py:202 __exit__
   934|         0|            0|            0|  0.00%|                else:
   935|         0|            0|            0|  0.00%|                    if timeout is not None:
   936|         0|            0|            0|  0.00%|                        timeout = deadline - time.monotonic()
   937|         0|            0|            0|  0.00%|                        if timeout < 0:
   938|         0|            0|            0|  0.00%|                            return ready
   939|         0|            0|            0|  0.00%|
   940|         0|            0|            0|  0.00%|#
   941|         0|            0|            0|  0.00%|# Make connection and socket objects sharable if possible
   942|         0|            0|            0|  0.00%|#
   943|         0|            0|            0|  0.00%|
   944|         0|            0|            0|  0.00%|if sys.platform == 'win32':
   945|         0|            0|            0|  0.00%|    def reduce_connection(conn):
   946|         0|            0|            0|  0.00%|        handle = conn.fileno()
   947|         0|            0|            0|  0.00%|        with socket.fromfd(handle, socket.AF_INET, socket.SOCK_STREAM) as s:
   948|         0|            0|            0|  0.00%|            from . import resource_sharer
   949|         0|            0|            0|  0.00%|            ds = resource_sharer.DupSocket(s)
   950|         0|            0|            0|  0.00%|            return rebuild_connection, (ds, conn.readable, conn.writable)
   951|         0|            0|            0|  0.00%|    def rebuild_connection(ds, readable, writable):
   952|         0|            0|            0|  0.00%|        sock = ds.detach()
   953|         0|            0|            0|  0.00%|        return Connection(sock.detach(), readable, writable)
   954|         0|            0|            0|  0.00%|    reduction.register(Connection, reduce_connection)
   955|         0|            0|            0|  0.00%|
   956|         0|            0|            0|  0.00%|    def reduce_pipe_connection(conn):
   957|         0|            0|            0|  0.00%|        access = ((_winapi.FILE_GENERIC_READ if conn.readable else 0) |
   958|         0|            0|            0|  0.00%|                  (_winapi.FILE_GENERIC_WRITE if conn.writable else 0))
   959|         0|            0|            0|  0.00%|        dh = reduction.DupHandle(conn.fileno(), access)
   960|         0|            0|            0|  0.00%|        return rebuild_pipe_connection, (dh, conn.readable, conn.writable)
   961|         0|            0|            0|  0.00%|    def rebuild_pipe_connection(dh, readable, writable):
   962|         0|            0|            0|  0.00%|        handle = dh.detach()
   963|         0|            0|            0|  0.00%|        return PipeConnection(handle, readable, writable)
   964|         0|            0|            0|  0.00%|    reduction.register(PipeConnection, reduce_pipe_connection)
   965|         0|            0|            0|  0.00%|
   966|         0|            0|            0|  0.00%|else:
   967|         0|            0|            0|  0.00%|    def reduce_connection(conn):
   968|         0|            0|            0|  0.00%|        df = reduction.DupFd(conn.fileno())
   969|         0|            0|            0|  0.00%|        return rebuild_connection, (df, conn.readable, conn.writable)
   970|         0|            0|            0|  0.00%|    def rebuild_connection(df, readable, writable):
   971|         0|            0|            0|  0.00%|        fd = df.detach()
   972|         0|            0|            0|  0.00%|        return Connection(fd, readable, writable)
   973|         0|            0|            0|  0.00%|    reduction.register(Connection, reduce_connection)
File: /opt/conda/lib/python3.8/selectors.py
File duration: 0.464144s (0.87%)
Line #|      Hits|         Time| Time per hit|      %|Source code
------+----------+-------------+-------------+-------+-----------
     1|         0|            0|            0|  0.00%|"""Selectors module.
     2|         0|            0|            0|  0.00%|
     3|         0|            0|            0|  0.00%|This module allows high-level and efficient I/O multiplexing, built upon the
     4|         0|            0|            0|  0.00%|`select` module primitives.
     5|         0|            0|            0|  0.00%|"""
     6|         0|            0|            0|  0.00%|
     7|         0|            0|            0|  0.00%|
     8|         0|            0|            0|  0.00%|from abc import ABCMeta, abstractmethod
     9|         0|            0|            0|  0.00%|from collections import namedtuple
    10|         0|            0|            0|  0.00%|from collections.abc import Mapping
    11|         0|            0|            0|  0.00%|import math
    12|         0|            0|            0|  0.00%|import select
    13|         0|            0|            0|  0.00%|import sys
    14|         0|            0|            0|  0.00%|
    15|         0|            0|            0|  0.00%|
    16|         0|            0|            0|  0.00%|# generic events, that must be mapped to implementation-specific ones
    17|         0|            0|            0|  0.00%|EVENT_READ = (1 << 0)
    18|         0|            0|            0|  0.00%|EVENT_WRITE = (1 << 1)
    19|         0|            0|            0|  0.00%|
    20|         0|            0|            0|  0.00%|
    21|       108|  0.000599623|  5.55206e-06|  0.00%|def _fileobj_to_fd(fileobj):
    22|         0|            0|            0|  0.00%|    """Return a file descriptor from a file object.
    23|         0|            0|            0|  0.00%|
    24|         0|            0|            0|  0.00%|    Parameters:
    25|         0|            0|            0|  0.00%|    fileobj -- file object or file descriptor
    26|         0|            0|            0|  0.00%|
    27|         0|            0|            0|  0.00%|    Returns:
    28|         0|            0|            0|  0.00%|    corresponding file descriptor
    29|         0|            0|            0|  0.00%|
    30|         0|            0|            0|  0.00%|    Raises:
    31|         0|            0|            0|  0.00%|    ValueError if the object is invalid
    32|         0|            0|            0|  0.00%|    """
    33|       108|  0.000919342|  8.51243e-06|  0.00%|    if isinstance(fileobj, int):
    34|         8|  3.33786e-05|  4.17233e-06|  0.00%|        fd = fileobj
    35|         0|            0|            0|  0.00%|    else:
    36|       100|  0.000475407|  4.75407e-06|  0.00%|        try:
    37|       100|   0.00153208|  1.53208e-05|  0.00%|            fd = int(fileobj.fileno())
(call)|       100|   0.00310326|  3.10326e-05|  0.01%|# /opt/conda/lib/python3.8/multiprocessing/connection.py:168 fileno
    38|         0|            0|            0|  0.00%|        except (AttributeError, TypeError, ValueError):
    39|         0|            0|            0|  0.00%|            raise ValueError("Invalid file object: "
    40|         0|            0|            0|  0.00%|                             "{!r}".format(fileobj)) from None
    41|       108|  0.000502825|  4.65579e-06|  0.00%|    if fd < 0:
    42|         0|            0|            0|  0.00%|        raise ValueError("Invalid file descriptor: {}".format(fd))
    43|       108|  0.000466585|  4.32023e-06|  0.00%|    return fd
    44|         0|            0|            0|  0.00%|
    45|         0|            0|            0|  0.00%|
    46|         0|            0|            0|  0.00%|SelectorKey = namedtuple('SelectorKey', ['fileobj', 'fd', 'events', 'data'])
    47|         0|            0|            0|  0.00%|
    48|         0|            0|            0|  0.00%|SelectorKey.__doc__ = """SelectorKey(fileobj, fd, events, data)
    49|         0|            0|            0|  0.00%|
    50|         0|            0|            0|  0.00%|    Object used to associate a file object to its backing
    51|         0|            0|            0|  0.00%|    file descriptor, selected event mask, and attached data.
    52|         0|            0|            0|  0.00%|"""
    53|         0|            0|            0|  0.00%|if sys.version_info >= (3, 5):
    54|         0|            0|            0|  0.00%|    SelectorKey.fileobj.__doc__ = 'File object registered.'
    55|         0|            0|            0|  0.00%|    SelectorKey.fd.__doc__ = 'Underlying file descriptor.'
    56|         0|            0|            0|  0.00%|    SelectorKey.events.__doc__ = 'Events that must be waited for on this file object.'
    57|         0|            0|            0|  0.00%|    SelectorKey.data.__doc__ = ('''Optional opaque data associated to this file object.
    58|         0|            0|            0|  0.00%|    For example, this could be used to store a per-client session ID.''')
    59|         0|            0|            0|  0.00%|
    60|         0|            0|            0|  0.00%|class _SelectorMapping(Mapping):
    61|         0|            0|            0|  0.00%|    """Mapping of file objects to selector keys."""
    62|         0|            0|            0|  0.00%|
    63|       108|  0.000545263|  5.04873e-06|  0.00%|    def __init__(self, selector):
    64|       108|   0.00060463|  5.59842e-06|  0.00%|        self._selector = selector
    65|         0|            0|            0|  0.00%|
    66|         0|            0|            0|  0.00%|    def __len__(self):
    67|         0|            0|            0|  0.00%|        return len(self._selector._fd_to_key)
    68|         0|            0|            0|  0.00%|
    69|         0|            0|            0|  0.00%|    def __getitem__(self, fileobj):
    70|         0|            0|            0|  0.00%|        try:
    71|         0|            0|            0|  0.00%|            fd = self._selector._fileobj_lookup(fileobj)
    72|         0|            0|            0|  0.00%|            return self._selector._fd_to_key[fd]
    73|         0|            0|            0|  0.00%|        except KeyError:
    74|         0|            0|            0|  0.00%|            raise KeyError("{!r} is not registered".format(fileobj)) from None
    75|         0|            0|            0|  0.00%|
    76|         0|            0|            0|  0.00%|    def __iter__(self):
    77|         0|            0|            0|  0.00%|        return iter(self._selector._fd_to_key)
    78|         0|            0|            0|  0.00%|
    79|         0|            0|            0|  0.00%|
    80|         0|            0|            0|  0.00%|class BaseSelector(metaclass=ABCMeta):
    81|         0|            0|            0|  0.00%|    """Selector abstract base class.
    82|         0|            0|            0|  0.00%|
    83|         0|            0|            0|  0.00%|    A selector supports registering file objects to be monitored for specific
    84|         0|            0|            0|  0.00%|    I/O events.
    85|         0|            0|            0|  0.00%|
    86|         0|            0|            0|  0.00%|    A file object is a file descriptor or any object with a `fileno()` method.
    87|         0|            0|            0|  0.00%|    An arbitrary object can be attached to the file object, which can be used
    88|         0|            0|            0|  0.00%|    for example to store context information, a callback, etc.
    89|         0|            0|            0|  0.00%|
    90|         0|            0|            0|  0.00%|    A selector can use various implementations (select(), poll(), epoll()...)
    91|         0|            0|            0|  0.00%|    depending on the platform. The default `Selector` class uses the most
    92|         0|            0|            0|  0.00%|    efficient implementation on the current platform.
    93|         0|            0|            0|  0.00%|    """
    94|         0|            0|            0|  0.00%|
    95|         0|            0|            0|  0.00%|    @abstractmethod
    96|         0|            0|            0|  0.00%|    def register(self, fileobj, events, data=None):
    97|         0|            0|            0|  0.00%|        """Register a file object.
    98|         0|            0|            0|  0.00%|
    99|         0|            0|            0|  0.00%|        Parameters:
   100|         0|            0|            0|  0.00%|        fileobj -- file object or file descriptor
   101|         0|            0|            0|  0.00%|        events  -- events to monitor (bitwise mask of EVENT_READ|EVENT_WRITE)
   102|         0|            0|            0|  0.00%|        data    -- attached data
   103|         0|            0|            0|  0.00%|
   104|         0|            0|            0|  0.00%|        Returns:
   105|         0|            0|            0|  0.00%|        SelectorKey instance
   106|         0|            0|            0|  0.00%|
   107|         0|            0|            0|  0.00%|        Raises:
   108|         0|            0|            0|  0.00%|        ValueError if events is invalid
   109|         0|            0|            0|  0.00%|        KeyError if fileobj is already registered
   110|         0|            0|            0|  0.00%|        OSError if fileobj is closed or otherwise is unacceptable to
   111|         0|            0|            0|  0.00%|                the underlying system call (if a system call is made)
   112|         0|            0|            0|  0.00%|
   113|         0|            0|            0|  0.00%|        Note:
   114|         0|            0|            0|  0.00%|        OSError may or may not be raised
   115|         0|            0|            0|  0.00%|        """
   116|         0|            0|            0|  0.00%|        raise NotImplementedError
   117|         0|            0|            0|  0.00%|
   118|         0|            0|            0|  0.00%|    @abstractmethod
   119|         0|            0|            0|  0.00%|    def unregister(self, fileobj):
   120|         0|            0|            0|  0.00%|        """Unregister a file object.
   121|         0|            0|            0|  0.00%|
   122|         0|            0|            0|  0.00%|        Parameters:
   123|         0|            0|            0|  0.00%|        fileobj -- file object or file descriptor
   124|         0|            0|            0|  0.00%|
   125|         0|            0|            0|  0.00%|        Returns:
   126|         0|            0|            0|  0.00%|        SelectorKey instance
   127|         0|            0|            0|  0.00%|
   128|         0|            0|            0|  0.00%|        Raises:
   129|         0|            0|            0|  0.00%|        KeyError if fileobj is not registered
   130|         0|            0|            0|  0.00%|
   131|         0|            0|            0|  0.00%|        Note:
   132|         0|            0|            0|  0.00%|        If fileobj is registered but has since been closed this does
   133|         0|            0|            0|  0.00%|        *not* raise OSError (even if the wrapped syscall does)
   134|         0|            0|            0|  0.00%|        """
   135|         0|            0|            0|  0.00%|        raise NotImplementedError
   136|         0|            0|            0|  0.00%|
   137|         0|            0|            0|  0.00%|    def modify(self, fileobj, events, data=None):
   138|         0|            0|            0|  0.00%|        """Change a registered file object monitored events or attached data.
   139|         0|            0|            0|  0.00%|
   140|         0|            0|            0|  0.00%|        Parameters:
   141|         0|            0|            0|  0.00%|        fileobj -- file object or file descriptor
   142|         0|            0|            0|  0.00%|        events  -- events to monitor (bitwise mask of EVENT_READ|EVENT_WRITE)
   143|         0|            0|            0|  0.00%|        data    -- attached data
   144|         0|            0|            0|  0.00%|
   145|         0|            0|            0|  0.00%|        Returns:
   146|         0|            0|            0|  0.00%|        SelectorKey instance
   147|         0|            0|            0|  0.00%|
   148|         0|            0|            0|  0.00%|        Raises:
   149|         0|            0|            0|  0.00%|        Anything that unregister() or register() raises
   150|         0|            0|            0|  0.00%|        """
   151|         0|            0|            0|  0.00%|        self.unregister(fileobj)
   152|         0|            0|            0|  0.00%|        return self.register(fileobj, events, data)
   153|         0|            0|            0|  0.00%|
   154|         0|            0|            0|  0.00%|    @abstractmethod
   155|         0|            0|            0|  0.00%|    def select(self, timeout=None):
   156|         0|            0|            0|  0.00%|        """Perform the actual selection, until some monitored file objects are
   157|         0|            0|            0|  0.00%|        ready or a timeout expires.
   158|         0|            0|            0|  0.00%|
   159|         0|            0|            0|  0.00%|        Parameters:
   160|         0|            0|            0|  0.00%|        timeout -- if timeout > 0, this specifies the maximum wait time, in
   161|         0|            0|            0|  0.00%|                   seconds
   162|         0|            0|            0|  0.00%|                   if timeout <= 0, the select() call won't block, and will
   163|         0|            0|            0|  0.00%|                   report the currently ready file objects
   164|         0|            0|            0|  0.00%|                   if timeout is None, select() will block until a monitored
   165|         0|            0|            0|  0.00%|                   file object becomes ready
   166|         0|            0|            0|  0.00%|
   167|         0|            0|            0|  0.00%|        Returns:
   168|         0|            0|            0|  0.00%|        list of (key, events) for ready file objects
   169|         0|            0|            0|  0.00%|        `events` is a bitwise mask of EVENT_READ|EVENT_WRITE
   170|         0|            0|            0|  0.00%|        """
   171|         0|            0|            0|  0.00%|        raise NotImplementedError
   172|         0|            0|            0|  0.00%|
   173|         0|            0|            0|  0.00%|    def close(self):
   174|         0|            0|            0|  0.00%|        """Close the selector.
   175|         0|            0|            0|  0.00%|
   176|         0|            0|            0|  0.00%|        This must be called to make sure that any underlying resource is freed.
   177|         0|            0|            0|  0.00%|        """
   178|         0|            0|            0|  0.00%|        pass
   179|         0|            0|            0|  0.00%|
   180|         0|            0|            0|  0.00%|    def get_key(self, fileobj):
   181|         0|            0|            0|  0.00%|        """Return the key associated to a registered file object.
   182|         0|            0|            0|  0.00%|
   183|         0|            0|            0|  0.00%|        Returns:
   184|         0|            0|            0|  0.00%|        SelectorKey for this file object
   185|         0|            0|            0|  0.00%|        """
   186|         0|            0|            0|  0.00%|        mapping = self.get_map()
   187|         0|            0|            0|  0.00%|        if mapping is None:
   188|         0|            0|            0|  0.00%|            raise RuntimeError('Selector is closed')
   189|         0|            0|            0|  0.00%|        try:
   190|         0|            0|            0|  0.00%|            return mapping[fileobj]
   191|         0|            0|            0|  0.00%|        except KeyError:
   192|         0|            0|            0|  0.00%|            raise KeyError("{!r} is not registered".format(fileobj)) from None
   193|         0|            0|            0|  0.00%|
   194|         0|            0|            0|  0.00%|    @abstractmethod
   195|         0|            0|            0|  0.00%|    def get_map(self):
   196|         0|            0|            0|  0.00%|        """Return a mapping of file objects to selector keys."""
   197|         0|            0|            0|  0.00%|        raise NotImplementedError
   198|         0|            0|            0|  0.00%|
   199|       108|  0.000504971|  4.67565e-06|  0.00%|    def __enter__(self):
   200|       108|  0.000473022|  4.37984e-06|  0.00%|        return self
   201|         0|            0|            0|  0.00%|
   202|       108|  0.000538588|  4.98692e-06|  0.00%|    def __exit__(self, *args):
   203|       108|   0.00145721|  1.34927e-05|  0.00%|        self.close()
(call)|       108|   0.00190163|  1.76077e-05|  0.00%|# /opt/conda/lib/python3.8/selectors.py:268 close
   204|         0|            0|            0|  0.00%|
   205|         0|            0|            0|  0.00%|
   206|         0|            0|            0|  0.00%|class _BaseSelectorImpl(BaseSelector):
   207|         0|            0|            0|  0.00%|    """Base selector implementation."""
   208|         0|            0|            0|  0.00%|
   209|       108|  0.000582457|  5.39312e-06|  0.00%|    def __init__(self):
   210|         0|            0|            0|  0.00%|        # this maps file descriptors to keys
   211|       108|  0.000616789|  5.71101e-06|  0.00%|        self._fd_to_key = {}
   212|         0|            0|            0|  0.00%|        # read-only mapping returned by get_map()
   213|       108|   0.00180197|  1.66849e-05|  0.00%|        self._map = _SelectorMapping(self)
(call)|       108|   0.00114989|  1.06472e-05|  0.00%|# /opt/conda/lib/python3.8/selectors.py:63 __init__
   214|         0|            0|            0|  0.00%|
   215|       108|  0.000591755|  5.47921e-06|  0.00%|    def _fileobj_lookup(self, fileobj):
   216|         0|            0|            0|  0.00%|        """Return a file descriptor from a file object.
   217|         0|            0|            0|  0.00%|
   218|         0|            0|            0|  0.00%|        This wraps _fileobj_to_fd() to do an exhaustive search in case
   219|         0|            0|            0|  0.00%|        the object is invalid but we still have it in our map.  This
   220|         0|            0|            0|  0.00%|        is used by unregister() so we can unregister an object that
   221|         0|            0|            0|  0.00%|        was previously registered even if it is closed.  It is also
   222|         0|            0|            0|  0.00%|        used by _SelectorMapping.
   223|         0|            0|            0|  0.00%|        """
   224|       108|  0.000500202|   4.6315e-06|  0.00%|        try:
   225|       108|   0.00149965|  1.38857e-05|  0.00%|            return _fileobj_to_fd(fileobj)
(call)|       108|   0.00763249|  7.06712e-05|  0.01%|# /opt/conda/lib/python3.8/selectors.py:21 _fileobj_to_fd
   226|         0|            0|            0|  0.00%|        except ValueError:
   227|         0|            0|            0|  0.00%|            # Do an exhaustive search.
   228|         0|            0|            0|  0.00%|            for key in self._fd_to_key.values():
   229|         0|            0|            0|  0.00%|                if key.fileobj is fileobj:
   230|         0|            0|            0|  0.00%|                    return key.fd
   231|         0|            0|            0|  0.00%|            # Raise ValueError after all.
   232|         0|            0|            0|  0.00%|            raise
   233|         0|            0|            0|  0.00%|
   234|       108|  0.000674009|  6.24083e-06|  0.00%|    def register(self, fileobj, events, data=None):
   235|       108|  0.000841379|  7.79055e-06|  0.00%|        if (not events) or (events & ~(EVENT_READ | EVENT_WRITE)):
   236|         0|            0|            0|  0.00%|            raise ValueError("Invalid events: {!r}".format(events))
   237|         0|            0|            0|  0.00%|
   238|       108|     0.002882|  2.66852e-05|  0.01%|        key = SelectorKey(fileobj, self._fileobj_lookup(fileobj), events, data)
(call)|       108|    0.0102241|  9.46676e-05|  0.02%|# /opt/conda/lib/python3.8/selectors.py:215 _fileobj_lookup
(call)|       108|   0.00169754|   1.5718e-05|  0.00%|# <string>:1 __new__
   239|         0|            0|            0|  0.00%|
   240|       108|  0.000601768|  5.57193e-06|  0.00%|        if key.fd in self._fd_to_key:
   241|         0|            0|            0|  0.00%|            raise KeyError("{!r} (FD {}) is already registered"
   242|         0|            0|            0|  0.00%|                           .format(fileobj, key.fd))
   243|         0|            0|            0|  0.00%|
   244|       108|   0.00050807|  4.70435e-06|  0.00%|        self._fd_to_key[key.fd] = key
   245|       108|  0.000460386|  4.26284e-06|  0.00%|        return key
   246|         0|            0|            0|  0.00%|
   247|         0|            0|            0|  0.00%|    def unregister(self, fileobj):
   248|         0|            0|            0|  0.00%|        try:
   249|         0|            0|            0|  0.00%|            key = self._fd_to_key.pop(self._fileobj_lookup(fileobj))
   250|         0|            0|            0|  0.00%|        except KeyError:
   251|         0|            0|            0|  0.00%|            raise KeyError("{!r} is not registered".format(fileobj)) from None
   252|         0|            0|            0|  0.00%|        return key
   253|         0|            0|            0|  0.00%|
   254|         0|            0|            0|  0.00%|    def modify(self, fileobj, events, data=None):
   255|         0|            0|            0|  0.00%|        try:
   256|         0|            0|            0|  0.00%|            key = self._fd_to_key[self._fileobj_lookup(fileobj)]
   257|         0|            0|            0|  0.00%|        except KeyError:
   258|         0|            0|            0|  0.00%|            raise KeyError("{!r} is not registered".format(fileobj)) from None
   259|         0|            0|            0|  0.00%|        if events != key.events:
   260|         0|            0|            0|  0.00%|            self.unregister(fileobj)
   261|         0|            0|            0|  0.00%|            key = self.register(fileobj, events, data)
   262|         0|            0|            0|  0.00%|        elif data != key.data:
   263|         0|            0|            0|  0.00%|            # Use a shortcut to update the data.
   264|         0|            0|            0|  0.00%|            key = key._replace(data=data)
   265|         0|            0|            0|  0.00%|            self._fd_to_key[key.fd] = key
   266|         0|            0|            0|  0.00%|        return key
   267|         0|            0|            0|  0.00%|
   268|       108|  0.000471354|  4.36438e-06|  0.00%|    def close(self):
   269|       108|    0.0007689|  7.11944e-06|  0.00%|        self._fd_to_key.clear()
   270|       108|  0.000661373|  6.12383e-06|  0.00%|        self._map = None
   271|         0|            0|            0|  0.00%|
   272|         0|            0|            0|  0.00%|    def get_map(self):
   273|         0|            0|            0|  0.00%|        return self._map
   274|         0|            0|            0|  0.00%|
   275|       108|  0.000579357|  5.36442e-06|  0.00%|    def _key_from_fd(self, fd):
   276|         0|            0|            0|  0.00%|        """Return the key associated to a given file descriptor.
   277|         0|            0|            0|  0.00%|
   278|         0|            0|            0|  0.00%|        Parameters:
   279|         0|            0|            0|  0.00%|        fd -- file descriptor
   280|         0|            0|            0|  0.00%|
   281|         0|            0|            0|  0.00%|        Returns:
   282|         0|            0|            0|  0.00%|        corresponding key, or None if not found
   283|         0|            0|            0|  0.00%|        """
   284|       108|  0.000459433|  4.25401e-06|  0.00%|        try:
   285|       108|  0.000468969|  4.34231e-06|  0.00%|            return self._fd_to_key[fd]
   286|         0|            0|            0|  0.00%|        except KeyError:
   287|         0|            0|            0|  0.00%|            return None
   288|         0|            0|            0|  0.00%|
   289|         0|            0|            0|  0.00%|
   290|         0|            0|            0|  0.00%|class SelectSelector(_BaseSelectorImpl):
   291|         0|            0|            0|  0.00%|    """Select-based selector."""
   292|         0|            0|            0|  0.00%|
   293|         0|            0|            0|  0.00%|    def __init__(self):
   294|         0|            0|            0|  0.00%|        super().__init__()
   295|         0|            0|            0|  0.00%|        self._readers = set()
   296|         0|            0|            0|  0.00%|        self._writers = set()
   297|         0|            0|            0|  0.00%|
   298|         0|            0|            0|  0.00%|    def register(self, fileobj, events, data=None):
   299|         0|            0|            0|  0.00%|        key = super().register(fileobj, events, data)
   300|         0|            0|            0|  0.00%|        if events & EVENT_READ:
   301|         0|            0|            0|  0.00%|            self._readers.add(key.fd)
   302|         0|            0|            0|  0.00%|        if events & EVENT_WRITE:
   303|         0|            0|            0|  0.00%|            self._writers.add(key.fd)
   304|         0|            0|            0|  0.00%|        return key
   305|         0|            0|            0|  0.00%|
   306|         0|            0|            0|  0.00%|    def unregister(self, fileobj):
   307|         0|            0|            0|  0.00%|        key = super().unregister(fileobj)
   308|         0|            0|            0|  0.00%|        self._readers.discard(key.fd)
   309|         0|            0|            0|  0.00%|        self._writers.discard(key.fd)
   310|         0|            0|            0|  0.00%|        return key
   311|         0|            0|            0|  0.00%|
   312|         0|            0|            0|  0.00%|    if sys.platform == 'win32':
   313|         0|            0|            0|  0.00%|        def _select(self, r, w, _, timeout=None):
   314|         0|            0|            0|  0.00%|            r, w, x = select.select(r, w, w, timeout)
   315|         0|            0|            0|  0.00%|            return r, w + x, []
   316|         0|            0|            0|  0.00%|    else:
   317|         0|            0|            0|  0.00%|        _select = select.select
   318|         0|            0|            0|  0.00%|
   319|         0|            0|            0|  0.00%|    def select(self, timeout=None):
   320|         0|            0|            0|  0.00%|        timeout = None if timeout is None else max(timeout, 0)
   321|         0|            0|            0|  0.00%|        ready = []
   322|         0|            0|            0|  0.00%|        try:
   323|         0|            0|            0|  0.00%|            r, w, _ = self._select(self._readers, self._writers, [], timeout)
   324|         0|            0|            0|  0.00%|        except InterruptedError:
   325|         0|            0|            0|  0.00%|            return ready
   326|         0|            0|            0|  0.00%|        r = set(r)
   327|         0|            0|            0|  0.00%|        w = set(w)
   328|         0|            0|            0|  0.00%|        for fd in r | w:
   329|         0|            0|            0|  0.00%|            events = 0
   330|         0|            0|            0|  0.00%|            if fd in r:
   331|         0|            0|            0|  0.00%|                events |= EVENT_READ
   332|         0|            0|            0|  0.00%|            if fd in w:
   333|         0|            0|            0|  0.00%|                events |= EVENT_WRITE
   334|         0|            0|            0|  0.00%|
   335|         0|            0|            0|  0.00%|            key = self._key_from_fd(fd)
   336|         0|            0|            0|  0.00%|            if key:
   337|         0|            0|            0|  0.00%|                ready.append((key, events & key.events))
   338|         0|            0|            0|  0.00%|        return ready
   339|         0|            0|            0|  0.00%|
   340|         0|            0|            0|  0.00%|
   341|         0|            0|            0|  0.00%|class _PollLikeSelector(_BaseSelectorImpl):
   342|         0|            0|            0|  0.00%|    """Base class shared between poll, epoll and devpoll selectors."""
   343|         0|            0|            0|  0.00%|    _selector_cls = None
   344|         0|            0|            0|  0.00%|    _EVENT_READ = None
   345|         0|            0|            0|  0.00%|    _EVENT_WRITE = None
   346|         0|            0|            0|  0.00%|
   347|       108|  0.000748396|  6.92959e-06|  0.00%|    def __init__(self):
   348|       108|    0.0020504|  1.89852e-05|  0.00%|        super().__init__()
(call)|       108|   0.00415111|  3.84362e-05|  0.01%|# /opt/conda/lib/python3.8/selectors.py:209 __init__
   349|       108|  0.000910044|  8.42633e-06|  0.00%|        self._selector = self._selector_cls()
   350|         0|            0|            0|  0.00%|
   351|       108|  0.000709772|  6.57196e-06|  0.00%|    def register(self, fileobj, events, data=None):
   352|       108|   0.00182796|  1.69255e-05|  0.00%|        key = super().register(fileobj, events, data)
(call)|       108|    0.0178893|  0.000165641|  0.03%|# /opt/conda/lib/python3.8/selectors.py:234 register
   353|       108|  0.000532866|  4.93394e-06|  0.00%|        poller_events = 0
   354|       108|   0.00053215|  4.92732e-06|  0.00%|        if events & EVENT_READ:
   355|       108|  0.000584126|  5.40857e-06|  0.00%|            poller_events |= self._EVENT_READ
   356|       108|  0.000496149|  4.59397e-06|  0.00%|        if events & EVENT_WRITE:
   357|         0|            0|            0|  0.00%|            poller_events |= self._EVENT_WRITE
   358|       108|  0.000451565|  4.18116e-06|  0.00%|        try:
   359|       108|  0.000950336|  8.79941e-06|  0.00%|            self._selector.register(key.fd, poller_events)
   360|         0|            0|            0|  0.00%|        except:
   361|         0|            0|            0|  0.00%|            super().unregister(fileobj)
   362|         0|            0|            0|  0.00%|            raise
   363|       108|  0.000498295|  4.61384e-06|  0.00%|        return key
   364|         0|            0|            0|  0.00%|
   365|         0|            0|            0|  0.00%|    def unregister(self, fileobj):
   366|         0|            0|            0|  0.00%|        key = super().unregister(fileobj)
   367|         0|            0|            0|  0.00%|        try:
   368|         0|            0|            0|  0.00%|            self._selector.unregister(key.fd)
   369|         0|            0|            0|  0.00%|        except OSError:
   370|         0|            0|            0|  0.00%|            # This can happen if the FD was closed since it
   371|         0|            0|            0|  0.00%|            # was registered.
   372|         0|            0|            0|  0.00%|            pass
   373|         0|            0|            0|  0.00%|        return key
   374|         0|            0|            0|  0.00%|
   375|         0|            0|            0|  0.00%|    def modify(self, fileobj, events, data=None):
   376|         0|            0|            0|  0.00%|        try:
   377|         0|            0|            0|  0.00%|            key = self._fd_to_key[self._fileobj_lookup(fileobj)]
   378|         0|            0|            0|  0.00%|        except KeyError:
   379|         0|            0|            0|  0.00%|            raise KeyError(f"{fileobj!r} is not registered") from None
   380|         0|            0|            0|  0.00%|
   381|         0|            0|            0|  0.00%|        changed = False
   382|         0|            0|            0|  0.00%|        if events != key.events:
   383|         0|            0|            0|  0.00%|            selector_events = 0
   384|         0|            0|            0|  0.00%|            if events & EVENT_READ:
   385|         0|            0|            0|  0.00%|                selector_events |= self._EVENT_READ
   386|         0|            0|            0|  0.00%|            if events & EVENT_WRITE:
   387|         0|            0|            0|  0.00%|                selector_events |= self._EVENT_WRITE
   388|         0|            0|            0|  0.00%|            try:
   389|         0|            0|            0|  0.00%|                self._selector.modify(key.fd, selector_events)
   390|         0|            0|            0|  0.00%|            except:
   391|         0|            0|            0|  0.00%|                super().unregister(fileobj)
   392|         0|            0|            0|  0.00%|                raise
   393|         0|            0|            0|  0.00%|            changed = True
   394|         0|            0|            0|  0.00%|        if data != key.data:
   395|         0|            0|            0|  0.00%|            changed = True
   396|         0|            0|            0|  0.00%|
   397|         0|            0|            0|  0.00%|        if changed:
   398|         0|            0|            0|  0.00%|            key = key._replace(events=events, data=data)
   399|         0|            0|            0|  0.00%|            self._fd_to_key[key.fd] = key
   400|         0|            0|            0|  0.00%|        return key
   401|         0|            0|            0|  0.00%|
   402|       108|  0.000766039|  7.09295e-06|  0.00%|    def select(self, timeout=None):
   403|         0|            0|            0|  0.00%|        # This is shared between poll() and epoll().
   404|         0|            0|            0|  0.00%|        # epoll() has a different signature and handling of timeout parameter.
   405|       108|  0.000631571|  5.84788e-06|  0.00%|        if timeout is None:
   406|         0|            0|            0|  0.00%|            timeout = None
   407|       108|  0.000601053|  5.56531e-06|  0.00%|        elif timeout <= 0:
   408|         0|            0|            0|  0.00%|            timeout = 0
   409|         0|            0|            0|  0.00%|        else:
   410|         0|            0|            0|  0.00%|            # poll() has a resolution of 1 millisecond, round away from
   411|         0|            0|            0|  0.00%|            # zero to wait *at least* timeout seconds.
   412|       108|   0.00124502|   1.1528e-05|  0.00%|            timeout = math.ceil(timeout * 1e3)
   413|       108|   0.00058794|  5.44389e-06|  0.00%|        ready = []
   414|       108|  0.000542402|  5.02224e-06|  0.00%|        try:
   415|       108|     0.418977|   0.00387941|  0.78%|            fd_event_list = self._selector.poll(timeout)
(call)|         4|  0.000327587|  8.18968e-05|  0.00%|# /opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/signal_handling.py:63 handler
   416|         0|            0|            0|  0.00%|        except InterruptedError:
   417|         0|            0|            0|  0.00%|            return ready
   418|       216|   0.00121427|   5.6216e-06|  0.00%|        for fd, event in fd_event_list:
   419|       108|  0.000511169|  4.73305e-06|  0.00%|            events = 0
   420|       108|  0.000598669|  5.54323e-06|  0.00%|            if event & ~self._EVENT_READ:
   421|         8|  4.00543e-05|  5.00679e-06|  0.00%|                events |= EVENT_WRITE
   422|       108|  0.000580549|  5.37546e-06|  0.00%|            if event & ~self._EVENT_WRITE:
   423|       108|  0.000522614|  4.83901e-06|  0.00%|                events |= EVENT_READ
   424|         0|            0|            0|  0.00%|
   425|       108|   0.00159812|  1.47974e-05|  0.00%|            key = self._key_from_fd(fd)
(call)|       108|   0.00150776|  1.39607e-05|  0.00%|# /opt/conda/lib/python3.8/selectors.py:275 _key_from_fd
   426|       108|  0.000546217|  5.05756e-06|  0.00%|            if key:
   427|       108|  0.000835419|  7.73536e-06|  0.00%|                ready.append((key, events & key.events))
   428|       108|  0.000432014|  4.00013e-06|  0.00%|        return ready
   429|         0|            0|            0|  0.00%|
   430|         0|            0|            0|  0.00%|
   431|         0|            0|            0|  0.00%|if hasattr(select, 'poll'):
   432|         0|            0|            0|  0.00%|
   433|         0|            0|            0|  0.00%|    class PollSelector(_PollLikeSelector):
   434|         0|            0|            0|  0.00%|        """Poll-based selector."""
   435|         0|            0|            0|  0.00%|        _selector_cls = select.poll
   436|         0|            0|            0|  0.00%|        _EVENT_READ = select.POLLIN
   437|         0|            0|            0|  0.00%|        _EVENT_WRITE = select.POLLOUT
   438|         0|            0|            0|  0.00%|
   439|         0|            0|            0|  0.00%|
   440|         0|            0|            0|  0.00%|if hasattr(select, 'epoll'):
   441|         0|            0|            0|  0.00%|
   442|         0|            0|            0|  0.00%|    class EpollSelector(_PollLikeSelector):
   443|         0|            0|            0|  0.00%|        """Epoll-based selector."""
   444|         0|            0|            0|  0.00%|        _selector_cls = select.epoll
   445|         0|            0|            0|  0.00%|        _EVENT_READ = select.EPOLLIN
   446|         0|            0|            0|  0.00%|        _EVENT_WRITE = select.EPOLLOUT
   447|         0|            0|            0|  0.00%|
   448|         0|            0|            0|  0.00%|        def fileno(self):
   449|         0|            0|            0|  0.00%|            return self._selector.fileno()
   450|         0|            0|            0|  0.00%|
   451|         0|            0|            0|  0.00%|        def select(self, timeout=None):
   452|         0|            0|            0|  0.00%|            if timeout is None:
   453|         0|            0|            0|  0.00%|                timeout = -1
   454|         0|            0|            0|  0.00%|            elif timeout <= 0:
   455|         0|            0|            0|  0.00%|                timeout = 0
   456|         0|            0|            0|  0.00%|            else:
   457|         0|            0|            0|  0.00%|                # epoll_wait() has a resolution of 1 millisecond, round away
   458|         0|            0|            0|  0.00%|                # from zero to wait *at least* timeout seconds.
   459|         0|            0|            0|  0.00%|                timeout = math.ceil(timeout * 1e3) * 1e-3
   460|         0|            0|            0|  0.00%|
   461|         0|            0|            0|  0.00%|            # epoll_wait() expects `maxevents` to be greater than zero;
   462|         0|            0|            0|  0.00%|            # we want to make sure that `select()` can be called when no
   463|         0|            0|            0|  0.00%|            # FD is registered.
   464|         0|            0|            0|  0.00%|            max_ev = max(len(self._fd_to_key), 1)
   465|         0|            0|            0|  0.00%|
   466|         0|            0|            0|  0.00%|            ready = []
   467|         0|            0|            0|  0.00%|            try:
   468|         0|            0|            0|  0.00%|                fd_event_list = self._selector.poll(timeout, max_ev)
   469|         0|            0|            0|  0.00%|            except InterruptedError:
   470|         0|            0|            0|  0.00%|                return ready
   471|         0|            0|            0|  0.00%|            for fd, event in fd_event_list:
   472|         0|            0|            0|  0.00%|                events = 0
   473|         0|            0|            0|  0.00%|                if event & ~select.EPOLLIN:
   474|         0|            0|            0|  0.00%|                    events |= EVENT_WRITE
   475|         0|            0|            0|  0.00%|                if event & ~select.EPOLLOUT:
   476|         0|            0|            0|  0.00%|                    events |= EVENT_READ
   477|         0|            0|            0|  0.00%|
   478|         0|            0|            0|  0.00%|                key = self._key_from_fd(fd)
   479|         0|            0|            0|  0.00%|                if key:
   480|         0|            0|            0|  0.00%|                    ready.append((key, events & key.events))
   481|         0|            0|            0|  0.00%|            return ready
   482|         0|            0|            0|  0.00%|
   483|         0|            0|            0|  0.00%|        def close(self):
   484|         0|            0|            0|  0.00%|            self._selector.close()
   485|         0|            0|            0|  0.00%|            super().close()
   486|         0|            0|            0|  0.00%|
   487|         0|            0|            0|  0.00%|
   488|         0|            0|            0|  0.00%|if hasattr(select, 'devpoll'):
   489|         0|            0|            0|  0.00%|
   490|         0|            0|            0|  0.00%|    class DevpollSelector(_PollLikeSelector):
   491|         0|            0|            0|  0.00%|        """Solaris /dev/poll selector."""
   492|         0|            0|            0|  0.00%|        _selector_cls = select.devpoll
   493|         0|            0|            0|  0.00%|        _EVENT_READ = select.POLLIN
   494|         0|            0|            0|  0.00%|        _EVENT_WRITE = select.POLLOUT
   495|         0|            0|            0|  0.00%|
   496|         0|            0|            0|  0.00%|        def fileno(self):
   497|         0|            0|            0|  0.00%|            return self._selector.fileno()
   498|         0|            0|            0|  0.00%|
   499|         0|            0|            0|  0.00%|        def close(self):
   500|         0|            0|            0|  0.00%|            self._selector.close()
   501|         0|            0|            0|  0.00%|            super().close()
   502|         0|            0|            0|  0.00%|
   503|         0|            0|            0|  0.00%|
   504|         0|            0|            0|  0.00%|if hasattr(select, 'kqueue'):
   505|         0|            0|            0|  0.00%|
   506|         0|            0|            0|  0.00%|    class KqueueSelector(_BaseSelectorImpl):
   507|         0|            0|            0|  0.00%|        """Kqueue-based selector."""
   508|         0|            0|            0|  0.00%|
   509|         0|            0|            0|  0.00%|        def __init__(self):
   510|         0|            0|            0|  0.00%|            super().__init__()
   511|         0|            0|            0|  0.00%|            self._selector = select.kqueue()
   512|         0|            0|            0|  0.00%|
   513|         0|            0|            0|  0.00%|        def fileno(self):
   514|         0|            0|            0|  0.00%|            return self._selector.fileno()
   515|         0|            0|            0|  0.00%|
   516|         0|            0|            0|  0.00%|        def register(self, fileobj, events, data=None):
   517|         0|            0|            0|  0.00%|            key = super().register(fileobj, events, data)
   518|         0|            0|            0|  0.00%|            try:
   519|         0|            0|            0|  0.00%|                if events & EVENT_READ:
   520|         0|            0|            0|  0.00%|                    kev = select.kevent(key.fd, select.KQ_FILTER_READ,
   521|         0|            0|            0|  0.00%|                                        select.KQ_EV_ADD)
   522|         0|            0|            0|  0.00%|                    self._selector.control([kev], 0, 0)
   523|         0|            0|            0|  0.00%|                if events & EVENT_WRITE:
   524|         0|            0|            0|  0.00%|                    kev = select.kevent(key.fd, select.KQ_FILTER_WRITE,
   525|         0|            0|            0|  0.00%|                                        select.KQ_EV_ADD)
   526|         0|            0|            0|  0.00%|                    self._selector.control([kev], 0, 0)
   527|         0|            0|            0|  0.00%|            except:
   528|         0|            0|            0|  0.00%|                super().unregister(fileobj)
   529|         0|            0|            0|  0.00%|                raise
   530|         0|            0|            0|  0.00%|            return key
   531|         0|            0|            0|  0.00%|
   532|         0|            0|            0|  0.00%|        def unregister(self, fileobj):
   533|         0|            0|            0|  0.00%|            key = super().unregister(fileobj)
   534|         0|            0|            0|  0.00%|            if key.events & EVENT_READ:
   535|         0|            0|            0|  0.00%|                kev = select.kevent(key.fd, select.KQ_FILTER_READ,
   536|         0|            0|            0|  0.00%|                                    select.KQ_EV_DELETE)
   537|         0|            0|            0|  0.00%|                try:
   538|         0|            0|            0|  0.00%|                    self._selector.control([kev], 0, 0)
   539|         0|            0|            0|  0.00%|                except OSError:
   540|         0|            0|            0|  0.00%|                    # This can happen if the FD was closed since it
   541|         0|            0|            0|  0.00%|                    # was registered.
   542|         0|            0|            0|  0.00%|                    pass
   543|         0|            0|            0|  0.00%|            if key.events & EVENT_WRITE:
   544|         0|            0|            0|  0.00%|                kev = select.kevent(key.fd, select.KQ_FILTER_WRITE,
   545|         0|            0|            0|  0.00%|                                    select.KQ_EV_DELETE)
   546|         0|            0|            0|  0.00%|                try:
   547|         0|            0|            0|  0.00%|                    self._selector.control([kev], 0, 0)
   548|         0|            0|            0|  0.00%|                except OSError:
   549|         0|            0|            0|  0.00%|                    # See comment above.
   550|         0|            0|            0|  0.00%|                    pass
   551|         0|            0|            0|  0.00%|            return key
   552|         0|            0|            0|  0.00%|
   553|         0|            0|            0|  0.00%|        def select(self, timeout=None):
   554|         0|            0|            0|  0.00%|            timeout = None if timeout is None else max(timeout, 0)
   555|         0|            0|            0|  0.00%|            max_ev = len(self._fd_to_key)
   556|         0|            0|            0|  0.00%|            ready = []
   557|         0|            0|            0|  0.00%|            try:
   558|         0|            0|            0|  0.00%|                kev_list = self._selector.control(None, max_ev, timeout)
   559|         0|            0|            0|  0.00%|            except InterruptedError:
   560|         0|            0|            0|  0.00%|                return ready
   561|         0|            0|            0|  0.00%|            for kev in kev_list:
   562|         0|            0|            0|  0.00%|                fd = kev.ident
   563|         0|            0|            0|  0.00%|                flag = kev.filter
   564|         0|            0|            0|  0.00%|                events = 0
   565|         0|            0|            0|  0.00%|                if flag == select.KQ_FILTER_READ:
   566|         0|            0|            0|  0.00%|                    events |= EVENT_READ
   567|         0|            0|            0|  0.00%|                if flag == select.KQ_FILTER_WRITE:
   568|         0|            0|            0|  0.00%|                    events |= EVENT_WRITE
   569|         0|            0|            0|  0.00%|
   570|         0|            0|            0|  0.00%|                key = self._key_from_fd(fd)
   571|         0|            0|            0|  0.00%|                if key:
   572|         0|            0|            0|  0.00%|                    ready.append((key, events & key.events))
   573|         0|            0|            0|  0.00%|            return ready
   574|         0|            0|            0|  0.00%|
   575|         0|            0|            0|  0.00%|        def close(self):
   576|         0|            0|            0|  0.00%|            self._selector.close()
   577|         0|            0|            0|  0.00%|            super().close()
   578|         0|            0|            0|  0.00%|
   579|         0|            0|            0|  0.00%|
   580|         0|            0|            0|  0.00%|# Choose the best implementation, roughly:
   581|         0|            0|            0|  0.00%|#    epoll|kqueue|devpoll > poll > select.
   582|         0|            0|            0|  0.00%|# select() also can't accept a FD > FD_SETSIZE (usually around 1024)
   583|         0|            0|            0|  0.00%|if 'KqueueSelector' in globals():
   584|         0|            0|            0|  0.00%|    DefaultSelector = KqueueSelector
   585|         0|            0|            0|  0.00%|elif 'EpollSelector' in globals():
   586|         0|            0|            0|  0.00%|    DefaultSelector = EpollSelector
   587|         0|            0|            0|  0.00%|elif 'DevpollSelector' in globals():
   588|         0|            0|            0|  0.00%|    DefaultSelector = DevpollSelector
   589|         0|            0|            0|  0.00%|elif 'PollSelector' in globals():
   590|         0|            0|            0|  0.00%|    DefaultSelector = PollSelector
   591|         0|            0|            0|  0.00%|else:
   592|         0|            0|            0|  0.00%|    DefaultSelector = SelectSelector
File: /opt/conda/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py
File duration: 0.463052s (0.86%)
Line #|      Hits|         Time| Time per hit|      %|Source code
------+----------+-------------+-------------+-------+-----------
     1|         0|            0|            0|  0.00%|from typing import Optional, Any
     2|         0|            0|            0|  0.00%|
     3|         0|            0|            0|  0.00%|import torch
     4|         0|            0|            0|  0.00%|from torch import Tensor
     5|         0|            0|            0|  0.00%|from torch.nn.parameter import Parameter, UninitializedParameter, UninitializedBuffer
     6|         0|            0|            0|  0.00%|
     7|         0|            0|            0|  0.00%|from .. import functional as F
     8|         0|            0|            0|  0.00%|from .. import init
     9|         0|            0|            0|  0.00%|from ._functions import SyncBatchNorm as sync_batch_norm
    10|         0|            0|            0|  0.00%|from .lazy import LazyModuleMixin
    11|         0|            0|            0|  0.00%|from .module import Module
    12|         0|            0|            0|  0.00%|
    13|         0|            0|            0|  0.00%|
    14|         0|            0|            0|  0.00%|class _NormBase(Module):
    15|         0|            0|            0|  0.00%|    """Common base of _InstanceNorm and _BatchNorm"""
    16|         0|            0|            0|  0.00%|
    17|         0|            0|            0|  0.00%|    _version = 2
    18|         0|            0|            0|  0.00%|    __constants__ = ["track_running_stats", "momentum", "eps", "num_features", "affine"]
    19|         0|            0|            0|  0.00%|    num_features: int
    20|         0|            0|            0|  0.00%|    eps: float
    21|         0|            0|            0|  0.00%|    momentum: float
    22|         0|            0|            0|  0.00%|    affine: bool
    23|         0|            0|            0|  0.00%|    track_running_stats: bool
    24|         0|            0|            0|  0.00%|    # WARNING: weight and bias purposely not defined here.
    25|         0|            0|            0|  0.00%|    # See https://github.com/pytorch/pytorch/issues/39670
    26|         0|            0|            0|  0.00%|
    27|         0|            0|            0|  0.00%|    def __init__(
    28|         0|            0|            0|  0.00%|        self,
    29|         0|            0|            0|  0.00%|        num_features: int,
    30|         0|            0|            0|  0.00%|        eps: float = 1e-5,
    31|         0|            0|            0|  0.00%|        momentum: float = 0.1,
    32|         0|            0|            0|  0.00%|        affine: bool = True,
    33|         0|            0|            0|  0.00%|        track_running_stats: bool = True,
    34|         0|            0|            0|  0.00%|        device=None,
    35|         0|            0|            0|  0.00%|        dtype=None
    36|         0|            0|            0|  0.00%|    ) -> None:
    37|         0|            0|            0|  0.00%|        factory_kwargs = {'device': device, 'dtype': dtype}
    38|         0|            0|            0|  0.00%|        super(_NormBase, self).__init__()
    39|         0|            0|            0|  0.00%|        self.num_features = num_features
    40|         0|            0|            0|  0.00%|        self.eps = eps
    41|         0|            0|            0|  0.00%|        self.momentum = momentum
    42|         0|            0|            0|  0.00%|        self.affine = affine
    43|         0|            0|            0|  0.00%|        self.track_running_stats = track_running_stats
    44|         0|            0|            0|  0.00%|        if self.affine:
    45|         0|            0|            0|  0.00%|            self.weight = Parameter(torch.empty(num_features, **factory_kwargs))
    46|         0|            0|            0|  0.00%|            self.bias = Parameter(torch.empty(num_features, **factory_kwargs))
    47|         0|            0|            0|  0.00%|        else:
    48|         0|            0|            0|  0.00%|            self.register_parameter("weight", None)
    49|         0|            0|            0|  0.00%|            self.register_parameter("bias", None)
    50|         0|            0|            0|  0.00%|        if self.track_running_stats:
    51|         0|            0|            0|  0.00%|            self.register_buffer('running_mean', torch.zeros(num_features, **factory_kwargs))
    52|         0|            0|            0|  0.00%|            self.register_buffer('running_var', torch.ones(num_features, **factory_kwargs))
    53|         0|            0|            0|  0.00%|            self.running_mean: Optional[Tensor]
    54|         0|            0|            0|  0.00%|            self.running_var: Optional[Tensor]
    55|         0|            0|            0|  0.00%|            self.register_buffer('num_batches_tracked',
    56|         0|            0|            0|  0.00%|                                 torch.tensor(0, dtype=torch.long,
    57|         0|            0|            0|  0.00%|                                              **{k: v for k, v in factory_kwargs.items() if k != 'dtype'}))
    58|         0|            0|            0|  0.00%|        else:
    59|         0|            0|            0|  0.00%|            self.register_buffer("running_mean", None)
    60|         0|            0|            0|  0.00%|            self.register_buffer("running_var", None)
    61|         0|            0|            0|  0.00%|            self.register_buffer("num_batches_tracked", None)
    62|         0|            0|            0|  0.00%|        self.reset_parameters()
    63|         0|            0|            0|  0.00%|
    64|         0|            0|            0|  0.00%|    def reset_running_stats(self) -> None:
    65|         0|            0|            0|  0.00%|        if self.track_running_stats:
    66|         0|            0|            0|  0.00%|            # running_mean/running_var/num_batches... are registered at runtime depending
    67|         0|            0|            0|  0.00%|            # if self.track_running_stats is on
    68|         0|            0|            0|  0.00%|            self.running_mean.zero_()  # type: ignore[union-attr]
    69|         0|            0|            0|  0.00%|            self.running_var.fill_(1)  # type: ignore[union-attr]
    70|         0|            0|            0|  0.00%|            self.num_batches_tracked.zero_()  # type: ignore[union-attr,operator]
    71|         0|            0|            0|  0.00%|
    72|         0|            0|            0|  0.00%|    def reset_parameters(self) -> None:
    73|         0|            0|            0|  0.00%|        self.reset_running_stats()
    74|         0|            0|            0|  0.00%|        if self.affine:
    75|         0|            0|            0|  0.00%|            init.ones_(self.weight)
    76|         0|            0|            0|  0.00%|            init.zeros_(self.bias)
    77|         0|            0|            0|  0.00%|
    78|         0|            0|            0|  0.00%|    def _check_input_dim(self, input):
    79|         0|            0|            0|  0.00%|        raise NotImplementedError
    80|         0|            0|            0|  0.00%|
    81|         0|            0|            0|  0.00%|    def extra_repr(self):
    82|         0|            0|            0|  0.00%|        return (
    83|         0|            0|            0|  0.00%|            "{num_features}, eps={eps}, momentum={momentum}, affine={affine}, "
    84|         0|            0|            0|  0.00%|            "track_running_stats={track_running_stats}".format(**self.__dict__)
    85|         0|            0|            0|  0.00%|        )
    86|         0|            0|            0|  0.00%|
    87|        20|  0.000123501|  6.17504e-06|  0.00%|    def _load_from_state_dict(
    88|         0|            0|            0|  0.00%|        self,
    89|         0|            0|            0|  0.00%|        state_dict,
    90|         0|            0|            0|  0.00%|        prefix,
    91|         0|            0|            0|  0.00%|        local_metadata,
    92|         0|            0|            0|  0.00%|        strict,
    93|         0|            0|            0|  0.00%|        missing_keys,
    94|         0|            0|            0|  0.00%|        unexpected_keys,
    95|         0|            0|            0|  0.00%|        error_msgs,
    96|         0|            0|            0|  0.00%|    ):
    97|        20|  0.000148058|   7.4029e-06|  0.00%|        version = local_metadata.get("version", None)
    98|         0|            0|            0|  0.00%|
    99|        20|  9.82285e-05|  4.91142e-06|  0.00%|        if (version is None or version < 2) and self.track_running_stats:
   100|         0|            0|            0|  0.00%|            # at version 2: added num_batches_tracked buffer
   101|         0|            0|            0|  0.00%|            #               this should have a default value of 0
   102|         0|            0|            0|  0.00%|            num_batches_tracked_key = prefix + "num_batches_tracked"
   103|         0|            0|            0|  0.00%|            if num_batches_tracked_key not in state_dict:
   104|         0|            0|            0|  0.00%|                state_dict[num_batches_tracked_key] = torch.tensor(0, dtype=torch.long)
   105|         0|            0|            0|  0.00%|
   106|        40|  0.000509262|  1.27316e-05|  0.00%|        super(_NormBase, self)._load_from_state_dict(
(call)|        20|     0.060416|    0.0030208|  0.11%|# /opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1276 _load_from_state_dict
   107|        20|  9.29832e-05|  4.64916e-06|  0.00%|            state_dict,
   108|        20|  8.46386e-05|  4.23193e-06|  0.00%|            prefix,
   109|        20|  8.46386e-05|  4.23193e-06|  0.00%|            local_metadata,
   110|        20|  8.34465e-05|  4.17233e-06|  0.00%|            strict,
   111|        20|  8.34465e-05|  4.17233e-06|  0.00%|            missing_keys,
   112|        20|  8.32081e-05|   4.1604e-06|  0.00%|            unexpected_keys,
   113|        20|  8.36849e-05|  4.18425e-06|  0.00%|            error_msgs,
   114|         0|            0|            0|  0.00%|        )
   115|         0|            0|            0|  0.00%|
   116|         0|            0|            0|  0.00%|
   117|         0|            0|            0|  0.00%|class _BatchNorm(_NormBase):
   118|         0|            0|            0|  0.00%|    def __init__(
   119|         0|            0|            0|  0.00%|        self,
   120|         0|            0|            0|  0.00%|        num_features,
   121|         0|            0|            0|  0.00%|        eps=1e-5,
   122|         0|            0|            0|  0.00%|        momentum=0.1,
   123|         0|            0|            0|  0.00%|        affine=True,
   124|         0|            0|            0|  0.00%|        track_running_stats=True,
   125|         0|            0|            0|  0.00%|        device=None,
   126|         0|            0|            0|  0.00%|        dtype=None
   127|         0|            0|            0|  0.00%|    ):
   128|         0|            0|            0|  0.00%|        factory_kwargs = {'device': device, 'dtype': dtype}
   129|         0|            0|            0|  0.00%|        super(_BatchNorm, self).__init__(
   130|         0|            0|            0|  0.00%|            num_features, eps, momentum, affine, track_running_stats, **factory_kwargs
   131|         0|            0|            0|  0.00%|        )
   132|         0|            0|            0|  0.00%|
   133|      2000|    0.0125089|  6.25443e-06|  0.02%|    def forward(self, input: Tensor) -> Tensor:
   134|      2000|    0.0305207|  1.52603e-05|  0.06%|        self._check_input_dim(input)
(call)|      2000|    0.0271578|  1.35789e-05|  0.05%|# /opt/conda/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py:405 _check_input_dim
   135|         0|            0|            0|  0.00%|
   136|         0|            0|            0|  0.00%|        # exponential_average_factor is set to self.momentum
   137|         0|            0|            0|  0.00%|        # (when it is available) only so that it gets updated
   138|         0|            0|            0|  0.00%|        # in ONNX graph when this node is exported to ONNX.
   139|      2000|    0.0108864|  5.44322e-06|  0.02%|        if self.momentum is None:
   140|         0|            0|            0|  0.00%|            exponential_average_factor = 0.0
   141|         0|            0|            0|  0.00%|        else:
   142|      2000|   0.00944972|  4.72486e-06|  0.02%|            exponential_average_factor = self.momentum
   143|         0|            0|            0|  0.00%|
   144|      2000|   0.00957966|  4.78983e-06|  0.02%|        if self.training and self.track_running_stats:
   145|         0|            0|            0|  0.00%|            # TODO: if statement only here to tell the jit to skip emitting this when it is None
   146|      1220|    0.0206792|  1.69502e-05|  0.04%|            if self.num_batches_tracked is not None:  # type: ignore[has-type]
(call)|      1220|    0.0478392|  3.92124e-05|  0.09%|# /opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1117 __getattr__
   147|      1220|    0.0678508|  5.56154e-05|  0.13%|                self.num_batches_tracked = self.num_batches_tracked + 1  # type: ignore[has-type]
(call)|      1220|    0.0466533|  3.82404e-05|  0.09%|# /opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1117 __getattr__
(call)|      1220|     0.108036|  8.85537e-05|  0.20%|# /opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1133 __setattr__
   148|      1220|   0.00703931|  5.76993e-06|  0.01%|                if self.momentum is None:  # use cumulative moving average
   149|         0|            0|            0|  0.00%|                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)
   150|         0|            0|            0|  0.00%|                else:  # use exponential moving average
   151|      1220|   0.00660062|  5.41034e-06|  0.01%|                    exponential_average_factor = self.momentum
   152|         0|            0|            0|  0.00%|
   153|         0|            0|            0|  0.00%|        r"""
   154|         0|            0|            0|  0.00%|        Decide whether the mini-batch stats should be used for normalization rather than the buffers.
   155|         0|            0|            0|  0.00%|        Mini-batch stats are used in training mode, and in eval mode when buffers are None.
   156|         0|            0|            0|  0.00%|        """
   157|      2000|   0.00983334|  4.91667e-06|  0.02%|        if self.training:
   158|      1220|   0.00605655|  4.96438e-06|  0.01%|            bn_training = True
   159|         0|            0|            0|  0.00%|        else:
   160|       780|    0.0123684|   1.5857e-05|  0.02%|            bn_training = (self.running_mean is None) and (self.running_var is None)
(call)|       780|    0.0290024|  3.71826e-05|  0.05%|# /opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1117 __getattr__
   161|         0|            0|            0|  0.00%|
   162|         0|            0|            0|  0.00%|        r"""
   163|         0|            0|            0|  0.00%|        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be
   164|         0|            0|            0|  0.00%|        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are
   165|         0|            0|            0|  0.00%|        used for normalization (i.e. in eval mode when buffers are not None).
   166|         0|            0|            0|  0.00%|        """
   167|      4000|    0.0550857|  1.37714e-05|  0.10%|        return F.batch_norm(
(call)|      2000|      1.49061|  0.000745304|  2.78%|# /opt/conda/lib/python3.8/site-packages/torch/nn/functional.py:2250 batch_norm
   168|      2000|   0.00898433|  4.49216e-06|  0.02%|            input,
   169|         0|            0|            0|  0.00%|            # If buffers are not to be tracked, ensure that they won't be updated
   170|      3220|     0.037524|  1.16534e-05|  0.07%|            self.running_mean
(call)|      2000|    0.0741661|   3.7083e-05|  0.14%|# /opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1117 __getattr__
   171|      3220|    0.0156171|  4.85004e-06|  0.03%|            if not self.training or self.track_running_stats
   172|         0|            0|            0|  0.00%|            else None,
   173|      2000|    0.0300345|  1.50173e-05|  0.06%|            self.running_var if not self.training or self.track_running_stats else None,
(call)|      2000|    0.0714684|  3.57342e-05|  0.13%|# /opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1117 __getattr__
   174|      2000|    0.0295436|  1.47718e-05|  0.06%|            self.weight,
(call)|      2000|    0.0453074|  2.26537e-05|  0.08%|# /opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1117 __getattr__
   175|      2000|    0.0289612|  1.44806e-05|  0.05%|            self.bias,
(call)|      2000|    0.0453644|  2.26822e-05|  0.08%|# /opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1117 __getattr__
   176|      2000|    0.0086453|  4.32265e-06|  0.02%|            bn_training,
   177|      2000|   0.00796199|  3.98099e-06|  0.01%|            exponential_average_factor,
   178|      2000|   0.00868726|  4.34363e-06|  0.02%|            self.eps,
   179|         0|            0|            0|  0.00%|        )
   180|         0|            0|            0|  0.00%|
   181|         0|            0|            0|  0.00%|
   182|         0|            0|            0|  0.00%|class _LazyBatchNorm(LazyModuleMixin, _BatchNorm):
   183|         0|            0|            0|  0.00%|
   184|         0|            0|            0|  0.00%|    weight: UninitializedParameter  # type: ignore[assignment]
   185|         0|            0|            0|  0.00%|    bias: UninitializedParameter  # type: ignore[assignment]
   186|         0|            0|            0|  0.00%|
   187|         0|            0|            0|  0.00%|    def __init__(self, eps=1e-5, momentum=0.1, affine=True, track_running_stats=True,
   188|         0|            0|            0|  0.00%|                 device=None, dtype=None) -> None:
   189|         0|            0|            0|  0.00%|        factory_kwargs = {'device': device, 'dtype': dtype}
   190|         0|            0|            0|  0.00%|        super(_LazyBatchNorm, self).__init__(
   191|         0|            0|            0|  0.00%|            # affine and track_running_stats are hardcoded to False to
   192|         0|            0|            0|  0.00%|            # avoid creating tensors that will soon be overwritten.
   193|         0|            0|            0|  0.00%|            0,
   194|         0|            0|            0|  0.00%|            eps,
   195|         0|            0|            0|  0.00%|            momentum,
   196|         0|            0|            0|  0.00%|            False,
   197|         0|            0|            0|  0.00%|            False,
   198|         0|            0|            0|  0.00%|            **factory_kwargs,
   199|         0|            0|            0|  0.00%|        )
   200|         0|            0|            0|  0.00%|        self.affine = affine
   201|         0|            0|            0|  0.00%|        self.track_running_stats = track_running_stats
   202|         0|            0|            0|  0.00%|        if self.affine:
   203|         0|            0|            0|  0.00%|            self.weight = UninitializedParameter(**factory_kwargs)
   204|         0|            0|            0|  0.00%|            self.bias = UninitializedParameter(**factory_kwargs)
   205|         0|            0|            0|  0.00%|        if self.track_running_stats:
   206|         0|            0|            0|  0.00%|            self.running_mean = UninitializedBuffer(**factory_kwargs)
   207|         0|            0|            0|  0.00%|            self.running_var = UninitializedBuffer(**factory_kwargs)
   208|         0|            0|            0|  0.00%|            self.num_batches_tracked = torch.tensor(
   209|         0|            0|            0|  0.00%|                0, dtype=torch.long, **{k: v for k, v in factory_kwargs.items() if k != 'dtype'})
   210|         0|            0|            0|  0.00%|
   211|         0|            0|            0|  0.00%|    def reset_parameters(self) -> None:
   212|         0|            0|            0|  0.00%|        if not self.has_uninitialized_params() and self.num_features != 0:
   213|         0|            0|            0|  0.00%|            super().reset_parameters()
   214|         0|            0|            0|  0.00%|
   215|         0|            0|            0|  0.00%|    def initialize_parameters(self, input) -> None:  # type: ignore[override]
   216|         0|            0|            0|  0.00%|        if self.has_uninitialized_params():
   217|         0|            0|            0|  0.00%|            self.num_features = input.shape[1]
   218|         0|            0|            0|  0.00%|            if self.affine:
   219|         0|            0|            0|  0.00%|                assert isinstance(self.weight, UninitializedParameter)
   220|         0|            0|            0|  0.00%|                assert isinstance(self.bias, UninitializedParameter)
   221|         0|            0|            0|  0.00%|                self.weight.materialize((self.num_features,))
   222|         0|            0|            0|  0.00%|                self.bias.materialize((self.num_features,))
   223|         0|            0|            0|  0.00%|            if self.track_running_stats:
   224|         0|            0|            0|  0.00%|                self.running_mean.materialize((self.num_features,))  # type:ignore[union-attr]
   225|         0|            0|            0|  0.00%|                self.running_var.materialize((self.num_features,))  # type:ignore[union-attr]
   226|         0|            0|            0|  0.00%|            self.reset_parameters()
   227|         0|            0|            0|  0.00%|
   228|         0|            0|            0|  0.00%|
   229|         0|            0|            0|  0.00%|class BatchNorm1d(_BatchNorm):
   230|         0|            0|            0|  0.00%|    r"""Applies Batch Normalization over a 2D or 3D input (a mini-batch of 1D
   231|         0|            0|            0|  0.00%|    inputs with optional additional channel dimension) as described in the paper
   232|         0|            0|            0|  0.00%|    `Batch Normalization: Accelerating Deep Network Training by Reducing
   233|         0|            0|            0|  0.00%|    Internal Covariate Shift <https://arxiv.org/abs/1502.03167>`__ .
   234|         0|            0|            0|  0.00%|
   235|         0|            0|            0|  0.00%|    .. math::
   236|         0|            0|            0|  0.00%|
   237|         0|            0|            0|  0.00%|        y = \frac{x - \mathrm{E}[x]}{\sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta
   238|         0|            0|            0|  0.00%|
   239|         0|            0|            0|  0.00%|    The mean and standard-deviation are calculated per-dimension over
   240|         0|            0|            0|  0.00%|    the mini-batches and :math:`\gamma` and :math:`\beta` are learnable parameter vectors
   241|         0|            0|            0|  0.00%|    of size `C` (where `C` is the input size). By default, the elements of :math:`\gamma` are set
   242|         0|            0|            0|  0.00%|    to 1 and the elements of :math:`\beta` are set to 0. The standard-deviation is calculated
   243|         0|            0|            0|  0.00%|    via the biased estimator, equivalent to `torch.var(input, unbiased=False)`.
   244|         0|            0|            0|  0.00%|
   245|         0|            0|            0|  0.00%|    Also by default, during training this layer keeps running estimates of its
   246|         0|            0|            0|  0.00%|    computed mean and variance, which are then used for normalization during
   247|         0|            0|            0|  0.00%|    evaluation. The running estimates are kept with a default :attr:`momentum`
   248|         0|            0|            0|  0.00%|    of 0.1.
   249|         0|            0|            0|  0.00%|
   250|         0|            0|            0|  0.00%|    If :attr:`track_running_stats` is set to ``False``, this layer then does not
   251|         0|            0|            0|  0.00%|    keep running estimates, and batch statistics are instead used during
   252|         0|            0|            0|  0.00%|    evaluation time as well.
   253|         0|            0|            0|  0.00%|
   254|         0|            0|            0|  0.00%|    .. note::
   255|         0|            0|            0|  0.00%|        This :attr:`momentum` argument is different from one used in optimizer
   256|         0|            0|            0|  0.00%|        classes and the conventional notion of momentum. Mathematically, the
   257|         0|            0|            0|  0.00%|        update rule for running statistics here is
   258|         0|            0|            0|  0.00%|        :math:`\hat{x}_\text{new} = (1 - \text{momentum}) \times \hat{x} + \text{momentum} \times x_t`,
   259|         0|            0|            0|  0.00%|        where :math:`\hat{x}` is the estimated statistic and :math:`x_t` is the
   260|         0|            0|            0|  0.00%|        new observed value.
   261|         0|            0|            0|  0.00%|
   262|         0|            0|            0|  0.00%|    Because the Batch Normalization is done over the `C` dimension, computing statistics
   263|         0|            0|            0|  0.00%|    on `(N, L)` slices, it's common terminology to call this Temporal Batch Normalization.
   264|         0|            0|            0|  0.00%|
   265|         0|            0|            0|  0.00%|    Args:
   266|         0|            0|            0|  0.00%|        num_features: :math:`C` from an expected input of size
   267|         0|            0|            0|  0.00%|            :math:`(N, C, L)` or :math:`L` from input of size :math:`(N, L)`
   268|         0|            0|            0|  0.00%|        eps: a value added to the denominator for numerical stability.
   269|         0|            0|            0|  0.00%|            Default: 1e-5
   270|         0|            0|            0|  0.00%|        momentum: the value used for the running_mean and running_var
   271|         0|            0|            0|  0.00%|            computation. Can be set to ``None`` for cumulative moving average
   272|         0|            0|            0|  0.00%|            (i.e. simple average). Default: 0.1
   273|         0|            0|            0|  0.00%|        affine: a boolean value that when set to ``True``, this module has
   274|         0|            0|            0|  0.00%|            learnable affine parameters. Default: ``True``
   275|         0|            0|            0|  0.00%|        track_running_stats: a boolean value that when set to ``True``, this
   276|         0|            0|            0|  0.00%|            module tracks the running mean and variance, and when set to ``False``,
   277|         0|            0|            0|  0.00%|            this module does not track such statistics, and initializes statistics
   278|         0|            0|            0|  0.00%|            buffers :attr:`running_mean` and :attr:`running_var` as ``None``.
   279|         0|            0|            0|  0.00%|            When these buffers are ``None``, this module always uses batch statistics.
   280|         0|            0|            0|  0.00%|            in both training and eval modes. Default: ``True``
   281|         0|            0|            0|  0.00%|
   282|         0|            0|            0|  0.00%|    Shape:
   283|         0|            0|            0|  0.00%|        - Input: :math:`(N, C)` or :math:`(N, C, L)`
   284|         0|            0|            0|  0.00%|        - Output: :math:`(N, C)` or :math:`(N, C, L)` (same shape as input)
   285|         0|            0|            0|  0.00%|
   286|         0|            0|            0|  0.00%|    Examples::
   287|         0|            0|            0|  0.00%|
   288|         0|            0|            0|  0.00%|        >>> # With Learnable Parameters
   289|         0|            0|            0|  0.00%|        >>> m = nn.BatchNorm1d(100)
   290|         0|            0|            0|  0.00%|        >>> # Without Learnable Parameters
   291|         0|            0|            0|  0.00%|        >>> m = nn.BatchNorm1d(100, affine=False)
   292|         0|            0|            0|  0.00%|        >>> input = torch.randn(20, 100)
   293|         0|            0|            0|  0.00%|        >>> output = m(input)
   294|         0|            0|            0|  0.00%|    """
   295|         0|            0|            0|  0.00%|
   296|         0|            0|            0|  0.00%|    def _check_input_dim(self, input):
   297|         0|            0|            0|  0.00%|        if input.dim() != 2 and input.dim() != 3:
   298|         0|            0|            0|  0.00%|            raise ValueError(
   299|         0|            0|            0|  0.00%|                "expected 2D or 3D input (got {}D input)".format(input.dim())
   300|         0|            0|            0|  0.00%|            )
   301|         0|            0|            0|  0.00%|
   302|         0|            0|            0|  0.00%|
   303|         0|            0|            0|  0.00%|class LazyBatchNorm1d(_LazyBatchNorm):
   304|         0|            0|            0|  0.00%|    r"""A :class:`torch.nn.BatchNorm1d` module with lazy initialization of
   305|         0|            0|            0|  0.00%|    the ``num_features`` argument of the :class:`BatchNorm1d` that is inferred
   306|         0|            0|            0|  0.00%|    from the ``input.size(1)``.
   307|         0|            0|            0|  0.00%|    The attributes that will be lazily initialized are `weight`, `bias`,
   308|         0|            0|            0|  0.00%|    `running_mean` and `running_var`.
   309|         0|            0|            0|  0.00%|
   310|         0|            0|            0|  0.00%|    Check the :class:`torch.nn.modules.lazy.LazyModuleMixin` for further documentation
   311|         0|            0|            0|  0.00%|    on lazy modules and their limitations.
   312|         0|            0|            0|  0.00%|
   313|         0|            0|            0|  0.00%|    Args:
   314|         0|            0|            0|  0.00%|        eps: a value added to the denominator for numerical stability.
   315|         0|            0|            0|  0.00%|            Default: 1e-5
   316|         0|            0|            0|  0.00%|        momentum: the value used for the running_mean and running_var
   317|         0|            0|            0|  0.00%|            computation. Can be set to ``None`` for cumulative moving average
   318|         0|            0|            0|  0.00%|            (i.e. simple average). Default: 0.1
   319|         0|            0|            0|  0.00%|        affine: a boolean value that when set to ``True``, this module has
   320|         0|            0|            0|  0.00%|            learnable affine parameters. Default: ``True``
   321|         0|            0|            0|  0.00%|        track_running_stats: a boolean value that when set to ``True``, this
   322|         0|            0|            0|  0.00%|            module tracks the running mean and variance, and when set to ``False``,
   323|         0|            0|            0|  0.00%|            this module does not track such statistics, and initializes statistics
   324|         0|            0|            0|  0.00%|            buffers :attr:`running_mean` and :attr:`running_var` as ``None``.
   325|         0|            0|            0|  0.00%|            When these buffers are ``None``, this module always uses batch statistics.
   326|         0|            0|            0|  0.00%|            in both training and eval modes. Default: ``True``
   327|         0|            0|            0|  0.00%|    """
   328|         0|            0|            0|  0.00%|
   329|         0|            0|            0|  0.00%|    cls_to_become = BatchNorm1d  # type: ignore[assignment]
   330|         0|            0|            0|  0.00%|
   331|         0|            0|            0|  0.00%|    def _check_input_dim(self, input):
   332|         0|            0|            0|  0.00%|        if input.dim() != 2 and input.dim() != 3:
   333|         0|            0|            0|  0.00%|            raise ValueError(
   334|         0|            0|            0|  0.00%|                "expected 2D or 3D input (got {}D input)".format(input.dim())
   335|         0|            0|            0|  0.00%|            )
   336|         0|            0|            0|  0.00%|
   337|         0|            0|            0|  0.00%|
   338|         0|            0|            0|  0.00%|class BatchNorm2d(_BatchNorm):
   339|         0|            0|            0|  0.00%|    r"""Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs
   340|         0|            0|            0|  0.00%|    with additional channel dimension) as described in the paper
   341|         0|            0|            0|  0.00%|    `Batch Normalization: Accelerating Deep Network Training by Reducing
   342|         0|            0|            0|  0.00%|    Internal Covariate Shift <https://arxiv.org/abs/1502.03167>`__ .
   343|         0|            0|            0|  0.00%|
   344|         0|            0|            0|  0.00%|    .. math::
   345|         0|            0|            0|  0.00%|
   346|         0|            0|            0|  0.00%|        y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta
   347|         0|            0|            0|  0.00%|
   348|         0|            0|            0|  0.00%|    The mean and standard-deviation are calculated per-dimension over
   349|         0|            0|            0|  0.00%|    the mini-batches and :math:`\gamma` and :math:`\beta` are learnable parameter vectors
   350|         0|            0|            0|  0.00%|    of size `C` (where `C` is the input size). By default, the elements of :math:`\gamma` are set
   351|         0|            0|            0|  0.00%|    to 1 and the elements of :math:`\beta` are set to 0. The standard-deviation is calculated
   352|         0|            0|            0|  0.00%|    via the biased estimator, equivalent to `torch.var(input, unbiased=False)`.
   353|         0|            0|            0|  0.00%|
   354|         0|            0|            0|  0.00%|    Also by default, during training this layer keeps running estimates of its
   355|         0|            0|            0|  0.00%|    computed mean and variance, which are then used for normalization during
   356|         0|            0|            0|  0.00%|    evaluation. The running estimates are kept with a default :attr:`momentum`
   357|         0|            0|            0|  0.00%|    of 0.1.
   358|         0|            0|            0|  0.00%|
   359|         0|            0|            0|  0.00%|    If :attr:`track_running_stats` is set to ``False``, this layer then does not
   360|         0|            0|            0|  0.00%|    keep running estimates, and batch statistics are instead used during
   361|         0|            0|            0|  0.00%|    evaluation time as well.
   362|         0|            0|            0|  0.00%|
   363|         0|            0|            0|  0.00%|    .. note::
   364|         0|            0|            0|  0.00%|        This :attr:`momentum` argument is different from one used in optimizer
   365|         0|            0|            0|  0.00%|        classes and the conventional notion of momentum. Mathematically, the
   366|         0|            0|            0|  0.00%|        update rule for running statistics here is
   367|         0|            0|            0|  0.00%|        :math:`\hat{x}_\text{new} = (1 - \text{momentum}) \times \hat{x} + \text{momentum} \times x_t`,
   368|         0|            0|            0|  0.00%|        where :math:`\hat{x}` is the estimated statistic and :math:`x_t` is the
   369|         0|            0|            0|  0.00%|        new observed value.
   370|         0|            0|            0|  0.00%|
   371|         0|            0|            0|  0.00%|    Because the Batch Normalization is done over the `C` dimension, computing statistics
   372|         0|            0|            0|  0.00%|    on `(N, H, W)` slices, it's common terminology to call this Spatial Batch Normalization.
   373|         0|            0|            0|  0.00%|
   374|         0|            0|            0|  0.00%|    Args:
   375|         0|            0|            0|  0.00%|        num_features: :math:`C` from an expected input of size
   376|         0|            0|            0|  0.00%|            :math:`(N, C, H, W)`
   377|         0|            0|            0|  0.00%|        eps: a value added to the denominator for numerical stability.
   378|         0|            0|            0|  0.00%|            Default: 1e-5
   379|         0|            0|            0|  0.00%|        momentum: the value used for the running_mean and running_var
   380|         0|            0|            0|  0.00%|            computation. Can be set to ``None`` for cumulative moving average
   381|         0|            0|            0|  0.00%|            (i.e. simple average). Default: 0.1
   382|         0|            0|            0|  0.00%|        affine: a boolean value that when set to ``True``, this module has
   383|         0|            0|            0|  0.00%|            learnable affine parameters. Default: ``True``
   384|         0|            0|            0|  0.00%|        track_running_stats: a boolean value that when set to ``True``, this
   385|         0|            0|            0|  0.00%|            module tracks the running mean and variance, and when set to ``False``,
   386|         0|            0|            0|  0.00%|            this module does not track such statistics, and initializes statistics
   387|         0|            0|            0|  0.00%|            buffers :attr:`running_mean` and :attr:`running_var` as ``None``.
   388|         0|            0|            0|  0.00%|            When these buffers are ``None``, this module always uses batch statistics.
   389|         0|            0|            0|  0.00%|            in both training and eval modes. Default: ``True``
   390|         0|            0|            0|  0.00%|
   391|         0|            0|            0|  0.00%|    Shape:
   392|         0|            0|            0|  0.00%|        - Input: :math:`(N, C, H, W)`
   393|         0|            0|            0|  0.00%|        - Output: :math:`(N, C, H, W)` (same shape as input)
   394|         0|            0|            0|  0.00%|
   395|         0|            0|            0|  0.00%|    Examples::
   396|         0|            0|            0|  0.00%|
   397|         0|            0|            0|  0.00%|        >>> # With Learnable Parameters
   398|         0|            0|            0|  0.00%|        >>> m = nn.BatchNorm2d(100)
   399|         0|            0|            0|  0.00%|        >>> # Without Learnable Parameters
   400|         0|            0|            0|  0.00%|        >>> m = nn.BatchNorm2d(100, affine=False)
   401|         0|            0|            0|  0.00%|        >>> input = torch.randn(20, 100, 35, 45)
   402|         0|            0|            0|  0.00%|        >>> output = m(input)
   403|         0|            0|            0|  0.00%|    """
   404|         0|            0|            0|  0.00%|
   405|      2000|   0.00982046|  4.91023e-06|  0.02%|    def _check_input_dim(self, input):
   406|      2000|    0.0173373|  8.66866e-06|  0.03%|        if input.dim() != 4:
   407|         0|            0|            0|  0.00%|            raise ValueError("expected 4D input (got {}D input)".format(input.dim()))
   408|         0|            0|            0|  0.00%|
   409|         0|            0|            0|  0.00%|
   410|         0|            0|            0|  0.00%|class LazyBatchNorm2d(_LazyBatchNorm):
   411|         0|            0|            0|  0.00%|    r"""A :class:`torch.nn.BatchNorm2d` module with lazy initialization of
   412|         0|            0|            0|  0.00%|    the ``num_features`` argument of the :class:`BatchNorm2d` that is inferred
   413|         0|            0|            0|  0.00%|    from the ``input.size(1)``.
   414|         0|            0|            0|  0.00%|    The attributes that will be lazily initialized are `weight`, `bias`,
   415|         0|            0|            0|  0.00%|    `running_mean` and `running_var`.
   416|         0|            0|            0|  0.00%|
   417|         0|            0|            0|  0.00%|    Check the :class:`torch.nn.modules.lazy.LazyModuleMixin` for further documentation
   418|         0|            0|            0|  0.00%|    on lazy modules and their limitations.
   419|         0|            0|            0|  0.00%|
   420|         0|            0|            0|  0.00%|    Args:
   421|         0|            0|            0|  0.00%|        eps: a value added to the denominator for numerical stability.
   422|         0|            0|            0|  0.00%|            Default: 1e-5
   423|         0|            0|            0|  0.00%|        momentum: the value used for the running_mean and running_var
   424|         0|            0|            0|  0.00%|            computation. Can be set to ``None`` for cumulative moving average
   425|         0|            0|            0|  0.00%|            (i.e. simple average). Default: 0.1
   426|         0|            0|            0|  0.00%|        affine: a boolean value that when set to ``True``, this module has
   427|         0|            0|            0|  0.00%|            learnable affine parameters. Default: ``True``
   428|         0|            0|            0|  0.00%|        track_running_stats: a boolean value that when set to ``True``, this
   429|         0|            0|            0|  0.00%|            module tracks the running mean and variance, and when set to ``False``,
   430|         0|            0|            0|  0.00%|            this module does not track such statistics, and initializes statistics
   431|         0|            0|            0|  0.00%|            buffers :attr:`running_mean` and :attr:`running_var` as ``None``.
   432|         0|            0|            0|  0.00%|            When these buffers are ``None``, this module always uses batch statistics.
   433|         0|            0|            0|  0.00%|            in both training and eval modes. Default: ``True``
   434|         0|            0|            0|  0.00%|    """
   435|         0|            0|            0|  0.00%|
   436|         0|            0|            0|  0.00%|    cls_to_become = BatchNorm2d  # type: ignore[assignment]
   437|         0|            0|            0|  0.00%|
   438|         0|            0|            0|  0.00%|    def _check_input_dim(self, input):
   439|         0|            0|            0|  0.00%|        if input.dim() != 4:
   440|         0|            0|            0|  0.00%|            raise ValueError("expected 4D input (got {}D input)".format(input.dim()))
   441|         0|            0|            0|  0.00%|
   442|         0|            0|            0|  0.00%|
   443|         0|            0|            0|  0.00%|class BatchNorm3d(_BatchNorm):
   444|         0|            0|            0|  0.00%|    r"""Applies Batch Normalization over a 5D input (a mini-batch of 3D inputs
   445|         0|            0|            0|  0.00%|    with additional channel dimension) as described in the paper
   446|         0|            0|            0|  0.00%|    `Batch Normalization: Accelerating Deep Network Training by Reducing
   447|         0|            0|            0|  0.00%|    Internal Covariate Shift <https://arxiv.org/abs/1502.03167>`__ .
   448|         0|            0|            0|  0.00%|
   449|         0|            0|            0|  0.00%|    .. math::
   450|         0|            0|            0|  0.00%|
   451|         0|            0|            0|  0.00%|        y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta
   452|         0|            0|            0|  0.00%|
   453|         0|            0|            0|  0.00%|    The mean and standard-deviation are calculated per-dimension over
   454|         0|            0|            0|  0.00%|    the mini-batches and :math:`\gamma` and :math:`\beta` are learnable parameter vectors
   455|         0|            0|            0|  0.00%|    of size `C` (where `C` is the input size). By default, the elements of :math:`\gamma` are set
   456|         0|            0|            0|  0.00%|    to 1 and the elements of :math:`\beta` are set to 0. The standard-deviation is calculated
   457|         0|            0|            0|  0.00%|    via the biased estimator, equivalent to `torch.var(input, unbiased=False)`.
   458|         0|            0|            0|  0.00%|
   459|         0|            0|            0|  0.00%|    Also by default, during training this layer keeps running estimates of its
   460|         0|            0|            0|  0.00%|    computed mean and variance, which are then used for normalization during
   461|         0|            0|            0|  0.00%|    evaluation. The running estimates are kept with a default :attr:`momentum`
   462|         0|            0|            0|  0.00%|    of 0.1.
   463|         0|            0|            0|  0.00%|
   464|         0|            0|            0|  0.00%|    If :attr:`track_running_stats` is set to ``False``, this layer then does not
   465|         0|            0|            0|  0.00%|    keep running estimates, and batch statistics are instead used during
   466|         0|            0|            0|  0.00%|    evaluation time as well.
   467|         0|            0|            0|  0.00%|
   468|         0|            0|            0|  0.00%|    .. note::
   469|         0|            0|            0|  0.00%|        This :attr:`momentum` argument is different from one used in optimizer
   470|         0|            0|            0|  0.00%|        classes and the conventional notion of momentum. Mathematically, the
   471|         0|            0|            0|  0.00%|        update rule for running statistics here is
   472|         0|            0|            0|  0.00%|        :math:`\hat{x}_\text{new} = (1 - \text{momentum}) \times \hat{x} + \text{momentum} \times x_t`,
   473|         0|            0|            0|  0.00%|        where :math:`\hat{x}` is the estimated statistic and :math:`x_t` is the
   474|         0|            0|            0|  0.00%|        new observed value.
   475|         0|            0|            0|  0.00%|
   476|         0|            0|            0|  0.00%|    Because the Batch Normalization is done over the `C` dimension, computing statistics
   477|         0|            0|            0|  0.00%|    on `(N, D, H, W)` slices, it's common terminology to call this Volumetric Batch Normalization
   478|         0|            0|            0|  0.00%|    or Spatio-temporal Batch Normalization.
   479|         0|            0|            0|  0.00%|
   480|         0|            0|            0|  0.00%|    Args:
   481|         0|            0|            0|  0.00%|        num_features: :math:`C` from an expected input of size
   482|         0|            0|            0|  0.00%|            :math:`(N, C, D, H, W)`
   483|         0|            0|            0|  0.00%|        eps: a value added to the denominator for numerical stability.
   484|         0|            0|            0|  0.00%|            Default: 1e-5
   485|         0|            0|            0|  0.00%|        momentum: the value used for the running_mean and running_var
   486|         0|            0|            0|  0.00%|            computation. Can be set to ``None`` for cumulative moving average
   487|         0|            0|            0|  0.00%|            (i.e. simple average). Default: 0.1
   488|         0|            0|            0|  0.00%|        affine: a boolean value that when set to ``True``, this module has
   489|         0|            0|            0|  0.00%|            learnable affine parameters. Default: ``True``
   490|         0|            0|            0|  0.00%|        track_running_stats: a boolean value that when set to ``True``, this
   491|         0|            0|            0|  0.00%|            module tracks the running mean and variance, and when set to ``False``,
   492|         0|            0|            0|  0.00%|            this module does not track such statistics, and initializes statistics
   493|         0|            0|            0|  0.00%|            buffers :attr:`running_mean` and :attr:`running_var` as ``None``.
   494|         0|            0|            0|  0.00%|            When these buffers are ``None``, this module always uses batch statistics.
   495|         0|            0|            0|  0.00%|            in both training and eval modes. Default: ``True``
   496|         0|            0|            0|  0.00%|
   497|         0|            0|            0|  0.00%|    Shape:
   498|         0|            0|            0|  0.00%|        - Input: :math:`(N, C, D, H, W)`
   499|         0|            0|            0|  0.00%|        - Output: :math:`(N, C, D, H, W)` (same shape as input)
   500|         0|            0|            0|  0.00%|
   501|         0|            0|            0|  0.00%|    Examples::
   502|         0|            0|            0|  0.00%|
   503|         0|            0|            0|  0.00%|        >>> # With Learnable Parameters
   504|         0|            0|            0|  0.00%|        >>> m = nn.BatchNorm3d(100)
   505|         0|            0|            0|  0.00%|        >>> # Without Learnable Parameters
   506|         0|            0|            0|  0.00%|        >>> m = nn.BatchNorm3d(100, affine=False)
   507|         0|            0|            0|  0.00%|        >>> input = torch.randn(20, 100, 35, 45, 10)
   508|         0|            0|            0|  0.00%|        >>> output = m(input)
   509|         0|            0|            0|  0.00%|    """
   510|         0|            0|            0|  0.00%|
   511|         0|            0|            0|  0.00%|    def _check_input_dim(self, input):
   512|         0|            0|            0|  0.00%|        if input.dim() != 5:
   513|         0|            0|            0|  0.00%|            raise ValueError("expected 5D input (got {}D input)".format(input.dim()))
   514|         0|            0|            0|  0.00%|
   515|         0|            0|            0|  0.00%|
   516|         0|            0|            0|  0.00%|class LazyBatchNorm3d(_LazyBatchNorm):
   517|         0|            0|            0|  0.00%|    r"""A :class:`torch.nn.BatchNorm3d` module with lazy initialization of
   518|         0|            0|            0|  0.00%|    the ``num_features`` argument of the :class:`BatchNorm3d` that is inferred
   519|         0|            0|            0|  0.00%|    from the ``input.size(1)``.
   520|         0|            0|            0|  0.00%|    The attributes that will be lazily initialized are `weight`, `bias`,
   521|         0|            0|            0|  0.00%|    `running_mean` and `running_var`.
   522|         0|            0|            0|  0.00%|
   523|         0|            0|            0|  0.00%|    Check the :class:`torch.nn.modules.lazy.LazyModuleMixin` for further documentation
   524|         0|            0|            0|  0.00%|    on lazy modules and their limitations.
   525|         0|            0|            0|  0.00%|
   526|         0|            0|            0|  0.00%|    Args:
   527|         0|            0|            0|  0.00%|        eps: a value added to the denominator for numerical stability.
   528|         0|            0|            0|  0.00%|            Default: 1e-5
   529|         0|            0|            0|  0.00%|        momentum: the value used for the running_mean and running_var
   530|         0|            0|            0|  0.00%|            computation. Can be set to ``None`` for cumulative moving average
   531|         0|            0|            0|  0.00%|            (i.e. simple average). Default: 0.1
   532|         0|            0|            0|  0.00%|        affine: a boolean value that when set to ``True``, this module has
   533|         0|            0|            0|  0.00%|            learnable affine parameters. Default: ``True``
   534|         0|            0|            0|  0.00%|        track_running_stats: a boolean value that when set to ``True``, this
   535|         0|            0|            0|  0.00%|            module tracks the running mean and variance, and when set to ``False``,
   536|         0|            0|            0|  0.00%|            this module does not track such statistics, and initializes statistics
   537|         0|            0|            0|  0.00%|            buffers :attr:`running_mean` and :attr:`running_var` as ``None``.
   538|         0|            0|            0|  0.00%|            When these buffers are ``None``, this module always uses batch statistics.
   539|         0|            0|            0|  0.00%|            in both training and eval modes. Default: ``True``
   540|         0|            0|            0|  0.00%|    """
   541|         0|            0|            0|  0.00%|
   542|         0|            0|            0|  0.00%|    cls_to_become = BatchNorm3d  # type: ignore[assignment]
   543|         0|            0|            0|  0.00%|
   544|         0|            0|            0|  0.00%|    def _check_input_dim(self, input):
   545|         0|            0|            0|  0.00%|        if input.dim() != 5:
   546|         0|            0|            0|  0.00%|            raise ValueError("expected 5D input (got {}D input)".format(input.dim()))
   547|         0|            0|            0|  0.00%|
   548|         0|            0|            0|  0.00%|
   549|         0|            0|            0|  0.00%|class SyncBatchNorm(_BatchNorm):
   550|         0|            0|            0|  0.00%|    r"""Applies Batch Normalization over a N-Dimensional input (a mini-batch of [N-2]D inputs
   551|         0|            0|            0|  0.00%|    with additional channel dimension) as described in the paper
   552|         0|            0|            0|  0.00%|    `Batch Normalization: Accelerating Deep Network Training by Reducing
   553|         0|            0|            0|  0.00%|    Internal Covariate Shift <https://arxiv.org/abs/1502.03167>`__ .
   554|         0|            0|            0|  0.00%|
   555|         0|            0|            0|  0.00%|    .. math::
   556|         0|            0|            0|  0.00%|
   557|         0|            0|            0|  0.00%|        y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta
   558|         0|            0|            0|  0.00%|
   559|         0|            0|            0|  0.00%|    The mean and standard-deviation are calculated per-dimension over all
   560|         0|            0|            0|  0.00%|    mini-batches of the same process groups. :math:`\gamma` and :math:`\beta`
   561|         0|            0|            0|  0.00%|    are learnable parameter vectors of size `C` (where `C` is the input size).
   562|         0|            0|            0|  0.00%|    By default, the elements of :math:`\gamma` are sampled from
   563|         0|            0|            0|  0.00%|    :math:`\mathcal{U}(0, 1)` and the elements of :math:`\beta` are set to 0.
   564|         0|            0|            0|  0.00%|    The standard-deviation is calculated via the biased estimator, equivalent to
   565|         0|            0|            0|  0.00%|    `torch.var(input, unbiased=False)`.
   566|         0|            0|            0|  0.00%|
   567|         0|            0|            0|  0.00%|    Also by default, during training this layer keeps running estimates of its
   568|         0|            0|            0|  0.00%|    computed mean and variance, which are then used for normalization during
   569|         0|            0|            0|  0.00%|    evaluation. The running estimates are kept with a default :attr:`momentum`
   570|         0|            0|            0|  0.00%|    of 0.1.
   571|         0|            0|            0|  0.00%|
   572|         0|            0|            0|  0.00%|    If :attr:`track_running_stats` is set to ``False``, this layer then does not
   573|         0|            0|            0|  0.00%|    keep running estimates, and batch statistics are instead used during
   574|         0|            0|            0|  0.00%|    evaluation time as well.
   575|         0|            0|            0|  0.00%|
   576|         0|            0|            0|  0.00%|    .. note::
   577|         0|            0|            0|  0.00%|        This :attr:`momentum` argument is different from one used in optimizer
   578|         0|            0|            0|  0.00%|        classes and the conventional notion of momentum. Mathematically, the
   579|         0|            0|            0|  0.00%|        update rule for running statistics here is
   580|         0|            0|            0|  0.00%|        :math:`\hat{x}_\text{new} = (1 - \text{momentum}) \times \hat{x} + \text{momentum} \times x_t`,
   581|         0|            0|            0|  0.00%|        where :math:`\hat{x}` is the estimated statistic and :math:`x_t` is the
   582|         0|            0|            0|  0.00%|        new observed value.
   583|         0|            0|            0|  0.00%|
   584|         0|            0|            0|  0.00%|    Because the Batch Normalization is done for each channel in the ``C`` dimension, computing
   585|         0|            0|            0|  0.00%|    statistics on ``(N, +)`` slices, it's common terminology to call this Volumetric Batch
   586|         0|            0|            0|  0.00%|    Normalization or Spatio-temporal Batch Normalization.
   587|         0|            0|            0|  0.00%|
   588|         0|            0|            0|  0.00%|    Currently :class:`SyncBatchNorm` only supports
   589|         0|            0|            0|  0.00%|    :class:`~torch.nn.DistributedDataParallel` (DDP) with single GPU per process. Use
   590|         0|            0|            0|  0.00%|    :meth:`torch.nn.SyncBatchNorm.convert_sync_batchnorm()` to convert
   591|         0|            0|            0|  0.00%|    :attr:`BatchNorm*D` layer to :class:`SyncBatchNorm` before wrapping
   592|         0|            0|            0|  0.00%|    Network with DDP.
   593|         0|            0|            0|  0.00%|
   594|         0|            0|            0|  0.00%|    Args:
   595|         0|            0|            0|  0.00%|        num_features: :math:`C` from an expected input of size
   596|         0|            0|            0|  0.00%|            :math:`(N, C, +)`
   597|         0|            0|            0|  0.00%|        eps: a value added to the denominator for numerical stability.
   598|         0|            0|            0|  0.00%|            Default: ``1e-5``
   599|         0|            0|            0|  0.00%|        momentum: the value used for the running_mean and running_var
   600|         0|            0|            0|  0.00%|            computation. Can be set to ``None`` for cumulative moving average
   601|         0|            0|            0|  0.00%|            (i.e. simple average). Default: 0.1
   602|         0|            0|            0|  0.00%|        affine: a boolean value that when set to ``True``, this module has
   603|         0|            0|            0|  0.00%|            learnable affine parameters. Default: ``True``
   604|         0|            0|            0|  0.00%|        track_running_stats: a boolean value that when set to ``True``, this
   605|         0|            0|            0|  0.00%|            module tracks the running mean and variance, and when set to ``False``,
   606|         0|            0|            0|  0.00%|            this module does not track such statistics, and initializes statistics
   607|         0|            0|            0|  0.00%|            buffers :attr:`running_mean` and :attr:`running_var` as ``None``.
   608|         0|            0|            0|  0.00%|            When these buffers are ``None``, this module always uses batch statistics.
   609|         0|            0|            0|  0.00%|            in both training and eval modes. Default: ``True``
   610|         0|            0|            0|  0.00%|        process_group: synchronization of stats happen within each process group
   611|         0|            0|            0|  0.00%|            individually. Default behavior is synchronization across the whole
   612|         0|            0|            0|  0.00%|            world
   613|         0|            0|            0|  0.00%|
   614|         0|            0|            0|  0.00%|    Shape:
   615|         0|            0|            0|  0.00%|        - Input: :math:`(N, C, +)`
   616|         0|            0|            0|  0.00%|        - Output: :math:`(N, C, +)` (same shape as input)
   617|         0|            0|            0|  0.00%|
   618|         0|            0|            0|  0.00%|    .. note::
   619|         0|            0|            0|  0.00%|        Synchronization of batchnorm statistics occurs only while training, i.e.
   620|         0|            0|            0|  0.00%|        synchronization is disabled when ``model.eval()`` is set or if
   621|         0|            0|            0|  0.00%|        ``self.training`` is otherwise ``False``.
   622|         0|            0|            0|  0.00%|
   623|         0|            0|            0|  0.00%|    Examples::
   624|         0|            0|            0|  0.00%|
   625|         0|            0|            0|  0.00%|        >>> # With Learnable Parameters
   626|         0|            0|            0|  0.00%|        >>> m = nn.SyncBatchNorm(100)
   627|         0|            0|            0|  0.00%|        >>> # creating process group (optional)
   628|         0|            0|            0|  0.00%|        >>> # ranks is a list of int identifying rank ids.
   629|         0|            0|            0|  0.00%|        >>> ranks = list(range(8))
   630|         0|            0|            0|  0.00%|        >>> r1, r2 = ranks[:4], ranks[4:]
   631|         0|            0|            0|  0.00%|        >>> # Note: every rank calls into new_group for every
   632|         0|            0|            0|  0.00%|        >>> # process group created, even if that rank is not
   633|         0|            0|            0|  0.00%|        >>> # part of the group.
   634|         0|            0|            0|  0.00%|        >>> process_groups = [torch.distributed.new_group(pids) for pids in [r1, r2]]
   635|         0|            0|            0|  0.00%|        >>> process_group = process_groups[0 if dist.get_rank() <= 3 else 1]
   636|         0|            0|            0|  0.00%|        >>> # Without Learnable Parameters
   637|         0|            0|            0|  0.00%|        >>> m = nn.BatchNorm3d(100, affine=False, process_group=process_group)
   638|         0|            0|            0|  0.00%|        >>> input = torch.randn(20, 100, 35, 45, 10)
   639|         0|            0|            0|  0.00%|        >>> output = m(input)
   640|         0|            0|            0|  0.00%|
   641|         0|            0|            0|  0.00%|        >>> # network is nn.BatchNorm layer
   642|         0|            0|            0|  0.00%|        >>> sync_bn_network = nn.SyncBatchNorm.convert_sync_batchnorm(network, process_group)
   643|         0|            0|            0|  0.00%|        >>> # only single gpu per process is currently supported
   644|         0|            0|            0|  0.00%|        >>> ddp_sync_bn_network = torch.nn.parallel.DistributedDataParallel(
   645|         0|            0|            0|  0.00%|        >>>                         sync_bn_network,
   646|         0|            0|            0|  0.00%|        >>>                         device_ids=[args.local_rank],
   647|         0|            0|            0|  0.00%|        >>>                         output_device=args.local_rank)
   648|         0|            0|            0|  0.00%|    """
   649|         0|            0|            0|  0.00%|
   650|         0|            0|            0|  0.00%|    def __init__(
   651|         0|            0|            0|  0.00%|        self,
   652|         0|            0|            0|  0.00%|        num_features: int,
   653|         0|            0|            0|  0.00%|        eps: float = 1e-5,
   654|         0|            0|            0|  0.00%|        momentum: float = 0.1,
   655|         0|            0|            0|  0.00%|        affine: bool = True,
   656|         0|            0|            0|  0.00%|        track_running_stats: bool = True,
   657|         0|            0|            0|  0.00%|        process_group: Optional[Any] = None,
   658|         0|            0|            0|  0.00%|        device=None,
   659|         0|            0|            0|  0.00%|        dtype=None
   660|         0|            0|            0|  0.00%|    ) -> None:
   661|         0|            0|            0|  0.00%|        factory_kwargs = {'device': device, 'dtype': dtype}
   662|         0|            0|            0|  0.00%|        super(SyncBatchNorm, self).__init__(
   663|         0|            0|            0|  0.00%|            num_features, eps, momentum, affine, track_running_stats, **factory_kwargs
   664|         0|            0|            0|  0.00%|        )
   665|         0|            0|            0|  0.00%|        self.process_group = process_group
   666|         0|            0|            0|  0.00%|
   667|         0|            0|            0|  0.00%|    def _check_input_dim(self, input):
   668|         0|            0|            0|  0.00%|        if input.dim() < 2:
   669|         0|            0|            0|  0.00%|            raise ValueError(
   670|         0|            0|            0|  0.00%|                "expected at least 2D input (got {}D input)".format(input.dim())
   671|         0|            0|            0|  0.00%|            )
   672|         0|            0|            0|  0.00%|
   673|         0|            0|            0|  0.00%|    def _check_non_zero_input_channels(self, input):
   674|         0|            0|            0|  0.00%|        if input.size(1) == 0:
   675|         0|            0|            0|  0.00%|            raise ValueError(
   676|         0|            0|            0|  0.00%|                "SyncBatchNorm number of input channels should be non-zero"
   677|         0|            0|            0|  0.00%|            )
   678|         0|            0|            0|  0.00%|
   679|         0|            0|            0|  0.00%|    def forward(self, input: Tensor) -> Tensor:
   680|         0|            0|            0|  0.00%|        # currently only GPU input is supported
   681|         0|            0|            0|  0.00%|        if not input.is_cuda:
   682|         0|            0|            0|  0.00%|            raise ValueError("SyncBatchNorm expected input tensor to be on GPU")
   683|         0|            0|            0|  0.00%|
   684|         0|            0|            0|  0.00%|        self._check_input_dim(input)
   685|         0|            0|            0|  0.00%|        self._check_non_zero_input_channels(input)
   686|         0|            0|            0|  0.00%|
   687|         0|            0|            0|  0.00%|        # exponential_average_factor is set to self.momentum
   688|         0|            0|            0|  0.00%|        # (when it is available) only so that it gets updated
   689|         0|            0|            0|  0.00%|        # in ONNX graph when this node is exported to ONNX.
   690|         0|            0|            0|  0.00%|        if self.momentum is None:
   691|         0|            0|            0|  0.00%|            exponential_average_factor = 0.0
   692|         0|            0|            0|  0.00%|        else:
   693|         0|            0|            0|  0.00%|            exponential_average_factor = self.momentum
   694|         0|            0|            0|  0.00%|
   695|         0|            0|            0|  0.00%|        if self.training and self.track_running_stats:
   696|         0|            0|            0|  0.00%|            assert self.num_batches_tracked is not None
   697|         0|            0|            0|  0.00%|            self.num_batches_tracked = self.num_batches_tracked + 1
   698|         0|            0|            0|  0.00%|            if self.momentum is None:  # use cumulative moving average
   699|         0|            0|            0|  0.00%|                exponential_average_factor = 1.0 / self.num_batches_tracked.item()
   700|         0|            0|            0|  0.00%|            else:  # use exponential moving average
   701|         0|            0|            0|  0.00%|                exponential_average_factor = self.momentum
   702|         0|            0|            0|  0.00%|
   703|         0|            0|            0|  0.00%|        r"""
   704|         0|            0|            0|  0.00%|        Decide whether the mini-batch stats should be used for normalization rather than the buffers.
   705|         0|            0|            0|  0.00%|        Mini-batch stats are used in training mode, and in eval mode when buffers are None.
   706|         0|            0|            0|  0.00%|        """
   707|         0|            0|            0|  0.00%|        if self.training:
   708|         0|            0|            0|  0.00%|            bn_training = True
   709|         0|            0|            0|  0.00%|        else:
   710|         0|            0|            0|  0.00%|            bn_training = (self.running_mean is None) and (self.running_var is None)
   711|         0|            0|            0|  0.00%|
   712|         0|            0|            0|  0.00%|        r"""
   713|         0|            0|            0|  0.00%|        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be
   714|         0|            0|            0|  0.00%|        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are
   715|         0|            0|            0|  0.00%|        used for normalization (i.e. in eval mode when buffers are not None).
   716|         0|            0|            0|  0.00%|        """
   717|         0|            0|            0|  0.00%|        # If buffers are not to be tracked, ensure that they won't be updated
   718|         0|            0|            0|  0.00%|        running_mean = (
   719|         0|            0|            0|  0.00%|            self.running_mean if not self.training or self.track_running_stats else None
   720|         0|            0|            0|  0.00%|        )
   721|         0|            0|            0|  0.00%|        running_var = (
   722|         0|            0|            0|  0.00%|            self.running_var if not self.training or self.track_running_stats else None
   723|         0|            0|            0|  0.00%|        )
   724|         0|            0|            0|  0.00%|
   725|         0|            0|            0|  0.00%|        # Don't sync batchnorm stats in inference mode (model.eval()).
   726|         0|            0|            0|  0.00%|        need_sync = (bn_training and self.training)
   727|         0|            0|            0|  0.00%|        if need_sync:
   728|         0|            0|            0|  0.00%|            process_group = torch.distributed.group.WORLD
   729|         0|            0|            0|  0.00%|            if self.process_group:
   730|         0|            0|            0|  0.00%|                process_group = self.process_group
   731|         0|            0|            0|  0.00%|            world_size = torch.distributed.get_world_size(process_group)
   732|         0|            0|            0|  0.00%|            need_sync = world_size > 1
   733|         0|            0|            0|  0.00%|
   734|         0|            0|            0|  0.00%|        # fallback to framework BN when synchronization is not necessary
   735|         0|            0|            0|  0.00%|        if not need_sync:
   736|         0|            0|            0|  0.00%|            return F.batch_norm(
   737|         0|            0|            0|  0.00%|                input,
   738|         0|            0|            0|  0.00%|                running_mean,
   739|         0|            0|            0|  0.00%|                running_var,
   740|         0|            0|            0|  0.00%|                self.weight,
   741|         0|            0|            0|  0.00%|                self.bias,
   742|         0|            0|            0|  0.00%|                bn_training,
   743|         0|            0|            0|  0.00%|                exponential_average_factor,
   744|         0|            0|            0|  0.00%|                self.eps,
   745|         0|            0|            0|  0.00%|            )
   746|         0|            0|            0|  0.00%|        else:
   747|         0|            0|            0|  0.00%|            assert bn_training
   748|         0|            0|            0|  0.00%|            return sync_batch_norm.apply(
   749|         0|            0|            0|  0.00%|                input,
   750|         0|            0|            0|  0.00%|                self.weight,
   751|         0|            0|            0|  0.00%|                self.bias,
   752|         0|            0|            0|  0.00%|                running_mean,
   753|         0|            0|            0|  0.00%|                running_var,
   754|         0|            0|            0|  0.00%|                self.eps,
   755|         0|            0|            0|  0.00%|                exponential_average_factor,
   756|         0|            0|            0|  0.00%|                process_group,
   757|         0|            0|            0|  0.00%|                world_size,
   758|         0|            0|            0|  0.00%|            )
   759|         0|            0|            0|  0.00%|
   760|         0|            0|            0|  0.00%|    @classmethod
   761|         0|            0|            0|  0.00%|    def convert_sync_batchnorm(cls, module, process_group=None):
   762|         0|            0|            0|  0.00%|        r"""Helper function to convert all :attr:`BatchNorm*D` layers in the model to
   763|         0|            0|            0|  0.00%|        :class:`torch.nn.SyncBatchNorm` layers.
   764|         0|            0|            0|  0.00%|
   765|         0|            0|            0|  0.00%|        Args:
   766|         0|            0|            0|  0.00%|            module (nn.Module): module containing one or more attr:`BatchNorm*D` layers
   767|         0|            0|            0|  0.00%|            process_group (optional): process group to scope synchronization,
   768|         0|            0|            0|  0.00%|                default is the whole world
   769|         0|            0|            0|  0.00%|
   770|         0|            0|            0|  0.00%|        Returns:
   771|         0|            0|            0|  0.00%|            The original :attr:`module` with the converted :class:`torch.nn.SyncBatchNorm`
   772|         0|            0|            0|  0.00%|            layers. If the original :attr:`module` is a :attr:`BatchNorm*D` layer,
   773|         0|            0|            0|  0.00%|            a new :class:`torch.nn.SyncBatchNorm` layer object will be returned
   774|         0|            0|            0|  0.00%|            instead.
   775|         0|            0|            0|  0.00%|
   776|         0|            0|            0|  0.00%|        Example::
   777|         0|            0|            0|  0.00%|
   778|         0|            0|            0|  0.00%|            >>> # Network with nn.BatchNorm layer
   779|         0|            0|            0|  0.00%|            >>> module = torch.nn.Sequential(
   780|         0|            0|            0|  0.00%|            >>>            torch.nn.Linear(20, 100),
   781|         0|            0|            0|  0.00%|            >>>            torch.nn.BatchNorm1d(100),
   782|         0|            0|            0|  0.00%|            >>>          ).cuda()
   783|         0|            0|            0|  0.00%|            >>> # creating process group (optional)
   784|         0|            0|            0|  0.00%|            >>> # ranks is a list of int identifying rank ids.
   785|         0|            0|            0|  0.00%|            >>> ranks = list(range(8))
   786|         0|            0|            0|  0.00%|            >>> r1, r2 = ranks[:4], ranks[4:]
   787|         0|            0|            0|  0.00%|            >>> # Note: every rank calls into new_group for every
   788|         0|            0|            0|  0.00%|            >>> # process group created, even if that rank is not
   789|         0|            0|            0|  0.00%|            >>> # part of the group.
   790|         0|            0|            0|  0.00%|            >>> process_groups = [torch.distributed.new_group(pids) for pids in [r1, r2]]
   791|         0|            0|            0|  0.00%|            >>> process_group = process_groups[0 if dist.get_rank() <= 3 else 1]
   792|         0|            0|            0|  0.00%|            >>> sync_bn_module = torch.nn.SyncBatchNorm.convert_sync_batchnorm(module, process_group)
   793|         0|            0|            0|  0.00%|
   794|         0|            0|            0|  0.00%|        """
   795|         0|            0|            0|  0.00%|        module_output = module
   796|         0|            0|            0|  0.00%|        if isinstance(module, torch.nn.modules.batchnorm._BatchNorm):
   797|         0|            0|            0|  0.00%|            module_output = torch.nn.SyncBatchNorm(
   798|         0|            0|            0|  0.00%|                module.num_features,
   799|         0|            0|            0|  0.00%|                module.eps,
   800|         0|            0|            0|  0.00%|                module.momentum,
   801|         0|            0|            0|  0.00%|                module.affine,
   802|         0|            0|            0|  0.00%|                module.track_running_stats,
   803|         0|            0|            0|  0.00%|                process_group,
   804|         0|            0|            0|  0.00%|            )
   805|         0|            0|            0|  0.00%|            if module.affine:
   806|         0|            0|            0|  0.00%|                with torch.no_grad():
   807|         0|            0|            0|  0.00%|                    module_output.weight = module.weight
   808|         0|            0|            0|  0.00%|                    module_output.bias = module.bias
   809|         0|            0|            0|  0.00%|            module_output.running_mean = module.running_mean
   810|         0|            0|            0|  0.00%|            module_output.running_var = module.running_var
   811|         0|            0|            0|  0.00%|            module_output.num_batches_tracked = module.num_batches_tracked
   812|         0|            0|            0|  0.00%|            if hasattr(module, "qconfig"):
   813|         0|            0|            0|  0.00%|                module_output.qconfig = module.qconfig
   814|         0|            0|            0|  0.00%|        for name, child in module.named_children():
   815|         0|            0|            0|  0.00%|            module_output.add_module(
   816|         0|            0|            0|  0.00%|                name, cls.convert_sync_batchnorm(child, process_group)
   817|         0|            0|            0|  0.00%|            )
   818|         0|            0|            0|  0.00%|        del module
   819|         0|            0|            0|  0.00%|        return module_output
File: /opt/conda/lib/python3.8/site-packages/torch/optim/sgd.py
File duration: 0.339441s (0.63%)
Line #|      Hits|         Time| Time per hit|      %|Source code
------+----------+-------------+-------------+-------+-----------
     1|         0|            0|            0|  0.00%|import torch
     2|         0|            0|            0|  0.00%|from . import _functional as F
     3|         0|            0|            0|  0.00%|from .optimizer import Optimizer, required
     4|         0|            0|            0|  0.00%|
     5|         0|            0|            0|  0.00%|
     6|         0|            0|            0|  0.00%|class SGD(Optimizer):
     7|         0|            0|            0|  0.00%|    r"""Implements stochastic gradient descent (optionally with momentum).
     8|         0|            0|            0|  0.00%|
     9|         0|            0|            0|  0.00%|    Nesterov momentum is based on the formula from
    10|         0|            0|            0|  0.00%|    `On the importance of initialization and momentum in deep learning`__.
    11|         0|            0|            0|  0.00%|
    12|         0|            0|            0|  0.00%|    Args:
    13|         0|            0|            0|  0.00%|        params (iterable): iterable of parameters to optimize or dicts defining
    14|         0|            0|            0|  0.00%|            parameter groups
    15|         0|            0|            0|  0.00%|        lr (float): learning rate
    16|         0|            0|            0|  0.00%|        momentum (float, optional): momentum factor (default: 0)
    17|         0|            0|            0|  0.00%|        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)
    18|         0|            0|            0|  0.00%|        dampening (float, optional): dampening for momentum (default: 0)
    19|         0|            0|            0|  0.00%|        nesterov (bool, optional): enables Nesterov momentum (default: False)
    20|         0|            0|            0|  0.00%|
    21|         0|            0|            0|  0.00%|    Example:
    22|         0|            0|            0|  0.00%|        >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)
    23|         0|            0|            0|  0.00%|        >>> optimizer.zero_grad()
    24|         0|            0|            0|  0.00%|        >>> loss_fn(model(input), target).backward()
    25|         0|            0|            0|  0.00%|        >>> optimizer.step()
    26|         0|            0|            0|  0.00%|
    27|         0|            0|            0|  0.00%|    __ http://www.cs.toronto.edu/%7Ehinton/absps/momentum.pdf
    28|         0|            0|            0|  0.00%|
    29|         0|            0|            0|  0.00%|    .. note::
    30|         0|            0|            0|  0.00%|        The implementation of SGD with Momentum/Nesterov subtly differs from
    31|         0|            0|            0|  0.00%|        Sutskever et. al. and implementations in some other frameworks.
    32|         0|            0|            0|  0.00%|
    33|         0|            0|            0|  0.00%|        Considering the specific case of Momentum, the update can be written as
    34|         0|            0|            0|  0.00%|
    35|         0|            0|            0|  0.00%|        .. math::
    36|         0|            0|            0|  0.00%|            \begin{aligned}
    37|         0|            0|            0|  0.00%|                v_{t+1} & = \mu * v_{t} + g_{t+1}, \\
    38|         0|            0|            0|  0.00%|                p_{t+1} & = p_{t} - \text{lr} * v_{t+1},
    39|         0|            0|            0|  0.00%|            \end{aligned}
    40|         0|            0|            0|  0.00%|
    41|         0|            0|            0|  0.00%|        where :math:`p`, :math:`g`, :math:`v` and :math:`\mu` denote the
    42|         0|            0|            0|  0.00%|        parameters, gradient, velocity, and momentum respectively.
    43|         0|            0|            0|  0.00%|
    44|         0|            0|            0|  0.00%|        This is in contrast to Sutskever et. al. and
    45|         0|            0|            0|  0.00%|        other frameworks which employ an update of the form
    46|         0|            0|            0|  0.00%|
    47|         0|            0|            0|  0.00%|        .. math::
    48|         0|            0|            0|  0.00%|            \begin{aligned}
    49|         0|            0|            0|  0.00%|                v_{t+1} & = \mu * v_{t} + \text{lr} * g_{t+1}, \\
    50|         0|            0|            0|  0.00%|                p_{t+1} & = p_{t} - v_{t+1}.
    51|         0|            0|            0|  0.00%|            \end{aligned}
    52|         0|            0|            0|  0.00%|
    53|         0|            0|            0|  0.00%|        The Nesterov version is analogously modified.
    54|         0|            0|            0|  0.00%|    """
    55|         0|            0|            0|  0.00%|
    56|         0|            0|            0|  0.00%|    def __init__(self, params, lr=required, momentum=0, dampening=0,
    57|         0|            0|            0|  0.00%|                 weight_decay=0, nesterov=False):
    58|         0|            0|            0|  0.00%|        if lr is not required and lr < 0.0:
    59|         0|            0|            0|  0.00%|            raise ValueError("Invalid learning rate: {}".format(lr))
    60|         0|            0|            0|  0.00%|        if momentum < 0.0:
    61|         0|            0|            0|  0.00%|            raise ValueError("Invalid momentum value: {}".format(momentum))
    62|         0|            0|            0|  0.00%|        if weight_decay < 0.0:
    63|         0|            0|            0|  0.00%|            raise ValueError("Invalid weight_decay value: {}".format(weight_decay))
    64|         0|            0|            0|  0.00%|
    65|         0|            0|            0|  0.00%|        defaults = dict(lr=lr, momentum=momentum, dampening=dampening,
    66|         0|            0|            0|  0.00%|                        weight_decay=weight_decay, nesterov=nesterov)
    67|         0|            0|            0|  0.00%|        if nesterov and (momentum <= 0 or dampening != 0):
    68|         0|            0|            0|  0.00%|            raise ValueError("Nesterov momentum requires a momentum and zero dampening")
    69|         0|            0|            0|  0.00%|        super(SGD, self).__init__(params, defaults)
    70|         0|            0|            0|  0.00%|
    71|         0|            0|            0|  0.00%|    def __setstate__(self, state):
    72|         0|            0|            0|  0.00%|        super(SGD, self).__setstate__(state)
    73|         0|            0|            0|  0.00%|        for group in self.param_groups:
    74|         0|            0|            0|  0.00%|            group.setdefault('nesterov', False)
    75|         0|            0|            0|  0.00%|
    76|        61|  0.000808954|  1.32615e-05|  0.00%|    @torch.no_grad()
    77|         0|            0|            0|  0.00%|    def step(self, closure=None):
    78|         0|            0|            0|  0.00%|        """Performs a single optimization step.
    79|         0|            0|            0|  0.00%|
    80|         0|            0|            0|  0.00%|        Args:
    81|         0|            0|            0|  0.00%|            closure (callable, optional): A closure that reevaluates the model
    82|         0|            0|            0|  0.00%|                and returns the loss.
    83|         0|            0|            0|  0.00%|        """
    84|        61|   0.00056386|  9.24361e-06|  0.00%|        loss = None
    85|        61|  0.000395536|   6.4842e-06|  0.00%|        if closure is not None:
    86|         0|            0|            0|  0.00%|            with torch.enable_grad():
    87|         0|            0|            0|  0.00%|                loss = closure()
    88|         0|            0|            0|  0.00%|
    89|       122|  0.000728607|  5.97219e-06|  0.00%|        for group in self.param_groups:
    90|        61|  0.000371695|  6.09335e-06|  0.00%|            params_with_grad = []
    91|        61|  0.000425577|  6.97667e-06|  0.00%|            d_p_list = []
    92|        61|   0.00035429|  5.80803e-06|  0.00%|            momentum_buffer_list = []
    93|        61|   0.00036025|  5.90575e-06|  0.00%|            weight_decay = group['weight_decay']
    94|        61|  0.000357866|  5.86666e-06|  0.00%|            momentum = group['momentum']
    95|        61|  0.000340462|  5.58134e-06|  0.00%|            dampening = group['dampening']
    96|        61|   0.00037694|  6.17934e-06|  0.00%|            nesterov = group['nesterov']
    97|        61|  0.000337839|  5.53835e-06|  0.00%|            lr = group['lr']
    98|         0|            0|            0|  0.00%|
    99|      3843|    0.0180616|  4.69988e-06|  0.03%|            for p in group['params']:
   100|      3782|    0.0516684|  1.36617e-05|  0.10%|                if p.grad is not None:
(call)|      3782|    0.0821986|  2.17342e-05|  0.15%|# /opt/conda/lib/python3.8/site-packages/torch/_tensor.py:967 grad
   101|      3782|    0.0253193|   6.6947e-06|  0.05%|                    params_with_grad.append(p)
   102|      3782|    0.0569229|   1.5051e-05|  0.11%|                    d_p_list.append(p.grad)
(call)|      3782|    0.0785379|  2.07662e-05|  0.15%|# /opt/conda/lib/python3.8/site-packages/torch/_tensor.py:967 grad
   103|         0|            0|            0|  0.00%|
   104|      3782|    0.0509884|  1.34819e-05|  0.10%|                    state = self.state[p]
(call)|      3844|    0.0577645|  1.50272e-05|  0.11%|# /opt/conda/lib/python3.8/site-packages/torch/_tensor.py:615 __hash__
   105|      3782|    0.0182779|  4.83286e-06|  0.03%|                    if 'momentum_buffer' not in state:
   106|        62|  0.000414848|   6.6911e-06|  0.00%|                        momentum_buffer_list.append(None)
   107|         0|            0|            0|  0.00%|                    else:
   108|      3720|    0.0245297|    6.594e-06|  0.05%|                        momentum_buffer_list.append(state['momentum_buffer'])
   109|         0|            0|            0|  0.00%|
   110|       122|    0.0026257|  2.15222e-05|  0.00%|            F.sgd(params_with_grad,
(call)|        61|     0.806853|    0.0132271|  1.51%|# /opt/conda/lib/python3.8/site-packages/torch/optim/_functional.py:146 sgd
   111|        61|  0.000308514|   5.0576e-06|  0.00%|                  d_p_list,
   112|        61|  0.000282049|  4.62376e-06|  0.00%|                  momentum_buffer_list,
   113|        61|  0.000278473|  4.56513e-06|  0.00%|                  weight_decay=weight_decay,
   114|        61|  0.000302076|  4.95207e-06|  0.00%|                  momentum=momentum,
   115|        61|  0.000270844|  4.44006e-06|  0.00%|                  lr=lr,
   116|        61|  0.000284195|  4.65893e-06|  0.00%|                  dampening=dampening,
   117|        61|  0.000274658|  4.50259e-06|  0.00%|                  nesterov=nesterov)
   118|         0|            0|            0|  0.00%|
   119|         0|            0|            0|  0.00%|            # update momentum_buffers in state
   120|      3843|     0.017483|  4.54931e-06|  0.03%|            for p, momentum_buffer in zip(params_with_grad, momentum_buffer_list):
   121|      3782|    0.0479531|  1.26793e-05|  0.09%|                state = self.state[p]
(call)|      3782|    0.0571036|  1.50988e-05|  0.11%|# /opt/conda/lib/python3.8/site-packages/torch/_tensor.py:615 __hash__
   122|      3782|    0.0174787|  4.62155e-06|  0.03%|                state['momentum_buffer'] = momentum_buffer
   123|         0|            0|            0|  0.00%|
   124|        61|  0.000294447|    4.827e-06|  0.00%|        return loss
File: /opt/conda/lib/python3.8/site-packages/torchvision/models/resnet.py
File duration: 0.306095s (0.57%)
Line #|      Hits|         Time| Time per hit|      %|Source code
------+----------+-------------+-------------+-------+-----------
     1|         0|            0|            0|  0.00%|import torch
     2|         0|            0|            0|  0.00%|from torch import Tensor
     3|         0|            0|            0|  0.00%|import torch.nn as nn
     4|         0|            0|            0|  0.00%|from .utils import load_state_dict_from_url
     5|         0|            0|            0|  0.00%|from typing import Type, Any, Callable, Union, List, Optional
     6|         0|            0|            0|  0.00%|
     7|         0|            0|            0|  0.00%|
     8|         0|            0|            0|  0.00%|__all__ = ['ResNet', 'resnet18', 'resnet34', 'resnet50', 'resnet101',
     9|         0|            0|            0|  0.00%|           'resnet152', 'resnext50_32x4d', 'resnext101_32x8d',
    10|         0|            0|            0|  0.00%|           'wide_resnet50_2', 'wide_resnet101_2']
    11|         0|            0|            0|  0.00%|
    12|         0|            0|            0|  0.00%|
    13|         0|            0|            0|  0.00%|model_urls = {
    14|         0|            0|            0|  0.00%|    'resnet18': 'https://download.pytorch.org/models/resnet18-f37072fd.pth',
    15|         0|            0|            0|  0.00%|    'resnet34': 'https://download.pytorch.org/models/resnet34-b627a593.pth',
    16|         0|            0|            0|  0.00%|    'resnet50': 'https://download.pytorch.org/models/resnet50-0676ba61.pth',
    17|         0|            0|            0|  0.00%|    'resnet101': 'https://download.pytorch.org/models/resnet101-63fe2227.pth',
    18|         0|            0|            0|  0.00%|    'resnet152': 'https://download.pytorch.org/models/resnet152-394f9c45.pth',
    19|         0|            0|            0|  0.00%|    'resnext50_32x4d': 'https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth',
    20|         0|            0|            0|  0.00%|    'resnext101_32x8d': 'https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth',
    21|         0|            0|            0|  0.00%|    'wide_resnet50_2': 'https://download.pytorch.org/models/wide_resnet50_2-95faca4d.pth',
    22|         0|            0|            0|  0.00%|    'wide_resnet101_2': 'https://download.pytorch.org/models/wide_resnet101_2-32ee1156.pth',
    23|         0|            0|            0|  0.00%|}
    24|         0|            0|            0|  0.00%|
    25|         0|            0|            0|  0.00%|
    26|         0|            0|            0|  0.00%|def conv3x3(in_planes: int, out_planes: int, stride: int = 1, groups: int = 1, dilation: int = 1) -> nn.Conv2d:
    27|         0|            0|            0|  0.00%|    """3x3 convolution with padding"""
    28|         0|            0|            0|  0.00%|    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,
    29|         0|            0|            0|  0.00%|                     padding=dilation, groups=groups, bias=False, dilation=dilation)
    30|         0|            0|            0|  0.00%|
    31|         0|            0|            0|  0.00%|
    32|         0|            0|            0|  0.00%|def conv1x1(in_planes: int, out_planes: int, stride: int = 1) -> nn.Conv2d:
    33|         0|            0|            0|  0.00%|    """1x1 convolution"""
    34|         0|            0|            0|  0.00%|    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)
    35|         0|            0|            0|  0.00%|
    36|         0|            0|            0|  0.00%|
    37|         0|            0|            0|  0.00%|class BasicBlock(nn.Module):
    38|         0|            0|            0|  0.00%|    expansion: int = 1
    39|         0|            0|            0|  0.00%|
    40|         0|            0|            0|  0.00%|    def __init__(
    41|         0|            0|            0|  0.00%|        self,
    42|         0|            0|            0|  0.00%|        inplanes: int,
    43|         0|            0|            0|  0.00%|        planes: int,
    44|         0|            0|            0|  0.00%|        stride: int = 1,
    45|         0|            0|            0|  0.00%|        downsample: Optional[nn.Module] = None,
    46|         0|            0|            0|  0.00%|        groups: int = 1,
    47|         0|            0|            0|  0.00%|        base_width: int = 64,
    48|         0|            0|            0|  0.00%|        dilation: int = 1,
    49|         0|            0|            0|  0.00%|        norm_layer: Optional[Callable[..., nn.Module]] = None
    50|         0|            0|            0|  0.00%|    ) -> None:
    51|         0|            0|            0|  0.00%|        super(BasicBlock, self).__init__()
    52|         0|            0|            0|  0.00%|        if norm_layer is None:
    53|         0|            0|            0|  0.00%|            norm_layer = nn.BatchNorm2d
    54|         0|            0|            0|  0.00%|        if groups != 1 or base_width != 64:
    55|         0|            0|            0|  0.00%|            raise ValueError('BasicBlock only supports groups=1 and base_width=64')
    56|         0|            0|            0|  0.00%|        if dilation > 1:
    57|         0|            0|            0|  0.00%|            raise NotImplementedError("Dilation > 1 not supported in BasicBlock")
    58|         0|            0|            0|  0.00%|        # Both self.conv1 and self.downsample layers downsample the input when stride != 1
    59|         0|            0|            0|  0.00%|        self.conv1 = conv3x3(inplanes, planes, stride)
    60|         0|            0|            0|  0.00%|        self.bn1 = norm_layer(planes)
    61|         0|            0|            0|  0.00%|        self.relu = nn.ReLU(inplace=True)
    62|         0|            0|            0|  0.00%|        self.conv2 = conv3x3(planes, planes)
    63|         0|            0|            0|  0.00%|        self.bn2 = norm_layer(planes)
    64|         0|            0|            0|  0.00%|        self.downsample = downsample
    65|         0|            0|            0|  0.00%|        self.stride = stride
    66|         0|            0|            0|  0.00%|
    67|       800|    0.0043335|  5.41687e-06|  0.01%|    def forward(self, x: Tensor) -> Tensor:
    68|       800|   0.00368357|  4.60446e-06|  0.01%|        identity = x
    69|         0|            0|            0|  0.00%|
    70|       800|    0.0229213|  2.86517e-05|  0.04%|        out = self.conv1(x)
(call)|       800|    0.0403075|  5.03844e-05|  0.08%|# /opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1117 __getattr__
(call)|       800|      4.75285|   0.00594106|  8.87%|# /opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1045 _call_impl
    71|       800|    0.0272744|   3.4093e-05|  0.05%|        out = self.bn1(out)
(call)|       800|    0.0411639|  5.14549e-05|  0.08%|# /opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1117 __getattr__
(call)|       800|     0.932034|   0.00116504|  1.74%|# /opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1045 _call_impl
    72|       800|    0.0246735|  3.08418e-05|  0.05%|        out = self.relu(out)
(call)|       800|    0.0402718|  5.03397e-05|  0.08%|# /opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1117 __getattr__
(call)|       800|     0.148202|  0.000185252|  0.28%|# /opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1045 _call_impl
    73|         0|            0|            0|  0.00%|
    74|       800|    0.0261629|  3.27036e-05|  0.05%|        out = self.conv2(out)
(call)|       800|    0.0408461|  5.10576e-05|  0.08%|# /opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1117 __getattr__
(call)|       800|      5.71105|   0.00713881| 10.66%|# /opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1045 _call_impl
    75|       800|    0.0281622|  3.52028e-05|  0.05%|        out = self.bn2(out)
(call)|       800|    0.0415883|  5.19854e-05|  0.08%|# /opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1117 __getattr__
(call)|       800|     0.922436|   0.00115305|  1.72%|# /opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1045 _call_impl
    76|         0|            0|            0|  0.00%|
    77|       800|   0.00861049|  1.07631e-05|  0.02%|        if self.downsample is not None:
(call)|       300|      0.01495|  4.98335e-05|  0.03%|# /opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1117 __getattr__
    78|       300|   0.00800586|  2.66862e-05|  0.01%|            identity = self.downsample(x)
(call)|       300|    0.0140615|  4.68715e-05|  0.03%|# /opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1117 __getattr__
(call)|       300|     0.645009|   0.00215003|  1.20%|# /opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1045 _call_impl
    79|         0|            0|            0|  0.00%|
    80|       800|    0.0762622|  9.53278e-05|  0.14%|        out += identity
    81|       800|    0.0286634|  3.58292e-05|  0.05%|        out = self.relu(out)
(call)|       800|    0.0420821|  5.26026e-05|  0.08%|# /opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1117 __getattr__
(call)|       800|     0.129801|  0.000162251|  0.24%|# /opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1045 _call_impl
    82|         0|            0|            0|  0.00%|
    83|       800|    0.0036366|  4.54575e-06|  0.01%|        return out
    84|         0|            0|            0|  0.00%|
    85|         0|            0|            0|  0.00%|
    86|         0|            0|            0|  0.00%|class Bottleneck(nn.Module):
    87|         0|            0|            0|  0.00%|    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)
    88|         0|            0|            0|  0.00%|    # while original implementation places the stride at the first 1x1 convolution(self.conv1)
    89|         0|            0|            0|  0.00%|    # according to "Deep residual learning for image recognition"https://arxiv.org/abs/1512.03385.
    90|         0|            0|            0|  0.00%|    # This variant is also known as ResNet V1.5 and improves accuracy according to
    91|         0|            0|            0|  0.00%|    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.
    92|         0|            0|            0|  0.00%|
    93|         0|            0|            0|  0.00%|    expansion: int = 4
    94|         0|            0|            0|  0.00%|
    95|         0|            0|            0|  0.00%|    def __init__(
    96|         0|            0|            0|  0.00%|        self,
    97|         0|            0|            0|  0.00%|        inplanes: int,
    98|         0|            0|            0|  0.00%|        planes: int,
    99|         0|            0|            0|  0.00%|        stride: int = 1,
   100|         0|            0|            0|  0.00%|        downsample: Optional[nn.Module] = None,
   101|         0|            0|            0|  0.00%|        groups: int = 1,
   102|         0|            0|            0|  0.00%|        base_width: int = 64,
   103|         0|            0|            0|  0.00%|        dilation: int = 1,
   104|         0|            0|            0|  0.00%|        norm_layer: Optional[Callable[..., nn.Module]] = None
   105|         0|            0|            0|  0.00%|    ) -> None:
   106|         0|            0|            0|  0.00%|        super(Bottleneck, self).__init__()
   107|         0|            0|            0|  0.00%|        if norm_layer is None:
   108|         0|            0|            0|  0.00%|            norm_layer = nn.BatchNorm2d
   109|         0|            0|            0|  0.00%|        width = int(planes * (base_width / 64.)) * groups
   110|         0|            0|            0|  0.00%|        # Both self.conv2 and self.downsample layers downsample the input when stride != 1
   111|         0|            0|            0|  0.00%|        self.conv1 = conv1x1(inplanes, width)
   112|         0|            0|            0|  0.00%|        self.bn1 = norm_layer(width)
   113|         0|            0|            0|  0.00%|        self.conv2 = conv3x3(width, width, stride, groups, dilation)
   114|         0|            0|            0|  0.00%|        self.bn2 = norm_layer(width)
   115|         0|            0|            0|  0.00%|        self.conv3 = conv1x1(width, planes * self.expansion)
   116|         0|            0|            0|  0.00%|        self.bn3 = norm_layer(planes * self.expansion)
   117|         0|            0|            0|  0.00%|        self.relu = nn.ReLU(inplace=True)
   118|         0|            0|            0|  0.00%|        self.downsample = downsample
   119|         0|            0|            0|  0.00%|        self.stride = stride
   120|         0|            0|            0|  0.00%|
   121|         0|            0|            0|  0.00%|    def forward(self, x: Tensor) -> Tensor:
   122|         0|            0|            0|  0.00%|        identity = x
   123|         0|            0|            0|  0.00%|
   124|         0|            0|            0|  0.00%|        out = self.conv1(x)
   125|         0|            0|            0|  0.00%|        out = self.bn1(out)
   126|         0|            0|            0|  0.00%|        out = self.relu(out)
   127|         0|            0|            0|  0.00%|
   128|         0|            0|            0|  0.00%|        out = self.conv2(out)
   129|         0|            0|            0|  0.00%|        out = self.bn2(out)
   130|         0|            0|            0|  0.00%|        out = self.relu(out)
   131|         0|            0|            0|  0.00%|
   132|         0|            0|            0|  0.00%|        out = self.conv3(out)
   133|         0|            0|            0|  0.00%|        out = self.bn3(out)
   134|         0|            0|            0|  0.00%|
   135|         0|            0|            0|  0.00%|        if self.downsample is not None:
   136|         0|            0|            0|  0.00%|            identity = self.downsample(x)
   137|         0|            0|            0|  0.00%|
   138|         0|            0|            0|  0.00%|        out += identity
   139|         0|            0|            0|  0.00%|        out = self.relu(out)
   140|         0|            0|            0|  0.00%|
   141|         0|            0|            0|  0.00%|        return out
   142|         0|            0|            0|  0.00%|
   143|         0|            0|            0|  0.00%|
   144|         0|            0|            0|  0.00%|class ResNet(nn.Module):
   145|         0|            0|            0|  0.00%|
   146|         0|            0|            0|  0.00%|    def __init__(
   147|         0|            0|            0|  0.00%|        self,
   148|         0|            0|            0|  0.00%|        block: Type[Union[BasicBlock, Bottleneck]],
   149|         0|            0|            0|  0.00%|        layers: List[int],
   150|         0|            0|            0|  0.00%|        num_classes: int = 1000,
   151|         0|            0|            0|  0.00%|        zero_init_residual: bool = False,
   152|         0|            0|            0|  0.00%|        groups: int = 1,
   153|         0|            0|            0|  0.00%|        width_per_group: int = 64,
   154|         0|            0|            0|  0.00%|        replace_stride_with_dilation: Optional[List[bool]] = None,
   155|         0|            0|            0|  0.00%|        norm_layer: Optional[Callable[..., nn.Module]] = None
   156|         0|            0|            0|  0.00%|    ) -> None:
   157|         0|            0|            0|  0.00%|        super(ResNet, self).__init__()
   158|         0|            0|            0|  0.00%|        if norm_layer is None:
   159|         0|            0|            0|  0.00%|            norm_layer = nn.BatchNorm2d
   160|         0|            0|            0|  0.00%|        self._norm_layer = norm_layer
   161|         0|            0|            0|  0.00%|
   162|         0|            0|            0|  0.00%|        self.inplanes = 64
   163|         0|            0|            0|  0.00%|        self.dilation = 1
   164|         0|            0|            0|  0.00%|        if replace_stride_with_dilation is None:
   165|         0|            0|            0|  0.00%|            # each element in the tuple indicates if we should replace
   166|         0|            0|            0|  0.00%|            # the 2x2 stride with a dilated convolution instead
   167|         0|            0|            0|  0.00%|            replace_stride_with_dilation = [False, False, False]
   168|         0|            0|            0|  0.00%|        if len(replace_stride_with_dilation) != 3:
   169|         0|            0|            0|  0.00%|            raise ValueError("replace_stride_with_dilation should be None "
   170|         0|            0|            0|  0.00%|                             "or a 3-element tuple, got {}".format(replace_stride_with_dilation))
   171|         0|            0|            0|  0.00%|        self.groups = groups
   172|         0|            0|            0|  0.00%|        self.base_width = width_per_group
   173|         0|            0|            0|  0.00%|        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,
   174|         0|            0|            0|  0.00%|                               bias=False)
   175|         0|            0|            0|  0.00%|        self.bn1 = norm_layer(self.inplanes)
   176|         0|            0|            0|  0.00%|        self.relu = nn.ReLU(inplace=True)
   177|         0|            0|            0|  0.00%|        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
   178|         0|            0|            0|  0.00%|        self.layer1 = self._make_layer(block, 64, layers[0])
   179|         0|            0|            0|  0.00%|        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,
   180|         0|            0|            0|  0.00%|                                       dilate=replace_stride_with_dilation[0])
   181|         0|            0|            0|  0.00%|        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,
   182|         0|            0|            0|  0.00%|                                       dilate=replace_stride_with_dilation[1])
   183|         0|            0|            0|  0.00%|        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,
   184|         0|            0|            0|  0.00%|                                       dilate=replace_stride_with_dilation[2])
   185|         0|            0|            0|  0.00%|        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
   186|         0|            0|            0|  0.00%|        self.fc = nn.Linear(512 * block.expansion, num_classes)
   187|         0|            0|            0|  0.00%|
   188|         0|            0|            0|  0.00%|        for m in self.modules():
   189|         0|            0|            0|  0.00%|            if isinstance(m, nn.Conv2d):
   190|         0|            0|            0|  0.00%|                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
   191|         0|            0|            0|  0.00%|            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):
   192|         0|            0|            0|  0.00%|                nn.init.constant_(m.weight, 1)
   193|         0|            0|            0|  0.00%|                nn.init.constant_(m.bias, 0)
   194|         0|            0|            0|  0.00%|
   195|         0|            0|            0|  0.00%|        # Zero-initialize the last BN in each residual branch,
   196|         0|            0|            0|  0.00%|        # so that the residual branch starts with zeros, and each residual block behaves like an identity.
   197|         0|            0|            0|  0.00%|        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677
   198|         0|            0|            0|  0.00%|        if zero_init_residual:
   199|         0|            0|            0|  0.00%|            for m in self.modules():
   200|         0|            0|            0|  0.00%|                if isinstance(m, Bottleneck):
   201|         0|            0|            0|  0.00%|                    nn.init.constant_(m.bn3.weight, 0)  # type: ignore[arg-type]
   202|         0|            0|            0|  0.00%|                elif isinstance(m, BasicBlock):
   203|         0|            0|            0|  0.00%|                    nn.init.constant_(m.bn2.weight, 0)  # type: ignore[arg-type]
   204|         0|            0|            0|  0.00%|
   205|         0|            0|            0|  0.00%|    def _make_layer(self, block: Type[Union[BasicBlock, Bottleneck]], planes: int, blocks: int,
   206|         0|            0|            0|  0.00%|                    stride: int = 1, dilate: bool = False) -> nn.Sequential:
   207|         0|            0|            0|  0.00%|        norm_layer = self._norm_layer
   208|         0|            0|            0|  0.00%|        downsample = None
   209|         0|            0|            0|  0.00%|        previous_dilation = self.dilation
   210|         0|            0|            0|  0.00%|        if dilate:
   211|         0|            0|            0|  0.00%|            self.dilation *= stride
   212|         0|            0|            0|  0.00%|            stride = 1
   213|         0|            0|            0|  0.00%|        if stride != 1 or self.inplanes != planes * block.expansion:
   214|         0|            0|            0|  0.00%|            downsample = nn.Sequential(
   215|         0|            0|            0|  0.00%|                conv1x1(self.inplanes, planes * block.expansion, stride),
   216|         0|            0|            0|  0.00%|                norm_layer(planes * block.expansion),
   217|         0|            0|            0|  0.00%|            )
   218|         0|            0|            0|  0.00%|
   219|         0|            0|            0|  0.00%|        layers = []
   220|         0|            0|            0|  0.00%|        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,
   221|         0|            0|            0|  0.00%|                            self.base_width, previous_dilation, norm_layer))
   222|         0|            0|            0|  0.00%|        self.inplanes = planes * block.expansion
   223|         0|            0|            0|  0.00%|        for _ in range(1, blocks):
   224|         0|            0|            0|  0.00%|            layers.append(block(self.inplanes, planes, groups=self.groups,
   225|         0|            0|            0|  0.00%|                                base_width=self.base_width, dilation=self.dilation,
   226|         0|            0|            0|  0.00%|                                norm_layer=norm_layer))
   227|         0|            0|            0|  0.00%|
   228|         0|            0|            0|  0.00%|        return nn.Sequential(*layers)
   229|         0|            0|            0|  0.00%|
   230|       100|  0.000591278|  5.91278e-06|  0.00%|    def _forward_impl(self, x: Tensor) -> Tensor:
   231|         0|            0|            0|  0.00%|        # See note [TorchScript super()]
   232|       100|   0.00408649|  4.08649e-05|  0.01%|        x = self.conv1(x)
(call)|       100|   0.00580454|  5.80454e-05|  0.01%|# /opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1117 __getattr__
(call)|       100|     0.941341|   0.00941341|  1.76%|# /opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1045 _call_impl
   233|       100|   0.00438666|  4.38666e-05|  0.01%|        x = self.bn1(x)
(call)|       100|   0.00566721|  5.66721e-05|  0.01%|# /opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1117 __getattr__
(call)|       100|     0.369097|   0.00369097|  0.69%|# /opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1045 _call_impl
   234|       100|   0.00358987|  3.58987e-05|  0.01%|        x = self.relu(x)
(call)|       100|   0.00538921|  5.38921e-05|  0.01%|# /opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1117 __getattr__
(call)|       100|    0.0521073|  0.000521073|  0.10%|# /opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1045 _call_impl
   235|       100|   0.00355768|  3.55768e-05|  0.01%|        x = self.maxpool(x)
(call)|       100|   0.00517464|  5.17464e-05|  0.01%|# /opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1117 __getattr__
(call)|       100|     0.717541|   0.00717541|  1.34%|# /opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1045 _call_impl
   236|         0|            0|            0|  0.00%|
   237|       100|   0.00383902|  3.83902e-05|  0.01%|        x = self.layer1(x)
(call)|       100|   0.00533509|  5.33509e-05|  0.01%|# /opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1117 __getattr__
(call)|       100|      3.47732|    0.0347732|  6.49%|# /opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1045 _call_impl
   238|       100|    0.0034616|   3.4616e-05|  0.01%|        x = self.layer2(x)
(call)|       100|   0.00521374|  5.21374e-05|  0.01%|# /opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1117 __getattr__
(call)|       100|      3.23573|    0.0323573|  6.04%|# /opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1045 _call_impl
   239|       100|   0.00331426|  3.31426e-05|  0.01%|        x = self.layer3(x)
(call)|       100|    0.0052321|   5.2321e-05|  0.01%|# /opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1117 __getattr__
(call)|       100|      3.30774|    0.0330774|  6.17%|# /opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1045 _call_impl
   240|       100|   0.00349212|  3.49212e-05|  0.01%|        x = self.layer4(x)
(call)|       100|   0.00523019|  5.23019e-05|  0.01%|# /opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1117 __getattr__
(call)|       100|      3.86981|    0.0386981|  7.22%|# /opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1045 _call_impl
   241|         0|            0|            0|  0.00%|
   242|       100|   0.00338578|  3.38578e-05|  0.01%|        x = self.avgpool(x)
(call)|       100|   0.00524235|  5.24235e-05|  0.01%|# /opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1117 __getattr__
(call)|       100|     0.030303|   0.00030303|  0.06%|# /opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1045 _call_impl
   243|       100|   0.00341725|  3.41725e-05|  0.01%|        x = torch.flatten(x, 1)
   244|       100|   0.00354743|  3.54743e-05|  0.01%|        x = self.fc(x)
(call)|       100|   0.00520682|  5.20682e-05|  0.01%|# /opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1117 __getattr__
(call)|       100|    0.0270321|  0.000270321|  0.05%|# /opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1045 _call_impl
   245|         0|            0|            0|  0.00%|
   246|       100|    0.0005126|    5.126e-06|  0.00%|        return x
   247|         0|            0|            0|  0.00%|
   248|       100|  0.000661612|  6.61612e-06|  0.00%|    def forward(self, x: Tensor) -> Tensor:
   249|       100|   0.00186181|  1.86181e-05|  0.00%|        return self._forward_impl(x)
(call)|       100|      16.1227|     0.161227| 30.09%|# /opt/conda/lib/python3.8/site-packages/torchvision/models/resnet.py:230 _forward_impl
   250|         0|            0|            0|  0.00%|
   251|         0|            0|            0|  0.00%|
   252|         0|            0|            0|  0.00%|def _resnet(
   253|         0|            0|            0|  0.00%|    arch: str,
   254|         0|            0|            0|  0.00%|    block: Type[Union[BasicBlock, Bottleneck]],
   255|         0|            0|            0|  0.00%|    layers: List[int],
   256|         0|            0|            0|  0.00%|    pretrained: bool,
   257|         0|            0|            0|  0.00%|    progress: bool,
   258|         0|            0|            0|  0.00%|    **kwargs: Any
   259|         0|            0|            0|  0.00%|) -> ResNet:
   260|         0|            0|            0|  0.00%|    model = ResNet(block, layers, **kwargs)
   261|         0|            0|            0|  0.00%|    if pretrained:
   262|         0|            0|            0|  0.00%|        state_dict = load_state_dict_from_url(model_urls[arch],
   263|         0|            0|            0|  0.00%|                                              progress=progress)
   264|         0|            0|            0|  0.00%|        model.load_state_dict(state_dict)
   265|         0|            0|            0|  0.00%|    return model
   266|         0|            0|            0|  0.00%|
   267|         0|            0|            0|  0.00%|
   268|         0|            0|            0|  0.00%|def resnet18(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:
   269|         0|            0|            0|  0.00%|    r"""ResNet-18 model from
   270|         0|            0|            0|  0.00%|    `"Deep Residual Learning for Image Recognition" <https://arxiv.org/pdf/1512.03385.pdf>`_.
   271|         0|            0|            0|  0.00%|
   272|         0|            0|            0|  0.00%|    Args:
   273|         0|            0|            0|  0.00%|        pretrained (bool): If True, returns a model pre-trained on ImageNet
   274|         0|            0|            0|  0.00%|        progress (bool): If True, displays a progress bar of the download to stderr
   275|         0|            0|            0|  0.00%|    """
   276|         0|            0|            0|  0.00%|    return _resnet('resnet18', BasicBlock, [2, 2, 2, 2], pretrained, progress,
   277|         0|            0|            0|  0.00%|                   **kwargs)
   278|         0|            0|            0|  0.00%|
   279|         0|            0|            0|  0.00%|
   280|         0|            0|            0|  0.00%|def resnet34(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:
   281|         0|            0|            0|  0.00%|    r"""ResNet-34 model from
   282|         0|            0|            0|  0.00%|    `"Deep Residual Learning for Image Recognition" <https://arxiv.org/pdf/1512.03385.pdf>`_.
   283|         0|            0|            0|  0.00%|
   284|         0|            0|            0|  0.00%|    Args:
   285|         0|            0|            0|  0.00%|        pretrained (bool): If True, returns a model pre-trained on ImageNet
   286|         0|            0|            0|  0.00%|        progress (bool): If True, displays a progress bar of the download to stderr
   287|         0|            0|            0|  0.00%|    """
   288|         0|            0|            0|  0.00%|    return _resnet('resnet34', BasicBlock, [3, 4, 6, 3], pretrained, progress,
   289|         0|            0|            0|  0.00%|                   **kwargs)
   290|         0|            0|            0|  0.00%|
   291|         0|            0|            0|  0.00%|
   292|         0|            0|            0|  0.00%|def resnet50(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:
   293|         0|            0|            0|  0.00%|    r"""ResNet-50 model from
   294|         0|            0|            0|  0.00%|    `"Deep Residual Learning for Image Recognition" <https://arxiv.org/pdf/1512.03385.pdf>`_.
   295|         0|            0|            0|  0.00%|
   296|         0|            0|            0|  0.00%|    Args:
   297|         0|            0|            0|  0.00%|        pretrained (bool): If True, returns a model pre-trained on ImageNet
   298|         0|            0|            0|  0.00%|        progress (bool): If True, displays a progress bar of the download to stderr
   299|         0|            0|            0|  0.00%|    """
   300|         0|            0|            0|  0.00%|    return _resnet('resnet50', Bottleneck, [3, 4, 6, 3], pretrained, progress,
   301|         0|            0|            0|  0.00%|                   **kwargs)
   302|         0|            0|            0|  0.00%|
   303|         0|            0|            0|  0.00%|
   304|         0|            0|            0|  0.00%|def resnet101(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:
   305|         0|            0|            0|  0.00%|    r"""ResNet-101 model from
   306|         0|            0|            0|  0.00%|    `"Deep Residual Learning for Image Recognition" <https://arxiv.org/pdf/1512.03385.pdf>`_.
   307|         0|            0|            0|  0.00%|
   308|         0|            0|            0|  0.00%|    Args:
   309|         0|            0|            0|  0.00%|        pretrained (bool): If True, returns a model pre-trained on ImageNet
   310|         0|            0|            0|  0.00%|        progress (bool): If True, displays a progress bar of the download to stderr
   311|         0|            0|            0|  0.00%|    """
   312|         0|            0|            0|  0.00%|    return _resnet('resnet101', Bottleneck, [3, 4, 23, 3], pretrained, progress,
   313|         0|            0|            0|  0.00%|                   **kwargs)
   314|         0|            0|            0|  0.00%|
   315|         0|            0|            0|  0.00%|
   316|         0|            0|            0|  0.00%|def resnet152(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:
   317|         0|            0|            0|  0.00%|    r"""ResNet-152 model from
   318|         0|            0|            0|  0.00%|    `"Deep Residual Learning for Image Recognition" <https://arxiv.org/pdf/1512.03385.pdf>`_.
   319|         0|            0|            0|  0.00%|
   320|         0|            0|            0|  0.00%|    Args:
   321|         0|            0|            0|  0.00%|        pretrained (bool): If True, returns a model pre-trained on ImageNet
   322|         0|            0|            0|  0.00%|        progress (bool): If True, displays a progress bar of the download to stderr
   323|         0|            0|            0|  0.00%|    """
   324|         0|            0|            0|  0.00%|    return _resnet('resnet152', Bottleneck, [3, 8, 36, 3], pretrained, progress,
   325|         0|            0|            0|  0.00%|                   **kwargs)
   326|         0|            0|            0|  0.00%|
   327|         0|            0|            0|  0.00%|
   328|         0|            0|            0|  0.00%|def resnext50_32x4d(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:
   329|         0|            0|            0|  0.00%|    r"""ResNeXt-50 32x4d model from
   330|         0|            0|            0|  0.00%|    `"Aggregated Residual Transformation for Deep Neural Networks" <https://arxiv.org/pdf/1611.05431.pdf>`_.
   331|         0|            0|            0|  0.00%|
   332|         0|            0|            0|  0.00%|    Args:
   333|         0|            0|            0|  0.00%|        pretrained (bool): If True, returns a model pre-trained on ImageNet
   334|         0|            0|            0|  0.00%|        progress (bool): If True, displays a progress bar of the download to stderr
   335|         0|            0|            0|  0.00%|    """
   336|         0|            0|            0|  0.00%|    kwargs['groups'] = 32
   337|         0|            0|            0|  0.00%|    kwargs['width_per_group'] = 4
   338|         0|            0|            0|  0.00%|    return _resnet('resnext50_32x4d', Bottleneck, [3, 4, 6, 3],
   339|         0|            0|            0|  0.00%|                   pretrained, progress, **kwargs)
   340|         0|            0|            0|  0.00%|
   341|         0|            0|            0|  0.00%|
   342|         0|            0|            0|  0.00%|def resnext101_32x8d(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:
   343|         0|            0|            0|  0.00%|    r"""ResNeXt-101 32x8d model from
   344|         0|            0|            0|  0.00%|    `"Aggregated Residual Transformation for Deep Neural Networks" <https://arxiv.org/pdf/1611.05431.pdf>`_.
   345|         0|            0|            0|  0.00%|
   346|         0|            0|            0|  0.00%|    Args:
   347|         0|            0|            0|  0.00%|        pretrained (bool): If True, returns a model pre-trained on ImageNet
   348|         0|            0|            0|  0.00%|        progress (bool): If True, displays a progress bar of the download to stderr
   349|         0|            0|            0|  0.00%|    """
   350|         0|            0|            0|  0.00%|    kwargs['groups'] = 32
   351|         0|            0|            0|  0.00%|    kwargs['width_per_group'] = 8
   352|         0|            0|            0|  0.00%|    return _resnet('resnext101_32x8d', Bottleneck, [3, 4, 23, 3],
   353|         0|            0|            0|  0.00%|                   pretrained, progress, **kwargs)
   354|         0|            0|            0|  0.00%|
   355|         0|            0|            0|  0.00%|
   356|         0|            0|            0|  0.00%|def wide_resnet50_2(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:
   357|         0|            0|            0|  0.00%|    r"""Wide ResNet-50-2 model from
   358|         0|            0|            0|  0.00%|    `"Wide Residual Networks" <https://arxiv.org/pdf/1605.07146.pdf>`_.
   359|         0|            0|            0|  0.00%|
   360|         0|            0|            0|  0.00%|    The model is the same as ResNet except for the bottleneck number of channels
   361|         0|            0|            0|  0.00%|    which is twice larger in every block. The number of channels in outer 1x1
   362|         0|            0|            0|  0.00%|    convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048
   363|         0|            0|            0|  0.00%|    channels, and in Wide ResNet-50-2 has 2048-1024-2048.
   364|         0|            0|            0|  0.00%|
   365|         0|            0|            0|  0.00%|    Args:
   366|         0|            0|            0|  0.00%|        pretrained (bool): If True, returns a model pre-trained on ImageNet
   367|         0|            0|            0|  0.00%|        progress (bool): If True, displays a progress bar of the download to stderr
   368|         0|            0|            0|  0.00%|    """
   369|         0|            0|            0|  0.00%|    kwargs['width_per_group'] = 64 * 2
   370|         0|            0|            0|  0.00%|    return _resnet('wide_resnet50_2', Bottleneck, [3, 4, 6, 3],
   371|         0|            0|            0|  0.00%|                   pretrained, progress, **kwargs)
   372|         0|            0|            0|  0.00%|
   373|         0|            0|            0|  0.00%|
   374|         0|            0|            0|  0.00%|def wide_resnet101_2(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:
   375|         0|            0|            0|  0.00%|    r"""Wide ResNet-101-2 model from
   376|         0|            0|            0|  0.00%|    `"Wide Residual Networks" <https://arxiv.org/pdf/1605.07146.pdf>`_.
   377|         0|            0|            0|  0.00%|
   378|         0|            0|            0|  0.00%|    The model is the same as ResNet except for the bottleneck number of channels
   379|         0|            0|            0|  0.00%|    which is twice larger in every block. The number of channels in outer 1x1
   380|         0|            0|            0|  0.00%|    convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048
   381|         0|            0|            0|  0.00%|    channels, and in Wide ResNet-50-2 has 2048-1024-2048.
   382|         0|            0|            0|  0.00%|
   383|         0|            0|            0|  0.00%|    Args:
   384|         0|            0|            0|  0.00%|        pretrained (bool): If True, returns a model pre-trained on ImageNet
   385|         0|            0|            0|  0.00%|        progress (bool): If True, displays a progress bar of the download to stderr
   386|         0|            0|            0|  0.00%|    """
   387|         0|            0|            0|  0.00%|    kwargs['width_per_group'] = 64 * 2
   388|         0|            0|            0|  0.00%|    return _resnet('wide_resnet101_2', Bottleneck, [3, 4, 23, 3],
   389|         0|            0|            0|  0.00%|                   pretrained, progress, **kwargs)
File: main.py
File duration: 0.225292s (0.42%)
Line #|      Hits|         Time| Time per hit|      %|Source code
------+----------+-------------+-------------+-------+-----------
     1|         0|            0|            0|  0.00%|# Source: https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html
     2|         0|            0|            0|  0.00%|# --WITH CHANGES
     3|         0|            0|            0|  0.00%|
     4|         0|            0|            0|  0.00%|# License: BSD
     5|         0|            0|            0|  0.00%|# Author: Sasank Chilamkurthy
     6|         0|            0|            0|  0.00%|
     7|         0|            0|            0|  0.00%|from __future__ import print_function, division
     8|         0|            0|            0|  0.00%|import realtime_monitoring
     9|         0|            0|            0|  0.00%|import torch
    10|         0|            0|            0|  0.00%|import rapl
    11|         0|            0|            0|  0.00%|from memory_profiler import profile
    12|         0|            0|            0|  0.00%|import io
    13|         0|            0|            0|  0.00%|import pprofile
    14|         0|            0|            0|  0.00%|import cProfile
    15|         0|            0|            0|  0.00%|import pstats
    16|         0|            0|            0|  0.00%|import sys
    17|         0|            0|            0|  0.00%|import pyRAPL
    18|         0|            0|            0|  0.00%|import psutil
    19|         0|            0|            0|  0.00%|import pandas as pd
    20|         0|            0|            0|  0.00%|import zipfile
    21|         0|            0|            0|  0.00%|import torch.nn as nn
    22|         0|            0|            0|  0.00%|import torch.optim as optim
    23|         0|            0|            0|  0.00%|from torch.optim import lr_scheduler
    24|         0|            0|            0|  0.00%|import numpy as np
    25|         0|            0|            0|  0.00%|import torchvision
    26|         0|            0|            0|  0.00%|from torchvision import datasets, models, transforms
    27|         0|            0|            0|  0.00%|import matplotlib.pyplot as plt
    28|         0|            0|            0|  0.00%|import time
    29|         0|            0|            0|  0.00%|import os
    30|         0|            0|            0|  0.00%|from torch.nn.parallel import DistributedDataParallel as DDP
    31|         0|            0|            0|  0.00%|import torch.multiprocessing as mp
    32|         0|            0|            0|  0.00%|import torch.distributed as dist
    33|         0|            0|            0|  0.00%|import copy
    34|         0|            0|            0|  0.00%|
    35|         0|            0|            0|  0.00%|# pyRAPL.setup()
    36|         0|            0|            0|  0.00%|
    37|         0|            0|            0|  0.00%|#csv_output = pyRAPL.outputs.CSVOutput('energy_pyRAPL.csv')
    38|         0|            0|            0|  0.00%|
    39|         0|            0|            0|  0.00%|# @pyRAPL.measure(output=csv_output)
    40|         0|            0|            0|  0.00%|# @profile(precision=3)
    41|         1|  4.24385e-05|  4.24385e-05|  0.00%|def train_model(model, criterion, optimizer, scheduler, num_epochs=1):
    42|         1|   6.7234e-05|   6.7234e-05|  0.00%|    since = time.time()
    43|         0|            0|            0|  0.00%|    # -------------------------------------------------------------------
    44|         0|            0|            0|  0.00%|    #rank = 3
    45|         0|            0|            0|  0.00%|    # dist.init_process_group(
    46|         0|            0|            0|  0.00%|    #	backend='gloo',
    47|         0|            0|            0|  0.00%|    #		init_method='env://',
    48|         0|            0|            0|  0.00%|    #	world_size=4,
    49|         0|            0|            0|  0.00%|    #	rank=rank
    50|         0|            0|            0|  0.00%|    # )
    51|         0|            0|            0|  0.00%|    # -------------------------------------------------------------------
    52|         1|  0.000180483|  0.000180483|  0.00%|    best_model_wts = copy.deepcopy(model.state_dict())
(call)|         1|   0.00910616|   0.00910616|  0.02%|# /opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1236 state_dict
(call)|         1|     0.184232|     0.184232|  0.34%|# /opt/conda/lib/python3.8/copy.py:128 deepcopy
    53|         1|  2.98023e-05|  2.98023e-05|  0.00%|    best_acc = 0.0
    54|         0|            0|            0|  0.00%|
    55|         2|  2.71797e-05|  1.35899e-05|  0.00%|    for epoch in range(num_epochs):
    56|         0|            0|            0|  0.00%|        # s = realtime_monitoring.intermediate()
    57|         1|  7.60555e-05|  7.60555e-05|  0.00%|        print('Epoch {}/{}'.format(epoch, num_epochs - 1))
    58|         1|  2.83718e-05|  2.83718e-05|  0.00%|        print('-' * 10)
    59|         0|            0|            0|  0.00%|
    60|         0|            0|            0|  0.00%|        # Each epoch has a training and validation phase
    61|         3|  5.98431e-05|  1.99477e-05|  0.00%|        for phase in ['train', 'val']:
    62|         2|  2.28882e-05|  1.14441e-05|  0.00%|            if phase == 'train':
    63|         1|   2.6226e-05|   2.6226e-05|  0.00%|                model.train()  # Set model to training mode
(call)|         1|    0.0138621|    0.0138621|  0.03%|# /opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1622 train
    64|         0|            0|            0|  0.00%|            else:
    65|         1|   3.6478e-05|   3.6478e-05|  0.00%|                model.eval()   # Set model to evaluate mode
(call)|         1|    0.0141013|    0.0141013|  0.03%|# /opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1644 eval
    66|         0|            0|            0|  0.00%|
    67|         2|  1.93119e-05|  9.65595e-06|  0.00%|            running_loss = 0.0
    68|         2|  1.81198e-05|  9.05991e-06|  0.00%|            running_corrects = 0
    69|         0|            0|            0|  0.00%|
    70|         0|            0|            0|  0.00%|            # Iterate over data.
    71|       102|     0.137346|   0.00134653|  0.26%|            for inputs, labels in dataloaders[phase]:
(call)|         2|      1.12035|     0.560173|  2.09%|# /opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py:346 __iter__
(call)|       102|      1.46583|    0.0143709|  2.74%|# /opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py:517 __next__
(call)|         2|  0.000105143|  5.25713e-05|  0.00%|# /opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1327 __del__
(call)|        40|   0.00107908|  2.69771e-05|  0.00%|# /opt/conda/lib/python3.8/weakref.py:103 remove
(call)|        15|   0.00033021|   2.2014e-05|  0.00%|# /opt/conda/lib/python3.8/multiprocessing/connection.py:130 __del__
(call)|        11|  0.000310183|  2.81984e-05|  0.00%|# /opt/conda/lib/python3.8/_weakrefset.py:38 _remove
(call)|         8|   0.00114965|  0.000143707|  0.00%|# /opt/conda/lib/python3.8/multiprocessing/util.py:205 __call__
    72|       100|   0.00326443|  3.26443e-05|  0.01%|                inputs = inputs.to(device)
    73|       100|    0.0017941|   1.7941e-05|  0.00%|                labels = labels.to(device)
    74|         0|            0|            0|  0.00%|
    75|         0|            0|            0|  0.00%|                # zero the parameter gradients
    76|       100|   0.00333667|  3.33667e-05|  0.01%|                optimizer.zero_grad()
(call)|       100|      1.25615|    0.0125615|  2.34%|# /opt/conda/lib/python3.8/site-packages/torch/optim/optimizer.py:188 zero_grad
    77|         0|            0|            0|  0.00%|
    78|         0|            0|            0|  0.00%|                # forward
    79|         0|            0|            0|  0.00%|                # track history if only in train
    80|       100|   0.00348759|  3.48759e-05|  0.01%|                with torch.set_grad_enabled(phase == 'train'):
(call)|       100|    0.0027144|   2.7144e-05|  0.01%|# /opt/conda/lib/python3.8/site-packages/torch/autograd/grad_mode.py:213 __init__
(call)|       100|   0.00103068|  1.03068e-05|  0.00%|# /opt/conda/lib/python3.8/site-packages/torch/autograd/grad_mode.py:217 __enter__
    81|       100|   0.00344181|  3.44181e-05|  0.01%|                    outputs = model(inputs)
(call)|       100|       16.133|      0.16133| 30.11%|# /opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1045 _call_impl
    82|       100|   0.00696802|  6.96802e-05|  0.01%|                    _, preds = torch.max(outputs, 1)
    83|       100|    0.0293489|  0.000293489|  0.05%|                    loss = criterion(outputs, labels)
(call)|       100|    0.0319078|  0.000319078|  0.06%|# /opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1045 _call_impl
    84|         0|            0|            0|  0.00%|
    85|         0|            0|            0|  0.00%|                    # backward + optimize only if in training phase
    86|       100|   0.00149441|  1.49441e-05|  0.00%|                    if phase == 'train':
(call)|        39|  0.000734329|   1.8829e-05|  0.00%|# /opt/conda/lib/python3.8/site-packages/torch/autograd/grad_mode.py:220 __exit__
    87|        61|   0.00222015|   3.6396e-05|  0.00%|                        loss.backward()
(call)|        61|      31.3201|     0.513444| 58.46%|# /opt/conda/lib/python3.8/site-packages/torch/_tensor.py:205 backward
    88|        61|   0.00292468|  4.79456e-05|  0.01%|                        optimizer.step()
(call)|        61|      1.45998|    0.0239341|  2.73%|# /opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60 wrapper
(call)|        61|  0.000890017|  1.45904e-05|  0.00%|# /opt/conda/lib/python3.8/site-packages/torch/autograd/grad_mode.py:220 __exit__
    89|         0|            0|            0|  0.00%|
    90|         0|            0|            0|  0.00%|                # statistics
    91|       100|   0.00279522|  2.79522e-05|  0.01%|                running_loss += loss.item() * inputs.size(0)
    92|       100|   0.00920677|  9.20677e-05|  0.02%|                running_corrects += torch.sum(preds == labels.data)
    93|         2|  1.52588e-05|  7.62939e-06|  0.00%|            if phase == 'train':
    94|         1|  3.76701e-05|  3.76701e-05|  0.00%|                scheduler.step()
(call)|         1|  0.000575066|  0.000575066|  0.00%|# /opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:117 step
    95|         0|            0|            0|  0.00%|
    96|         2|  3.24249e-05|  1.62125e-05|  0.00%|            epoch_loss = running_loss / dataset_sizes[phase]
    97|         2|  0.000966787|  0.000483394|  0.00%|            epoch_acc = running_corrects.double() / dataset_sizes[phase]
    98|         0|            0|            0|  0.00%|
    99|         4|  0.000251532|  6.28829e-05|  0.00%|            print('{} Loss: {:.4f} Acc: {:.4f}'.format(
(call)|         2|  0.000139236|  6.96182e-05|  0.00%|# /opt/conda/lib/python3.8/site-packages/torch/_tensor.py:556 __format__
   100|         2|  1.45435e-05|  7.27177e-06|  0.00%|                phase, epoch_loss, epoch_acc))
   101|         0|            0|            0|  0.00%|
   102|         0|            0|            0|  0.00%|            # deep copy the model
   103|         2|  0.000216722|  0.000108361|  0.00%|            if phase == 'val' and epoch_acc > best_acc:
   104|         1|  1.07288e-05|  1.07288e-05|  0.00%|                best_acc = epoch_acc
   105|         1|    0.0151908|    0.0151908|  0.03%|                best_model_wts = copy.deepcopy(model.state_dict())
(call)|         1|   0.00938892|   0.00938892|  0.02%|# /opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1236 state_dict
(call)|         1|     0.144516|     0.144516|  0.27%|# /opt/conda/lib/python3.8/copy.py:128 deepcopy
   106|         0|            0|            0|  0.00%|
   107|         1|  6.07967e-05|  6.07967e-05|  0.00%|        print()
   108|         0|            0|            0|  0.00%|
   109|         1|  1.57356e-05|  1.57356e-05|  0.00%|    time_elapsed = time.time() - since
   110|         2|  7.10487e-05|  3.55244e-05|  0.00%|    print('Training complete in {:.0f}m {:.0f}s'.format(
   111|         1|  2.07424e-05|  2.07424e-05|  0.00%|        time_elapsed // 60, time_elapsed % 60))
   112|         1|  5.17368e-05|  5.17368e-05|  0.00%|    print('Best val Acc: {:4f}'.format(best_acc))
(call)|         1|  6.55651e-05|  6.55651e-05|  0.00%|# /opt/conda/lib/python3.8/site-packages/torch/_tensor.py:556 __format__
   113|         0|            0|            0|  0.00%|
   114|         0|            0|            0|  0.00%|    # load best model weights
   115|         1|  6.24657e-05|  6.24657e-05|  0.00%|    model.load_state_dict(best_model_wts)
(call)|         1|     0.178973|     0.178973|  0.33%|# /opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1354 load_state_dict
   116|         1|  1.00136e-05|  1.00136e-05|  0.00%|    return model
   117|         0|            0|            0|  0.00%|
   118|         0|            0|            0|  0.00%|
   119|         0|            0|            0|  0.00%|if __name__ == '__main__':
   120|         0|            0|            0|  0.00%|
   121|         0|            0|            0|  0.00%|    plt.ion()   # interactive mode
   122|         0|            0|            0|  0.00%|
   123|         0|            0|            0|  0.00%|    # ------------------------
   124|         0|            0|            0|  0.00%|    #os.environ['MASTER_ADDR'] = '10.57.23.164'
   125|         0|            0|            0|  0.00%|    #os.environ['MASTER_PORT'] = '8888'
   126|         0|            0|            0|  0.00%|
   127|         0|            0|            0|  0.00%|    # create default process group
   128|         0|            0|            0|  0.00%|    #dist.init_process_group("gloo", rank=3, world_size=4)
   129|         0|            0|            0|  0.00%|    #rank = 3
   130|         0|            0|            0|  0.00%|    # dist.init_process_group(
   131|         0|            0|            0|  0.00%|    #    backend='gloo',
   132|         0|            0|            0|  0.00%|    #            init_method='env://',
   133|         0|            0|            0|  0.00%|    #    world_size=4,
   134|         0|            0|            0|  0.00%|    #    rank=rank
   135|         0|            0|            0|  0.00%|    # )
   136|         0|            0|            0|  0.00%|    # -----------------------
   137|         0|            0|            0|  0.00%|
   138|         0|            0|            0|  0.00%|    zip = zipfile.ZipFile('hymenoptera_data.zip')
   139|         0|            0|            0|  0.00%|    zip.extractall()
   140|         0|            0|            0|  0.00%|
   141|         0|            0|            0|  0.00%|    torch.set_num_threads(4)
   142|         0|            0|            0|  0.00%|    torch.set_num_interop_threads(80)
   143|         0|            0|            0|  0.00%|    num_thred = torch.get_num_threads()
   144|         0|            0|            0|  0.00%|    num_interop_thred = torch.get_num_interop_threads()
   145|         0|            0|            0|  0.00%|    print("Num Threads:",num_thred," ",num_interop_thred)
   146|         0|            0|            0|  0.00%|
   147|         0|            0|            0|  0.00%|
   148|         0|            0|            0|  0.00%|    # Data augmentation and normalization for training
   149|         0|            0|            0|  0.00%|    # Just normalization for validation
   150|         0|            0|            0|  0.00%|    data_transforms = {
   151|         0|            0|            0|  0.00%|        'train': transforms.Compose([
   152|         0|            0|            0|  0.00%|            transforms.RandomResizedCrop(224),
   153|         0|            0|            0|  0.00%|            transforms.RandomHorizontalFlip(),
   154|         0|            0|            0|  0.00%|            transforms.ToTensor(),
   155|         0|            0|            0|  0.00%|            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
   156|         0|            0|            0|  0.00%|        ]),
   157|         0|            0|            0|  0.00%|        'val': transforms.Compose([
   158|         0|            0|            0|  0.00%|            transforms.Resize(256),
   159|         0|            0|            0|  0.00%|            transforms.CenterCrop(224),
   160|         0|            0|            0|  0.00%|            transforms.ToTensor(),
   161|         0|            0|            0|  0.00%|            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
   162|         0|            0|            0|  0.00%|        ]),
   163|         0|            0|            0|  0.00%|    }
   164|         0|            0|            0|  0.00%|
   165|         0|            0|            0|  0.00%|    data_dir = 'hymenoptera_data'
   166|         0|            0|            0|  0.00%|    image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),
   167|         0|            0|            0|  0.00%|                                              data_transforms[x])
   168|         0|            0|            0|  0.00%|                      for x in ['train', 'val']}
   169|         0|            0|            0|  0.00%|
   170|         0|            0|            0|  0.00%|    # ------------------------------------------------------------
   171|         0|            0|            0|  0.00%|    # train_sampler = torch.utils.data.distributed.DistributedSampler(
   172|         0|            0|            0|  0.00%|    #	image_datasets,
   173|         0|            0|            0|  0.00%|    #    num_replicas=4,
   174|         0|            0|            0|  0.00%|    #	rank=3)
   175|         0|            0|            0|  0.00%|    # ------------------------------------------------------------
   176|         0|            0|            0|  0.00%|
   177|         0|            0|            0|  0.00%|    dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,
   178|         0|            0|            0|  0.00%|                                                  shuffle=True, num_workers=4)  # ,pin_memory=True,
   179|         0|            0|            0|  0.00%|                   # sampler=train_sampler)
   180|         0|            0|            0|  0.00%|                   for x in ['train', 'val']}
   181|         0|            0|            0|  0.00%|    dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}
   182|         0|            0|            0|  0.00%|    class_names = image_datasets['train'].classes
   183|         0|            0|            0|  0.00%|
   184|         0|            0|            0|  0.00%|    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
   185|         0|            0|            0|  0.00%|
   186|         0|            0|            0|  0.00%|    # Get a batch of training data
   187|         0|            0|            0|  0.00%|    inputs, classes = next(iter(dataloaders['train']))
   188|         0|            0|            0|  0.00%|
   189|         0|            0|            0|  0.00%|    # Make a grid from batch
   190|         0|            0|            0|  0.00%|    out = torchvision.utils.make_grid(inputs)
   191|         0|            0|            0|  0.00%|
   192|         0|            0|            0|  0.00%|    # imshow(out, title=[class_names[x] for x in classes])
   193|         0|            0|            0|  0.00%|
   194|         0|            0|            0|  0.00%|    # model_ft = models.resnet18(pretrained=True)
   195|         0|            0|            0|  0.00%|    # torch.save(model_ft, "resnet18.pt")
   196|         0|            0|            0|  0.00%|    model_ft = torch.load("resnet18.pt")
   197|         0|            0|            0|  0.00%|    num_ftrs = model_ft.fc.in_features
   198|         0|            0|            0|  0.00%|    # Here the size of each output sample is set to 2.
   199|         0|            0|            0|  0.00%|    # Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).
   200|         0|            0|            0|  0.00%|    model_ft.fc = nn.Linear(num_ftrs, 2)
   201|         0|            0|            0|  0.00%|
   202|         0|            0|            0|  0.00%|    model_ft = model_ft.to(device)
   203|         0|            0|            0|  0.00%|
   204|         0|            0|            0|  0.00%|    # -------------------------------------------
   205|         0|            0|            0|  0.00%|    # model_ft = nn.parallel.DistributedDataParallel(model_ft,
   206|         0|            0|            0|  0.00%|    #                                            device_ids=None)
   207|         0|            0|            0|  0.00%|    # -------------------------------------------
   208|         0|            0|            0|  0.00%|
   209|         0|            0|            0|  0.00%|    criterion = nn.CrossEntropyLoss()
   210|         0|            0|            0|  0.00%|
   211|         0|            0|            0|  0.00%|    # Observe that all parameters are being optimized
   212|         0|            0|            0|  0.00%|    optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)
   213|         0|            0|            0|  0.00%|
   214|         0|            0|            0|  0.00%|    # Decay LR by a factor of 0.1 every 7 epochs
   215|         0|            0|            0|  0.00%|    exp_lr_scheduler = lr_scheduler.StepLR(
   216|         0|            0|            0|  0.00%|        optimizer_ft, step_size=7, gamma=0.1)
   217|         0|            0|            0|  0.00%|
   218|         0|            0|            0|  0.00%|    # gives a single float value
   219|         0|            0|            0|  0.00%|    cpu_per_b = psutil.cpu_percent()
   220|         0|            0|            0|  0.00%|    # gives an object with many fields
   221|         0|            0|            0|  0.00%|    # vir_mem_b = psutil.virtual_memory()
   222|         0|            0|            0|  0.00%|    # you can convert that object to a dictionary
   223|         0|            0|            0|  0.00%|    vir_mem_b = dict(psutil.virtual_memory()._asdict())
   224|         0|            0|            0|  0.00%|    # you can have the percentage of used RAM
   225|         0|            0|            0|  0.00%|    vir_mem_per_b = psutil.virtual_memory().percent
   226|         0|            0|            0|  0.00%|    # you can calculate percentage of available memory
   227|         0|            0|            0|  0.00%|    mem_av_per_b = psutil.virtual_memory().available * 100 / \
   228|         0|            0|            0|  0.00%|        psutil.virtual_memory().total
   229|         0|            0|            0|  0.00%|
   230|         0|            0|            0|  0.00%|    num_epochs = 1
   231|         0|            0|            0|  0.00%|    #torch.set_num_threads(4)
   232|         0|            0|            0|  0.00%|    #torch.set_num_interop_threads(80)
   233|         0|            0|            0|  0.00%|    #num_thred = torch.get_num_threads()
   234|         0|            0|            0|  0.00%|    #num_interop_thred = torch.get_num_interop_threads()
   235|         0|            0|            0|  0.00%|    #print("Num Threads:",num_thred," ",num_interop_thred)
   236|         0|            0|            0|  0.00%|    profiler = pprofile.Profile()
   237|         0|            0|            0|  0.00%|    pr = cProfile.Profile()
   238|         0|            0|            0|  0.00%|    #s1 = rapl.RAPLMonitor.sample()
   239|         0|            0|            0|  0.00%|    pr.enable()
   240|         0|            0|            0|  0.00%|    start_train = time.time()
   241|         0|            0|            0|  0.00%|    with profiler:
   242|         0|            0|            0|  0.00%|        model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,
(call)|         1|      53.5759|      53.5759|100.00%|# main.py:41 train_model
   243|         0|            0|            0|  0.00%|                               num_epochs=1)
   244|         0|            0|            0|  0.00%|        # mp.spawn(train_model, nprocs=4, args=(criterion,optimizer_ft,exp_lr_scheduler,
   245|         0|            0|            0|  0.00%|        #    num_epochs))
   246|         0|            0|            0|  0.00%|        # csv_output.save()
   247|         0|            0|            0|  0.00%|    end_train = time.time()
   248|         0|            0|            0|  0.00%|    pr.disable()
   249|         0|            0|            0|  0.00%|    #s2 = rapl.RAPLMonitor.sample()
   250|         0|            0|            0|  0.00%|    #diff = s2 - s1
   251|         0|            0|            0|  0.00%|    #print(diff)
   252|         0|            0|            0|  0.00%|    # Print the difference in microjoules
   253|         0|            0|            0|  0.00%|    #print("CPU Energy Usage(microjoules): ",diff.energy("package-0", "core", rapl.UJOULES))
   254|         0|            0|            0|  0.00%|
   255|         0|            0|            0|  0.00%|    # Print the average power
   256|         0|            0|            0|  0.00%|    #print("Average Power(CPU Usage): ",diff.average_power("package-0", "core"))
   257|         0|            0|            0|  0.00%|   # for d in diff.domains:
   258|         0|            0|            0|  0.00%|   #     print(d)
   259|         0|            0|            0|  0.00%|   #     domain = diff.domains[d]
   260|         0|            0|            0|  0.00%|   #     power = diff.average_power(package=domain.name)
   261|         0|            0|            0|  0.00%|   #     print("%s - %0.2f W" % (domain.name, power))
   262|         0|            0|            0|  0.00%|
   263|         0|            0|            0|  0.00%|   #     for sd in domain.subdomains:
   264|         0|            0|            0|  0.00%|   #         subdomain = domain.subdomains[sd]
   265|         0|            0|            0|  0.00%|   #         power = diff.average_power(
   266|         0|            0|            0|  0.00%|   #             package=domain.name, domain=subdomain.name)
   267|         0|            0|            0|  0.00%|   #         print("\t%s - %0.2f W" % (subdomain.name, power))
   268|         0|            0|            0|  0.00%|    elapsed_time = end_train-start_train
   269|         0|            0|            0|  0.00%|    with open('Elapsed_time.txt', 'w') as f1:
   270|         0|            0|            0|  0.00%|        f1.write("Training Time(Elapsed):")
   271|         0|            0|            0|  0.00%|        f1.write(str(elapsed_time))
   272|         0|            0|            0|  0.00%|    f1.close()
   273|         0|            0|            0|  0.00%|    # gives a single float value
   274|         0|            0|            0|  0.00%|    profiler.dump_stats("exec_time.txt")
   275|         0|            0|            0|  0.00%|    cpu_per_a = psutil.cpu_percent()
   276|         0|            0|            0|  0.00%|    # gives an object with many fields
   277|         0|            0|            0|  0.00%|    # you can convert that object to a dictionary
   278|         0|            0|            0|  0.00%|    vir_mem_a = dict(psutil.virtual_memory()._asdict())
   279|         0|            0|            0|  0.00%|    # you can have the percentage of used RAM
   280|         0|            0|            0|  0.00%|    vir_mem_per_a = psutil.virtual_memory().percent
   281|         0|            0|            0|  0.00%|    # you can calculate percentage of available memory
   282|         0|            0|            0|  0.00%|    mem_av_per_a = psutil.virtual_memory().available * 100 / \
   283|         0|            0|            0|  0.00%|        psutil.virtual_memory().total
   284|         0|            0|            0|  0.00%|    with open('CPU_table.txt', 'w') as f:
   285|         0|            0|            0|  0.00%|        f.write("BEFORE TRAINING:--------\n")
   286|         0|            0|            0|  0.00%|        f.write("CPU USAGE(%):")
   287|         0|            0|            0|  0.00%|        f.write(str(cpu_per_b))
   288|         0|            0|            0|  0.00%|        f.write("\n")
   289|         0|            0|            0|  0.00%|        f.write("MEMORY USE:")
   290|         0|            0|            0|  0.00%|        f.write(str(vir_mem_b))
   291|         0|            0|            0|  0.00%|        f.write("\n")
   292|         0|            0|            0|  0.00%|        f.write("MEMORY USE(%):")
   293|         0|            0|            0|  0.00%|        f.write(str(vir_mem_per_b))
   294|         0|            0|            0|  0.00%|        f.write("\n")
   295|         0|            0|            0|  0.00%|        f.write("MEMORY AVAIL(%):")
   296|         0|            0|            0|  0.00%|        f.write(str(mem_av_per_b))
   297|         0|            0|            0|  0.00%|        f.write("\n")
   298|         0|            0|            0|  0.00%|        f.write("\n\n\n\n")
   299|         0|            0|            0|  0.00%|        f.write("AFTER TRAINING:---------\n")
   300|         0|            0|            0|  0.00%|        f.write("CPU USAGE(%):")
   301|         0|            0|            0|  0.00%|        f.write(str(cpu_per_a))
   302|         0|            0|            0|  0.00%|        f.write("\n")
   303|         0|            0|            0|  0.00%|        f.write("MEMORY USE:")
   304|         0|            0|            0|  0.00%|        f.write(str(vir_mem_a))
   305|         0|            0|            0|  0.00%|        f.write("\n")
   306|         0|            0|            0|  0.00%|        f.write("MEMORY USE(%):")
   307|         0|            0|            0|  0.00%|        f.write(str(vir_mem_per_a))
   308|         0|            0|            0|  0.00%|        f.write("\n")
   309|         0|            0|            0|  0.00%|        f.write("MEMORY AVAIL(%):")
   310|         0|            0|            0|  0.00%|        f.write(str(mem_av_per_a))
   311|         0|            0|            0|  0.00%|        f.write("\n")
   312|         0|            0|            0|  0.00%|    f.close()
   313|         0|            0|            0|  0.00%|
   314|         0|            0|            0|  0.00%|    result = io.StringIO()
   315|         0|            0|            0|  0.00%|    pstats.Stats(pr, stream=result).print_stats()
   316|         0|            0|            0|  0.00%|    result = result.getvalue()
   317|         0|            0|            0|  0.00%|    # chop the string into a csv-like buffer
   318|         0|            0|            0|  0.00%|    result = 'ncalls' + result.split('ncalls')[-1]
   319|         0|            0|            0|  0.00%|    result = '\n'.join([','.join(line.rstrip().split(None, 5))
   320|         0|            0|            0|  0.00%|                       for line in result.split('\n')])
   321|         0|            0|            0|  0.00%|    # save it to disk
   322|         0|            0|            0|  0.00%|
   323|         0|            0|            0|  0.00%|    with open('memory_logs.csv', 'w+') as f:
   324|         0|            0|            0|  0.00%|        # f=open(result.rsplit('.')[0]+'.csv','w')
   325|         0|            0|            0|  0.00%|        f.write(result)
   326|         0|            0|            0|  0.00%|        f.close()
File: /opt/conda/lib/python3.8/site-packages/torch/multiprocessing/reductions.py
File duration: 0.11572s (0.22%)
Line #|      Hits|         Time| Time per hit|      %|Source code
------+----------+-------------+-------------+-------+-----------
     1|         0|            0|            0|  0.00%|import torch
     2|         0|            0|            0|  0.00%|import torch.utils.hooks
     3|         0|            0|            0|  0.00%|from torch._namedtensor_internals import check_serializing_named_tensor
     4|         0|            0|            0|  0.00%|import os
     5|         0|            0|            0|  0.00%|import threading
     6|         0|            0|            0|  0.00%|import multiprocessing
     7|         0|            0|            0|  0.00%|from multiprocessing.util import register_after_fork
     8|         0|            0|            0|  0.00%|from multiprocessing.reduction import ForkingPickler
     9|         0|            0|            0|  0.00%|try:
    10|         0|            0|            0|  0.00%|    # Early load resource_sharer to prevent a partially initialized instance
    11|         0|            0|            0|  0.00%|    # from being inherited in a forked child process. The reduce_storage method
    12|         0|            0|            0|  0.00%|    # requires this module indirectly through DupFd(). The built-in mp.Queue
    13|         0|            0|            0|  0.00%|    # class pickles arguments in a background thread which may overlap with the
    14|         0|            0|            0|  0.00%|    # fork.
    15|         0|            0|            0|  0.00%|    import multiprocessing.resource_sharer
    16|         0|            0|            0|  0.00%|except ImportError:
    17|         0|            0|            0|  0.00%|    pass
    18|         0|            0|            0|  0.00%|
    19|         0|            0|            0|  0.00%|
    20|         0|            0|            0|  0.00%|class StorageWeakRef(object):
    21|         0|            0|            0|  0.00%|    r"""A weak reference to a Storage.
    22|         0|            0|            0|  0.00%|
    23|         0|            0|            0|  0.00%|    The cdata member is a Python number containing the integer representation of
    24|         0|            0|            0|  0.00%|    the Storage pointer."""
    25|         0|            0|            0|  0.00%|
    26|       200|   0.00105119|  5.25594e-06|  0.00%|    def __init__(self, storage):
    27|       200|   0.00180483|  9.02414e-06|  0.00%|        self.cdata = storage._weak_ref()
    28|         0|            0|            0|  0.00%|        # Save a direct reference to _free_weak_ref because the `torch` module
    29|         0|            0|            0|  0.00%|        # might be cleared during Python shutdown before this module is cleared.
    30|       200|    0.0011766|  5.88298e-06|  0.00%|        self._free_weak_ref = torch.Storage._free_weak_ref  # type: ignore[attr-defined]
    31|         0|            0|            0|  0.00%|
    32|       129|   0.00052619|  4.07899e-06|  0.00%|    def expired(self):
    33|       129|  0.000896931|  6.95295e-06|  0.00%|        return torch.Storage._expired(self.cdata)  # type: ignore[attr-defined]
    34|         0|            0|            0|  0.00%|
    35|       124|  0.000517607|  4.17425e-06|  0.00%|    def __del__(self):
    36|       124|   0.00118017|  9.51752e-06|  0.00%|        self._free_weak_ref(self.cdata)
    37|         0|            0|            0|  0.00%|
    38|         0|            0|            0|  0.00%|
    39|         0|            0|            0|  0.00%|class SharedCache(dict):
    40|         0|            0|            0|  0.00%|    """dictionary from multiprocessing handles to StorageWeakRef"""
    41|         0|            0|            0|  0.00%|
    42|         0|            0|            0|  0.00%|    def __init__(self):
    43|         0|            0|            0|  0.00%|        # free_dead_references() is called if the len exceeds the current
    44|         0|            0|            0|  0.00%|        # limit. The limit scales with the number of remaining live objects.
    45|         0|            0|            0|  0.00%|        self.limit = 128
    46|         0|            0|            0|  0.00%|        # `fork` inherits lock state, so in case we fork when the lock is held,
    47|         0|            0|            0|  0.00%|        # we register a function to reset the lock to a new object to avoid
    48|         0|            0|            0|  0.00%|        # possible deadlocks, following python multiprocessing library design.
    49|         0|            0|            0|  0.00%|        self._after_fork()
    50|         0|            0|            0|  0.00%|        register_after_fork(self, SharedCache._after_fork)
    51|         0|            0|            0|  0.00%|
    52|         0|            0|            0|  0.00%|    def _after_fork(self):
    53|         0|            0|            0|  0.00%|        self.lock = threading.Lock()
    54|         0|            0|            0|  0.00%|
    55|       200|  0.000859261|   4.2963e-06|  0.00%|    def get(self, key):
    56|       200|   0.00121808|   6.0904e-06|  0.00%|        with self.lock:
    57|       200|   0.00155258|  7.76291e-06|  0.00%|            return dict.get(self, key)
    58|         0|            0|            0|  0.00%|
    59|       200|  0.000969887|  4.84943e-06|  0.00%|    def __setitem__(self, key, storage_ref):
    60|       200|   0.00107288|  5.36442e-06|  0.00%|        with self.lock:
    61|       200|   0.00119424|  5.97119e-06|  0.00%|            dict.__setitem__(self, key, storage_ref)
    62|       200|    0.0014739|  7.36952e-06|  0.00%|            if len(self) > self.limit:
    63|         1|  0.000351906|  0.000351906|  0.00%|                self.free_dead_references()
(call)|         1|    0.0069766|    0.0069766|  0.01%|# /opt/conda/lib/python3.8/site-packages/torch/multiprocessing/reductions.py:65 free_dead_references
    64|         0|            0|            0|  0.00%|
    65|         1|   2.6226e-05|   2.6226e-05|  0.00%|    def free_dead_references(self):
    66|         1|  1.95503e-05|  1.95503e-05|  0.00%|        live = 0
    67|       130|   0.00166583|  1.28141e-05|  0.00%|        for key, storage_ref in list(self.items()):
(call)|       124|   0.00169778|  1.36918e-05|  0.00%|# /opt/conda/lib/python3.8/site-packages/torch/multiprocessing/reductions.py:35 __del__
    68|       129|   0.00156593|   1.2139e-05|  0.00%|            if storage_ref.expired():
(call)|       129|   0.00142312|  1.10319e-05|  0.00%|# /opt/conda/lib/python3.8/site-packages/torch/multiprocessing/reductions.py:32 expired
    69|       124|  0.000546694|  4.40882e-06|  0.00%|                del self[key]
    70|         0|            0|            0|  0.00%|            else:
    71|         5|  1.93119e-05|  3.86238e-06|  0.00%|                live += 1
    72|         1|  1.21593e-05|  1.21593e-05|  0.00%|        self.limit = max(128, live * 2)
    73|         0|            0|            0|  0.00%|
    74|         0|            0|            0|  0.00%|
    75|         0|            0|            0|  0.00%|# mapping from handles to StorageWeakRef objects
    76|         0|            0|            0|  0.00%|shared_cache = SharedCache()
    77|         0|            0|            0|  0.00%|
    78|         0|            0|            0|  0.00%|
    79|         0|            0|            0|  0.00%|def rebuild_event(device, handle):
    80|         0|            0|            0|  0.00%|    return torch.cuda.Event.from_ipc_handle(device, handle)
    81|         0|            0|            0|  0.00%|
    82|         0|            0|            0|  0.00%|
    83|         0|            0|            0|  0.00%|def reduce_event(event):
    84|         0|            0|            0|  0.00%|    handle = event.ipc_handle()
    85|         0|            0|            0|  0.00%|    return (rebuild_event, (event.device, handle))
    86|         0|            0|            0|  0.00%|
    87|         0|            0|            0|  0.00%|
    88|       200|   0.00136971|  6.84857e-06|  0.00%|def rebuild_tensor(cls, storage, metadata):
    89|       200|   0.00115752|  5.78761e-06|  0.00%|    storage_offset, size, stride, requires_grad = metadata
    90|       200|   0.00370598|  1.85299e-05|  0.01%|    t = torch._utils._rebuild_tensor(storage, storage_offset, size, stride)
(call)|       200|    0.0131717|  6.58584e-05|  0.02%|# /opt/conda/lib/python3.8/site-packages/torch/_utils.py:130 _rebuild_tensor
    91|       200|   0.00139046|  6.95229e-06|  0.00%|    if cls == torch.nn.parameter.Parameter:
    92|         0|            0|            0|  0.00%|        # we have to pass requires_grad into constructor, rather than set it as an
    93|         0|            0|            0|  0.00%|        # attribute later, because it's an important check for Integer Tensors to
    94|         0|            0|            0|  0.00%|        # have requires_grad=False (or else they raise an error)
    95|         0|            0|            0|  0.00%|        t = torch.nn.parameter.Parameter(t, requires_grad=requires_grad)
    96|         0|            0|            0|  0.00%|    else:
    97|       200|    0.0012691|  6.34551e-06|  0.00%|        t.requires_grad = requires_grad
    98|       200|  0.000873089|  4.36544e-06|  0.00%|    return t
    99|         0|            0|            0|  0.00%|
   100|         0|            0|            0|  0.00%|
   101|         0|            0|            0|  0.00%|def rebuild_cuda_tensor(tensor_cls, tensor_size, tensor_stride, tensor_offset,
   102|         0|            0|            0|  0.00%|                        storage_cls, storage_device, storage_handle, storage_size_bytes, storage_offset_bytes,
   103|         0|            0|            0|  0.00%|                        requires_grad, ref_counter_handle, ref_counter_offset, event_handle, event_sync_required):
   104|         0|            0|            0|  0.00%|    # If storage_handle is None, storage points to nullptr.
   105|         0|            0|            0|  0.00%|    if storage_handle is None or storage_size_bytes == 0:
   106|         0|            0|            0|  0.00%|        storage = storage_cls(0)
   107|         0|            0|            0|  0.00%|    else:
   108|         0|            0|            0|  0.00%|        storage = storage_from_cache(storage_cls, (storage_handle, storage_offset_bytes))
   109|         0|            0|            0|  0.00%|        if storage is None:
   110|         0|            0|            0|  0.00%|            torch.cuda._lazy_init()
   111|         0|            0|            0|  0.00%|            storage = storage_cls._new_shared_cuda(
   112|         0|            0|            0|  0.00%|                storage_device,
   113|         0|            0|            0|  0.00%|                storage_handle,
   114|         0|            0|            0|  0.00%|                storage_size_bytes,
   115|         0|            0|            0|  0.00%|                storage_offset_bytes,
   116|         0|            0|            0|  0.00%|                ref_counter_handle,
   117|         0|            0|            0|  0.00%|                ref_counter_offset,
   118|         0|            0|            0|  0.00%|                event_handle,
   119|         0|            0|            0|  0.00%|                event_sync_required)
   120|         0|            0|            0|  0.00%|            shared_cache[(storage_handle, storage_offset_bytes)] = StorageWeakRef(storage)
   121|         0|            0|            0|  0.00%|        else:
   122|         0|            0|            0|  0.00%|            # We already ref counting this Storage, but producer needs new ref-counters to be released.
   123|         0|            0|            0|  0.00%|            storage_cls._release_ipc_counter(ref_counter_handle, ref_counter_offset)
   124|         0|            0|            0|  0.00%|
   125|         0|            0|            0|  0.00%|    t = torch._utils._rebuild_tensor(storage, tensor_offset, tensor_size, tensor_stride)
   126|         0|            0|            0|  0.00%|
   127|         0|            0|            0|  0.00%|    if tensor_cls == torch.nn.parameter.Parameter:
   128|         0|            0|            0|  0.00%|        # It is crucial for integer tensors to receive
   129|         0|            0|            0|  0.00%|        # the requires_grad=False as an argument in the constructor
   130|         0|            0|            0|  0.00%|        t = torch.nn.parameter.Parameter(t, requires_grad=requires_grad)
   131|         0|            0|            0|  0.00%|    else:
   132|         0|            0|            0|  0.00%|        t.requires_grad = requires_grad
   133|         0|            0|            0|  0.00%|
   134|         0|            0|            0|  0.00%|    return t
   135|         0|            0|            0|  0.00%|
   136|         0|            0|            0|  0.00%|
   137|         0|            0|            0|  0.00%|def reduce_tensor(tensor):
   138|         0|            0|            0|  0.00%|    storage = tensor.storage()
   139|         0|            0|            0|  0.00%|
   140|         0|            0|            0|  0.00%|    if tensor.requires_grad and not tensor.is_leaf:
   141|         0|            0|            0|  0.00%|        raise RuntimeError("Cowardly refusing to serialize non-leaf tensor which requires_grad, "
   142|         0|            0|            0|  0.00%|                           "since autograd does not support crossing process boundaries.  "
   143|         0|            0|            0|  0.00%|                           "If you just want to transfer the data, call detach() on the tensor "
   144|         0|            0|            0|  0.00%|                           "before serializing (e.g., putting it on the queue).")
   145|         0|            0|            0|  0.00%|
   146|         0|            0|            0|  0.00%|    check_serializing_named_tensor(tensor)
   147|         0|            0|            0|  0.00%|    torch.utils.hooks.warn_if_has_hooks(tensor)
   148|         0|            0|            0|  0.00%|
   149|         0|            0|            0|  0.00%|    # Note [CUDA IPC and the caching allocator]
   150|         0|            0|            0|  0.00%|    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
   151|         0|            0|            0|  0.00%|    # When you send a CUDA tensor over IPC, you might expect that you will
   152|         0|            0|            0|  0.00%|    # get out the same storage from the other end.  However, the CUDA caching
   153|         0|            0|            0|  0.00%|    # allocator makes it difficult to preserve this invariant.  Consider
   154|         0|            0|            0|  0.00%|    # the following situation: a tensor of size 0x100 points to offset 0x20 of
   155|         0|            0|            0|  0.00%|    # a storage at 0xA100 of size 0x100.  (For simplicity, all of these
   156|         0|            0|            0|  0.00%|    # sizes are given in bytes).  HOWEVER, with the caching allocator, this storage
   157|         0|            0|            0|  0.00%|    # might be part of a larger cudaMalloc allocation 0xA000 of size 0x4000.
   158|         0|            0|            0|  0.00%|    #
   159|         0|            0|            0|  0.00%|    # When we want to send this CUDA tensor over IPC, we must send the
   160|         0|            0|            0|  0.00%|    # *entire* cudaMalloc allocation, i.e., the 0xA000 region, not just
   161|         0|            0|            0|  0.00%|    # the storage 0xA100 (because that is what CUDA supports).  So, on the
   162|         0|            0|            0|  0.00%|    # other end, there simply isn't any way to say, "Wait, you gave me
   163|         0|            0|            0|  0.00%|    # a bigger region (0xA000) than the one I wanted (0xA100)".
   164|         0|            0|            0|  0.00%|    #
   165|         0|            0|            0|  0.00%|    # OK, so if you sent the cudaMalloc allocation, can you just wrap that up as
   166|         0|            0|            0|  0.00%|    # one storage itself? No, because this cudaMalloc allocation might contain
   167|         0|            0|            0|  0.00%|    # storages of mixed types: float, bytes, double... If you make the entire
   168|         0|            0|            0|  0.00%|    # allocation a single storage of a type A, we'll hit an error when constructing
   169|         0|            0|            0|  0.00%|    # a tensor of type B on the storage.
   170|         0|            0|            0|  0.00%|    #
   171|         0|            0|            0|  0.00%|    # cudaIpcMemHandle is an identifier to access the sender cudaMalloc allocation on the
   172|         0|            0|            0|  0.00%|    # receiver side. However, cudaIpcMemHandles from each device in a given process may
   173|         0|            0|            0|  0.00%|    # only be opened by one context per device per other process.
   174|         0|            0|            0|  0.00%|    # If we open and close a memory handle multiples times in a process, CUDA is allowed
   175|         0|            0|            0|  0.00%|    # to give it a different address; similarly, once we close the memory, we're not
   176|         0|            0|            0|  0.00%|    # allowed to access it(and the storage/tensor built on top of it), even if it is
   177|         0|            0|            0|  0.00%|    # still live in the original process. As we cannot make a cudaMalloc allocation
   178|         0|            0|            0|  0.00%|    # to a single storage in one go, this requires us to cache the device pointer for
   179|         0|            0|            0|  0.00%|    # each cudaIpcMemHandle on C++ side to reconstruct types of storages, while keep
   180|         0|            0|            0|  0.00%|    # the old ones alives.
   181|         0|            0|            0|  0.00%|    # See [https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__DEVICE.html]
   182|         0|            0|            0|  0.00%|    #
   183|         0|            0|            0|  0.00%|    # This is fine, because all we need to do is to save our position in the allocation,
   184|         0|            0|            0|  0.00%|    # and reconstruct storage and tensor from it.
   185|         0|            0|            0|  0.00%|    # 0xA000 ->  -------CUDA Allocation------
   186|         0|            0|            0|  0.00%|    #           |                            |
   187|         0|            0|            0|  0.00%|    #           |                            |
   188|         0|            0|            0|  0.00%|    #           |                            |
   189|         0|            0|            0|  0.00%|    #           |                            |
   190|         0|            0|            0|  0.00%|    # 0xA100 ->  --------storage1 begin------
   191|         0|            0|            0|  0.00%|    #           |                            |
   192|         0|            0|            0|  0.00%|    # 0xA120 ->  --------tensor1 begin ------
   193|         0|            0|            0|  0.00%|    #           |                            |
   194|         0|            0|            0|  0.00%|    #           |                            |
   195|         0|            0|            0|  0.00%|    #           |                            |
   196|         0|            0|            0|  0.00%|    #           |                            |
   197|         0|            0|            0|  0.00%|    #           |                            |
   198|         0|            0|            0|  0.00%|    # 0xA160 ->  --------tensor1 end---------
   199|         0|            0|            0|  0.00%|    #           |                            |
   200|         0|            0|            0|  0.00%|    #           |                            |
   201|         0|            0|            0|  0.00%|    #           |                            |
   202|         0|            0|            0|  0.00%|    # 0xA200 ->  --------storage1 end--------
   203|         0|            0|            0|  0.00%|    #           |                            |
   204|         0|            0|            0|  0.00%|    # 0xE000 ->  --------CUDA allocation-----
   205|         0|            0|            0|  0.00%|    #
   206|         0|            0|            0|  0.00%|    # To send tensor1, the following info are required from sender to receiver for
   207|         0|            0|            0|  0.00%|    # storage recontruction.
   208|         0|            0|            0|  0.00%|    #   1. cudaIpcMemHandle of 0xA000(which can be mapped to a basePtr in receiver process).
   209|         0|            0|            0|  0.00%|    #      basePtr may not be exactly 0xA000 since it's a different process.
   210|         0|            0|            0|  0.00%|    #   2. offset(0xA100) of storage1 in the CUDA allocation.
   211|         0|            0|            0|  0.00%|    #   3. size of storage1(0x100).
   212|         0|            0|            0|  0.00%|    #
   213|         0|            0|            0|  0.00%|    # On receiver side:
   214|         0|            0|            0|  0.00%|    #   1. Get the devPtr of the MemHandle to access the memory, reconstruct a storage
   215|         0|            0|            0|  0.00%|    #      of the same type using (basePtr, offset, size).
   216|         0|            0|            0|  0.00%|    #   2. we can reconstruct the tensor on top of the reconstructed storage
   217|         0|            0|            0|  0.00%|    #   Tensor(size=0x040, offset=0x020, storage=Storage(data=basePtr+0xA100, size=0x0100))
   218|         0|            0|            0|  0.00%|    #
   219|         0|            0|            0|  0.00%|    # This strategy has a few implications:
   220|         0|            0|            0|  0.00%|    #
   221|         0|            0|            0|  0.00%|    # 1. When we serialize a CUDA tensor for IPC, we cannot do it all in one
   222|         0|            0|            0|  0.00%|    #    go (non-compositionally), and this requires to have a global map
   223|         0|            0|            0|  0.00%|    #    memHandle -> devPtr for each process.
   224|         0|            0|            0|  0.00%|    #
   225|         0|            0|            0|  0.00%|    # 2. We MUST NOT let the new IPC tensor be resizable.  Originally, a resize
   226|         0|            0|            0|  0.00%|    #    of the storage beyond 0x100 would merely have caused us to do a
   227|         0|            0|            0|  0.00%|    #    reallocation.  You don't really want to do this, but if you did,
   228|         0|            0|            0|  0.00%|    #    all that would happen is that you would lose IPC sharing.  But if
   229|         0|            0|            0|  0.00%|    #    you do this in the new world, we will happily let you write out of
   230|         0|            0|            0|  0.00%|    #    bounds of your "allocation", clobbering unrelated data in the cached
   231|         0|            0|            0|  0.00%|    #    allocator block.  BAD!
   232|         0|            0|            0|  0.00%|    #
   233|         0|            0|            0|  0.00%|    # By the way, in old versions of PyTorch, we supported this situation
   234|         0|            0|            0|  0.00%|    # natively using a "storage view", which permitted multiple storages to be
   235|         0|            0|            0|  0.00%|    # views on each other.  But this was the *only* use of storage views, so we
   236|         0|            0|            0|  0.00%|    # eliminated it so that we could just use tensor views to implement the same
   237|         0|            0|            0|  0.00%|    # thing.
   238|         0|            0|            0|  0.00%|    #
   239|         0|            0|            0|  0.00%|    if storage.is_cuda:
   240|         0|            0|            0|  0.00%|        (device,
   241|         0|            0|            0|  0.00%|         handle,
   242|         0|            0|            0|  0.00%|         storage_size_bytes,
   243|         0|            0|            0|  0.00%|         storage_offset_bytes,
   244|         0|            0|            0|  0.00%|         ref_counter_handle,
   245|         0|            0|            0|  0.00%|         ref_counter_offset,
   246|         0|            0|            0|  0.00%|         event_handle,
   247|         0|            0|            0|  0.00%|         event_sync_required) = storage._share_cuda_()
   248|         0|            0|            0|  0.00%|        tensor_offset = tensor.storage_offset()
   249|         0|            0|            0|  0.00%|        shared_cache[handle] = StorageWeakRef(storage)
   250|         0|            0|            0|  0.00%|        # _backward_hooks purposely omitted here, see
   251|         0|            0|            0|  0.00%|        # Note [Don't serialize hooks]
   252|         0|            0|            0|  0.00%|        return (rebuild_cuda_tensor,
   253|         0|            0|            0|  0.00%|                (type(tensor),
   254|         0|            0|            0|  0.00%|                 tensor.size(),
   255|         0|            0|            0|  0.00%|                 tensor.stride(),
   256|         0|            0|            0|  0.00%|                 tensor_offset,  # tensor offset in its storage
   257|         0|            0|            0|  0.00%|                 type(storage),
   258|         0|            0|            0|  0.00%|                 device,
   259|         0|            0|            0|  0.00%|                 handle,  # identifier which CUDA allocation is the storage in.
   260|         0|            0|            0|  0.00%|                 storage_size_bytes,  # size(in bytes) of the storage
   261|         0|            0|            0|  0.00%|                 storage_offset_bytes,  # offset(in bytes) of the storage in the CUDA allocation
   262|         0|            0|            0|  0.00%|                 tensor.requires_grad,
   263|         0|            0|            0|  0.00%|                 ref_counter_handle,
   264|         0|            0|            0|  0.00%|                 ref_counter_offset,
   265|         0|            0|            0|  0.00%|                 event_handle,
   266|         0|            0|            0|  0.00%|                 event_sync_required))
   267|         0|            0|            0|  0.00%|
   268|         0|            0|            0|  0.00%|    # _backward_hooks purposely omitted here, see Note [Don't serialize hooks]
   269|         0|            0|            0|  0.00%|    metadata = (tensor.storage_offset(), tensor.size(), tensor.stride(), tensor.requires_grad)
   270|         0|            0|            0|  0.00%|    return (rebuild_tensor, (type(tensor), storage, metadata))
   271|         0|            0|            0|  0.00%|
   272|         0|            0|            0|  0.00%|
   273|       400|   0.00189495|  4.73738e-06|  0.00%|def fd_id(fd):
   274|         0|            0|            0|  0.00%|    # Returns a tuple which uniquely identifies a file descriptor. In Mac OS,
   275|         0|            0|            0|  0.00%|    # this doesn't work with shared memory handles, which is why we don't
   276|         0|            0|            0|  0.00%|    # support the "file_descriptor" sharing method on that platform.
   277|       400|    0.0047276|   1.1819e-05|  0.01%|    stat = os.fstat(fd)
   278|       400|   0.00191116|  4.77791e-06|  0.00%|    return (stat.st_ino, stat.st_dev)
   279|         0|            0|            0|  0.00%|
   280|         0|            0|            0|  0.00%|
   281|       200|   0.00102067|  5.10335e-06|  0.00%|def storage_from_cache(cls, key):
   282|       200|   0.00271463|  1.35732e-05|  0.01%|    storage_ref = shared_cache.get(key)
(call)|       200|   0.00362992|  1.81496e-05|  0.01%|# /opt/conda/lib/python3.8/site-packages/torch/multiprocessing/reductions.py:55 get
   283|       200|  0.000835657|  4.17829e-06|  0.00%|    if storage_ref is None:
   284|       200|   0.00077033|  3.85165e-06|  0.00%|        return None
   285|         0|            0|            0|  0.00%|    return cls._new_with_weak_ptr(storage_ref.cdata)
   286|         0|            0|            0|  0.00%|
   287|         0|            0|            0|  0.00%|
   288|       200|   0.00147057|  7.35283e-06|  0.00%|def rebuild_storage_fd(cls, df, size):
   289|       200|   0.00521088|  2.60544e-05|  0.01%|    fd = df.detach()
(call)|       200|     0.691057|   0.00345529|  1.29%|# /opt/conda/lib/python3.8/multiprocessing/resource_sharer.py:55 detach
(call)|       200|   0.00185466|  9.27329e-06|  0.00%|# /opt/conda/lib/python3.8/multiprocessing/connection.py:130 __del__
   290|       200|  0.000935078|  4.67539e-06|  0.00%|    try:
   291|       200|   0.00450683|  2.25341e-05|  0.01%|        storage = storage_from_cache(cls, fd_id(fd))
(call)|       200|   0.00467682|  2.33841e-05|  0.01%|# /opt/conda/lib/python3.8/site-packages/torch/multiprocessing/reductions.py:273 fd_id
(call)|       200|   0.00897121|  4.48561e-05|  0.02%|# /opt/conda/lib/python3.8/site-packages/torch/multiprocessing/reductions.py:281 storage_from_cache
   292|       200|  0.000834942|  4.17471e-06|  0.00%|        if storage is not None:
   293|         0|            0|            0|  0.00%|            return storage
   294|       200|    0.0491524|  0.000245762|  0.09%|        storage = cls._new_shared_fd(fd, size)
   295|       200|   0.00700116|  3.50058e-05|  0.01%|        shared_cache[fd_id(fd)] = StorageWeakRef(storage)
(call)|       200|   0.00403261|  2.01631e-05|  0.01%|# /opt/conda/lib/python3.8/site-packages/torch/multiprocessing/reductions.py:26 __init__
(call)|       200|    0.0038569|  1.92845e-05|  0.01%|# /opt/conda/lib/python3.8/site-packages/torch/multiprocessing/reductions.py:273 fd_id
(call)|       200|    0.0120394|  6.01971e-05|  0.02%|# /opt/conda/lib/python3.8/site-packages/torch/multiprocessing/reductions.py:59 __setitem__
   296|       400|   0.00171947|  4.29869e-06|  0.00%|        return storage
   297|         0|            0|            0|  0.00%|    finally:
   298|       200|   0.00154614|  7.73072e-06|  0.00%|        os.close(fd)
   299|         0|            0|            0|  0.00%|
   300|         0|            0|            0|  0.00%|
   301|         0|            0|            0|  0.00%|def rebuild_storage_filename(cls, manager, handle, size):
   302|         0|            0|            0|  0.00%|    storage = storage_from_cache(cls, handle)
   303|         0|            0|            0|  0.00%|    if storage is not None:
   304|         0|            0|            0|  0.00%|        return storage._shared_decref()
   305|         0|            0|            0|  0.00%|    storage = cls._new_shared_filename(manager, handle, size)
   306|         0|            0|            0|  0.00%|    shared_cache[handle] = StorageWeakRef(storage)
   307|         0|            0|            0|  0.00%|    return storage._shared_decref()
   308|         0|            0|            0|  0.00%|
   309|         0|            0|            0|  0.00%|
   310|         0|            0|            0|  0.00%|def rebuild_storage_empty(cls):
   311|         0|            0|            0|  0.00%|    return cls()
   312|         0|            0|            0|  0.00%|
   313|         0|            0|            0|  0.00%|
   314|         0|            0|            0|  0.00%|def reduce_storage(storage):
   315|         0|            0|            0|  0.00%|    from . import get_sharing_strategy
   316|         0|            0|            0|  0.00%|    if storage.is_cuda:
   317|         0|            0|            0|  0.00%|        raise RuntimeError("Cannot pickle CUDA storage; try pickling a CUDA tensor instead")
   318|         0|            0|            0|  0.00%|    elif get_sharing_strategy() == 'file_system':
   319|         0|            0|            0|  0.00%|        metadata = storage._share_filename_()
   320|         0|            0|            0|  0.00%|        cache_key = metadata[1]
   321|         0|            0|            0|  0.00%|        rebuild = rebuild_storage_filename
   322|         0|            0|            0|  0.00%|        storage._shared_incref()
   323|         0|            0|            0|  0.00%|    elif storage.size() == 0:
   324|         0|            0|            0|  0.00%|        # This is special cased because Empty tensors
   325|         0|            0|            0|  0.00%|        # (with size 0) cannot be mmapped.
   326|         0|            0|            0|  0.00%|        return (rebuild_storage_empty, (type(storage),))
   327|         0|            0|            0|  0.00%|    else:
   328|         0|            0|            0|  0.00%|        fd, size = storage._share_fd_()
   329|         0|            0|            0|  0.00%|        df = multiprocessing.reduction.DupFd(fd)
   330|         0|            0|            0|  0.00%|        cache_key = fd_id(fd)
   331|         0|            0|            0|  0.00%|        metadata = (df, size)
   332|         0|            0|            0|  0.00%|        rebuild = rebuild_storage_fd  # type: ignore[assignment]
   333|         0|            0|            0|  0.00%|
   334|         0|            0|            0|  0.00%|    shared_cache[cache_key] = StorageWeakRef(storage)
   335|         0|            0|            0|  0.00%|    return (rebuild, (type(storage),) + metadata)
   336|         0|            0|            0|  0.00%|
   337|         0|            0|            0|  0.00%|
   338|         0|            0|            0|  0.00%|def init_reductions():
   339|         0|            0|            0|  0.00%|    ForkingPickler.register(torch.cuda.Event, reduce_event)
   340|         0|            0|            0|  0.00%|
   341|         0|            0|            0|  0.00%|    for t in torch._storage_classes:
   342|         0|            0|            0|  0.00%|        ForkingPickler.register(t, reduce_storage)
   343|         0|            0|            0|  0.00%|
   344|         0|            0|            0|  0.00%|    for t in torch._tensor_classes:
   345|         0|            0|            0|  0.00%|        ForkingPickler.register(t, reduce_tensor)
   346|         0|            0|            0|  0.00%|
   347|         0|            0|            0|  0.00%|    # TODO: Maybe this should be in tensor_classes? :)
   348|         0|            0|            0|  0.00%|    ForkingPickler.register(torch.Tensor, reduce_tensor)
   349|         0|            0|            0|  0.00%|    ForkingPickler.register(torch.nn.parameter.Parameter, reduce_tensor)
File: /opt/conda/lib/python3.8/copy.py
File duration: 0.113578s (0.21%)
Line #|      Hits|         Time| Time per hit|      %|Source code
------+----------+-------------+-------------+-------+-----------
     1|         0|            0|            0|  0.00%|"""Generic (shallow and deep) copying operations.
     2|         0|            0|            0|  0.00%|
     3|         0|            0|            0|  0.00%|Interface summary:
     4|         0|            0|            0|  0.00%|
     5|         0|            0|            0|  0.00%|        import copy
     6|         0|            0|            0|  0.00%|
     7|         0|            0|            0|  0.00%|        x = copy.copy(y)        # make a shallow copy of y
     8|         0|            0|            0|  0.00%|        x = copy.deepcopy(y)    # make a deep copy of y
     9|         0|            0|            0|  0.00%|
    10|         0|            0|            0|  0.00%|For module specific errors, copy.Error is raised.
    11|         0|            0|            0|  0.00%|
    12|         0|            0|            0|  0.00%|The difference between shallow and deep copying is only relevant for
    13|         0|            0|            0|  0.00%|compound objects (objects that contain other objects, like lists or
    14|         0|            0|            0|  0.00%|class instances).
    15|         0|            0|            0|  0.00%|
    16|         0|            0|            0|  0.00%|- A shallow copy constructs a new compound object and then (to the
    17|         0|            0|            0|  0.00%|  extent possible) inserts *the same objects* into it that the
    18|         0|            0|            0|  0.00%|  original contains.
    19|         0|            0|            0|  0.00%|
    20|         0|            0|            0|  0.00%|- A deep copy constructs a new compound object and then, recursively,
    21|         0|            0|            0|  0.00%|  inserts *copies* into it of the objects found in the original.
    22|         0|            0|            0|  0.00%|
    23|         0|            0|            0|  0.00%|Two problems often exist with deep copy operations that don't exist
    24|         0|            0|            0|  0.00%|with shallow copy operations:
    25|         0|            0|            0|  0.00%|
    26|         0|            0|            0|  0.00%| a) recursive objects (compound objects that, directly or indirectly,
    27|         0|            0|            0|  0.00%|    contain a reference to themselves) may cause a recursive loop
    28|         0|            0|            0|  0.00%|
    29|         0|            0|            0|  0.00%| b) because deep copy copies *everything* it may copy too much, e.g.
    30|         0|            0|            0|  0.00%|    administrative data structures that should be shared even between
    31|         0|            0|            0|  0.00%|    copies
    32|         0|            0|            0|  0.00%|
    33|         0|            0|            0|  0.00%|Python's deep copy operation avoids these problems by:
    34|         0|            0|            0|  0.00%|
    35|         0|            0|            0|  0.00%| a) keeping a table of objects already copied during the current
    36|         0|            0|            0|  0.00%|    copying pass
    37|         0|            0|            0|  0.00%|
    38|         0|            0|            0|  0.00%| b) letting user-defined classes override the copying operation or the
    39|         0|            0|            0|  0.00%|    set of components copied
    40|         0|            0|            0|  0.00%|
    41|         0|            0|            0|  0.00%|This version does not copy types like module, class, function, method,
    42|         0|            0|            0|  0.00%|nor stack trace, stack frame, nor file, socket, window, nor array, nor
    43|         0|            0|            0|  0.00%|any similar types.
    44|         0|            0|            0|  0.00%|
    45|         0|            0|            0|  0.00%|Classes can use the same interfaces to control copying that they use
    46|         0|            0|            0|  0.00%|to control pickling: they can define methods called __getinitargs__(),
    47|         0|            0|            0|  0.00%|__getstate__() and __setstate__().  See the documentation for module
    48|         0|            0|            0|  0.00%|"pickle" for information on these methods.
    49|         0|            0|            0|  0.00%|"""
    50|         0|            0|            0|  0.00%|
    51|         0|            0|            0|  0.00%|import types
    52|         0|            0|            0|  0.00%|import weakref
    53|         0|            0|            0|  0.00%|from copyreg import dispatch_table
    54|         0|            0|            0|  0.00%|
    55|         0|            0|            0|  0.00%|class Error(Exception):
    56|         0|            0|            0|  0.00%|    pass
    57|         0|            0|            0|  0.00%|error = Error   # backward compatibility
    58|         0|            0|            0|  0.00%|
    59|         0|            0|            0|  0.00%|try:
    60|         0|            0|            0|  0.00%|    from org.python.core import PyStringMap
    61|         0|            0|            0|  0.00%|except ImportError:
    62|         0|            0|            0|  0.00%|    PyStringMap = None
    63|         0|            0|            0|  0.00%|
    64|         0|            0|            0|  0.00%|__all__ = ["Error", "copy", "deepcopy"]
    65|         0|            0|            0|  0.00%|
    66|         0|            0|            0|  0.00%|def copy(x):
    67|         0|            0|            0|  0.00%|    """Shallow copy operation on arbitrary Python objects.
    68|         0|            0|            0|  0.00%|
    69|         0|            0|            0|  0.00%|    See the module's __doc__ string for more info.
    70|         0|            0|            0|  0.00%|    """
    71|         0|            0|            0|  0.00%|
    72|         0|            0|            0|  0.00%|    cls = type(x)
    73|         0|            0|            0|  0.00%|
    74|         0|            0|            0|  0.00%|    copier = _copy_dispatch.get(cls)
    75|         0|            0|            0|  0.00%|    if copier:
    76|         0|            0|            0|  0.00%|        return copier(x)
    77|         0|            0|            0|  0.00%|
    78|         0|            0|            0|  0.00%|    if issubclass(cls, type):
    79|         0|            0|            0|  0.00%|        # treat it as a regular class:
    80|         0|            0|            0|  0.00%|        return _copy_immutable(x)
    81|         0|            0|            0|  0.00%|
    82|         0|            0|            0|  0.00%|    copier = getattr(cls, "__copy__", None)
    83|         0|            0|            0|  0.00%|    if copier is not None:
    84|         0|            0|            0|  0.00%|        return copier(x)
    85|         0|            0|            0|  0.00%|
    86|         0|            0|            0|  0.00%|    reductor = dispatch_table.get(cls)
    87|         0|            0|            0|  0.00%|    if reductor is not None:
    88|         0|            0|            0|  0.00%|        rv = reductor(x)
    89|         0|            0|            0|  0.00%|    else:
    90|         0|            0|            0|  0.00%|        reductor = getattr(x, "__reduce_ex__", None)
    91|         0|            0|            0|  0.00%|        if reductor is not None:
    92|         0|            0|            0|  0.00%|            rv = reductor(4)
    93|         0|            0|            0|  0.00%|        else:
    94|         0|            0|            0|  0.00%|            reductor = getattr(x, "__reduce__", None)
    95|         0|            0|            0|  0.00%|            if reductor:
    96|         0|            0|            0|  0.00%|                rv = reductor()
    97|         0|            0|            0|  0.00%|            else:
    98|         0|            0|            0|  0.00%|                raise Error("un(shallow)copyable object of type %s" % cls)
    99|         0|            0|            0|  0.00%|
   100|         0|            0|            0|  0.00%|    if isinstance(rv, str):
   101|         0|            0|            0|  0.00%|        return x
   102|         0|            0|            0|  0.00%|    return _reconstruct(x, None, *rv)
   103|         0|            0|            0|  0.00%|
   104|         0|            0|            0|  0.00%|
   105|         0|            0|            0|  0.00%|_copy_dispatch = d = {}
   106|         0|            0|            0|  0.00%|
   107|         0|            0|            0|  0.00%|def _copy_immutable(x):
   108|         0|            0|            0|  0.00%|    return x
   109|         0|            0|            0|  0.00%|for t in (type(None), int, float, bool, complex, str, tuple,
   110|         0|            0|            0|  0.00%|          bytes, frozenset, type, range, slice, property,
   111|         0|            0|            0|  0.00%|          types.BuiltinFunctionType, type(Ellipsis), type(NotImplemented),
   112|         0|            0|            0|  0.00%|          types.FunctionType, weakref.ref):
   113|         0|            0|            0|  0.00%|    d[t] = _copy_immutable
   114|         0|            0|            0|  0.00%|t = getattr(types, "CodeType", None)
   115|         0|            0|            0|  0.00%|if t is not None:
   116|         0|            0|            0|  0.00%|    d[t] = _copy_immutable
   117|         0|            0|            0|  0.00%|
   118|         0|            0|            0|  0.00%|d[list] = list.copy
   119|         0|            0|            0|  0.00%|d[dict] = dict.copy
   120|         0|            0|            0|  0.00%|d[set] = set.copy
   121|         0|            0|            0|  0.00%|d[bytearray] = bytearray.copy
   122|         0|            0|            0|  0.00%|
   123|         0|            0|            0|  0.00%|if PyStringMap is not None:
   124|         0|            0|            0|  0.00%|    d[PyStringMap] = PyStringMap.copy
   125|         0|            0|            0|  0.00%|
   126|         0|            0|            0|  0.00%|del d, t
   127|         0|            0|            0|  0.00%|
   128|      1040|    0.0054493|  5.23971e-06|  0.01%|def deepcopy(x, memo=None, _nil=[]):
   129|         0|            0|            0|  0.00%|    """Deep copy operation on arbitrary Python objects.
   130|         0|            0|            0|  0.00%|
   131|         0|            0|            0|  0.00%|    See the module's __doc__ string for more info.
   132|         0|            0|            0|  0.00%|    """
   133|         0|            0|            0|  0.00%|
   134|      1040|   0.00517392|  4.97492e-06|  0.01%|    if memo is None:
   135|         2|  1.33514e-05|  6.67572e-06|  0.00%|        memo = {}
   136|         0|            0|            0|  0.00%|
   137|      1040|   0.00675344|   6.4937e-06|  0.01%|    d = id(x)
   138|      1040|    0.0068779|  6.61336e-06|  0.01%|    y = memo.get(d, _nil)
   139|      1040|   0.00469828|  4.51757e-06|  0.01%|    if y is not _nil:
   140|         0|            0|            0|  0.00%|        return y
   141|         0|            0|            0|  0.00%|
   142|      1040|   0.00499463|  4.80253e-06|  0.01%|    cls = type(x)
   143|         0|            0|            0|  0.00%|
   144|      1040|   0.00662136|  6.36669e-06|  0.01%|    copier = _deepcopy_dispatch.get(cls)
   145|      1040|   0.00459194|  4.41533e-06|  0.01%|    if copier is not None:
   146|       792|   0.00979304|   1.2365e-05|  0.02%|        y = copier(x, memo)
(call)|       654|   0.00517416|  7.91156e-06|  0.01%|# /opt/conda/lib/python3.8/copy.py:182 _deepcopy_atomic
(call)|       138|    0.0556655|  0.000403373|  0.10%|# /opt/conda/lib/python3.8/copy.py:226 _deepcopy_dict
   147|         0|            0|            0|  0.00%|    else:
   148|       248|   0.00155497|  6.27002e-06|  0.00%|        if issubclass(cls, type):
   149|         0|            0|            0|  0.00%|            y = _deepcopy_atomic(x, memo)
   150|         0|            0|            0|  0.00%|        else:
   151|       248|   0.00165248|  6.66322e-06|  0.00%|            copier = getattr(x, "__deepcopy__", None)
   152|       248|   0.00110245|  4.44535e-06|  0.00%|            if copier is not None:
   153|       244|   0.00358725|  1.47018e-05|  0.01%|                y = copier(memo)
(call)|       244|      0.21517|  0.000881844|  0.40%|# /opt/conda/lib/python3.8/site-packages/torch/_tensor.py:51 __deepcopy__
   154|         0|            0|            0|  0.00%|            else:
   155|         4|  4.02927e-05|  1.00732e-05|  0.00%|                reductor = dispatch_table.get(cls)
   156|         4|  2.09808e-05|  5.24521e-06|  0.00%|                if reductor:
   157|         0|            0|            0|  0.00%|                    rv = reductor(x)
   158|         0|            0|            0|  0.00%|                else:
   159|         4|  2.93255e-05|  7.33137e-06|  0.00%|                    reductor = getattr(x, "__reduce_ex__", None)
   160|         4|  2.00272e-05|  5.00679e-06|  0.00%|                    if reductor is not None:
   161|         4|   4.8399e-05|  1.20997e-05|  0.00%|                        rv = reductor(4)
   162|         0|            0|            0|  0.00%|                    else:
   163|         0|            0|            0|  0.00%|                        reductor = getattr(x, "__reduce__", None)
   164|         0|            0|            0|  0.00%|                        if reductor:
   165|         0|            0|            0|  0.00%|                            rv = reductor()
   166|         0|            0|            0|  0.00%|                        else:
   167|         0|            0|            0|  0.00%|                            raise Error(
   168|         0|            0|            0|  0.00%|                                "un(deep)copyable object of type %s" % cls)
   169|         4|  3.31402e-05|  8.28505e-06|  0.00%|                if isinstance(rv, str):
   170|         0|            0|            0|  0.00%|                    y = x
   171|         0|            0|            0|  0.00%|                else:
   172|         4|  0.000113487|  2.83718e-05|  0.00%|                    y = _reconstruct(x, memo, *rv)
(call)|         4|     0.328224|     0.082056|  0.61%|# /opt/conda/lib/python3.8/copy.py:258 _reconstruct
   173|         0|            0|            0|  0.00%|
   174|         0|            0|            0|  0.00%|    # If is its own copy, don't memoize.
   175|      1040|   0.00479698|  4.61248e-06|  0.01%|    if y is not x:
   176|       386|   0.00176239|  4.56578e-06|  0.00%|        memo[d] = y
   177|       386|   0.00495243|  1.28301e-05|  0.01%|        _keep_alive(x, memo) # Make sure x lives at least as long as d
(call)|       386|   0.00611091|  1.58314e-05|  0.01%|# /opt/conda/lib/python3.8/copy.py:242 _keep_alive
   178|      1040|   0.00447941|  4.30712e-06|  0.01%|    return y
   179|         0|            0|            0|  0.00%|
   180|         0|            0|            0|  0.00%|_deepcopy_dispatch = d = {}
   181|         0|            0|            0|  0.00%|
   182|       654|   0.00270605|  4.13769e-06|  0.01%|def _deepcopy_atomic(x, memo):
   183|       654|   0.00246811|  3.77387e-06|  0.00%|    return x
   184|         0|            0|            0|  0.00%|d[type(None)] = _deepcopy_atomic
   185|         0|            0|            0|  0.00%|d[type(Ellipsis)] = _deepcopy_atomic
   186|         0|            0|            0|  0.00%|d[type(NotImplemented)] = _deepcopy_atomic
   187|         0|            0|            0|  0.00%|d[int] = _deepcopy_atomic
   188|         0|            0|            0|  0.00%|d[float] = _deepcopy_atomic
   189|         0|            0|            0|  0.00%|d[bool] = _deepcopy_atomic
   190|         0|            0|            0|  0.00%|d[complex] = _deepcopy_atomic
   191|         0|            0|            0|  0.00%|d[bytes] = _deepcopy_atomic
   192|         0|            0|            0|  0.00%|d[str] = _deepcopy_atomic
   193|         0|            0|            0|  0.00%|d[types.CodeType] = _deepcopy_atomic
   194|         0|            0|            0|  0.00%|d[type] = _deepcopy_atomic
   195|         0|            0|            0|  0.00%|d[types.BuiltinFunctionType] = _deepcopy_atomic
   196|         0|            0|            0|  0.00%|d[types.FunctionType] = _deepcopy_atomic
   197|         0|            0|            0|  0.00%|d[weakref.ref] = _deepcopy_atomic
   198|         0|            0|            0|  0.00%|d[property] = _deepcopy_atomic
   199|         0|            0|            0|  0.00%|
   200|         0|            0|            0|  0.00%|def _deepcopy_list(x, memo, deepcopy=deepcopy):
   201|         0|            0|            0|  0.00%|    y = []
   202|         0|            0|            0|  0.00%|    memo[id(x)] = y
   203|         0|            0|            0|  0.00%|    append = y.append
   204|         0|            0|            0|  0.00%|    for a in x:
   205|         0|            0|            0|  0.00%|        append(deepcopy(a, memo))
   206|         0|            0|            0|  0.00%|    return y
   207|         0|            0|            0|  0.00%|d[list] = _deepcopy_list
   208|         0|            0|            0|  0.00%|
   209|         0|            0|            0|  0.00%|def _deepcopy_tuple(x, memo, deepcopy=deepcopy):
   210|         0|            0|            0|  0.00%|    y = [deepcopy(a, memo) for a in x]
   211|         0|            0|            0|  0.00%|    # We're not going to put the tuple in the memo, but it's still important we
   212|         0|            0|            0|  0.00%|    # check for it, in case the tuple contains recursive mutable structures.
   213|         0|            0|            0|  0.00%|    try:
   214|         0|            0|            0|  0.00%|        return memo[id(x)]
   215|         0|            0|            0|  0.00%|    except KeyError:
   216|         0|            0|            0|  0.00%|        pass
   217|         0|            0|            0|  0.00%|    for k, j in zip(x, y):
   218|         0|            0|            0|  0.00%|        if k is not j:
   219|         0|            0|            0|  0.00%|            y = tuple(y)
   220|         0|            0|            0|  0.00%|            break
   221|         0|            0|            0|  0.00%|    else:
   222|         0|            0|            0|  0.00%|        y = x
   223|         0|            0|            0|  0.00%|    return y
   224|         0|            0|            0|  0.00%|d[tuple] = _deepcopy_tuple
   225|         0|            0|            0|  0.00%|
   226|       138|  0.000643492|  4.66298e-06|  0.00%|def _deepcopy_dict(x, memo, deepcopy=deepcopy):
   227|       138|  0.000599623|  4.34509e-06|  0.00%|    y = {}
   228|       138|  0.000822306|  5.95874e-06|  0.00%|    memo[id(x)] = y
   229|       276|     0.001477|  5.35146e-06|  0.00%|    for key, value in x.items():
   230|       138|   0.00310469|  2.24977e-05|  0.01%|        y[deepcopy(key, memo)] = deepcopy(value, memo)
(call)|       276|    0.0555186|  0.000201154|  0.10%|# /opt/conda/lib/python3.8/copy.py:128 deepcopy
   231|       138|  0.000511169|  3.70413e-06|  0.00%|    return y
   232|         0|            0|            0|  0.00%|d[dict] = _deepcopy_dict
   233|         0|            0|            0|  0.00%|if PyStringMap is not None:
   234|         0|            0|            0|  0.00%|    d[PyStringMap] = _deepcopy_dict
   235|         0|            0|            0|  0.00%|
   236|         0|            0|            0|  0.00%|def _deepcopy_method(x, memo): # Copy instance methods
   237|         0|            0|            0|  0.00%|    return type(x)(x.__func__, deepcopy(x.__self__, memo))
   238|         0|            0|            0|  0.00%|d[types.MethodType] = _deepcopy_method
   239|         0|            0|            0|  0.00%|
   240|         0|            0|            0|  0.00%|del d
   241|         0|            0|            0|  0.00%|
   242|       386|   0.00166583|  4.31562e-06|  0.00%|def _keep_alive(x, memo):
   243|         0|            0|            0|  0.00%|    """Keeps a reference to the object x in the memo.
   244|         0|            0|            0|  0.00%|
   245|         0|            0|            0|  0.00%|    Because we remember objects by their id, we have
   246|         0|            0|            0|  0.00%|    to assure that possibly temporary objects are kept
   247|         0|            0|            0|  0.00%|    alive by referencing them.
   248|         0|            0|            0|  0.00%|    We store a reference at the id of the memo, which should
   249|         0|            0|            0|  0.00%|    normally not be used unless someone tries to deepcopy
   250|         0|            0|            0|  0.00%|    the memo itself...
   251|         0|            0|            0|  0.00%|    """
   252|       386|   0.00145626|   3.7727e-06|  0.00%|    try:
   253|       386|    0.0029645|  7.68004e-06|  0.01%|        memo[id(memo)].append(x)
   254|         2|   1.0252e-05|    5.126e-06|  0.00%|    except KeyError:
   255|         0|            0|            0|  0.00%|        # aha, this is the first one :-)
   256|         2|  1.40667e-05|  7.03335e-06|  0.00%|        memo[id(memo)]=[x]
   257|         0|            0|            0|  0.00%|
   258|         4|  4.45843e-05|  1.11461e-05|  0.00%|def _reconstruct(x, memo, func, args,
   259|         0|            0|            0|  0.00%|                 state=None, listiter=None, dictiter=None,
   260|         0|            0|            0|  0.00%|                 deepcopy=deepcopy):
   261|         4|  2.74181e-05|  6.85453e-06|  0.00%|    deep = memo is not None
   262|         4|  2.28882e-05|  5.72205e-06|  0.00%|    if deep and args:
   263|         0|            0|            0|  0.00%|        args = (deepcopy(arg, memo) for arg in args)
   264|         4|  2.40803e-05|  6.02007e-06|  0.00%|    y = func(*args)
   265|         4|   2.3365e-05|  5.84126e-06|  0.00%|    if deep:
   266|         4|  3.09944e-05|   7.7486e-06|  0.00%|        memo[id(x)] = y
   267|         0|            0|            0|  0.00%|
   268|         4|  2.26498e-05|  5.66244e-06|  0.00%|    if state is not None:
   269|         2|  1.88351e-05|  9.41753e-06|  0.00%|        if deep:
   270|         2|  3.52859e-05|   1.7643e-05|  0.00%|            state = deepcopy(state, memo)
(call)|         2|   0.00640631|   0.00320315|  0.01%|# /opt/conda/lib/python3.8/copy.py:128 deepcopy
   271|         2|  2.21729e-05|  1.10865e-05|  0.00%|        if hasattr(y, '__setstate__'):
   272|         0|            0|            0|  0.00%|            y.__setstate__(state)
   273|         0|            0|            0|  0.00%|        else:
   274|         2|  1.88351e-05|  9.41753e-06|  0.00%|            if isinstance(state, tuple) and len(state) == 2:
   275|         0|            0|            0|  0.00%|                state, slotstate = state
   276|         0|            0|            0|  0.00%|            else:
   277|         2|  1.28746e-05|   6.4373e-06|  0.00%|                slotstate = None
   278|         2|  1.23978e-05|  6.19888e-06|  0.00%|            if state is not None:
   279|         2|  2.16961e-05|   1.0848e-05|  0.00%|                y.__dict__.update(state)
   280|         2|  1.28746e-05|   6.4373e-06|  0.00%|            if slotstate is not None:
   281|         0|            0|            0|  0.00%|                for key, value in slotstate.items():
   282|         0|            0|            0|  0.00%|                    setattr(y, key, value)
   283|         0|            0|            0|  0.00%|
   284|         4|  2.19345e-05|  5.48363e-06|  0.00%|    if listiter is not None:
   285|         0|            0|            0|  0.00%|        if deep:
   286|         0|            0|            0|  0.00%|            for item in listiter:
   287|         0|            0|            0|  0.00%|                item = deepcopy(item, memo)
   288|         0|            0|            0|  0.00%|                y.append(item)
   289|         0|            0|            0|  0.00%|        else:
   290|         0|            0|            0|  0.00%|            for item in listiter:
   291|         0|            0|            0|  0.00%|                y.append(item)
   292|         4|  2.21729e-05|  5.54323e-06|  0.00%|    if dictiter is not None:
   293|         4|  2.12193e-05|  5.30481e-06|  0.00%|        if deep:
   294|       384|   0.00199723|  5.20113e-06|  0.00%|            for key, value in dictiter:
   295|       380|   0.00554895|  1.46025e-05|  0.01%|                key = deepcopy(key, memo)
(call)|       380|    0.0277603|  7.30533e-05|  0.05%|# /opt/conda/lib/python3.8/copy.py:128 deepcopy
   296|       380|   0.00586176|  1.54257e-05|  0.01%|                value = deepcopy(value, memo)
(call)|       380|     0.283663|   0.00074648|  0.53%|# /opt/conda/lib/python3.8/copy.py:128 deepcopy
   297|       380|   0.00213099|  5.60786e-06|  0.00%|                y[key] = value
   298|         0|            0|            0|  0.00%|        else:
   299|         0|            0|            0|  0.00%|            for key, value in dictiter:
   300|         0|            0|            0|  0.00%|                y[key] = value
   301|         4|  1.83582e-05|  4.58956e-06|  0.00%|    return y
   302|         0|            0|            0|  0.00%|
   303|         0|            0|            0|  0.00%|del types, weakref, PyStringMap
File: /opt/conda/lib/python3.8/site-packages/torch/storage.py
File duration: 0.107162s (0.20%)
Line #|      Hits|         Time| Time per hit|      %|Source code
------+----------+-------------+-------------+-------+-----------
     1|         0|            0|            0|  0.00%|import io
     2|         0|            0|            0|  0.00%|
     3|         0|            0|            0|  0.00%|import torch
     4|         0|            0|            0|  0.00%|from ._utils import _type, _cuda
     5|         0|            0|            0|  0.00%|from typing import Any, TypeVar, Type
     6|         0|            0|            0|  0.00%|
     7|         0|            0|            0|  0.00%|T = TypeVar('T', bound='_StorageBase')
     8|         0|            0|            0|  0.00%|class _StorageBase(object):
     9|         0|            0|            0|  0.00%|    _cdata: Any
    10|         0|            0|            0|  0.00%|    is_cuda: bool = False
    11|         0|            0|            0|  0.00%|    is_sparse: bool = False
    12|         0|            0|            0|  0.00%|
    13|       488|    0.0023849|  4.88709e-06|  0.00%|    def __init__(self, *args, **kwargs): ...  # noqa: E704
    14|         0|            0|            0|  0.00%|    def __len__(self) -> int: ...  # noqa: E704
    15|         0|            0|            0|  0.00%|    def __getitem__(self, idx): ...  # noqa: E704
    16|         0|            0|            0|  0.00%|    def copy_(self, source: T) -> T: ...  # noqa: E704
    17|         0|            0|            0|  0.00%|    def size(self) -> int: ...  # noqa: E704
    18|         0|            0|            0|  0.00%|    def type(self, dtype: str = None, non_blocking: bool = False) -> T: ...  # noqa: E704
    19|         0|            0|            0|  0.00%|    def cuda(self, device=None, non_blocking=False, **kwargs) -> T: ...  # noqa: E704
    20|         0|            0|            0|  0.00%|    def element_size(self) -> int: ...  # noqa: E704
    21|         0|            0|            0|  0.00%|    def get_device(self) -> int: ...  # noqa: E704
    22|         0|            0|            0|  0.00%|
    23|         0|            0|            0|  0.00%|    # Defined in torch/csrc/generic/StorageSharing.cpp
    24|         0|            0|            0|  0.00%|    def _share_filename_(self): ...  # noqa: E704
    25|         0|            0|            0|  0.00%|    def _share_fd_(self): ...  # noqa: E704
    26|         0|            0|            0|  0.00%|    @classmethod
    27|         0|            0|            0|  0.00%|    def _new_using_filename(cls: Type[T], size: int) -> T: ...  # noqa: E704
    28|         0|            0|            0|  0.00%|    @classmethod
    29|         0|            0|            0|  0.00%|    def _new_using_fd(cls: Type[T], size: int) -> T: ...  # noqa: E704
    30|         0|            0|            0|  0.00%|
    31|         0|            0|            0|  0.00%|    def __str__(self):
    32|         0|            0|            0|  0.00%|        content = ' ' + '\n '.join(str(self[i]) for i in range(len(self)))
    33|         0|            0|            0|  0.00%|        return content + f'\n[{torch.typename(self)} of size {len(self)}]'
    34|         0|            0|            0|  0.00%|
    35|         0|            0|            0|  0.00%|    def __repr__(self):
    36|         0|            0|            0|  0.00%|        return str(self)
    37|         0|            0|            0|  0.00%|
    38|         0|            0|            0|  0.00%|    def __iter__(self):
    39|         0|            0|            0|  0.00%|        return iter(map(lambda i: self[i], range(self.size())))
    40|         0|            0|            0|  0.00%|
    41|         0|            0|            0|  0.00%|    def __copy__(self):
    42|         0|            0|            0|  0.00%|        return self.clone()
    43|         0|            0|            0|  0.00%|
    44|       244|   0.00129843|  5.32142e-06|  0.00%|    def __deepcopy__(self, memo):
    45|       244|   0.00173402|  7.10663e-06|  0.00%|        memo = memo.setdefault('torch', {})
    46|       244|    0.0010972|  4.49673e-06|  0.00%|        if self._cdata in memo:
    47|         0|            0|            0|  0.00%|            return memo[self._cdata]
    48|       244|   0.00293636|  1.20343e-05|  0.01%|        new_storage = self.clone()
(call)|       244|     0.132753|  0.000544069|  0.25%|# /opt/conda/lib/python3.8/site-packages/torch/storage.py:60 clone
    49|       244|   0.00105834|  4.33746e-06|  0.00%|        memo[self._cdata] = new_storage
    50|       244|  0.000886202|  3.63197e-06|  0.00%|        return new_storage
    51|         0|            0|            0|  0.00%|
    52|         0|            0|            0|  0.00%|    def __reduce__(self):
    53|         0|            0|            0|  0.00%|        b = io.BytesIO()
    54|         0|            0|            0|  0.00%|        torch.save(self, b, _use_new_zipfile_serialization=False)
    55|         0|            0|            0|  0.00%|        return (_load_from_bytes, (b.getvalue(),))
    56|         0|            0|            0|  0.00%|
    57|         0|            0|            0|  0.00%|    def __sizeof__(self):
    58|         0|            0|            0|  0.00%|        return super(_StorageBase, self).__sizeof__() + self.element_size() * self.size()
    59|         0|            0|            0|  0.00%|
    60|       244|   0.00107408|  4.40195e-06|  0.00%|    def clone(self):
    61|         0|            0|            0|  0.00%|        """Returns a copy of this storage"""
    62|       244|  0.000994444|  4.07559e-06|  0.00%|        device = self.get_device() if self.is_cuda else -1
    63|       244|   0.00523305|  2.14469e-05|  0.01%|        with torch.cuda.device(device):
(call)|       244|     0.028322|  0.000116074|  0.05%|# /opt/conda/lib/python3.8/site-packages/torch/cuda/__init__.py:218 __init__
(call)|       244|   0.00292087|  1.19708e-05|  0.01%|# /opt/conda/lib/python3.8/site-packages/torch/cuda/__init__.py:222 __enter__
    64|       244|     0.088465|  0.000362561|  0.17%|            return type(self)(self.size()).copy_(self)
(call)|       244|    0.0023849|  9.77418e-06|  0.00%|# /opt/conda/lib/python3.8/site-packages/torch/storage.py:13 __init__
(call)|       244|    0.0033586|  1.37648e-05|  0.01%|# /opt/conda/lib/python3.8/site-packages/torch/cuda/__init__.py:231 __exit__
    65|         0|            0|            0|  0.00%|
    66|         0|            0|            0|  0.00%|    def tolist(self):
    67|         0|            0|            0|  0.00%|        """Returns a list containing the elements of this storage"""
    68|         0|            0|            0|  0.00%|        return list(self)
    69|         0|            0|            0|  0.00%|
    70|         0|            0|            0|  0.00%|    def cpu(self):
    71|         0|            0|            0|  0.00%|        """Returns a CPU copy of this storage if it's not already on the CPU"""
    72|         0|            0|            0|  0.00%|        return self.type(getattr(torch, self.__class__.__name__))
    73|         0|            0|            0|  0.00%|
    74|         0|            0|            0|  0.00%|    def double(self):
    75|         0|            0|            0|  0.00%|        """Casts this storage to double type"""
    76|         0|            0|            0|  0.00%|        return self.type(type(self).__module__ + '.DoubleStorage')
    77|         0|            0|            0|  0.00%|
    78|         0|            0|            0|  0.00%|    def float(self):
    79|         0|            0|            0|  0.00%|        """Casts this storage to float type"""
    80|         0|            0|            0|  0.00%|        return self.type(type(self).__module__ + '.FloatStorage')
    81|         0|            0|            0|  0.00%|
    82|         0|            0|            0|  0.00%|    def half(self):
    83|         0|            0|            0|  0.00%|        """Casts this storage to half type"""
    84|         0|            0|            0|  0.00%|        return self.type(type(self).__module__ + '.HalfStorage')
    85|         0|            0|            0|  0.00%|
    86|         0|            0|            0|  0.00%|    def long(self):
    87|         0|            0|            0|  0.00%|        """Casts this storage to long type"""
    88|         0|            0|            0|  0.00%|        return self.type(type(self).__module__ + '.LongStorage')
    89|         0|            0|            0|  0.00%|
    90|         0|            0|            0|  0.00%|    def int(self):
    91|         0|            0|            0|  0.00%|        """Casts this storage to int type"""
    92|         0|            0|            0|  0.00%|        return self.type(type(self).__module__ + '.IntStorage')
    93|         0|            0|            0|  0.00%|
    94|         0|            0|            0|  0.00%|    def short(self):
    95|         0|            0|            0|  0.00%|        """Casts this storage to short type"""
    96|         0|            0|            0|  0.00%|        return self.type(type(self).__module__ + '.ShortStorage')
    97|         0|            0|            0|  0.00%|
    98|         0|            0|            0|  0.00%|    def char(self):
    99|         0|            0|            0|  0.00%|        """Casts this storage to char type"""
   100|         0|            0|            0|  0.00%|        return self.type(type(self).__module__ + '.CharStorage')
   101|         0|            0|            0|  0.00%|
   102|         0|            0|            0|  0.00%|    def byte(self):
   103|         0|            0|            0|  0.00%|        """Casts this storage to byte type"""
   104|         0|            0|            0|  0.00%|        return self.type(type(self).__module__ + '.ByteStorage')
   105|         0|            0|            0|  0.00%|
   106|         0|            0|            0|  0.00%|    def bool(self):
   107|         0|            0|            0|  0.00%|        """Casts this storage to bool type"""
   108|         0|            0|            0|  0.00%|        return self.type(type(self).__module__ + '.BoolStorage')
   109|         0|            0|            0|  0.00%|
   110|         0|            0|            0|  0.00%|    def bfloat16(self):
   111|         0|            0|            0|  0.00%|        """Casts this storage to bfloat16 type"""
   112|         0|            0|            0|  0.00%|        return self.type(type(self).__module__ + '.BFloat16Storage')
   113|         0|            0|            0|  0.00%|
   114|         0|            0|            0|  0.00%|    def complex_double(self):
   115|         0|            0|            0|  0.00%|        """Casts this storage to complex double type"""
   116|         0|            0|            0|  0.00%|        return self.type(type(self).__module__ + '.ComplexDoubleStorage')
   117|         0|            0|            0|  0.00%|
   118|         0|            0|            0|  0.00%|    def complex_float(self):
   119|         0|            0|            0|  0.00%|        """Casts this storage to complex float type"""
   120|         0|            0|            0|  0.00%|        return self.type(type(self).__module__ + '.ComplexFloatStorage')
   121|         0|            0|            0|  0.00%|
   122|         0|            0|            0|  0.00%|    def pin_memory(self):
   123|         0|            0|            0|  0.00%|        """Copies the storage to pinned memory, if it's not already pinned."""
   124|         0|            0|            0|  0.00%|        if self.is_cuda:
   125|         0|            0|            0|  0.00%|            raise TypeError(f"cannot pin '{self.type()}' only CPU memory can be pinned")
   126|         0|            0|            0|  0.00%|        import torch.cuda
   127|         0|            0|            0|  0.00%|        allocator = torch.cuda._host_allocator()  # type: ignore[attr-defined]
   128|         0|            0|            0|  0.00%|        return type(self)(self.size(), allocator=allocator).copy_(self)
   129|         0|            0|            0|  0.00%|
   130|         0|            0|            0|  0.00%|    def share_memory_(self):
   131|         0|            0|            0|  0.00%|        """Moves the storage to shared memory.
   132|         0|            0|            0|  0.00%|
   133|         0|            0|            0|  0.00%|        This is a no-op for storages already in shared memory and for CUDA
   134|         0|            0|            0|  0.00%|        storages, which do not need to be moved for sharing across processes.
   135|         0|            0|            0|  0.00%|        Storages in shared memory cannot be resized.
   136|         0|            0|            0|  0.00%|
   137|         0|            0|            0|  0.00%|        Returns: self
   138|         0|            0|            0|  0.00%|        """
   139|         0|            0|            0|  0.00%|        from torch.multiprocessing import get_sharing_strategy
   140|         0|            0|            0|  0.00%|        if self.is_cuda:
   141|         0|            0|            0|  0.00%|            pass  # CUDA doesn't use POSIX shared memory
   142|         0|            0|            0|  0.00%|        elif get_sharing_strategy() == 'file_system':
   143|         0|            0|            0|  0.00%|            self._share_filename_()
   144|         0|            0|            0|  0.00%|        else:
   145|         0|            0|            0|  0.00%|            self._share_fd_()
   146|         0|            0|            0|  0.00%|        return self
   147|         0|            0|            0|  0.00%|
   148|         0|            0|            0|  0.00%|    @classmethod
   149|         0|            0|            0|  0.00%|    def _new_shared(cls, size):
   150|         0|            0|            0|  0.00%|        """Creates a new storage in shared memory with the same data type"""
   151|         0|            0|            0|  0.00%|        from torch.multiprocessing import get_sharing_strategy
   152|         0|            0|            0|  0.00%|        if cls.is_cuda:
   153|         0|            0|            0|  0.00%|            return cls(size)
   154|         0|            0|            0|  0.00%|        elif get_sharing_strategy() == 'file_system':
   155|         0|            0|            0|  0.00%|            return cls._new_using_filename(size)
   156|         0|            0|            0|  0.00%|        else:
   157|         0|            0|            0|  0.00%|            return cls._new_using_fd(size)
   158|         0|            0|            0|  0.00%|
   159|         0|            0|            0|  0.00%|
   160|         0|            0|            0|  0.00%|def _load_from_bytes(b):
   161|         0|            0|            0|  0.00%|    return torch.load(io.BytesIO(b))
   162|         0|            0|            0|  0.00%|
   163|         0|            0|            0|  0.00%|
   164|         0|            0|            0|  0.00%|_StorageBase.type = _type  # type: ignore[assignment]
   165|         0|            0|            0|  0.00%|_StorageBase.cuda = _cuda  # type: ignore[assignment]
File: /opt/conda/lib/python3.8/hmac.py
File duration: 0.105253s (0.20%)
Line #|      Hits|         Time| Time per hit|      %|Source code
------+----------+-------------+-------------+-------+-----------
     1|         0|            0|            0|  0.00%|"""HMAC (Keyed-Hashing for Message Authentication) module.
     2|         0|            0|            0|  0.00%|
     3|         0|            0|            0|  0.00%|Implements the HMAC algorithm as described by RFC 2104.
     4|         0|            0|            0|  0.00%|"""
     5|         0|            0|            0|  0.00%|
     6|         0|            0|            0|  0.00%|import warnings as _warnings
     7|         0|            0|            0|  0.00%|from _operator import _compare_digest as compare_digest
     8|         0|            0|            0|  0.00%|try:
     9|         0|            0|            0|  0.00%|    import _hashlib as _hashopenssl
    10|         0|            0|            0|  0.00%|except ImportError:
    11|         0|            0|            0|  0.00%|    _hashopenssl = None
    12|         0|            0|            0|  0.00%|    _openssl_md_meths = None
    13|         0|            0|            0|  0.00%|else:
    14|         0|            0|            0|  0.00%|    _openssl_md_meths = frozenset(_hashopenssl.openssl_md_meth_names)
    15|         0|            0|            0|  0.00%|import hashlib as _hashlib
    16|         0|            0|            0|  0.00%|
    17|         0|            0|            0|  0.00%|trans_5C = bytes((x ^ 0x5C) for x in range(256))
    18|         0|            0|            0|  0.00%|trans_36 = bytes((x ^ 0x36) for x in range(256))
    19|         0|            0|            0|  0.00%|
    20|         0|            0|            0|  0.00%|# The size of the digests returned by HMAC depends on the underlying
    21|         0|            0|            0|  0.00%|# hashing module used.  Use digest_size from the instance of HMAC instead.
    22|         0|            0|            0|  0.00%|digest_size = None
    23|         0|            0|            0|  0.00%|
    24|         0|            0|            0|  0.00%|
    25|         0|            0|            0|  0.00%|
    26|         0|            0|            0|  0.00%|class HMAC:
    27|         0|            0|            0|  0.00%|    """RFC 2104 HMAC class.  Also complies with RFC 4231.
    28|         0|            0|            0|  0.00%|
    29|         0|            0|            0|  0.00%|    This supports the API for Cryptographic Hash Functions (PEP 247).
    30|         0|            0|            0|  0.00%|    """
    31|         0|            0|            0|  0.00%|    blocksize = 64  # 512-bit HMAC; can be changed in subclasses.
    32|         0|            0|            0|  0.00%|
    33|       400|   0.00261617|  6.54042e-06|  0.00%|    def __init__(self, key, msg=None, digestmod=''):
    34|         0|            0|            0|  0.00%|        """Create a new HMAC object.
    35|         0|            0|            0|  0.00%|
    36|         0|            0|            0|  0.00%|        key: bytes or buffer, key for the keyed hash object.
    37|         0|            0|            0|  0.00%|        msg: bytes or buffer, Initial input for the hash or None.
    38|         0|            0|            0|  0.00%|        digestmod: A hash name suitable for hashlib.new(). *OR*
    39|         0|            0|            0|  0.00%|                   A hashlib constructor returning a new hash object. *OR*
    40|         0|            0|            0|  0.00%|                   A module supporting PEP 247.
    41|         0|            0|            0|  0.00%|
    42|         0|            0|            0|  0.00%|                   Required as of 3.8, despite its position after the optional
    43|         0|            0|            0|  0.00%|                   msg argument.  Passing it as a keyword argument is
    44|         0|            0|            0|  0.00%|                   recommended, though not required for legacy API reasons.
    45|         0|            0|            0|  0.00%|        """
    46|         0|            0|            0|  0.00%|
    47|       400|   0.00304961|  7.62403e-06|  0.01%|        if not isinstance(key, (bytes, bytearray)):
    48|         0|            0|            0|  0.00%|            raise TypeError("key: expected bytes or bytearray, but got %r" % type(key).__name__)
    49|         0|            0|            0|  0.00%|
    50|       400|   0.00201583|  5.03957e-06|  0.00%|        if not digestmod:
    51|         0|            0|            0|  0.00%|            raise TypeError("Missing required parameter 'digestmod'.")
    52|         0|            0|            0|  0.00%|
    53|       400|   0.00270391|  6.75976e-06|  0.01%|        if callable(digestmod):
    54|         0|            0|            0|  0.00%|            self.digest_cons = digestmod
    55|       400|   0.00264049|  6.60121e-06|  0.00%|        elif isinstance(digestmod, str):
    56|      2000|    0.0169821|  8.49104e-06|  0.03%|            self.digest_cons = lambda d=b'': _hashlib.new(digestmod, d)
(call)|       800|    0.0168066|  2.10083e-05|  0.03%|# /opt/conda/lib/python3.8/hashlib.py:146 __hash_new
    57|         0|            0|            0|  0.00%|        else:
    58|         0|            0|            0|  0.00%|            self.digest_cons = lambda d=b'': digestmod.new(d)
    59|         0|            0|            0|  0.00%|
    60|       400|   0.00593042|  1.48261e-05|  0.01%|        self.outer = self.digest_cons()
(call)|       400|    0.0169857|  4.24641e-05|  0.03%|# /opt/conda/lib/python3.8/hmac.py:56 <lambda>
    61|       400|   0.00575757|  1.43939e-05|  0.01%|        self.inner = self.digest_cons()
(call)|       400|    0.0144343|  3.60858e-05|  0.03%|# /opt/conda/lib/python3.8/hmac.py:56 <lambda>
    62|       400|   0.00214767|  5.36919e-06|  0.00%|        self.digest_size = self.inner.digest_size
    63|         0|            0|            0|  0.00%|
    64|       400|   0.00286579|  7.16448e-06|  0.01%|        if hasattr(self.inner, 'block_size'):
    65|       400|   0.00197697|  4.94242e-06|  0.00%|            blocksize = self.inner.block_size
    66|       400|   0.00187278|  4.68194e-06|  0.00%|            if blocksize < 16:
    67|         0|            0|            0|  0.00%|                _warnings.warn('block_size of %d seems too small; using our '
    68|         0|            0|            0|  0.00%|                               'default of %d.' % (blocksize, self.blocksize),
    69|         0|            0|            0|  0.00%|                               RuntimeWarning, 2)
    70|         0|            0|            0|  0.00%|                blocksize = self.blocksize
    71|         0|            0|            0|  0.00%|        else:
    72|         0|            0|            0|  0.00%|            _warnings.warn('No block_size attribute on given digest object; '
    73|         0|            0|            0|  0.00%|                           'Assuming %d.' % (self.blocksize),
    74|         0|            0|            0|  0.00%|                           RuntimeWarning, 2)
    75|         0|            0|            0|  0.00%|            blocksize = self.blocksize
    76|         0|            0|            0|  0.00%|
    77|         0|            0|            0|  0.00%|        # self.blocksize is the default blocksize. self.block_size is
    78|         0|            0|            0|  0.00%|        # effective block size as well as the public API attribute.
    79|       400|   0.00185084|  4.62711e-06|  0.00%|        self.block_size = blocksize
    80|         0|            0|            0|  0.00%|
    81|       400|   0.00264883|  6.62208e-06|  0.00%|        if len(key) > blocksize:
    82|         0|            0|            0|  0.00%|            key = self.digest_cons(key).digest()
    83|         0|            0|            0|  0.00%|
    84|       400|   0.00300312|   7.5078e-06|  0.01%|        key = key.ljust(blocksize, b'\0')
    85|       400|   0.00389194|  9.72986e-06|  0.01%|        self.outer.update(key.translate(trans_5C))
    86|       400|   0.00359559|  8.98898e-06|  0.01%|        self.inner.update(key.translate(trans_36))
    87|       400|   0.00192881|  4.82202e-06|  0.00%|        if msg is not None:
    88|       400|   0.00560617|  1.40154e-05|  0.01%|            self.update(msg)
(call)|       400|   0.00453043|  1.13261e-05|  0.01%|# /opt/conda/lib/python3.8/hmac.py:94 update
    89|         0|            0|            0|  0.00%|
    90|         0|            0|            0|  0.00%|    @property
    91|         0|            0|            0|  0.00%|    def name(self):
    92|         0|            0|            0|  0.00%|        return "hmac-" + self.inner.name
    93|         0|            0|            0|  0.00%|
    94|       400|    0.0020678|  5.16951e-06|  0.00%|    def update(self, msg):
    95|         0|            0|            0|  0.00%|        """Feed data from msg into this hashing object."""
    96|       400|   0.00246263|  6.15656e-06|  0.00%|        self.inner.update(msg)
    97|         0|            0|            0|  0.00%|
    98|         0|            0|            0|  0.00%|    def copy(self):
    99|         0|            0|            0|  0.00%|        """Return a separate copy of this hashing object.
   100|         0|            0|            0|  0.00%|
   101|         0|            0|            0|  0.00%|        An update to this copy won't affect the original object.
   102|         0|            0|            0|  0.00%|        """
   103|         0|            0|            0|  0.00%|        # Call __new__ directly to avoid the expensive __init__.
   104|         0|            0|            0|  0.00%|        other = self.__class__.__new__(self.__class__)
   105|         0|            0|            0|  0.00%|        other.digest_cons = self.digest_cons
   106|         0|            0|            0|  0.00%|        other.digest_size = self.digest_size
   107|         0|            0|            0|  0.00%|        other.inner = self.inner.copy()
   108|         0|            0|            0|  0.00%|        other.outer = self.outer.copy()
   109|         0|            0|            0|  0.00%|        return other
   110|         0|            0|            0|  0.00%|
   111|       400|   0.00172901|  4.32253e-06|  0.00%|    def _current(self):
   112|         0|            0|            0|  0.00%|        """Return a hash object for the current state.
   113|         0|            0|            0|  0.00%|
   114|         0|            0|            0|  0.00%|        To be used only internally with digest() and hexdigest().
   115|         0|            0|            0|  0.00%|        """
   116|       400|   0.00280523|  7.01308e-06|  0.01%|        h = self.outer.copy()
   117|       400|   0.00369358|  9.23395e-06|  0.01%|        h.update(self.inner.digest())
   118|       400|   0.00161529|  4.03821e-06|  0.00%|        return h
   119|         0|            0|            0|  0.00%|
   120|       400|   0.00195289|  4.88222e-06|  0.00%|    def digest(self):
   121|         0|            0|            0|  0.00%|        """Return the hash value of this hashing object.
   122|         0|            0|            0|  0.00%|
   123|         0|            0|            0|  0.00%|        This returns the hmac value as bytes.  The object is
   124|         0|            0|            0|  0.00%|        not altered in any way by this function; you can continue
   125|         0|            0|            0|  0.00%|        updating the object after calling this function.
   126|         0|            0|            0|  0.00%|        """
   127|       400|   0.00493622|  1.23405e-05|  0.01%|        h = self._current()
(call)|       400|   0.00984311|  2.46078e-05|  0.02%|# /opt/conda/lib/python3.8/hmac.py:111 _current
   128|       400|   0.00257325|  6.43313e-06|  0.00%|        return h.digest()
   129|         0|            0|            0|  0.00%|
   130|         0|            0|            0|  0.00%|    def hexdigest(self):
   131|         0|            0|            0|  0.00%|        """Like digest(), but returns a string of hexadecimal digits instead.
   132|         0|            0|            0|  0.00%|        """
   133|         0|            0|            0|  0.00%|        h = self._current()
   134|         0|            0|            0|  0.00%|        return h.hexdigest()
   135|         0|            0|            0|  0.00%|
   136|       400|   0.00201964|  5.04911e-06|  0.00%|def new(key, msg=None, digestmod=''):
   137|         0|            0|            0|  0.00%|    """Create a new hashing object and return it.
   138|         0|            0|            0|  0.00%|
   139|         0|            0|            0|  0.00%|    key: bytes or buffer, The starting key for the hash.
   140|         0|            0|            0|  0.00%|    msg: bytes or buffer, Initial input for the hash, or None.
   141|         0|            0|            0|  0.00%|    digestmod: A hash name suitable for hashlib.new(). *OR*
   142|         0|            0|            0|  0.00%|               A hashlib constructor returning a new hash object. *OR*
   143|         0|            0|            0|  0.00%|               A module supporting PEP 247.
   144|         0|            0|            0|  0.00%|
   145|         0|            0|            0|  0.00%|               Required as of 3.8, despite its position after the optional
   146|         0|            0|            0|  0.00%|               msg argument.  Passing it as a keyword argument is
   147|         0|            0|            0|  0.00%|               recommended, though not required for legacy API reasons.
   148|         0|            0|            0|  0.00%|
   149|         0|            0|            0|  0.00%|    You can now feed arbitrary bytes into the object using its update()
   150|         0|            0|            0|  0.00%|    method, and can ask for the hash value at any time by calling its digest()
   151|         0|            0|            0|  0.00%|    or hexdigest() methods.
   152|         0|            0|            0|  0.00%|    """
   153|       400|   0.00631332|  1.57833e-05|  0.01%|    return HMAC(key, msg, digestmod)
(call)|       400|    0.0944216|  0.000236054|  0.18%|# /opt/conda/lib/python3.8/hmac.py:33 __init__
   154|         0|            0|            0|  0.00%|
   155|         0|            0|            0|  0.00%|
   156|         0|            0|            0|  0.00%|def digest(key, msg, digest):
   157|         0|            0|            0|  0.00%|    """Fast inline implementation of HMAC.
   158|         0|            0|            0|  0.00%|
   159|         0|            0|            0|  0.00%|    key: bytes or buffer, The key for the keyed hash object.
   160|         0|            0|            0|  0.00%|    msg: bytes or buffer, Input message.
   161|         0|            0|            0|  0.00%|    digest: A hash name suitable for hashlib.new() for best performance. *OR*
   162|         0|            0|            0|  0.00%|            A hashlib constructor returning a new hash object. *OR*
   163|         0|            0|            0|  0.00%|            A module supporting PEP 247.
   164|         0|            0|            0|  0.00%|    """
   165|         0|            0|            0|  0.00%|    if (_hashopenssl is not None and
   166|         0|            0|            0|  0.00%|            isinstance(digest, str) and digest in _openssl_md_meths):
   167|         0|            0|            0|  0.00%|        return _hashopenssl.hmac_digest(key, msg, digest)
   168|         0|            0|            0|  0.00%|
   169|         0|            0|            0|  0.00%|    if callable(digest):
   170|         0|            0|            0|  0.00%|        digest_cons = digest
   171|         0|            0|            0|  0.00%|    elif isinstance(digest, str):
   172|         0|            0|            0|  0.00%|        digest_cons = lambda d=b'': _hashlib.new(digest, d)
   173|         0|            0|            0|  0.00%|    else:
   174|         0|            0|            0|  0.00%|        digest_cons = lambda d=b'': digest.new(d)
   175|         0|            0|            0|  0.00%|
   176|         0|            0|            0|  0.00%|    inner = digest_cons()
   177|         0|            0|            0|  0.00%|    outer = digest_cons()
   178|         0|            0|            0|  0.00%|    blocksize = getattr(inner, 'block_size', 64)
   179|         0|            0|            0|  0.00%|    if len(key) > blocksize:
   180|         0|            0|            0|  0.00%|        key = digest_cons(key).digest()
   181|         0|            0|            0|  0.00%|    key = key + b'\x00' * (blocksize - len(key))
   182|         0|            0|            0|  0.00%|    inner.update(key.translate(trans_36))
   183|         0|            0|            0|  0.00%|    outer.update(key.translate(trans_5C))
   184|         0|            0|            0|  0.00%|    inner.update(msg)
   185|         0|            0|            0|  0.00%|    outer.update(inner.digest())
   186|         0|            0|            0|  0.00%|    return outer.digest()
File: /opt/conda/lib/python3.8/site-packages/torch/nn/modules/container.py
File duration: 0.0614235s (0.11%)
Line #|      Hits|         Time| Time per hit|      %|Source code
------+----------+-------------+-------------+-------+-----------
     1|         0|            0|            0|  0.00%|import warnings
     2|         0|            0|            0|  0.00%|from collections import OrderedDict, abc as container_abcs
     3|         0|            0|            0|  0.00%|from itertools import islice
     4|         0|            0|            0|  0.00%|import operator
     5|         0|            0|            0|  0.00%|
     6|         0|            0|            0|  0.00%|import torch
     7|         0|            0|            0|  0.00%|from .module import Module
     8|         0|            0|            0|  0.00%|from torch._jit_internal import _copy_to_script_wrapper
     9|         0|            0|            0|  0.00%|
    10|         0|            0|            0|  0.00%|from typing import Any, Iterable, Iterator, Mapping, Optional, TYPE_CHECKING, overload, Tuple, TypeVar, Union
    11|         0|            0|            0|  0.00%|
    12|         0|            0|            0|  0.00%|if TYPE_CHECKING:
    13|         0|            0|            0|  0.00%|    from torch.nn import Parameter
    14|         0|            0|            0|  0.00%|
    15|         0|            0|            0|  0.00%|T = TypeVar('T', bound=Module)
    16|         0|            0|            0|  0.00%|
    17|         0|            0|            0|  0.00%|
    18|         0|            0|            0|  0.00%|class Container(Module):
    19|         0|            0|            0|  0.00%|
    20|         0|            0|            0|  0.00%|    def __init__(self, **kwargs: Any) -> None:
    21|         0|            0|            0|  0.00%|        super(Container, self).__init__()
    22|         0|            0|            0|  0.00%|        # DeprecationWarning is ignored by default <sigh>
    23|         0|            0|            0|  0.00%|        warnings.warn("nn.Container is deprecated. All of it's functionality "
    24|         0|            0|            0|  0.00%|                      "is now implemented in nn.Module. Subclass that instead.")
    25|         0|            0|            0|  0.00%|        for key, value in kwargs.items():
    26|         0|            0|            0|  0.00%|            self.add_module(key, value)
    27|         0|            0|            0|  0.00%|
    28|         0|            0|            0|  0.00%|
    29|         0|            0|            0|  0.00%|class Sequential(Module):
    30|         0|            0|            0|  0.00%|    r"""A sequential container.
    31|         0|            0|            0|  0.00%|    Modules will be added to it in the order they are passed in the
    32|         0|            0|            0|  0.00%|    constructor. Alternatively, an ``OrderedDict`` of modules can be
    33|         0|            0|            0|  0.00%|    passed in. The ``forward()`` method of ``Sequential`` accepts any
    34|         0|            0|            0|  0.00%|    input and forwards it to the first module it contains. It then
    35|         0|            0|            0|  0.00%|    "chains" outputs to inputs sequentially for each subsequent module,
    36|         0|            0|            0|  0.00%|    finally returning the output of the last module.
    37|         0|            0|            0|  0.00%|
    38|         0|            0|            0|  0.00%|    The value a ``Sequential`` provides over manually calling a sequence
    39|         0|            0|            0|  0.00%|    of modules is that it allows treating the whole container as a
    40|         0|            0|            0|  0.00%|    single module, such that performing a transformation on the
    41|         0|            0|            0|  0.00%|    ``Sequential`` applies to each of the modules it stores (which are
    42|         0|            0|            0|  0.00%|    each a registered submodule of the ``Sequential``).
    43|         0|            0|            0|  0.00%|
    44|         0|            0|            0|  0.00%|    What's the difference between a ``Sequential`` and a
    45|         0|            0|            0|  0.00%|    :class:`torch.nn.ModuleList`? A ``ModuleList`` is exactly what it
    46|         0|            0|            0|  0.00%|    sounds like--a list for storing ``Module`` s! On the other hand,
    47|         0|            0|            0|  0.00%|    the layers in a ``Sequential`` are connected in a cascading way.
    48|         0|            0|            0|  0.00%|
    49|         0|            0|            0|  0.00%|    Example::
    50|         0|            0|            0|  0.00%|
    51|         0|            0|            0|  0.00%|        # Using Sequential to create a small model. When `model` is run,
    52|         0|            0|            0|  0.00%|        # input will first be passed to `Conv2d(1,20,5)`. The output of
    53|         0|            0|            0|  0.00%|        # `Conv2d(1,20,5)` will be used as the input to the first
    54|         0|            0|            0|  0.00%|        # `ReLU`; the output of the first `ReLU` will become the input
    55|         0|            0|            0|  0.00%|        # for `Conv2d(20,64,5)`. Finally, the output of
    56|         0|            0|            0|  0.00%|        # `Conv2d(20,64,5)` will be used as input to the second `ReLU`
    57|         0|            0|            0|  0.00%|        model = nn.Sequential(
    58|         0|            0|            0|  0.00%|                  nn.Conv2d(1,20,5),
    59|         0|            0|            0|  0.00%|                  nn.ReLU(),
    60|         0|            0|            0|  0.00%|                  nn.Conv2d(20,64,5),
    61|         0|            0|            0|  0.00%|                  nn.ReLU()
    62|         0|            0|            0|  0.00%|                )
    63|         0|            0|            0|  0.00%|
    64|         0|            0|            0|  0.00%|        # Using Sequential with OrderedDict. This is functionally the
    65|         0|            0|            0|  0.00%|        # same as the above code
    66|         0|            0|            0|  0.00%|        model = nn.Sequential(OrderedDict([
    67|         0|            0|            0|  0.00%|                  ('conv1', nn.Conv2d(1,20,5)),
    68|         0|            0|            0|  0.00%|                  ('relu1', nn.ReLU()),
    69|         0|            0|            0|  0.00%|                  ('conv2', nn.Conv2d(20,64,5)),
    70|         0|            0|            0|  0.00%|                  ('relu2', nn.ReLU())
    71|         0|            0|            0|  0.00%|                ]))
    72|         0|            0|            0|  0.00%|    """
    73|         0|            0|            0|  0.00%|
    74|         0|            0|            0|  0.00%|    @overload
    75|         0|            0|            0|  0.00%|    def __init__(self, *args: Module) -> None:
    76|         0|            0|            0|  0.00%|        ...
    77|         0|            0|            0|  0.00%|
    78|         0|            0|            0|  0.00%|    @overload
    79|         0|            0|            0|  0.00%|    def __init__(self, arg: 'OrderedDict[str, Module]') -> None:
    80|         0|            0|            0|  0.00%|        ...
    81|         0|            0|            0|  0.00%|
    82|         0|            0|            0|  0.00%|    def __init__(self, *args):
    83|         0|            0|            0|  0.00%|        super(Sequential, self).__init__()
    84|         0|            0|            0|  0.00%|        if len(args) == 1 and isinstance(args[0], OrderedDict):
    85|         0|            0|            0|  0.00%|            for key, module in args[0].items():
    86|         0|            0|            0|  0.00%|                self.add_module(key, module)
    87|         0|            0|            0|  0.00%|        else:
    88|         0|            0|            0|  0.00%|            for idx, module in enumerate(args):
    89|         0|            0|            0|  0.00%|                self.add_module(str(idx), module)
    90|         0|            0|            0|  0.00%|
    91|         0|            0|            0|  0.00%|    def _get_item_by_idx(self, iterator, idx) -> T:
    92|         0|            0|            0|  0.00%|        """Get the idx-th item of the iterator"""
    93|         0|            0|            0|  0.00%|        size = len(self)
    94|         0|            0|            0|  0.00%|        idx = operator.index(idx)
    95|         0|            0|            0|  0.00%|        if not -size <= idx < size:
    96|         0|            0|            0|  0.00%|            raise IndexError('index {} is out of range'.format(idx))
    97|         0|            0|            0|  0.00%|        idx %= size
    98|         0|            0|            0|  0.00%|        return next(islice(iterator, idx, None))
    99|         0|            0|            0|  0.00%|
   100|         0|            0|            0|  0.00%|    @_copy_to_script_wrapper
   101|         0|            0|            0|  0.00%|    def __getitem__(self, idx) -> Union['Sequential', T]:
   102|         0|            0|            0|  0.00%|        if isinstance(idx, slice):
   103|         0|            0|            0|  0.00%|            return self.__class__(OrderedDict(list(self._modules.items())[idx]))
   104|         0|            0|            0|  0.00%|        else:
   105|         0|            0|            0|  0.00%|            return self._get_item_by_idx(self._modules.values(), idx)
   106|         0|            0|            0|  0.00%|
   107|         0|            0|            0|  0.00%|    def __setitem__(self, idx: int, module: Module) -> None:
   108|         0|            0|            0|  0.00%|        key: str = self._get_item_by_idx(self._modules.keys(), idx)
   109|         0|            0|            0|  0.00%|        return setattr(self, key, module)
   110|         0|            0|            0|  0.00%|
   111|         0|            0|            0|  0.00%|    def __delitem__(self, idx: Union[slice, int]) -> None:
   112|         0|            0|            0|  0.00%|        if isinstance(idx, slice):
   113|         0|            0|            0|  0.00%|            for key in list(self._modules.keys())[idx]:
   114|         0|            0|            0|  0.00%|                delattr(self, key)
   115|         0|            0|            0|  0.00%|        else:
   116|         0|            0|            0|  0.00%|            key = self._get_item_by_idx(self._modules.keys(), idx)
   117|         0|            0|            0|  0.00%|            delattr(self, key)
   118|         0|            0|            0|  0.00%|
   119|         0|            0|            0|  0.00%|    @_copy_to_script_wrapper
   120|         0|            0|            0|  0.00%|    def __len__(self) -> int:
   121|         0|            0|            0|  0.00%|        return len(self._modules)
   122|         0|            0|            0|  0.00%|
   123|         0|            0|            0|  0.00%|    @_copy_to_script_wrapper
   124|         0|            0|            0|  0.00%|    def __dir__(self):
   125|         0|            0|            0|  0.00%|        keys = super(Sequential, self).__dir__()
   126|         0|            0|            0|  0.00%|        keys = [key for key in keys if not key.isdigit()]
   127|         0|            0|            0|  0.00%|        return keys
   128|         0|            0|            0|  0.00%|
   129|       700|   0.00300241|  4.28915e-06|  0.01%|    @_copy_to_script_wrapper
   130|         0|            0|            0|  0.00%|    def __iter__(self) -> Iterator[Module]:
   131|       700|   0.00728345|  1.04049e-05|  0.01%|        return iter(self._modules.values())
   132|         0|            0|            0|  0.00%|
   133|         0|            0|            0|  0.00%|    # NB: We can't really type check this function as the type of input
   134|         0|            0|            0|  0.00%|    # may change dynamically (as is tested in
   135|         0|            0|            0|  0.00%|    # TestScript.test_sequential_intermediary_types).  Cannot annotate
   136|         0|            0|            0|  0.00%|    # with Any as TorchScript expects a more precise type
   137|       700|    0.0036819|  5.25985e-06|  0.01%|    def forward(self, input):
   138|      2100|    0.0177429|  8.44899e-06|  0.03%|        for module in self:
(call)|       700|    0.0102859|  1.46941e-05|  0.02%|# /opt/conda/lib/python3.8/site-packages/torch/nn/modules/container.py:129 __iter__
   139|      1400|    0.0268602|  1.91859e-05|  0.05%|            input = module(input)
(call)|      1400|      13.8299|   0.00987848| 25.81%|# /opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1045 _call_impl
   140|       700|   0.00285268|  4.07525e-06|  0.01%|        return input
   141|         0|            0|            0|  0.00%|
   142|         0|            0|            0|  0.00%|
   143|         0|            0|            0|  0.00%|class ModuleList(Module):
   144|         0|            0|            0|  0.00%|    r"""Holds submodules in a list.
   145|         0|            0|            0|  0.00%|
   146|         0|            0|            0|  0.00%|    :class:`~torch.nn.ModuleList` can be indexed like a regular Python list, but
   147|         0|            0|            0|  0.00%|    modules it contains are properly registered, and will be visible by all
   148|         0|            0|            0|  0.00%|    :class:`~torch.nn.Module` methods.
   149|         0|            0|            0|  0.00%|
   150|         0|            0|            0|  0.00%|    Args:
   151|         0|            0|            0|  0.00%|        modules (iterable, optional): an iterable of modules to add
   152|         0|            0|            0|  0.00%|
   153|         0|            0|            0|  0.00%|    Example::
   154|         0|            0|            0|  0.00%|
   155|         0|            0|            0|  0.00%|        class MyModule(nn.Module):
   156|         0|            0|            0|  0.00%|            def __init__(self):
   157|         0|            0|            0|  0.00%|                super(MyModule, self).__init__()
   158|         0|            0|            0|  0.00%|                self.linears = nn.ModuleList([nn.Linear(10, 10) for i in range(10)])
   159|         0|            0|            0|  0.00%|
   160|         0|            0|            0|  0.00%|            def forward(self, x):
   161|         0|            0|            0|  0.00%|                # ModuleList can act as an iterable, or be indexed using ints
   162|         0|            0|            0|  0.00%|                for i, l in enumerate(self.linears):
   163|         0|            0|            0|  0.00%|                    x = self.linears[i // 2](x) + l(x)
   164|         0|            0|            0|  0.00%|                return x
   165|         0|            0|            0|  0.00%|    """
   166|         0|            0|            0|  0.00%|
   167|         0|            0|            0|  0.00%|    def __init__(self, modules: Optional[Iterable[Module]] = None) -> None:
   168|         0|            0|            0|  0.00%|        super(ModuleList, self).__init__()
   169|         0|            0|            0|  0.00%|        if modules is not None:
   170|         0|            0|            0|  0.00%|            self += modules
   171|         0|            0|            0|  0.00%|
   172|         0|            0|            0|  0.00%|    def _get_abs_string_index(self, idx):
   173|         0|            0|            0|  0.00%|        """Get the absolute index for the list of modules"""
   174|         0|            0|            0|  0.00%|        idx = operator.index(idx)
   175|         0|            0|            0|  0.00%|        if not (-len(self) <= idx < len(self)):
   176|         0|            0|            0|  0.00%|            raise IndexError('index {} is out of range'.format(idx))
   177|         0|            0|            0|  0.00%|        if idx < 0:
   178|         0|            0|            0|  0.00%|            idx += len(self)
   179|         0|            0|            0|  0.00%|        return str(idx)
   180|         0|            0|            0|  0.00%|
   181|         0|            0|            0|  0.00%|    @_copy_to_script_wrapper
   182|         0|            0|            0|  0.00%|    def __getitem__(self, idx: int) -> Module:
   183|         0|            0|            0|  0.00%|        if isinstance(idx, slice):
   184|         0|            0|            0|  0.00%|            return self.__class__(list(self._modules.values())[idx])
   185|         0|            0|            0|  0.00%|        else:
   186|         0|            0|            0|  0.00%|            return self._modules[self._get_abs_string_index(idx)]
   187|         0|            0|            0|  0.00%|
   188|         0|            0|            0|  0.00%|    def __setitem__(self, idx: int, module: Module) -> None:
   189|         0|            0|            0|  0.00%|        idx = self._get_abs_string_index(idx)
   190|         0|            0|            0|  0.00%|        return setattr(self, str(idx), module)
   191|         0|            0|            0|  0.00%|
   192|         0|            0|            0|  0.00%|    def __delitem__(self, idx: Union[int, slice]) -> None:
   193|         0|            0|            0|  0.00%|        if isinstance(idx, slice):
   194|         0|            0|            0|  0.00%|            for k in range(len(self._modules))[idx]:
   195|         0|            0|            0|  0.00%|                delattr(self, str(k))
   196|         0|            0|            0|  0.00%|        else:
   197|         0|            0|            0|  0.00%|            delattr(self, self._get_abs_string_index(idx))
   198|         0|            0|            0|  0.00%|        # To preserve numbering, self._modules is being reconstructed with modules after deletion
   199|         0|            0|            0|  0.00%|        str_indices = [str(i) for i in range(len(self._modules))]
   200|         0|            0|            0|  0.00%|        self._modules = OrderedDict(list(zip(str_indices, self._modules.values())))
   201|         0|            0|            0|  0.00%|
   202|         0|            0|            0|  0.00%|    @_copy_to_script_wrapper
   203|         0|            0|            0|  0.00%|    def __len__(self) -> int:
   204|         0|            0|            0|  0.00%|        return len(self._modules)
   205|         0|            0|            0|  0.00%|
   206|         0|            0|            0|  0.00%|    @_copy_to_script_wrapper
   207|         0|            0|            0|  0.00%|    def __iter__(self) -> Iterator[Module]:
   208|         0|            0|            0|  0.00%|        return iter(self._modules.values())
   209|         0|            0|            0|  0.00%|
   210|         0|            0|            0|  0.00%|    def __iadd__(self, modules: Iterable[Module]) -> 'ModuleList':
   211|         0|            0|            0|  0.00%|        return self.extend(modules)
   212|         0|            0|            0|  0.00%|
   213|         0|            0|            0|  0.00%|    @_copy_to_script_wrapper
   214|         0|            0|            0|  0.00%|    def __dir__(self):
   215|         0|            0|            0|  0.00%|        keys = super(ModuleList, self).__dir__()
   216|         0|            0|            0|  0.00%|        keys = [key for key in keys if not key.isdigit()]
   217|         0|            0|            0|  0.00%|        return keys
   218|         0|            0|            0|  0.00%|
   219|         0|            0|            0|  0.00%|    def insert(self, index: int, module: Module) -> None:
   220|         0|            0|            0|  0.00%|        r"""Insert a given module before a given index in the list.
   221|         0|            0|            0|  0.00%|
   222|         0|            0|            0|  0.00%|        Args:
   223|         0|            0|            0|  0.00%|            index (int): index to insert.
   224|         0|            0|            0|  0.00%|            module (nn.Module): module to insert
   225|         0|            0|            0|  0.00%|        """
   226|         0|            0|            0|  0.00%|        for i in range(len(self._modules), index, -1):
   227|         0|            0|            0|  0.00%|            self._modules[str(i)] = self._modules[str(i - 1)]
   228|         0|            0|            0|  0.00%|        self._modules[str(index)] = module
   229|         0|            0|            0|  0.00%|
   230|         0|            0|            0|  0.00%|    def append(self, module: Module) -> 'ModuleList':
   231|         0|            0|            0|  0.00%|        r"""Appends a given module to the end of the list.
   232|         0|            0|            0|  0.00%|
   233|         0|            0|            0|  0.00%|        Args:
   234|         0|            0|            0|  0.00%|            module (nn.Module): module to append
   235|         0|            0|            0|  0.00%|        """
   236|         0|            0|            0|  0.00%|        self.add_module(str(len(self)), module)
   237|         0|            0|            0|  0.00%|        return self
   238|         0|            0|            0|  0.00%|
   239|         0|            0|            0|  0.00%|    def extend(self, modules: Iterable[Module]) -> 'ModuleList':
   240|         0|            0|            0|  0.00%|        r"""Appends modules from a Python iterable to the end of the list.
   241|         0|            0|            0|  0.00%|
   242|         0|            0|            0|  0.00%|        Args:
   243|         0|            0|            0|  0.00%|            modules (iterable): iterable of modules to append
   244|         0|            0|            0|  0.00%|        """
   245|         0|            0|            0|  0.00%|        if not isinstance(modules, container_abcs.Iterable):
   246|         0|            0|            0|  0.00%|            raise TypeError("ModuleList.extend should be called with an "
   247|         0|            0|            0|  0.00%|                            "iterable, but got " + type(modules).__name__)
   248|         0|            0|            0|  0.00%|        offset = len(self)
   249|         0|            0|            0|  0.00%|        for i, module in enumerate(modules):
   250|         0|            0|            0|  0.00%|            self.add_module(str(offset + i), module)
   251|         0|            0|            0|  0.00%|        return self
   252|         0|            0|            0|  0.00%|
   253|         0|            0|            0|  0.00%|    # remove forward alltogether to fallback on Module's _forward_unimplemented
   254|         0|            0|            0|  0.00%|
   255|         0|            0|            0|  0.00%|
   256|         0|            0|            0|  0.00%|class ModuleDict(Module):
   257|         0|            0|            0|  0.00%|    r"""Holds submodules in a dictionary.
   258|         0|            0|            0|  0.00%|
   259|         0|            0|            0|  0.00%|    :class:`~torch.nn.ModuleDict` can be indexed like a regular Python dictionary,
   260|         0|            0|            0|  0.00%|    but modules it contains are properly registered, and will be visible by all
   261|         0|            0|            0|  0.00%|    :class:`~torch.nn.Module` methods.
   262|         0|            0|            0|  0.00%|
   263|         0|            0|            0|  0.00%|    :class:`~torch.nn.ModuleDict` is an **ordered** dictionary that respects
   264|         0|            0|            0|  0.00%|
   265|         0|            0|            0|  0.00%|    * the order of insertion, and
   266|         0|            0|            0|  0.00%|
   267|         0|            0|            0|  0.00%|    * in :meth:`~torch.nn.ModuleDict.update`, the order of the merged
   268|         0|            0|            0|  0.00%|      ``OrderedDict``, ``dict`` (started from Python 3.6) or another
   269|         0|            0|            0|  0.00%|      :class:`~torch.nn.ModuleDict` (the argument to
   270|         0|            0|            0|  0.00%|      :meth:`~torch.nn.ModuleDict.update`).
   271|         0|            0|            0|  0.00%|
   272|         0|            0|            0|  0.00%|    Note that :meth:`~torch.nn.ModuleDict.update` with other unordered mapping
   273|         0|            0|            0|  0.00%|    types (e.g., Python's plain ``dict`` before Python version 3.6) does not
   274|         0|            0|            0|  0.00%|    preserve the order of the merged mapping.
   275|         0|            0|            0|  0.00%|
   276|         0|            0|            0|  0.00%|    Args:
   277|         0|            0|            0|  0.00%|        modules (iterable, optional): a mapping (dictionary) of (string: module)
   278|         0|            0|            0|  0.00%|            or an iterable of key-value pairs of type (string, module)
   279|         0|            0|            0|  0.00%|
   280|         0|            0|            0|  0.00%|    Example::
   281|         0|            0|            0|  0.00%|
   282|         0|            0|            0|  0.00%|        class MyModule(nn.Module):
   283|         0|            0|            0|  0.00%|            def __init__(self):
   284|         0|            0|            0|  0.00%|                super(MyModule, self).__init__()
   285|         0|            0|            0|  0.00%|                self.choices = nn.ModuleDict({
   286|         0|            0|            0|  0.00%|                        'conv': nn.Conv2d(10, 10, 3),
   287|         0|            0|            0|  0.00%|                        'pool': nn.MaxPool2d(3)
   288|         0|            0|            0|  0.00%|                })
   289|         0|            0|            0|  0.00%|                self.activations = nn.ModuleDict([
   290|         0|            0|            0|  0.00%|                        ['lrelu', nn.LeakyReLU()],
   291|         0|            0|            0|  0.00%|                        ['prelu', nn.PReLU()]
   292|         0|            0|            0|  0.00%|                ])
   293|         0|            0|            0|  0.00%|
   294|         0|            0|            0|  0.00%|            def forward(self, x, choice, act):
   295|         0|            0|            0|  0.00%|                x = self.choices[choice](x)
   296|         0|            0|            0|  0.00%|                x = self.activations[act](x)
   297|         0|            0|            0|  0.00%|                return x
   298|         0|            0|            0|  0.00%|    """
   299|         0|            0|            0|  0.00%|
   300|         0|            0|            0|  0.00%|    def __init__(self, modules: Optional[Mapping[str, Module]] = None) -> None:
   301|         0|            0|            0|  0.00%|        super(ModuleDict, self).__init__()
   302|         0|            0|            0|  0.00%|        if modules is not None:
   303|         0|            0|            0|  0.00%|            self.update(modules)
   304|         0|            0|            0|  0.00%|
   305|         0|            0|            0|  0.00%|    @_copy_to_script_wrapper
   306|         0|            0|            0|  0.00%|    def __getitem__(self, key: str) -> Module:
   307|         0|            0|            0|  0.00%|        return self._modules[key]
   308|         0|            0|            0|  0.00%|
   309|         0|            0|            0|  0.00%|    def __setitem__(self, key: str, module: Module) -> None:
   310|         0|            0|            0|  0.00%|        self.add_module(key, module)
   311|         0|            0|            0|  0.00%|
   312|         0|            0|            0|  0.00%|    def __delitem__(self, key: str) -> None:
   313|         0|            0|            0|  0.00%|        del self._modules[key]
   314|         0|            0|            0|  0.00%|
   315|         0|            0|            0|  0.00%|    @_copy_to_script_wrapper
   316|         0|            0|            0|  0.00%|    def __len__(self) -> int:
   317|         0|            0|            0|  0.00%|        return len(self._modules)
   318|         0|            0|            0|  0.00%|
   319|         0|            0|            0|  0.00%|    @_copy_to_script_wrapper
   320|         0|            0|            0|  0.00%|    def __iter__(self) -> Iterator[str]:
   321|         0|            0|            0|  0.00%|        return iter(self._modules)
   322|         0|            0|            0|  0.00%|
   323|         0|            0|            0|  0.00%|    @_copy_to_script_wrapper
   324|         0|            0|            0|  0.00%|    def __contains__(self, key: str) -> bool:
   325|         0|            0|            0|  0.00%|        return key in self._modules
   326|         0|            0|            0|  0.00%|
   327|         0|            0|            0|  0.00%|    def clear(self) -> None:
   328|         0|            0|            0|  0.00%|        """Remove all items from the ModuleDict.
   329|         0|            0|            0|  0.00%|        """
   330|         0|            0|            0|  0.00%|        self._modules.clear()
   331|         0|            0|            0|  0.00%|
   332|         0|            0|            0|  0.00%|    def pop(self, key: str) -> Module:
   333|         0|            0|            0|  0.00%|        r"""Remove key from the ModuleDict and return its module.
   334|         0|            0|            0|  0.00%|
   335|         0|            0|            0|  0.00%|        Args:
   336|         0|            0|            0|  0.00%|            key (string): key to pop from the ModuleDict
   337|         0|            0|            0|  0.00%|        """
   338|         0|            0|            0|  0.00%|        v = self[key]
   339|         0|            0|            0|  0.00%|        del self[key]
   340|         0|            0|            0|  0.00%|        return v
   341|         0|            0|            0|  0.00%|
   342|         0|            0|            0|  0.00%|    @_copy_to_script_wrapper
   343|         0|            0|            0|  0.00%|    def keys(self) -> Iterable[str]:
   344|         0|            0|            0|  0.00%|        r"""Return an iterable of the ModuleDict keys.
   345|         0|            0|            0|  0.00%|        """
   346|         0|            0|            0|  0.00%|        return self._modules.keys()
   347|         0|            0|            0|  0.00%|
   348|         0|            0|            0|  0.00%|    @_copy_to_script_wrapper
   349|         0|            0|            0|  0.00%|    def items(self) -> Iterable[Tuple[str, Module]]:
   350|         0|            0|            0|  0.00%|        r"""Return an iterable of the ModuleDict key/value pairs.
   351|         0|            0|            0|  0.00%|        """
   352|         0|            0|            0|  0.00%|        return self._modules.items()
   353|         0|            0|            0|  0.00%|
   354|         0|            0|            0|  0.00%|    @_copy_to_script_wrapper
   355|         0|            0|            0|  0.00%|    def values(self) -> Iterable[Module]:
   356|         0|            0|            0|  0.00%|        r"""Return an iterable of the ModuleDict values.
   357|         0|            0|            0|  0.00%|        """
   358|         0|            0|            0|  0.00%|        return self._modules.values()
   359|         0|            0|            0|  0.00%|
   360|         0|            0|            0|  0.00%|    def update(self, modules: Mapping[str, Module]) -> None:
   361|         0|            0|            0|  0.00%|        r"""Update the :class:`~torch.nn.ModuleDict` with the key-value pairs from a
   362|         0|            0|            0|  0.00%|        mapping or an iterable, overwriting existing keys.
   363|         0|            0|            0|  0.00%|
   364|         0|            0|            0|  0.00%|        .. note::
   365|         0|            0|            0|  0.00%|            If :attr:`modules` is an ``OrderedDict``, a :class:`~torch.nn.ModuleDict`, or
   366|         0|            0|            0|  0.00%|            an iterable of key-value pairs, the order of new elements in it is preserved.
   367|         0|            0|            0|  0.00%|
   368|         0|            0|            0|  0.00%|        Args:
   369|         0|            0|            0|  0.00%|            modules (iterable): a mapping (dictionary) from string to :class:`~torch.nn.Module`,
   370|         0|            0|            0|  0.00%|                or an iterable of key-value pairs of type (string, :class:`~torch.nn.Module`)
   371|         0|            0|            0|  0.00%|        """
   372|         0|            0|            0|  0.00%|        if not isinstance(modules, container_abcs.Iterable):
   373|         0|            0|            0|  0.00%|            raise TypeError("ModuleDict.update should be called with an "
   374|         0|            0|            0|  0.00%|                            "iterable of key/value pairs, but got " +
   375|         0|            0|            0|  0.00%|                            type(modules).__name__)
   376|         0|            0|            0|  0.00%|
   377|         0|            0|            0|  0.00%|        if isinstance(modules, (OrderedDict, ModuleDict, container_abcs.Mapping)):
   378|         0|            0|            0|  0.00%|            for key, module in modules.items():
   379|         0|            0|            0|  0.00%|                self[key] = module
   380|         0|            0|            0|  0.00%|        else:
   381|         0|            0|            0|  0.00%|            # modules here can be a list with two items
   382|         0|            0|            0|  0.00%|            for j, m in enumerate(modules):
   383|         0|            0|            0|  0.00%|                if not isinstance(m, container_abcs.Iterable):
   384|         0|            0|            0|  0.00%|                    raise TypeError("ModuleDict update sequence element "
   385|         0|            0|            0|  0.00%|                                    "#" + str(j) + " should be Iterable; is" +
   386|         0|            0|            0|  0.00%|                                    type(m).__name__)
   387|         0|            0|            0|  0.00%|                if not len(m) == 2:
   388|         0|            0|            0|  0.00%|                    raise ValueError("ModuleDict update sequence element "
   389|         0|            0|            0|  0.00%|                                     "#" + str(j) + " has length " + str(len(m)) +
   390|         0|            0|            0|  0.00%|                                     "; 2 is required")
   391|         0|            0|            0|  0.00%|                # modules can be Mapping (what it's typed at), or a list: [(name1, module1), (name2, module2)]
   392|         0|            0|            0|  0.00%|                # that's too cumbersome to type correctly with overloads, so we add an ignore here
   393|         0|            0|            0|  0.00%|                self[m[0]] = m[1]  # type: ignore[assignment]
   394|         0|            0|            0|  0.00%|
   395|         0|            0|            0|  0.00%|    # remove forward alltogether to fallback on Module's _forward_unimplemented
   396|         0|            0|            0|  0.00%|
   397|         0|            0|            0|  0.00%|
   398|         0|            0|            0|  0.00%|class ParameterList(Module):
   399|         0|            0|            0|  0.00%|    r"""Holds parameters in a list.
   400|         0|            0|            0|  0.00%|
   401|         0|            0|            0|  0.00%|    :class:`~torch.nn.ParameterList` can be indexed like a regular Python
   402|         0|            0|            0|  0.00%|    list, but parameters it contains are properly registered, and will be
   403|         0|            0|            0|  0.00%|    visible by all :class:`~torch.nn.Module` methods.
   404|         0|            0|            0|  0.00%|
   405|         0|            0|            0|  0.00%|    Args:
   406|         0|            0|            0|  0.00%|        parameters (iterable, optional): an iterable of :class:`~torch.nn.Parameter` to add
   407|         0|            0|            0|  0.00%|
   408|         0|            0|            0|  0.00%|    Example::
   409|         0|            0|            0|  0.00%|
   410|         0|            0|            0|  0.00%|        class MyModule(nn.Module):
   411|         0|            0|            0|  0.00%|            def __init__(self):
   412|         0|            0|            0|  0.00%|                super(MyModule, self).__init__()
   413|         0|            0|            0|  0.00%|                self.params = nn.ParameterList([nn.Parameter(torch.randn(10, 10)) for i in range(10)])
   414|         0|            0|            0|  0.00%|
   415|         0|            0|            0|  0.00%|            def forward(self, x):
   416|         0|            0|            0|  0.00%|                # ParameterList can act as an iterable, or be indexed using ints
   417|         0|            0|            0|  0.00%|                for i, p in enumerate(self.params):
   418|         0|            0|            0|  0.00%|                    x = self.params[i // 2].mm(x) + p.mm(x)
   419|         0|            0|            0|  0.00%|                return x
   420|         0|            0|            0|  0.00%|    """
   421|         0|            0|            0|  0.00%|
   422|         0|            0|            0|  0.00%|    def __init__(self, parameters: Optional[Iterable['Parameter']] = None) -> None:
   423|         0|            0|            0|  0.00%|        super(ParameterList, self).__init__()
   424|         0|            0|            0|  0.00%|        self._initialized = True
   425|         0|            0|            0|  0.00%|        if parameters is not None:
   426|         0|            0|            0|  0.00%|            self += parameters
   427|         0|            0|            0|  0.00%|
   428|         0|            0|            0|  0.00%|    def __setstate__(self, state):
   429|         0|            0|            0|  0.00%|        state['_initialized'] = False
   430|         0|            0|            0|  0.00%|        super(ParameterList, self).__setstate__(state)
   431|         0|            0|            0|  0.00%|        self._initialized = True
   432|         0|            0|            0|  0.00%|
   433|         0|            0|            0|  0.00%|    def _get_abs_string_index(self, idx):
   434|         0|            0|            0|  0.00%|        """Get the absolute index for the list of modules"""
   435|         0|            0|            0|  0.00%|        idx = operator.index(idx)
   436|         0|            0|            0|  0.00%|        if not (-len(self) <= idx < len(self)):
   437|         0|            0|            0|  0.00%|            raise IndexError('index {} is out of range'.format(idx))
   438|         0|            0|            0|  0.00%|        if idx < 0:
   439|         0|            0|            0|  0.00%|            idx += len(self)
   440|         0|            0|            0|  0.00%|        return str(idx)
   441|         0|            0|            0|  0.00%|
   442|         0|            0|            0|  0.00%|    @overload
   443|         0|            0|            0|  0.00%|    def __getitem__(self, idx: int) -> 'Parameter':
   444|         0|            0|            0|  0.00%|        ...
   445|         0|            0|            0|  0.00%|
   446|         0|            0|            0|  0.00%|    @overload
   447|         0|            0|            0|  0.00%|    def __getitem__(self: T, idx: slice) -> T:
   448|         0|            0|            0|  0.00%|        ...
   449|         0|            0|            0|  0.00%|
   450|         0|            0|            0|  0.00%|    def __getitem__(self, idx):
   451|         0|            0|            0|  0.00%|        if isinstance(idx, slice):
   452|         0|            0|            0|  0.00%|            return self.__class__(list(self._parameters.values())[idx])
   453|         0|            0|            0|  0.00%|        else:
   454|         0|            0|            0|  0.00%|            idx = self._get_abs_string_index(idx)
   455|         0|            0|            0|  0.00%|            return self._parameters[str(idx)]
   456|         0|            0|            0|  0.00%|
   457|         0|            0|            0|  0.00%|    def __setitem__(self, idx: int, param: 'Parameter') -> None:
   458|         0|            0|            0|  0.00%|        idx = self._get_abs_string_index(idx)
   459|         0|            0|            0|  0.00%|        return self.register_parameter(str(idx), param)
   460|         0|            0|            0|  0.00%|
   461|         0|            0|            0|  0.00%|    def __setattr__(self, key: Any, value: Any) -> None:
   462|         0|            0|            0|  0.00%|        if getattr(self, "_initialized", False):
   463|         0|            0|            0|  0.00%|            if not hasattr(self, key) and not isinstance(value, torch.nn.Parameter):
   464|         0|            0|            0|  0.00%|                warnings.warn("Setting attributes on ParameterList is not supported.")
   465|         0|            0|            0|  0.00%|        super(ParameterList, self).__setattr__(key, value)
   466|         0|            0|            0|  0.00%|
   467|         0|            0|            0|  0.00%|    def __len__(self) -> int:
   468|         0|            0|            0|  0.00%|        return len(self._parameters)
   469|         0|            0|            0|  0.00%|
   470|         0|            0|            0|  0.00%|    def __iter__(self) -> Iterator['Parameter']:
   471|         0|            0|            0|  0.00%|        return iter(self._parameters.values())
   472|         0|            0|            0|  0.00%|
   473|         0|            0|            0|  0.00%|    def __iadd__(self, parameters: Iterable['Parameter']) -> 'ParameterList':
   474|         0|            0|            0|  0.00%|        return self.extend(parameters)
   475|         0|            0|            0|  0.00%|
   476|         0|            0|            0|  0.00%|    def __dir__(self):
   477|         0|            0|            0|  0.00%|        keys = super(ParameterList, self).__dir__()
   478|         0|            0|            0|  0.00%|        keys = [key for key in keys if not key.isdigit()]
   479|         0|            0|            0|  0.00%|        return keys
   480|         0|            0|            0|  0.00%|
   481|         0|            0|            0|  0.00%|    def append(self, parameter: 'Parameter') -> 'ParameterList':
   482|         0|            0|            0|  0.00%|        """Appends a given parameter at the end of the list.
   483|         0|            0|            0|  0.00%|
   484|         0|            0|            0|  0.00%|        Args:
   485|         0|            0|            0|  0.00%|            parameter (nn.Parameter): parameter to append
   486|         0|            0|            0|  0.00%|        """
   487|         0|            0|            0|  0.00%|        self.register_parameter(str(len(self)), parameter)
   488|         0|            0|            0|  0.00%|        return self
   489|         0|            0|            0|  0.00%|
   490|         0|            0|            0|  0.00%|    def extend(self, parameters: Iterable['Parameter']) -> 'ParameterList':
   491|         0|            0|            0|  0.00%|        """Appends parameters from a Python iterable to the end of the list.
   492|         0|            0|            0|  0.00%|
   493|         0|            0|            0|  0.00%|        Args:
   494|         0|            0|            0|  0.00%|            parameters (iterable): iterable of parameters to append
   495|         0|            0|            0|  0.00%|        """
   496|         0|            0|            0|  0.00%|        if not isinstance(parameters, container_abcs.Iterable):
   497|         0|            0|            0|  0.00%|            raise TypeError("ParameterList.extend should be called with an "
   498|         0|            0|            0|  0.00%|                            "iterable, but got " + type(parameters).__name__)
   499|         0|            0|            0|  0.00%|        offset = len(self)
   500|         0|            0|            0|  0.00%|        for i, param in enumerate(parameters):
   501|         0|            0|            0|  0.00%|            self.register_parameter(str(offset + i), param)
   502|         0|            0|            0|  0.00%|        return self
   503|         0|            0|            0|  0.00%|
   504|         0|            0|            0|  0.00%|    def extra_repr(self) -> str:
   505|         0|            0|            0|  0.00%|        child_lines = []
   506|         0|            0|            0|  0.00%|        for k, p in self._parameters.items():
   507|         0|            0|            0|  0.00%|            size_str = 'x'.join(str(size) for size in p.size())
   508|         0|            0|            0|  0.00%|            device_str = '' if not p.is_cuda else ' (GPU {})'.format(p.get_device())
   509|         0|            0|            0|  0.00%|            parastr = 'Parameter containing: [{} of size {}{}]'.format(
   510|         0|            0|            0|  0.00%|                torch.typename(p), size_str, device_str)
   511|         0|            0|            0|  0.00%|            child_lines.append('  (' + str(k) + '): ' + parastr)
   512|         0|            0|            0|  0.00%|        tmpstr = '\n'.join(child_lines)
   513|         0|            0|            0|  0.00%|        return tmpstr
   514|         0|            0|            0|  0.00%|
   515|         0|            0|            0|  0.00%|    def __call__(self, input):
   516|         0|            0|            0|  0.00%|        raise RuntimeError('ParameterList should not be called.')
   517|         0|            0|            0|  0.00%|
   518|         0|            0|            0|  0.00%|    def _replicate_for_data_parallel(self):
   519|         0|            0|            0|  0.00%|        warnings.warn("nn.ParameterList is being used with DataParallel but this is not "
   520|         0|            0|            0|  0.00%|                      "supported. This list will appear empty for the models replicated "
   521|         0|            0|            0|  0.00%|                      "on each GPU except the original one.")
   522|         0|            0|            0|  0.00%|
   523|         0|            0|            0|  0.00%|        return super(ParameterList, self)._replicate_for_data_parallel()
   524|         0|            0|            0|  0.00%|
   525|         0|            0|            0|  0.00%|
   526|         0|            0|            0|  0.00%|class ParameterDict(Module):
   527|         0|            0|            0|  0.00%|    r"""Holds parameters in a dictionary.
   528|         0|            0|            0|  0.00%|
   529|         0|            0|            0|  0.00%|    ParameterDict can be indexed like a regular Python dictionary, but parameters it
   530|         0|            0|            0|  0.00%|    contains are properly registered, and will be visible by all Module methods.
   531|         0|            0|            0|  0.00%|
   532|         0|            0|            0|  0.00%|    :class:`~torch.nn.ParameterDict` is an **ordered** dictionary that respects
   533|         0|            0|            0|  0.00%|
   534|         0|            0|            0|  0.00%|    * the order of insertion, and
   535|         0|            0|            0|  0.00%|
   536|         0|            0|            0|  0.00%|    * in :meth:`~torch.nn.ParameterDict.update`, the order of the merged ``OrderedDict``
   537|         0|            0|            0|  0.00%|      or another :class:`~torch.nn.ParameterDict` (the argument to
   538|         0|            0|            0|  0.00%|      :meth:`~torch.nn.ParameterDict.update`).
   539|         0|            0|            0|  0.00%|
   540|         0|            0|            0|  0.00%|    Note that :meth:`~torch.nn.ParameterDict.update` with other unordered mapping
   541|         0|            0|            0|  0.00%|    types (e.g., Python's plain ``dict``) does not preserve the order of the
   542|         0|            0|            0|  0.00%|    merged mapping.
   543|         0|            0|            0|  0.00%|
   544|         0|            0|            0|  0.00%|    Args:
   545|         0|            0|            0|  0.00%|        parameters (iterable, optional): a mapping (dictionary) of
   546|         0|            0|            0|  0.00%|            (string : :class:`~torch.nn.Parameter`) or an iterable of key-value pairs
   547|         0|            0|            0|  0.00%|            of type (string, :class:`~torch.nn.Parameter`)
   548|         0|            0|            0|  0.00%|
   549|         0|            0|            0|  0.00%|    Example::
   550|         0|            0|            0|  0.00%|
   551|         0|            0|            0|  0.00%|        class MyModule(nn.Module):
   552|         0|            0|            0|  0.00%|            def __init__(self):
   553|         0|            0|            0|  0.00%|                super(MyModule, self).__init__()
   554|         0|            0|            0|  0.00%|                self.params = nn.ParameterDict({
   555|         0|            0|            0|  0.00%|                        'left': nn.Parameter(torch.randn(5, 10)),
   556|         0|            0|            0|  0.00%|                        'right': nn.Parameter(torch.randn(5, 10))
   557|         0|            0|            0|  0.00%|                })
   558|         0|            0|            0|  0.00%|
   559|         0|            0|            0|  0.00%|            def forward(self, x, choice):
   560|         0|            0|            0|  0.00%|                x = self.params[choice].mm(x)
   561|         0|            0|            0|  0.00%|                return x
   562|         0|            0|            0|  0.00%|    """
   563|         0|            0|            0|  0.00%|
   564|         0|            0|            0|  0.00%|    def __init__(self, parameters: Optional[Mapping[str, 'Parameter']] = None) -> None:
   565|         0|            0|            0|  0.00%|        super(ParameterDict, self).__init__()
   566|         0|            0|            0|  0.00%|        self._initialized = True
   567|         0|            0|            0|  0.00%|        if parameters is not None:
   568|         0|            0|            0|  0.00%|            self.update(parameters)
   569|         0|            0|            0|  0.00%|
   570|         0|            0|            0|  0.00%|    def __setstate__(self, state):
   571|         0|            0|            0|  0.00%|        state['_initialized'] = False
   572|         0|            0|            0|  0.00%|        super(ParameterDict, self).__setstate__(state)
   573|         0|            0|            0|  0.00%|        self._initialized = True
   574|         0|            0|            0|  0.00%|
   575|         0|            0|            0|  0.00%|    def __getitem__(self, key: str) -> 'Parameter':
   576|         0|            0|            0|  0.00%|        return self._parameters[key]
   577|         0|            0|            0|  0.00%|
   578|         0|            0|            0|  0.00%|    def __setitem__(self, key: str, parameter: 'Parameter') -> None:
   579|         0|            0|            0|  0.00%|        self.register_parameter(key, parameter)
   580|         0|            0|            0|  0.00%|
   581|         0|            0|            0|  0.00%|    def __delitem__(self, key: str) -> None:
   582|         0|            0|            0|  0.00%|        del self._parameters[key]
   583|         0|            0|            0|  0.00%|
   584|         0|            0|            0|  0.00%|    def __setattr__(self, key: Any, value: Any) -> None:
   585|         0|            0|            0|  0.00%|        if getattr(self, "_initialized", False):
   586|         0|            0|            0|  0.00%|            if not hasattr(self, key) and not isinstance(value, torch.nn.Parameter):
   587|         0|            0|            0|  0.00%|                warnings.warn("Setting attributes on ParameterDict is not supported.")
   588|         0|            0|            0|  0.00%|        super(ParameterDict, self).__setattr__(key, value)
   589|         0|            0|            0|  0.00%|
   590|         0|            0|            0|  0.00%|    def __len__(self) -> int:
   591|         0|            0|            0|  0.00%|        return len(self._parameters)
   592|         0|            0|            0|  0.00%|
   593|         0|            0|            0|  0.00%|    def __iter__(self) -> Iterator[str]:
   594|         0|            0|            0|  0.00%|        return iter(self._parameters.keys())
   595|         0|            0|            0|  0.00%|
   596|         0|            0|            0|  0.00%|    def __contains__(self, key: str) -> bool:
   597|         0|            0|            0|  0.00%|        return key in self._parameters
   598|         0|            0|            0|  0.00%|
   599|         0|            0|            0|  0.00%|    def clear(self) -> None:
   600|         0|            0|            0|  0.00%|        """Remove all items from the ParameterDict.
   601|         0|            0|            0|  0.00%|        """
   602|         0|            0|            0|  0.00%|        self._parameters.clear()
   603|         0|            0|            0|  0.00%|
   604|         0|            0|            0|  0.00%|    def pop(self, key: str) -> 'Parameter':
   605|         0|            0|            0|  0.00%|        r"""Remove key from the ParameterDict and return its parameter.
   606|         0|            0|            0|  0.00%|
   607|         0|            0|            0|  0.00%|        Args:
   608|         0|            0|            0|  0.00%|            key (string): key to pop from the ParameterDict
   609|         0|            0|            0|  0.00%|        """
   610|         0|            0|            0|  0.00%|        v = self[key]
   611|         0|            0|            0|  0.00%|        del self[key]
   612|         0|            0|            0|  0.00%|        return v
   613|         0|            0|            0|  0.00%|
   614|         0|            0|            0|  0.00%|    def keys(self) -> Iterable[str]:
   615|         0|            0|            0|  0.00%|        r"""Return an iterable of the ParameterDict keys.
   616|         0|            0|            0|  0.00%|        """
   617|         0|            0|            0|  0.00%|        return self._parameters.keys()
   618|         0|            0|            0|  0.00%|
   619|         0|            0|            0|  0.00%|    def items(self) -> Iterable[Tuple[str, 'Parameter']]:
   620|         0|            0|            0|  0.00%|        r"""Return an iterable of the ParameterDict key/value pairs.
   621|         0|            0|            0|  0.00%|        """
   622|         0|            0|            0|  0.00%|        return self._parameters.items()
   623|         0|            0|            0|  0.00%|
   624|         0|            0|            0|  0.00%|    def values(self) -> Iterable['Parameter']:
   625|         0|            0|            0|  0.00%|        r"""Return an iterable of the ParameterDict values.
   626|         0|            0|            0|  0.00%|        """
   627|         0|            0|            0|  0.00%|        return self._parameters.values()
   628|         0|            0|            0|  0.00%|
   629|         0|            0|            0|  0.00%|    def update(self, parameters: Mapping[str, 'Parameter']) -> None:
   630|         0|            0|            0|  0.00%|        r"""Update the :class:`~torch.nn.ParameterDict` with the key-value pairs from a
   631|         0|            0|            0|  0.00%|        mapping or an iterable, overwriting existing keys.
   632|         0|            0|            0|  0.00%|
   633|         0|            0|            0|  0.00%|        .. note::
   634|         0|            0|            0|  0.00%|            If :attr:`parameters` is an ``OrderedDict``, a :class:`~torch.nn.ParameterDict`, or
   635|         0|            0|            0|  0.00%|            an iterable of key-value pairs, the order of new elements in it is preserved.
   636|         0|            0|            0|  0.00%|
   637|         0|            0|            0|  0.00%|        Args:
   638|         0|            0|            0|  0.00%|            parameters (iterable): a mapping (dictionary) from string to
   639|         0|            0|            0|  0.00%|                :class:`~torch.nn.Parameter`, or an iterable of
   640|         0|            0|            0|  0.00%|                key-value pairs of type (string, :class:`~torch.nn.Parameter`)
   641|         0|            0|            0|  0.00%|        """
   642|         0|            0|            0|  0.00%|        if not isinstance(parameters, container_abcs.Iterable):
   643|         0|            0|            0|  0.00%|            raise TypeError("ParametersDict.update should be called with an "
   644|         0|            0|            0|  0.00%|                            "iterable of key/value pairs, but got " +
   645|         0|            0|            0|  0.00%|                            type(parameters).__name__)
   646|         0|            0|            0|  0.00%|
   647|         0|            0|            0|  0.00%|        if isinstance(parameters, (OrderedDict, ParameterDict)):
   648|         0|            0|            0|  0.00%|            for key, parameter in parameters.items():
   649|         0|            0|            0|  0.00%|                self[key] = parameter
   650|         0|            0|            0|  0.00%|        elif isinstance(parameters, container_abcs.Mapping):
   651|         0|            0|            0|  0.00%|            for key, parameter in sorted(parameters.items()):
   652|         0|            0|            0|  0.00%|                self[key] = parameter
   653|         0|            0|            0|  0.00%|        else:
   654|         0|            0|            0|  0.00%|            for j, p in enumerate(parameters):
   655|         0|            0|            0|  0.00%|                if not isinstance(p, container_abcs.Iterable):
   656|         0|            0|            0|  0.00%|                    raise TypeError("ParameterDict update sequence element "
   657|         0|            0|            0|  0.00%|                                    "#" + str(j) + " should be Iterable; is" +
   658|         0|            0|            0|  0.00%|                                    type(p).__name__)
   659|         0|            0|            0|  0.00%|                if not len(p) == 2:
   660|         0|            0|            0|  0.00%|                    raise ValueError("ParameterDict update sequence element "
   661|         0|            0|            0|  0.00%|                                     "#" + str(j) + " has length " + str(len(p)) +
   662|         0|            0|            0|  0.00%|                                     "; 2 is required")
   663|         0|            0|            0|  0.00%|                # parameters as length-2 list too cumbersome to type, see ModuleDict.update comment
   664|         0|            0|            0|  0.00%|                self[p[0]] = p[1]  # type: ignore[assignment]
   665|         0|            0|            0|  0.00%|
   666|         0|            0|            0|  0.00%|    def extra_repr(self) -> str:
   667|         0|            0|            0|  0.00%|        child_lines = []
   668|         0|            0|            0|  0.00%|        for k, p in self._parameters.items():
   669|         0|            0|            0|  0.00%|            size_str = 'x'.join(str(size) for size in p.size())
   670|         0|            0|            0|  0.00%|            device_str = '' if not p.is_cuda else ' (GPU {})'.format(p.get_device())
   671|         0|            0|            0|  0.00%|            parastr = 'Parameter containing: [{} of size {}{}]'.format(
   672|         0|            0|            0|  0.00%|                torch.typename(p), size_str, device_str)
   673|         0|            0|            0|  0.00%|            child_lines.append('  (' + k + '): ' + parastr)
   674|         0|            0|            0|  0.00%|        tmpstr = '\n'.join(child_lines)
   675|         0|            0|            0|  0.00%|        return tmpstr
   676|         0|            0|            0|  0.00%|
   677|         0|            0|            0|  0.00%|    def __call__(self, input):
   678|         0|            0|            0|  0.00%|        raise RuntimeError('ParameterDict should not be called.')
   679|         0|            0|            0|  0.00%|
   680|         0|            0|            0|  0.00%|    def _replicate_for_data_parallel(self):
   681|         0|            0|            0|  0.00%|        warnings.warn("nn.ParameterDict is being used with DataParallel but this is not "
   682|         0|            0|            0|  0.00%|                      "supported. This dict will appear empty for the models replicated "
   683|         0|            0|            0|  0.00%|                      "on each GPU except the original one.")
   684|         0|            0|            0|  0.00%|
   685|         0|            0|            0|  0.00%|        return super(ParameterDict, self)._replicate_for_data_parallel()
File: /opt/conda/lib/python3.8/site-packages/torch/autograd/grad_mode.py
File duration: 0.0588748s (0.11%)
Line #|      Hits|         Time| Time per hit|      %|Source code
------+----------+-------------+-------------+-------+-----------
     1|         0|            0|            0|  0.00%|import sys
     2|         0|            0|            0|  0.00%|import torch
     3|         0|            0|            0|  0.00%|import functools
     4|         0|            0|            0|  0.00%|import inspect
     5|         0|            0|            0|  0.00%|from typing import Any, Callable, TypeVar, cast
     6|         0|            0|            0|  0.00%|
     7|         0|            0|            0|  0.00%|
     8|         0|            0|            0|  0.00%|__all__ = ['no_grad', 'enable_grad', 'set_grad_enabled',
     9|         0|            0|            0|  0.00%|           'inference_mode']
    10|         0|            0|            0|  0.00%|
    11|         0|            0|            0|  0.00%|
    12|         0|            0|            0|  0.00%|# Used for annotating the decorator usage of 'no_grad' and 'enable_grad'.
    13|         0|            0|            0|  0.00%|# See https://mypy.readthedocs.io/en/latest/generics.html#declaring-decorators
    14|         0|            0|            0|  0.00%|FuncType = Callable[..., Any]
    15|         0|            0|            0|  0.00%|F = TypeVar('F', bound=FuncType)
    16|         0|            0|            0|  0.00%|
    17|         0|            0|            0|  0.00%|
    18|         0|            0|            0|  0.00%|class _DecoratorContextManager:
    19|         0|            0|            0|  0.00%|    """Allow a context manager to be used as a decorator"""
    20|         0|            0|            0|  0.00%|
    21|         0|            0|            0|  0.00%|    def __call__(self, func: F) -> F:
    22|         0|            0|            0|  0.00%|        if inspect.isgeneratorfunction(func):
    23|         0|            0|            0|  0.00%|            return self._wrap_generator(func)
    24|         0|            0|            0|  0.00%|
    25|        61|  0.000391006|  6.40994e-06|  0.00%|        @functools.wraps(func)
    26|         0|            0|            0|  0.00%|        def decorate_context(*args, **kwargs):
    27|        61|   0.00187302|  3.07052e-05|  0.00%|            with self.__class__():
(call)|        61|   0.00313854|  5.14515e-05|  0.01%|# /opt/conda/lib/python3.8/site-packages/torch/autograd/grad_mode.py:119 __init__
(call)|        61|   0.00327301|  5.36559e-05|  0.01%|# /opt/conda/lib/python3.8/site-packages/torch/autograd/grad_mode.py:124 __enter__
    28|        61|   0.00383544|  6.28761e-05|  0.01%|                return func(*args, **kwargs)
(call)|        61|       1.4219|    0.0233098|  2.65%|# /opt/conda/lib/python3.8/site-packages/torch/optim/sgd.py:76 step
(call)|        61|   0.00331163|  5.42891e-05|  0.01%|# /opt/conda/lib/python3.8/site-packages/torch/autograd/grad_mode.py:128 __exit__
    29|         0|            0|            0|  0.00%|        return cast(F, decorate_context)
    30|         0|            0|            0|  0.00%|
    31|         0|            0|            0|  0.00%|    def _wrap_generator(self, func):
    32|         0|            0|            0|  0.00%|        """Wrap each generator invocation with the context manager"""
    33|         0|            0|            0|  0.00%|        @functools.wraps(func)
    34|         0|            0|            0|  0.00%|        def generator_context(*args, **kwargs):
    35|         0|            0|            0|  0.00%|            gen = func(*args, **kwargs)
    36|         0|            0|            0|  0.00%|
    37|         0|            0|            0|  0.00%|            # Generators are suspended and unsuspended at `yield`, hence we
    38|         0|            0|            0|  0.00%|            # make sure the grad mode is properly set every time the execution
    39|         0|            0|            0|  0.00%|            # flow returns into the wrapped generator and restored when it
    40|         0|            0|            0|  0.00%|            # returns through our `yield` to our caller (see PR #49017).
    41|         0|            0|            0|  0.00%|            cls = type(self)
    42|         0|            0|            0|  0.00%|            try:
    43|         0|            0|            0|  0.00%|                # Issuing `None` to a generator fires it up
    44|         0|            0|            0|  0.00%|                with cls():
    45|         0|            0|            0|  0.00%|                    response = gen.send(None)
    46|         0|            0|            0|  0.00%|
    47|         0|            0|            0|  0.00%|                while True:
    48|         0|            0|            0|  0.00%|                    try:
    49|         0|            0|            0|  0.00%|                        # Forward the response to our caller and get its next request
    50|         0|            0|            0|  0.00%|                        request = yield response
    51|         0|            0|            0|  0.00%|
    52|         0|            0|            0|  0.00%|                    except GeneratorExit:
    53|         0|            0|            0|  0.00%|                        # Inform the still active generator about its imminent closure
    54|         0|            0|            0|  0.00%|                        with cls():
    55|         0|            0|            0|  0.00%|                            gen.close()
    56|         0|            0|            0|  0.00%|                        raise
    57|         0|            0|            0|  0.00%|
    58|         0|            0|            0|  0.00%|                    except BaseException:
    59|         0|            0|            0|  0.00%|                        # Propagate the exception thrown at us by the caller
    60|         0|            0|            0|  0.00%|                        with cls():
    61|         0|            0|            0|  0.00%|                            response = gen.throw(*sys.exc_info())
    62|         0|            0|            0|  0.00%|
    63|         0|            0|            0|  0.00%|                    else:
    64|         0|            0|            0|  0.00%|                        # Pass the last request to the generator and get its response
    65|         0|            0|            0|  0.00%|                        with cls():
    66|         0|            0|            0|  0.00%|                            response = gen.send(request)
    67|         0|            0|            0|  0.00%|
    68|         0|            0|            0|  0.00%|            # We let the exceptions raised above by the generator's `.throw` or
    69|         0|            0|            0|  0.00%|            # `.send` methods bubble up to our caller, except for StopIteration
    70|         0|            0|            0|  0.00%|            except StopIteration as e:
    71|         0|            0|            0|  0.00%|                # The generator informed us that it is done: take whatever its
    72|         0|            0|            0|  0.00%|                # returned value (if any) was and indicate that we're done too
    73|         0|            0|            0|  0.00%|                # by returning it (see docs for python's return-statement).
    74|         0|            0|            0|  0.00%|                return e.value
    75|         0|            0|            0|  0.00%|
    76|         0|            0|            0|  0.00%|        return generator_context
    77|         0|            0|            0|  0.00%|
    78|         0|            0|            0|  0.00%|    def __enter__(self) -> None:
    79|         0|            0|            0|  0.00%|        raise NotImplementedError
    80|         0|            0|            0|  0.00%|
    81|         0|            0|            0|  0.00%|    def __exit__(self, exc_type: Any, exc_value: Any, traceback: Any) -> None:
    82|         0|            0|            0|  0.00%|        raise NotImplementedError
    83|         0|            0|            0|  0.00%|
    84|         0|            0|            0|  0.00%|
    85|         0|            0|            0|  0.00%|class no_grad(_DecoratorContextManager):
    86|         0|            0|            0|  0.00%|    r"""Context-manager that disabled gradient calculation.
    87|         0|            0|            0|  0.00%|
    88|         0|            0|            0|  0.00%|    Disabling gradient calculation is useful for inference, when you are sure
    89|         0|            0|            0|  0.00%|    that you will not call :meth:`Tensor.backward()`. It will reduce memory
    90|         0|            0|            0|  0.00%|    consumption for computations that would otherwise have `requires_grad=True`.
    91|         0|            0|            0|  0.00%|
    92|         0|            0|            0|  0.00%|    In this mode, the result of every computation will have
    93|         0|            0|            0|  0.00%|    `requires_grad=False`, even when the inputs have `requires_grad=True`.
    94|         0|            0|            0|  0.00%|
    95|         0|            0|            0|  0.00%|    This context manager is thread local; it will not affect computation
    96|         0|            0|            0|  0.00%|    in other threads.
    97|         0|            0|            0|  0.00%|
    98|         0|            0|            0|  0.00%|    Also functions as a decorator. (Make sure to instantiate with parenthesis.)
    99|         0|            0|            0|  0.00%|
   100|         0|            0|            0|  0.00%|    .. note::
   101|         0|            0|            0|  0.00%|        No-grad is one of several mechanisms that can enable or
   102|         0|            0|            0|  0.00%|        disable gradients locally see :ref:`locally-disable-grad-doc` for
   103|         0|            0|            0|  0.00%|        more information on how they compare.
   104|         0|            0|            0|  0.00%|
   105|         0|            0|            0|  0.00%|    Example::
   106|         0|            0|            0|  0.00%|
   107|         0|            0|            0|  0.00%|        >>> x = torch.tensor([1], requires_grad=True)
   108|         0|            0|            0|  0.00%|        >>> with torch.no_grad():
   109|         0|            0|            0|  0.00%|        ...   y = x * 2
   110|         0|            0|            0|  0.00%|        >>> y.requires_grad
   111|         0|            0|            0|  0.00%|        False
   112|         0|            0|            0|  0.00%|        >>> @torch.no_grad()
   113|         0|            0|            0|  0.00%|        ... def doubler(x):
   114|         0|            0|            0|  0.00%|        ...     return x * 2
   115|         0|            0|            0|  0.00%|        >>> z = doubler(x)
   116|         0|            0|            0|  0.00%|        >>> z.requires_grad
   117|         0|            0|            0|  0.00%|        False
   118|         0|            0|            0|  0.00%|    """
   119|       427|   0.00216317|  5.06598e-06|  0.00%|    def __init__(self):
   120|       427|   0.00567627|  1.32934e-05|  0.01%|        if not torch._jit_internal.is_scripting():
(call)|       427|   0.00348663|  8.16542e-06|  0.01%|# /opt/conda/lib/python3.8/site-packages/torch/_jit_internal.py:881 is_scripting
   121|       427|   0.00260615|   6.1034e-06|  0.00%|            super().__init__()
   122|       427|   0.00195599|  4.58076e-06|  0.00%|        self.prev = False
   123|         0|            0|            0|  0.00%|
   124|       427|   0.00197864|  4.63381e-06|  0.00%|    def __enter__(self):
   125|       427|   0.00296354|  6.94038e-06|  0.01%|        self.prev = torch.is_grad_enabled()
   126|       427|   0.00584555|  1.36898e-05|  0.01%|        torch.set_grad_enabled(False)
(call)|       427|   0.00742054|  1.73783e-05|  0.01%|# /opt/conda/lib/python3.8/site-packages/torch/autograd/grad_mode.py:213 __init__
   127|         0|            0|            0|  0.00%|
   128|       427|   0.00248671|  5.82367e-06|  0.00%|    def __exit__(self, exc_type: Any, exc_value: Any, traceback: Any) -> None:
   129|       427|   0.00646019|  1.51293e-05|  0.01%|        torch.set_grad_enabled(self.prev)
(call)|       427|   0.00784922|  1.83822e-05|  0.01%|# /opt/conda/lib/python3.8/site-packages/torch/autograd/grad_mode.py:213 __init__
   130|         0|            0|            0|  0.00%|
   131|         0|            0|            0|  0.00%|
   132|         0|            0|            0|  0.00%|class enable_grad(_DecoratorContextManager):
   133|         0|            0|            0|  0.00%|    r"""Context-manager that enables gradient calculation.
   134|         0|            0|            0|  0.00%|
   135|         0|            0|            0|  0.00%|    Enables gradient calculation, if it has been disabled via :class:`~no_grad`
   136|         0|            0|            0|  0.00%|    or :class:`~set_grad_enabled`.
   137|         0|            0|            0|  0.00%|
   138|         0|            0|            0|  0.00%|    This context manager is thread local; it will not affect computation
   139|         0|            0|            0|  0.00%|    in other threads.
   140|         0|            0|            0|  0.00%|
   141|         0|            0|            0|  0.00%|    Also functions as a decorator. (Make sure to instantiate with parenthesis.)
   142|         0|            0|            0|  0.00%|
   143|         0|            0|            0|  0.00%|    .. note::
   144|         0|            0|            0|  0.00%|        enable_grad is one of several mechanisms that can enable or
   145|         0|            0|            0|  0.00%|        disable gradients locally see :ref:`locally-disable-grad-doc` for
   146|         0|            0|            0|  0.00%|        more information on how they compare.
   147|         0|            0|            0|  0.00%|
   148|         0|            0|            0|  0.00%|    Example::
   149|         0|            0|            0|  0.00%|
   150|         0|            0|            0|  0.00%|        >>> x = torch.tensor([1.], requires_grad=True)
   151|         0|            0|            0|  0.00%|        >>> with torch.no_grad():
   152|         0|            0|            0|  0.00%|        ...   with torch.enable_grad():
   153|         0|            0|            0|  0.00%|        ...     y = x * 2
   154|         0|            0|            0|  0.00%|        >>> y.requires_grad
   155|         0|            0|            0|  0.00%|        True
   156|         0|            0|            0|  0.00%|        >>> y.backward()
   157|         0|            0|            0|  0.00%|        >>> x.grad
   158|         0|            0|            0|  0.00%|        >>> @torch.enable_grad()
   159|         0|            0|            0|  0.00%|        ... def doubler(x):
   160|         0|            0|            0|  0.00%|        ...     return x * 2
   161|         0|            0|            0|  0.00%|        >>> with torch.no_grad():
   162|         0|            0|            0|  0.00%|        ...     z = doubler(x)
   163|         0|            0|            0|  0.00%|        >>> z.requires_grad
   164|         0|            0|            0|  0.00%|        True
   165|         0|            0|            0|  0.00%|
   166|         0|            0|            0|  0.00%|    """
   167|         0|            0|            0|  0.00%|    def __enter__(self) -> None:
   168|         0|            0|            0|  0.00%|        self.prev = torch.is_grad_enabled()
   169|         0|            0|            0|  0.00%|        torch._C._set_grad_enabled(True)
   170|         0|            0|            0|  0.00%|
   171|         0|            0|            0|  0.00%|    def __exit__(self, exc_type: Any, exc_value: Any, traceback: Any) -> None:
   172|         0|            0|            0|  0.00%|        torch._C._set_grad_enabled(self.prev)
   173|         0|            0|            0|  0.00%|
   174|         0|            0|            0|  0.00%|
   175|         0|            0|            0|  0.00%|class set_grad_enabled(object):
   176|         0|            0|            0|  0.00%|    r"""Context-manager that sets gradient calculation to on or off.
   177|         0|            0|            0|  0.00%|
   178|         0|            0|            0|  0.00%|    ``set_grad_enabled`` will enable or disable grads based on its argument :attr:`mode`.
   179|         0|            0|            0|  0.00%|    It can be used as a context-manager or as a function.
   180|         0|            0|            0|  0.00%|
   181|         0|            0|            0|  0.00%|    This context manager is thread local; it will not affect computation
   182|         0|            0|            0|  0.00%|    in other threads.
   183|         0|            0|            0|  0.00%|
   184|         0|            0|            0|  0.00%|    Args:
   185|         0|            0|            0|  0.00%|        mode (bool): Flag whether to enable grad (``True``), or disable
   186|         0|            0|            0|  0.00%|                     (``False``). This can be used to conditionally enable
   187|         0|            0|            0|  0.00%|                     gradients.
   188|         0|            0|            0|  0.00%|
   189|         0|            0|            0|  0.00%|    .. note::
   190|         0|            0|            0|  0.00%|        set_grad_enabled is one of several mechanisms that can enable or
   191|         0|            0|            0|  0.00%|        disable gradients locally see :ref:`locally-disable-grad-doc` for
   192|         0|            0|            0|  0.00%|        more information on how they compare.
   193|         0|            0|            0|  0.00%|
   194|         0|            0|            0|  0.00%|    Example::
   195|         0|            0|            0|  0.00%|
   196|         0|            0|            0|  0.00%|        >>> x = torch.tensor([1], requires_grad=True)
   197|         0|            0|            0|  0.00%|        >>> is_train = False
   198|         0|            0|            0|  0.00%|        >>> with torch.set_grad_enabled(is_train):
   199|         0|            0|            0|  0.00%|        ...   y = x * 2
   200|         0|            0|            0|  0.00%|        >>> y.requires_grad
   201|         0|            0|            0|  0.00%|        False
   202|         0|            0|            0|  0.00%|        >>> torch.set_grad_enabled(True)
   203|         0|            0|            0|  0.00%|        >>> y = x * 2
   204|         0|            0|            0|  0.00%|        >>> y.requires_grad
   205|         0|            0|            0|  0.00%|        True
   206|         0|            0|            0|  0.00%|        >>> torch.set_grad_enabled(False)
   207|         0|            0|            0|  0.00%|        >>> y = x * 2
   208|         0|            0|            0|  0.00%|        >>> y.requires_grad
   209|         0|            0|            0|  0.00%|        False
   210|         0|            0|            0|  0.00%|
   211|         0|            0|            0|  0.00%|    """
   212|         0|            0|            0|  0.00%|
   213|       954|   0.00443268|  4.64641e-06|  0.01%|    def __init__(self, mode: bool) -> None:
   214|       954|   0.00698638|  7.32325e-06|  0.01%|        self.prev = torch.is_grad_enabled()
   215|       954|   0.00656509|  6.88165e-06|  0.01%|        torch._C._set_grad_enabled(mode)
   216|         0|            0|            0|  0.00%|
   217|       100|  0.000560284|  5.60284e-06|  0.00%|    def __enter__(self) -> None:
   218|       100|    0.0004704|    4.704e-06|  0.00%|        pass
   219|         0|            0|            0|  0.00%|
   220|       100|  0.000691414|  6.91414e-06|  0.00%|    def __exit__(self, exc_type: Any, exc_value: Any, traceback: Any) -> None:
   221|       100|  0.000932932|  9.32932e-06|  0.00%|        torch._C._set_grad_enabled(self.prev)
   222|         0|            0|            0|  0.00%|
   223|         0|            0|            0|  0.00%|
   224|         0|            0|            0|  0.00%|class inference_mode(_DecoratorContextManager):
   225|         0|            0|            0|  0.00%|    r"""Context-manager that enables or disables inference mode
   226|         0|            0|            0|  0.00%|
   227|         0|            0|            0|  0.00%|    InferenceMode is a new context manager analogous to :class:`~no_grad`
   228|         0|            0|            0|  0.00%|    to be used when you are certain your operations will have no interactions
   229|         0|            0|            0|  0.00%|    with autograd (e.g., model training). Code run under this mode gets better
   230|         0|            0|            0|  0.00%|    performance by disabling view tracking and version counter bumps.
   231|         0|            0|            0|  0.00%|
   232|         0|            0|            0|  0.00%|    This context manager is thread local; it will not affect computation
   233|         0|            0|            0|  0.00%|    in other threads.
   234|         0|            0|            0|  0.00%|
   235|         0|            0|            0|  0.00%|    Also functions as a decorator. (Make sure to instantiate with parenthesis.)
   236|         0|            0|            0|  0.00%|
   237|         0|            0|            0|  0.00%|    .. note::
   238|         0|            0|            0|  0.00%|        Inference mode is one of several mechanisms that can enable or
   239|         0|            0|            0|  0.00%|        disable gradients locally see :ref:`locally-disable-grad-doc` for
   240|         0|            0|            0|  0.00%|        more information on how they compare.
   241|         0|            0|            0|  0.00%|
   242|         0|            0|            0|  0.00%|    Args:
   243|         0|            0|            0|  0.00%|        mode (bool): Flag whether to enable or disable inference mode
   244|         0|            0|            0|  0.00%|
   245|         0|            0|            0|  0.00%|    Example::
   246|         0|            0|            0|  0.00%|        >>> import torch
   247|         0|            0|            0|  0.00%|        >>> x = torch.ones(1, 2, 3, requires_grad=True)
   248|         0|            0|            0|  0.00%|        >>> with torch.inference_mode():
   249|         0|            0|            0|  0.00%|        ...   y = x * x
   250|         0|            0|            0|  0.00%|        >>> y.requires_grad
   251|         0|            0|            0|  0.00%|        False
   252|         0|            0|            0|  0.00%|        >>> y._version
   253|         0|            0|            0|  0.00%|        Traceback (most recent call last):
   254|         0|            0|            0|  0.00%|        File "<stdin>", line 1, in <module>
   255|         0|            0|            0|  0.00%|        RuntimeError: Inference tensors do not track version counter.
   256|         0|            0|            0|  0.00%|        >>> @torch.inference_mode()
   257|         0|            0|            0|  0.00%|        ... def func(x):
   258|         0|            0|            0|  0.00%|        ...   return x * x
   259|         0|            0|            0|  0.00%|        >>> out = func(x)
   260|         0|            0|            0|  0.00%|        >>> out.requires_grad
   261|         0|            0|            0|  0.00%|        False
   262|         0|            0|            0|  0.00%|
   263|         0|            0|            0|  0.00%|    """
   264|         0|            0|            0|  0.00%|    def __init__(self, mode=True):
   265|         0|            0|            0|  0.00%|        if not torch._jit_internal.is_scripting():
   266|         0|            0|            0|  0.00%|            super().__init__()
   267|         0|            0|            0|  0.00%|        # Holds a python binding to a RAII guard that can enable or disable
   268|         0|            0|            0|  0.00%|        # inference mode
   269|         0|            0|            0|  0.00%|        self._inference_mode_raii_guard = None
   270|         0|            0|            0|  0.00%|        self.mode = mode
   271|         0|            0|            0|  0.00%|
   272|         0|            0|            0|  0.00%|    def __enter__(self):
   273|         0|            0|            0|  0.00%|        self._inference_mode_raii_guard = torch._C._InferenceMode(self.mode)
   274|         0|            0|            0|  0.00%|
   275|         0|            0|            0|  0.00%|    def __exit__(self, exc_type: Any, exc_value: Any, traceback: Any) -> None:
   276|         0|            0|            0|  0.00%|        del self._inference_mode_raii_guard
File: /opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py
File duration: 0.0550752s (0.10%)
Line #|      Hits|         Time| Time per hit|      %|Source code
------+----------+-------------+-------------+-------+-----------
     1|         0|            0|            0|  0.00%|r"""Definition of the DataLoader and associated iterators that subclass _BaseDataLoaderIter
     2|         0|            0|            0|  0.00%|
     3|         0|            0|            0|  0.00%|To support these two classes, in `./_utils` we define many utility methods and
     4|         0|            0|            0|  0.00%|functions to be run in multiprocessing. E.g., the data loading worker loop is
     5|         0|            0|            0|  0.00%|in `./_utils/worker.py`.
     6|         0|            0|            0|  0.00%|"""
     7|         0|            0|            0|  0.00%|
     8|         0|            0|            0|  0.00%|import os
     9|         0|            0|            0|  0.00%|import threading
    10|         0|            0|            0|  0.00%|import itertools
    11|         0|            0|            0|  0.00%|import warnings
    12|         0|            0|            0|  0.00%|import queue
    13|         0|            0|            0|  0.00%|from typing import Any, Callable, TypeVar, Generic, Sequence, List, Optional
    14|         0|            0|            0|  0.00%|
    15|         0|            0|            0|  0.00%|import multiprocessing as python_multiprocessing
    16|         0|            0|            0|  0.00%|import torch
    17|         0|            0|            0|  0.00%|import torch.multiprocessing as multiprocessing
    18|         0|            0|            0|  0.00%|from torch._utils import ExceptionWrapper
    19|         0|            0|            0|  0.00%|from torch._six import string_classes
    20|         0|            0|            0|  0.00%|
    21|         0|            0|            0|  0.00%|from . import IterableDataset, Sampler, SequentialSampler, RandomSampler, BatchSampler, Dataset
    22|         0|            0|            0|  0.00%|from . import _utils
    23|         0|            0|            0|  0.00%|
    24|         0|            0|            0|  0.00%|T_co = TypeVar('T_co', covariant=True)
    25|         0|            0|            0|  0.00%|T = TypeVar('T')
    26|         0|            0|            0|  0.00%|_worker_init_fn_t = Callable[[int], None]
    27|         0|            0|            0|  0.00%|
    28|         0|            0|            0|  0.00%|# Ideally we would parameterize `DataLoader` by the return type of `collate_fn`, but there is currently no way to have that
    29|         0|            0|            0|  0.00%|# type parameter set to a default value if the user doesn't pass in a custom 'collate_fn'.
    30|         0|            0|            0|  0.00%|# See https://github.com/python/mypy/issues/3737.
    31|         0|            0|            0|  0.00%|_collate_fn_t = Callable[[List[T]], Any]
    32|         0|            0|            0|  0.00%|
    33|         0|            0|            0|  0.00%|
    34|         0|            0|            0|  0.00%|# This function used to be defined in this file. However, it was moved to
    35|         0|            0|            0|  0.00%|# _utils/collate.py. Although it is rather hard to access this from user land
    36|         0|            0|            0|  0.00%|# (one has to explicitly directly `import torch.utils.data.dataloader`), there
    37|         0|            0|            0|  0.00%|# probably is user code out there using it. This aliasing maintains BC in this
    38|         0|            0|            0|  0.00%|# aspect.
    39|         0|            0|            0|  0.00%|default_collate: _collate_fn_t = _utils.collate.default_collate
    40|         0|            0|            0|  0.00%|
    41|         0|            0|            0|  0.00%|get_worker_info = _utils.worker.get_worker_info
    42|         0|            0|            0|  0.00%|
    43|         0|            0|            0|  0.00%|class _DatasetKind(object):
    44|         0|            0|            0|  0.00%|    Map = 0
    45|         0|            0|            0|  0.00%|    Iterable = 1
    46|         0|            0|            0|  0.00%|
    47|         0|            0|            0|  0.00%|    @staticmethod
    48|         0|            0|            0|  0.00%|    def create_fetcher(kind, dataset, auto_collation, collate_fn, drop_last):
    49|         0|            0|            0|  0.00%|        if kind == _DatasetKind.Map:
    50|         0|            0|            0|  0.00%|            return _utils.fetch._MapDatasetFetcher(dataset, auto_collation, collate_fn, drop_last)
    51|         0|            0|            0|  0.00%|        else:
    52|         0|            0|            0|  0.00%|            return _utils.fetch._IterableDatasetFetcher(dataset, auto_collation, collate_fn, drop_last)
    53|         0|            0|            0|  0.00%|
    54|         0|            0|            0|  0.00%|
    55|         0|            0|            0|  0.00%|class _InfiniteConstantSampler(Sampler):
    56|         0|            0|            0|  0.00%|    r"""Analogous to ``itertools.repeat(None, None)``.
    57|         0|            0|            0|  0.00%|    Used as sampler for :class:`~torch.utils.data.IterableDataset`.
    58|         0|            0|            0|  0.00%|
    59|         0|            0|            0|  0.00%|    Args:
    60|         0|            0|            0|  0.00%|        data_source (Dataset): dataset to sample from
    61|         0|            0|            0|  0.00%|    """
    62|         0|            0|            0|  0.00%|
    63|         0|            0|            0|  0.00%|    def __init__(self):
    64|         0|            0|            0|  0.00%|        super(_InfiniteConstantSampler, self).__init__(None)
    65|         0|            0|            0|  0.00%|
    66|         0|            0|            0|  0.00%|    def __iter__(self):
    67|         0|            0|            0|  0.00%|        while True:
    68|         0|            0|            0|  0.00%|            yield None
    69|         0|            0|            0|  0.00%|
    70|         0|            0|            0|  0.00%|
    71|         0|            0|            0|  0.00%|class DataLoader(Generic[T_co]):
    72|         0|            0|            0|  0.00%|    r"""
    73|         0|            0|            0|  0.00%|    Data loader. Combines a dataset and a sampler, and provides an iterable over
    74|         0|            0|            0|  0.00%|    the given dataset.
    75|         0|            0|            0|  0.00%|
    76|         0|            0|            0|  0.00%|    The :class:`~torch.utils.data.DataLoader` supports both map-style and
    77|         0|            0|            0|  0.00%|    iterable-style datasets with single- or multi-process loading, customizing
    78|         0|            0|            0|  0.00%|    loading order and optional automatic batching (collation) and memory pinning.
    79|         0|            0|            0|  0.00%|
    80|         0|            0|            0|  0.00%|    See :py:mod:`torch.utils.data` documentation page for more details.
    81|         0|            0|            0|  0.00%|
    82|         0|            0|            0|  0.00%|    Args:
    83|         0|            0|            0|  0.00%|        dataset (Dataset): dataset from which to load the data.
    84|         0|            0|            0|  0.00%|        batch_size (int, optional): how many samples per batch to load
    85|         0|            0|            0|  0.00%|            (default: ``1``).
    86|         0|            0|            0|  0.00%|        shuffle (bool, optional): set to ``True`` to have the data reshuffled
    87|         0|            0|            0|  0.00%|            at every epoch (default: ``False``).
    88|         0|            0|            0|  0.00%|        sampler (Sampler or Iterable, optional): defines the strategy to draw
    89|         0|            0|            0|  0.00%|            samples from the dataset. Can be any ``Iterable`` with ``__len__``
    90|         0|            0|            0|  0.00%|            implemented. If specified, :attr:`shuffle` must not be specified.
    91|         0|            0|            0|  0.00%|        batch_sampler (Sampler or Iterable, optional): like :attr:`sampler`, but
    92|         0|            0|            0|  0.00%|            returns a batch of indices at a time. Mutually exclusive with
    93|         0|            0|            0|  0.00%|            :attr:`batch_size`, :attr:`shuffle`, :attr:`sampler`,
    94|         0|            0|            0|  0.00%|            and :attr:`drop_last`.
    95|         0|            0|            0|  0.00%|        num_workers (int, optional): how many subprocesses to use for data
    96|         0|            0|            0|  0.00%|            loading. ``0`` means that the data will be loaded in the main process.
    97|         0|            0|            0|  0.00%|            (default: ``0``)
    98|         0|            0|            0|  0.00%|        collate_fn (callable, optional): merges a list of samples to form a
    99|         0|            0|            0|  0.00%|            mini-batch of Tensor(s).  Used when using batched loading from a
   100|         0|            0|            0|  0.00%|            map-style dataset.
   101|         0|            0|            0|  0.00%|        pin_memory (bool, optional): If ``True``, the data loader will copy Tensors
   102|         0|            0|            0|  0.00%|            into CUDA pinned memory before returning them.  If your data elements
   103|         0|            0|            0|  0.00%|            are a custom type, or your :attr:`collate_fn` returns a batch that is a custom type,
   104|         0|            0|            0|  0.00%|            see the example below.
   105|         0|            0|            0|  0.00%|        drop_last (bool, optional): set to ``True`` to drop the last incomplete batch,
   106|         0|            0|            0|  0.00%|            if the dataset size is not divisible by the batch size. If ``False`` and
   107|         0|            0|            0|  0.00%|            the size of dataset is not divisible by the batch size, then the last batch
   108|         0|            0|            0|  0.00%|            will be smaller. (default: ``False``)
   109|         0|            0|            0|  0.00%|        timeout (numeric, optional): if positive, the timeout value for collecting a batch
   110|         0|            0|            0|  0.00%|            from workers. Should always be non-negative. (default: ``0``)
   111|         0|            0|            0|  0.00%|        worker_init_fn (callable, optional): If not ``None``, this will be called on each
   112|         0|            0|            0|  0.00%|            worker subprocess with the worker id (an int in ``[0, num_workers - 1]``) as
   113|         0|            0|            0|  0.00%|            input, after seeding and before data loading. (default: ``None``)
   114|         0|            0|            0|  0.00%|        generator (torch.Generator, optional): If not ``None``, this RNG will be used
   115|         0|            0|            0|  0.00%|            by RandomSampler to generate random indexes and multiprocessing to generate
   116|         0|            0|            0|  0.00%|            `base_seed` for workers. (default: ``None``)
   117|         0|            0|            0|  0.00%|        prefetch_factor (int, optional, keyword-only arg): Number of samples loaded
   118|         0|            0|            0|  0.00%|            in advance by each worker. ``2`` means there will be a total of
   119|         0|            0|            0|  0.00%|            2 * num_workers samples prefetched across all workers. (default: ``2``)
   120|         0|            0|            0|  0.00%|        persistent_workers (bool, optional): If ``True``, the data loader will not shutdown
   121|         0|            0|            0|  0.00%|            the worker processes after a dataset has been consumed once. This allows to
   122|         0|            0|            0|  0.00%|            maintain the workers `Dataset` instances alive. (default: ``False``)
   123|         0|            0|            0|  0.00%|
   124|         0|            0|            0|  0.00%|
   125|         0|            0|            0|  0.00%|    .. warning:: If the ``spawn`` start method is used, :attr:`worker_init_fn`
   126|         0|            0|            0|  0.00%|                 cannot be an unpicklable object, e.g., a lambda function. See
   127|         0|            0|            0|  0.00%|                 :ref:`multiprocessing-best-practices` on more details related
   128|         0|            0|            0|  0.00%|                 to multiprocessing in PyTorch.
   129|         0|            0|            0|  0.00%|
   130|         0|            0|            0|  0.00%|    .. warning:: ``len(dataloader)`` heuristic is based on the length of the sampler used.
   131|         0|            0|            0|  0.00%|                 When :attr:`dataset` is an :class:`~torch.utils.data.IterableDataset`,
   132|         0|            0|            0|  0.00%|                 it instead returns an estimate based on ``len(dataset) / batch_size``, with proper
   133|         0|            0|            0|  0.00%|                 rounding depending on :attr:`drop_last`, regardless of multi-process loading
   134|         0|            0|            0|  0.00%|                 configurations. This represents the best guess PyTorch can make because PyTorch
   135|         0|            0|            0|  0.00%|                 trusts user :attr:`dataset` code in correctly handling multi-process
   136|         0|            0|            0|  0.00%|                 loading to avoid duplicate data.
   137|         0|            0|            0|  0.00%|
   138|         0|            0|            0|  0.00%|                 However, if sharding results in multiple workers having incomplete last batches,
   139|         0|            0|            0|  0.00%|                 this estimate can still be inaccurate, because (1) an otherwise complete batch can
   140|         0|            0|            0|  0.00%|                 be broken into multiple ones and (2) more than one batch worth of samples can be
   141|         0|            0|            0|  0.00%|                 dropped when :attr:`drop_last` is set. Unfortunately, PyTorch can not detect such
   142|         0|            0|            0|  0.00%|                 cases in general.
   143|         0|            0|            0|  0.00%|
   144|         0|            0|            0|  0.00%|                 See `Dataset Types`_ for more details on these two types of datasets and how
   145|         0|            0|            0|  0.00%|                 :class:`~torch.utils.data.IterableDataset` interacts with
   146|         0|            0|            0|  0.00%|                 `Multi-process data loading`_.
   147|         0|            0|            0|  0.00%|
   148|         0|            0|            0|  0.00%|    .. warning:: See :ref:`reproducibility`, and :ref:`dataloader-workers-random-seed`, and
   149|         0|            0|            0|  0.00%|                 :ref:`data-loading-randomness` notes for random seed related questions.
   150|         0|            0|            0|  0.00%|    """
   151|         0|            0|            0|  0.00%|    dataset: Dataset[T_co]
   152|         0|            0|            0|  0.00%|    batch_size: Optional[int]
   153|         0|            0|            0|  0.00%|    num_workers: int
   154|         0|            0|            0|  0.00%|    pin_memory: bool
   155|         0|            0|            0|  0.00%|    drop_last: bool
   156|         0|            0|            0|  0.00%|    timeout: float
   157|         0|            0|            0|  0.00%|    sampler: Sampler
   158|         0|            0|            0|  0.00%|    prefetch_factor: int
   159|         0|            0|            0|  0.00%|    _iterator : Optional['_BaseDataLoaderIter']
   160|         0|            0|            0|  0.00%|    __initialized = False
   161|         0|            0|            0|  0.00%|
   162|         0|            0|            0|  0.00%|    def __init__(self, dataset: Dataset[T_co], batch_size: Optional[int] = 1,
   163|         0|            0|            0|  0.00%|                 shuffle: bool = False, sampler: Optional[Sampler[int]] = None,
   164|         0|            0|            0|  0.00%|                 batch_sampler: Optional[Sampler[Sequence[int]]] = None,
   165|         0|            0|            0|  0.00%|                 num_workers: int = 0, collate_fn: Optional[_collate_fn_t] = None,
   166|         0|            0|            0|  0.00%|                 pin_memory: bool = False, drop_last: bool = False,
   167|         0|            0|            0|  0.00%|                 timeout: float = 0, worker_init_fn: Optional[_worker_init_fn_t] = None,
   168|         0|            0|            0|  0.00%|                 multiprocessing_context=None, generator=None,
   169|         0|            0|            0|  0.00%|                 *, prefetch_factor: int = 2,
   170|         0|            0|            0|  0.00%|                 persistent_workers: bool = False):
   171|         0|            0|            0|  0.00%|        torch._C._log_api_usage_once("python.data_loader")
   172|         0|            0|            0|  0.00%|
   173|         0|            0|            0|  0.00%|        if num_workers < 0:
   174|         0|            0|            0|  0.00%|            raise ValueError('num_workers option should be non-negative; '
   175|         0|            0|            0|  0.00%|                             'use num_workers=0 to disable multiprocessing.')
   176|         0|            0|            0|  0.00%|
   177|         0|            0|            0|  0.00%|        if timeout < 0:
   178|         0|            0|            0|  0.00%|            raise ValueError('timeout option should be non-negative')
   179|         0|            0|            0|  0.00%|
   180|         0|            0|            0|  0.00%|        if num_workers == 0 and prefetch_factor != 2:
   181|         0|            0|            0|  0.00%|            raise ValueError('prefetch_factor option could only be specified in multiprocessing.'
   182|         0|            0|            0|  0.00%|                             'let num_workers > 0 to enable multiprocessing.')
   183|         0|            0|            0|  0.00%|        assert prefetch_factor > 0
   184|         0|            0|            0|  0.00%|
   185|         0|            0|            0|  0.00%|        if persistent_workers and num_workers == 0:
   186|         0|            0|            0|  0.00%|            raise ValueError('persistent_workers option needs num_workers > 0')
   187|         0|            0|            0|  0.00%|
   188|         0|            0|            0|  0.00%|        self.dataset = dataset
   189|         0|            0|            0|  0.00%|        self.num_workers = num_workers
   190|         0|            0|            0|  0.00%|        self.prefetch_factor = prefetch_factor
   191|         0|            0|            0|  0.00%|        self.pin_memory = pin_memory
   192|         0|            0|            0|  0.00%|        self.timeout = timeout
   193|         0|            0|            0|  0.00%|        self.worker_init_fn = worker_init_fn
   194|         0|            0|            0|  0.00%|        self.multiprocessing_context = multiprocessing_context
   195|         0|            0|            0|  0.00%|
   196|         0|            0|            0|  0.00%|        # Arg-check dataset related before checking samplers because we want to
   197|         0|            0|            0|  0.00%|        # tell users that iterable-style datasets are incompatible with custom
   198|         0|            0|            0|  0.00%|        # samplers first, so that they don't learn that this combo doesn't work
   199|         0|            0|            0|  0.00%|        # after spending time fixing the custom sampler errors.
   200|         0|            0|            0|  0.00%|        if isinstance(dataset, IterableDataset):
   201|         0|            0|            0|  0.00%|            self._dataset_kind = _DatasetKind.Iterable
   202|         0|            0|            0|  0.00%|            # NOTE [ Custom Samplers and IterableDataset ]
   203|         0|            0|            0|  0.00%|            #
   204|         0|            0|            0|  0.00%|            # `IterableDataset` does not support custom `batch_sampler` or
   205|         0|            0|            0|  0.00%|            # `sampler` since the key is irrelevant (unless we support
   206|         0|            0|            0|  0.00%|            # generator-style dataset one day...).
   207|         0|            0|            0|  0.00%|            #
   208|         0|            0|            0|  0.00%|            # For `sampler`, we always create a dummy sampler. This is an
   209|         0|            0|            0|  0.00%|            # infinite sampler even when the dataset may have an implemented
   210|         0|            0|            0|  0.00%|            # finite `__len__` because in multi-process data loading, naive
   211|         0|            0|            0|  0.00%|            # settings will return duplicated data (which may be desired), and
   212|         0|            0|            0|  0.00%|            # thus using a sampler with length matching that of dataset will
   213|         0|            0|            0|  0.00%|            # cause data lost (you may have duplicates of the first couple
   214|         0|            0|            0|  0.00%|            # batches, but never see anything afterwards). Therefore,
   215|         0|            0|            0|  0.00%|            # `Iterabledataset` always uses an infinite sampler, an instance of
   216|         0|            0|            0|  0.00%|            # `_InfiniteConstantSampler` defined above.
   217|         0|            0|            0|  0.00%|            #
   218|         0|            0|            0|  0.00%|            # A custom `batch_sampler` essentially only controls the batch size.
   219|         0|            0|            0|  0.00%|            # However, it is unclear how useful it would be since an iterable-style
   220|         0|            0|            0|  0.00%|            # dataset can handle that within itself. Moreover, it is pointless
   221|         0|            0|            0|  0.00%|            # in multi-process data loading as the assignment order of batches
   222|         0|            0|            0|  0.00%|            # to workers is an implementation detail so users can not control
   223|         0|            0|            0|  0.00%|            # how to batchify each worker's iterable. Thus, we disable this
   224|         0|            0|            0|  0.00%|            # option. If this turns out to be useful in future, we can re-enable
   225|         0|            0|            0|  0.00%|            # this, and support custom samplers that specify the assignments to
   226|         0|            0|            0|  0.00%|            # specific workers.
   227|         0|            0|            0|  0.00%|            if shuffle is not False:
   228|         0|            0|            0|  0.00%|                raise ValueError(
   229|         0|            0|            0|  0.00%|                    "DataLoader with IterableDataset: expected unspecified "
   230|         0|            0|            0|  0.00%|                    "shuffle option, but got shuffle={}".format(shuffle))
   231|         0|            0|            0|  0.00%|            elif sampler is not None:
   232|         0|            0|            0|  0.00%|                # See NOTE [ Custom Samplers and IterableDataset ]
   233|         0|            0|            0|  0.00%|                raise ValueError(
   234|         0|            0|            0|  0.00%|                    "DataLoader with IterableDataset: expected unspecified "
   235|         0|            0|            0|  0.00%|                    "sampler option, but got sampler={}".format(sampler))
   236|         0|            0|            0|  0.00%|            elif batch_sampler is not None:
   237|         0|            0|            0|  0.00%|                # See NOTE [ Custom Samplers and IterableDataset ]
   238|         0|            0|            0|  0.00%|                raise ValueError(
   239|         0|            0|            0|  0.00%|                    "DataLoader with IterableDataset: expected unspecified "
   240|         0|            0|            0|  0.00%|                    "batch_sampler option, but got batch_sampler={}".format(batch_sampler))
   241|         0|            0|            0|  0.00%|        else:
   242|         0|            0|            0|  0.00%|            self._dataset_kind = _DatasetKind.Map
   243|         0|            0|            0|  0.00%|
   244|         0|            0|            0|  0.00%|        if sampler is not None and shuffle:
   245|         0|            0|            0|  0.00%|            raise ValueError('sampler option is mutually exclusive with '
   246|         0|            0|            0|  0.00%|                             'shuffle')
   247|         0|            0|            0|  0.00%|
   248|         0|            0|            0|  0.00%|        if batch_sampler is not None:
   249|         0|            0|            0|  0.00%|            # auto_collation with custom batch_sampler
   250|         0|            0|            0|  0.00%|            if batch_size != 1 or shuffle or sampler is not None or drop_last:
   251|         0|            0|            0|  0.00%|                raise ValueError('batch_sampler option is mutually exclusive '
   252|         0|            0|            0|  0.00%|                                 'with batch_size, shuffle, sampler, and '
   253|         0|            0|            0|  0.00%|                                 'drop_last')
   254|         0|            0|            0|  0.00%|            batch_size = None
   255|         0|            0|            0|  0.00%|            drop_last = False
   256|         0|            0|            0|  0.00%|        elif batch_size is None:
   257|         0|            0|            0|  0.00%|            # no auto_collation
   258|         0|            0|            0|  0.00%|            if drop_last:
   259|         0|            0|            0|  0.00%|                raise ValueError('batch_size=None option disables auto-batching '
   260|         0|            0|            0|  0.00%|                                 'and is mutually exclusive with drop_last')
   261|         0|            0|            0|  0.00%|
   262|         0|            0|            0|  0.00%|        if sampler is None:  # give default samplers
   263|         0|            0|            0|  0.00%|            if self._dataset_kind == _DatasetKind.Iterable:
   264|         0|            0|            0|  0.00%|                # See NOTE [ Custom Samplers and IterableDataset ]
   265|         0|            0|            0|  0.00%|                sampler = _InfiniteConstantSampler()
   266|         0|            0|            0|  0.00%|            else:  # map-style
   267|         0|            0|            0|  0.00%|                if shuffle:
   268|         0|            0|            0|  0.00%|                    # Cannot statically verify that dataset is Sized
   269|         0|            0|            0|  0.00%|                    # Somewhat related: see NOTE [ Lack of Default `__len__` in Python Abstract Base Classes ]
   270|         0|            0|            0|  0.00%|                    sampler = RandomSampler(dataset, generator=generator)  # type: ignore[arg-type]
   271|         0|            0|            0|  0.00%|                else:
   272|         0|            0|            0|  0.00%|                    sampler = SequentialSampler(dataset)  # type: ignore[arg-type]
   273|         0|            0|            0|  0.00%|
   274|         0|            0|            0|  0.00%|        if batch_size is not None and batch_sampler is None:
   275|         0|            0|            0|  0.00%|            # auto_collation without custom batch_sampler
   276|         0|            0|            0|  0.00%|            batch_sampler = BatchSampler(sampler, batch_size, drop_last)
   277|         0|            0|            0|  0.00%|
   278|         0|            0|            0|  0.00%|        self.batch_size = batch_size
   279|         0|            0|            0|  0.00%|        self.drop_last = drop_last
   280|         0|            0|            0|  0.00%|        self.sampler = sampler
   281|         0|            0|            0|  0.00%|        self.batch_sampler = batch_sampler
   282|         0|            0|            0|  0.00%|        self.generator = generator
   283|         0|            0|            0|  0.00%|
   284|         0|            0|            0|  0.00%|        if collate_fn is None:
   285|         0|            0|            0|  0.00%|            if self._auto_collation:
   286|         0|            0|            0|  0.00%|                collate_fn = _utils.collate.default_collate
   287|         0|            0|            0|  0.00%|            else:
   288|         0|            0|            0|  0.00%|                collate_fn = _utils.collate.default_convert
   289|         0|            0|            0|  0.00%|
   290|         0|            0|            0|  0.00%|        self.collate_fn = collate_fn
   291|         0|            0|            0|  0.00%|        self.persistent_workers = persistent_workers
   292|         0|            0|            0|  0.00%|
   293|         0|            0|            0|  0.00%|        self.__initialized = True
   294|         0|            0|            0|  0.00%|        self._IterableDataset_len_called = None  # See NOTE [ IterableDataset and __len__ ]
   295|         0|            0|            0|  0.00%|
   296|         0|            0|            0|  0.00%|        self._iterator = None
   297|         0|            0|            0|  0.00%|
   298|         0|            0|            0|  0.00%|        self.check_worker_number_rationality()
   299|         0|            0|            0|  0.00%|
   300|         2|  1.16825e-05|  5.84126e-06|  0.00%|    def _get_iterator(self) -> '_BaseDataLoaderIter':
   301|         2|  1.12057e-05|  5.60284e-06|  0.00%|        if self.num_workers == 0:
   302|         0|            0|            0|  0.00%|            return _SingleProcessDataLoaderIter(self)
   303|         0|            0|            0|  0.00%|        else:
   304|         2|  5.03063e-05|  2.51532e-05|  0.00%|            self.check_worker_number_rationality()
(call)|         2|  0.000207901|  0.000103951|  0.00%|# /opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py:406 check_worker_number_rationality
   305|         2|  0.000187159|  9.35793e-05|  0.00%|            return _MultiProcessingDataLoaderIter(self)
(call)|         2|      1.11975|     0.559874|  2.09%|# /opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py:877 __init__
   306|         0|            0|            0|  0.00%|
   307|         2|  1.19209e-05|  5.96046e-06|  0.00%|    @property
   308|         0|            0|            0|  0.00%|    def multiprocessing_context(self):
   309|         2|  1.04904e-05|  5.24521e-06|  0.00%|        return self.__multiprocessing_context
   310|         0|            0|            0|  0.00%|
   311|         0|            0|            0|  0.00%|    @multiprocessing_context.setter
   312|         0|            0|            0|  0.00%|    def multiprocessing_context(self, multiprocessing_context):
   313|         0|            0|            0|  0.00%|        if multiprocessing_context is not None:
   314|         0|            0|            0|  0.00%|            if self.num_workers > 0:
   315|         0|            0|            0|  0.00%|                if isinstance(multiprocessing_context, string_classes):
   316|         0|            0|            0|  0.00%|                    valid_start_methods = multiprocessing.get_all_start_methods()
   317|         0|            0|            0|  0.00%|                    if multiprocessing_context not in valid_start_methods:
   318|         0|            0|            0|  0.00%|                        raise ValueError(
   319|         0|            0|            0|  0.00%|                            ('multiprocessing_context option '
   320|         0|            0|            0|  0.00%|                             'should specify a valid start method in {!r}, but got '
   321|         0|            0|            0|  0.00%|                             'multiprocessing_context={!r}').format(valid_start_methods, multiprocessing_context))
   322|         0|            0|            0|  0.00%|                    # error: Argument 1 to "get_context" has incompatible type "Union[str, bytes]"; expected "str"  [arg-type]
   323|         0|            0|            0|  0.00%|                    multiprocessing_context = multiprocessing.get_context(multiprocessing_context)  # type: ignore[arg-type]
   324|         0|            0|            0|  0.00%|
   325|         0|            0|            0|  0.00%|                if not isinstance(multiprocessing_context, python_multiprocessing.context.BaseContext):
   326|         0|            0|            0|  0.00%|                    raise TypeError(('multiprocessing_context option should be a valid context '
   327|         0|            0|            0|  0.00%|                                     'object or a string specifying the start method, but got '
   328|         0|            0|            0|  0.00%|                                     'multiprocessing_context={}').format(multiprocessing_context))
   329|         0|            0|            0|  0.00%|            else:
   330|         0|            0|            0|  0.00%|                raise ValueError(('multiprocessing_context can only be used with '
   331|         0|            0|            0|  0.00%|                                  'multi-process loading (num_workers > 0), but got '
   332|         0|            0|            0|  0.00%|                                  'num_workers={}').format(self.num_workers))
   333|         0|            0|            0|  0.00%|
   334|         0|            0|            0|  0.00%|        self.__multiprocessing_context = multiprocessing_context
   335|         0|            0|            0|  0.00%|
   336|         0|            0|            0|  0.00%|    def __setattr__(self, attr, val):
   337|         0|            0|            0|  0.00%|        if self.__initialized and attr in (
   338|         0|            0|            0|  0.00%|                'batch_size', 'batch_sampler', 'sampler', 'drop_last', 'dataset', 'persistent_workers'):
   339|         0|            0|            0|  0.00%|            raise ValueError('{} attribute should not be set after {} is '
   340|         0|            0|            0|  0.00%|                             'initialized'.format(attr, self.__class__.__name__))
   341|         0|            0|            0|  0.00%|
   342|         0|            0|            0|  0.00%|        super(DataLoader, self).__setattr__(attr, val)
   343|         0|            0|            0|  0.00%|
   344|         0|            0|            0|  0.00%|    # We quote '_BaseDataLoaderIter' since it isn't defined yet and the definition can't be moved up
   345|         0|            0|            0|  0.00%|    # since '_BaseDataLoaderIter' references 'DataLoader'.
   346|         2|  1.62125e-05|  8.10623e-06|  0.00%|    def __iter__(self) -> '_BaseDataLoaderIter':
   347|         0|            0|            0|  0.00%|        # When using a single worker the returned iterator should be
   348|         0|            0|            0|  0.00%|        # created everytime to avoid reseting its state
   349|         0|            0|            0|  0.00%|        # However, in the case of a multiple workers iterator
   350|         0|            0|            0|  0.00%|        # the iterator is only created once in the lifetime of the
   351|         0|            0|            0|  0.00%|        # DataLoader object so that workers can be reused
   352|         2|  3.79086e-05|  1.89543e-05|  0.00%|        if self.persistent_workers and self.num_workers > 0:
   353|         0|            0|            0|  0.00%|            if self._iterator is None:
   354|         0|            0|            0|  0.00%|                self._iterator = self._get_iterator()
   355|         0|            0|            0|  0.00%|            else:
   356|         0|            0|            0|  0.00%|                self._iterator._reset(self)
   357|         0|            0|            0|  0.00%|            return self._iterator
   358|         0|            0|            0|  0.00%|        else:
   359|         2|  7.58171e-05|  3.79086e-05|  0.00%|            return self._get_iterator()
(call)|         2|      1.12022|     0.560108|  2.09%|# /opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py:300 _get_iterator
   360|         0|            0|            0|  0.00%|
   361|         4|  2.12193e-05|  5.30481e-06|  0.00%|    @property
   362|         0|            0|            0|  0.00%|    def _auto_collation(self):
   363|         4|   2.6226e-05|  6.55651e-06|  0.00%|        return self.batch_sampler is not None
   364|         0|            0|            0|  0.00%|
   365|         2|  1.28746e-05|   6.4373e-06|  0.00%|    @property
   366|         0|            0|            0|  0.00%|    def _index_sampler(self):
   367|         0|            0|            0|  0.00%|        # The actual sampler used for generating indices for `_DatasetFetcher`
   368|         0|            0|            0|  0.00%|        # (see _utils/fetch.py) to read data at each time. This would be
   369|         0|            0|            0|  0.00%|        # `.batch_sampler` if in auto-collation mode, and `.sampler` otherwise.
   370|         0|            0|            0|  0.00%|        # We can't change `.sampler` and `.batch_sampler` attributes for BC
   371|         0|            0|            0|  0.00%|        # reasons.
   372|         2|  2.74181e-05|  1.37091e-05|  0.00%|        if self._auto_collation:
(call)|         2|  1.78814e-05|   8.9407e-06|  0.00%|# /opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py:361 _auto_collation
   373|         2|  1.00136e-05|  5.00679e-06|  0.00%|            return self.batch_sampler
   374|         0|            0|            0|  0.00%|        else:
   375|         0|            0|            0|  0.00%|            return self.sampler
   376|         0|            0|            0|  0.00%|
   377|         0|            0|            0|  0.00%|    def __len__(self) -> int:
   378|         0|            0|            0|  0.00%|        if self._dataset_kind == _DatasetKind.Iterable:
   379|         0|            0|            0|  0.00%|            # NOTE [ IterableDataset and __len__ ]
   380|         0|            0|            0|  0.00%|            #
   381|         0|            0|            0|  0.00%|            # For `IterableDataset`, `__len__` could be inaccurate when one naively
   382|         0|            0|            0|  0.00%|            # does multi-processing data loading, since the samples will be duplicated.
   383|         0|            0|            0|  0.00%|            # However, no real use case should be actually using that behavior, so
   384|         0|            0|            0|  0.00%|            # it should count as a user error. We should generally trust user
   385|         0|            0|            0|  0.00%|            # code to do the proper thing (e.g., configure each replica differently
   386|         0|            0|            0|  0.00%|            # in `__iter__`), and give us the correct `__len__` if they choose to
   387|         0|            0|            0|  0.00%|            # implement it (this will still throw if the dataset does not implement
   388|         0|            0|            0|  0.00%|            # a `__len__`).
   389|         0|            0|            0|  0.00%|            #
   390|         0|            0|            0|  0.00%|            # To provide a further warning, we track if `__len__` was called on the
   391|         0|            0|            0|  0.00%|            # `DataLoader`, save the returned value in `self._len_called`, and warn
   392|         0|            0|            0|  0.00%|            # if the iterator ends up yielding more than this number of samples.
   393|         0|            0|            0|  0.00%|
   394|         0|            0|            0|  0.00%|            # Cannot statically verify that dataset is Sized
   395|         0|            0|            0|  0.00%|            length = self._IterableDataset_len_called = len(self.dataset)  # type: ignore[assignment, arg-type]
   396|         0|            0|            0|  0.00%|            if self.batch_size is not None:  # IterableDataset doesn't allow custom sampler or batch_sampler
   397|         0|            0|            0|  0.00%|                from math import ceil
   398|         0|            0|            0|  0.00%|                if self.drop_last:
   399|         0|            0|            0|  0.00%|                    length = length // self.batch_size
   400|         0|            0|            0|  0.00%|                else:
   401|         0|            0|            0|  0.00%|                    length = ceil(length / self.batch_size)
   402|         0|            0|            0|  0.00%|            return length
   403|         0|            0|            0|  0.00%|        else:
   404|         0|            0|            0|  0.00%|            return len(self._index_sampler)
   405|         0|            0|            0|  0.00%|
   406|         2|  2.69413e-05|  1.34706e-05|  0.00%|    def check_worker_number_rationality(self):
   407|         0|            0|            0|  0.00%|        # This function check whether the dataloader's worker number is rational based on
   408|         0|            0|            0|  0.00%|        # current system's resource. Current rule is that if the number of workers this
   409|         0|            0|            0|  0.00%|        # Dataloader will create is bigger than the number of logical cpus that is allowed to
   410|         0|            0|            0|  0.00%|        # use, than we will pop up a warning to let user pay attention.
   411|         0|            0|            0|  0.00%|        #
   412|         0|            0|            0|  0.00%|        # eg. If current system has 2 physical CPUs with 16 cores each. And each core support 2
   413|         0|            0|            0|  0.00%|        #     threads, then the total logical cpus here is 2 * 16 * 2 = 64. Let's say current
   414|         0|            0|            0|  0.00%|        #     DataLoader process can use half of them which is 32, then the rational max number of
   415|         0|            0|            0|  0.00%|        #     worker that initiated from this process is 32.
   416|         0|            0|            0|  0.00%|        #     Now, let's say the created DataLoader has num_works = 40, which is bigger than 32.
   417|         0|            0|            0|  0.00%|        #     So the warning message is triggered to notify the user to lower the worker number if
   418|         0|            0|            0|  0.00%|        #     necessary.
   419|         0|            0|            0|  0.00%|        #
   420|         0|            0|            0|  0.00%|        #
   421|         0|            0|            0|  0.00%|        # [Note] Please note that this function repects `cpuset` only when os.sched_getaffinity is
   422|         0|            0|            0|  0.00%|        #        available (available in most of Linux system, but not OSX and Windows).
   423|         0|            0|            0|  0.00%|        #        When os.sched_getaffinity is not available, os.cpu_count() is called instead, but
   424|         0|            0|            0|  0.00%|        #        it doesn't repect cpuset.
   425|         0|            0|            0|  0.00%|        #        We don't take threading into account since each worker process is single threaded
   426|         0|            0|            0|  0.00%|        #        at this time.
   427|         0|            0|            0|  0.00%|        #
   428|         0|            0|            0|  0.00%|        #        We don't set any threading flags (eg. OMP_NUM_THREADS, MKL_NUM_THREADS, etc)
   429|         0|            0|            0|  0.00%|        #        other than `torch.set_num_threads` to 1 in the worker process, if the passing
   430|         0|            0|            0|  0.00%|        #        in functions use 3rd party modules that rely on those threading flags to determine
   431|         0|            0|            0|  0.00%|        #        how many thread to create (eg. numpy, etc), then it is caller's responsibility to
   432|         0|            0|            0|  0.00%|        #        set those flags correctly.
   433|         2|  1.35899e-05|  6.79493e-06|  0.00%|        def _create_warning_msg(num_worker_suggest, num_worker_created, cpuset_checked):
   434|         0|            0|            0|  0.00%|
   435|         0|            0|            0|  0.00%|            suggested_max_worker_msg = ((
   436|         0|            0|            0|  0.00%|                "Our suggested max number of worker in current system is {}{}, which is smaller "
   437|         0|            0|            0|  0.00%|                "than what this DataLoader is going to create.").format(
   438|         0|            0|            0|  0.00%|                    num_worker_suggest,
   439|         0|            0|            0|  0.00%|                    ("" if cpuset_checked else " (`cpuset` is not taken into account)"))
   440|         0|            0|            0|  0.00%|            ) if num_worker_suggest is not None else (
   441|         0|            0|            0|  0.00%|                "DataLoader is not able to compute a suggested max number of worker in current system.")
   442|         0|            0|            0|  0.00%|
   443|         0|            0|            0|  0.00%|            warn_msg = (
   444|         0|            0|            0|  0.00%|                "This DataLoader will create {} worker processes in total. {} "
   445|         0|            0|            0|  0.00%|                "Please be aware that excessive worker creation might get DataLoader running slow or even freeze, "
   446|         0|            0|            0|  0.00%|                "lower the worker number to avoid potential slowness/freeze if necessary.").format(
   447|         0|            0|            0|  0.00%|                    num_worker_created,
   448|         0|            0|            0|  0.00%|                    suggested_max_worker_msg)
   449|         0|            0|            0|  0.00%|            return warn_msg
   450|         0|            0|            0|  0.00%|
   451|         2|  1.16825e-05|  5.84126e-06|  0.00%|        if not self.num_workers or self.num_workers == 0:
   452|         0|            0|            0|  0.00%|            return
   453|         0|            0|            0|  0.00%|
   454|         0|            0|            0|  0.00%|        # try to compute a suggested max number of worker based on system's resource
   455|         2|  1.09673e-05|  5.48363e-06|  0.00%|        max_num_worker_suggest = None
   456|         2|  1.07288e-05|  5.36442e-06|  0.00%|        cpuset_checked = False
   457|         2|  1.78814e-05|   8.9407e-06|  0.00%|        if hasattr(os, 'sched_getaffinity'):
   458|         2|  1.12057e-05|  5.60284e-06|  0.00%|            try:
   459|         2|  5.76973e-05|  2.88486e-05|  0.00%|                max_num_worker_suggest = len(os.sched_getaffinity(0))
   460|         2|  1.43051e-05|  7.15256e-06|  0.00%|                cpuset_checked = True
   461|         0|            0|            0|  0.00%|            except Exception:
   462|         0|            0|            0|  0.00%|                pass
   463|         2|  1.07288e-05|  5.36442e-06|  0.00%|        if max_num_worker_suggest is None:
   464|         0|            0|            0|  0.00%|            # os.cpu_count() could return Optional[int]
   465|         0|            0|            0|  0.00%|            # get cpu count first and check None in order to satify mypy check
   466|         0|            0|            0|  0.00%|            cpu_count = os.cpu_count()
   467|         0|            0|            0|  0.00%|            if cpu_count is not None:
   468|         0|            0|            0|  0.00%|                max_num_worker_suggest = cpu_count
   469|         0|            0|            0|  0.00%|
   470|         2|  1.07288e-05|  5.36442e-06|  0.00%|        if max_num_worker_suggest is None:
   471|         0|            0|            0|  0.00%|            warnings.warn(_create_warning_msg(
   472|         0|            0|            0|  0.00%|                max_num_worker_suggest,
   473|         0|            0|            0|  0.00%|                self.num_workers,
   474|         0|            0|            0|  0.00%|                cpuset_checked))
   475|         0|            0|            0|  0.00%|            return
   476|         0|            0|            0|  0.00%|
   477|         2|  1.14441e-05|  5.72205e-06|  0.00%|        if self.num_workers > max_num_worker_suggest:
   478|         0|            0|            0|  0.00%|            warnings.warn(_create_warning_msg(
   479|         0|            0|            0|  0.00%|                max_num_worker_suggest,
   480|         0|            0|            0|  0.00%|                self.num_workers,
   481|         0|            0|            0|  0.00%|                cpuset_checked))
   482|         0|            0|            0|  0.00%|
   483|         0|            0|            0|  0.00%|
   484|         0|            0|            0|  0.00%|class _BaseDataLoaderIter(object):
   485|         2|  1.74046e-05|  8.70228e-06|  0.00%|    def __init__(self, loader: DataLoader) -> None:
   486|         2|  3.02792e-05|  1.51396e-05|  0.00%|        self._dataset = loader.dataset
   487|         2|  1.26362e-05|  6.31809e-06|  0.00%|        self._dataset_kind = loader._dataset_kind
   488|         2|  1.12057e-05|  5.60284e-06|  0.00%|        self._IterableDataset_len_called = loader._IterableDataset_len_called
   489|         2|  3.57628e-05|  1.78814e-05|  0.00%|        self._auto_collation = loader._auto_collation
(call)|         2|  2.95639e-05|   1.4782e-05|  0.00%|# /opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py:361 _auto_collation
   490|         2|  1.95503e-05|  9.77516e-06|  0.00%|        self._drop_last = loader.drop_last
   491|         2|  5.10216e-05|  2.55108e-05|  0.00%|        self._index_sampler = loader._index_sampler
(call)|         2|  6.81877e-05|  3.40939e-05|  0.00%|# /opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py:365 _index_sampler
   492|         2|  1.12057e-05|  5.60284e-06|  0.00%|        self._num_workers = loader.num_workers
   493|         2|  1.07288e-05|  5.36442e-06|  0.00%|        self._prefetch_factor = loader.prefetch_factor
   494|         2|  1.09673e-05|  5.48363e-06|  0.00%|        self._pin_memory = loader.pin_memory and torch.cuda.is_available()
   495|         2|  1.00136e-05|  5.00679e-06|  0.00%|        self._timeout = loader.timeout
   496|         2|   1.0252e-05|    5.126e-06|  0.00%|        self._collate_fn = loader.collate_fn
   497|         2|  2.14577e-05|  1.07288e-05|  0.00%|        self._sampler_iter = iter(self._index_sampler)
   498|         2|  0.000153303|  7.66516e-05|  0.00%|        self._base_seed = torch.empty((), dtype=torch.int64).random_(generator=loader.generator).item()
   499|         2|  1.81198e-05|  9.05991e-06|  0.00%|        self._persistent_workers = loader.persistent_workers
   500|         2|  1.09673e-05|  5.48363e-06|  0.00%|        self._num_yielded = 0
   501|         2|  2.14577e-05|  1.07288e-05|  0.00%|        self._profile_name = "enumerate(DataLoader)#{}.__next__".format(self.__class__.__name__)
   502|         0|            0|            0|  0.00%|
   503|         0|            0|            0|  0.00%|    def __iter__(self) -> '_BaseDataLoaderIter':
   504|         0|            0|            0|  0.00%|        return self
   505|         0|            0|            0|  0.00%|
   506|         2|  1.54972e-05|   7.7486e-06|  0.00%|    def _reset(self, loader, first_iter=False):
   507|         2|  0.000230074|  0.000115037|  0.00%|        self._sampler_iter = iter(self._index_sampler)
(call)|         2|  4.62532e-05|  2.31266e-05|  0.00%|# /opt/conda/lib/python3.8/site-packages/torch/utils/data/sampler.py:224 __iter__
   508|         2|  1.16825e-05|  5.84126e-06|  0.00%|        self._num_yielded = 0
   509|         2|  2.57492e-05|  1.28746e-05|  0.00%|        self._IterableDataset_len_called = loader._IterableDataset_len_called
   510|         0|            0|            0|  0.00%|
   511|       116|  0.000559092|  4.81975e-06|  0.00%|    def _next_index(self):
   512|       116|   0.00180674|  1.55753e-05|  0.00%|        return next(self._sampler_iter)  # may raise StopIteration
(call)|       102|    0.0152252|  0.000149266|  0.03%|# /opt/conda/lib/python3.8/site-packages/torch/utils/data/sampler.py:224 __iter__
   513|         0|            0|            0|  0.00%|
   514|         0|            0|            0|  0.00%|    def _next_data(self):
   515|         0|            0|            0|  0.00%|        raise NotImplementedError
   516|         0|            0|            0|  0.00%|
   517|       102|   0.00101733|  9.97384e-06|  0.00%|    def __next__(self) -> Any:
   518|       102|   0.00400782|  3.92923e-05|  0.01%|        with torch.autograd.profiler.record_function(self._profile_name):
(call)|       102|    0.0051024|  5.00235e-05|  0.01%|# /opt/conda/lib/python3.8/site-packages/torch/autograd/profiler.py:605 __init__
(call)|       102|   0.00595617|  5.83939e-05|  0.01%|# /opt/conda/lib/python3.8/site-packages/torch/autograd/profiler.py:613 __enter__
   519|       102|  0.000531197|  5.20781e-06|  0.00%|            if self._sampler_iter is None:
   520|         0|            0|            0|  0.00%|                self._reset()
   521|       102|   0.00182867|  1.79281e-05|  0.00%|            data = self._next_data()
(call)|       102|      1.44019|    0.0141195|  2.69%|# /opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1156 _next_data
(call)|         2|  0.000412703|  0.000206351|  0.00%|# /opt/conda/lib/python3.8/site-packages/torch/autograd/profiler.py:617 __exit__
   522|       100|  0.000506401|  5.06401e-06|  0.00%|            self._num_yielded += 1
   523|       100|  0.000504494|  5.04494e-06|  0.00%|            if self._dataset_kind == _DatasetKind.Iterable and \
   524|         0|            0|            0|  0.00%|                    self._IterableDataset_len_called is not None and \
   525|         0|            0|            0|  0.00%|                    self._num_yielded > self._IterableDataset_len_called:
   526|         0|            0|            0|  0.00%|                warn_msg = ("Length of IterableDataset {} was reported to be {} (when accessing len(dataloader)), but {} "
   527|         0|            0|            0|  0.00%|                            "samples have been fetched. ").format(self._dataset, self._IterableDataset_len_called,
   528|         0|            0|            0|  0.00%|                                                                  self._num_yielded)
   529|         0|            0|            0|  0.00%|                if self._num_workers > 0:
   530|         0|            0|            0|  0.00%|                    warn_msg += ("For multiprocessing data-loading, this could be caused by not properly configuring the "
   531|         0|            0|            0|  0.00%|                                 "IterableDataset replica at each worker. Please see "
   532|         0|            0|            0|  0.00%|                                 "https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.")
   533|         0|            0|            0|  0.00%|                warnings.warn(warn_msg)
   534|       100|   0.00208449|  2.08449e-05|  0.00%|            return data
(call)|       100|   0.00369143|  3.69143e-05|  0.01%|# /opt/conda/lib/python3.8/site-packages/torch/autograd/profiler.py:617 __exit__
   535|         0|            0|            0|  0.00%|
   536|         0|            0|            0|  0.00%|    next = __next__  # Python 2 compatibility
   537|         0|            0|            0|  0.00%|
   538|         0|            0|            0|  0.00%|    def __len__(self) -> int:
   539|         0|            0|            0|  0.00%|        return len(self._index_sampler)
   540|         0|            0|            0|  0.00%|
   541|         0|            0|            0|  0.00%|    def __getstate__(self):
   542|         0|            0|            0|  0.00%|        # TODO: add limited pickling support for sharing an iterator
   543|         0|            0|            0|  0.00%|        # across multiple threads for HOGWILD.
   544|         0|            0|            0|  0.00%|        # Probably the best way to do this is by moving the sample pushing
   545|         0|            0|            0|  0.00%|        # to a separate thread and then just sharing the data queue
   546|         0|            0|            0|  0.00%|        # but signalling the end is tricky without a non-blocking API
   547|         0|            0|            0|  0.00%|        raise NotImplementedError("{} cannot be pickled", self.__class__.__name__)
   548|         0|            0|            0|  0.00%|
   549|         0|            0|            0|  0.00%|
   550|         0|            0|            0|  0.00%|class _SingleProcessDataLoaderIter(_BaseDataLoaderIter):
   551|         0|            0|            0|  0.00%|    def __init__(self, loader):
   552|         0|            0|            0|  0.00%|        super(_SingleProcessDataLoaderIter, self).__init__(loader)
   553|         0|            0|            0|  0.00%|        assert self._timeout == 0
   554|         0|            0|            0|  0.00%|        assert self._num_workers == 0
   555|         0|            0|            0|  0.00%|
   556|         0|            0|            0|  0.00%|        self._dataset_fetcher = _DatasetKind.create_fetcher(
   557|         0|            0|            0|  0.00%|            self._dataset_kind, self._dataset, self._auto_collation, self._collate_fn, self._drop_last)
   558|         0|            0|            0|  0.00%|
   559|         0|            0|            0|  0.00%|    def _next_data(self):
   560|         0|            0|            0|  0.00%|        index = self._next_index()  # may raise StopIteration
   561|         0|            0|            0|  0.00%|        data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
   562|         0|            0|            0|  0.00%|        if self._pin_memory:
   563|         0|            0|            0|  0.00%|            data = _utils.pin_memory.pin_memory(data)
   564|         0|            0|            0|  0.00%|        return data
   565|         0|            0|            0|  0.00%|
   566|         0|            0|            0|  0.00%|
   567|         0|            0|            0|  0.00%|class _MultiProcessingDataLoaderIter(_BaseDataLoaderIter):
   568|         0|            0|            0|  0.00%|    r"""Iterates once over the DataLoader's dataset, as specified by the sampler"""
   569|         0|            0|            0|  0.00%|
   570|         0|            0|            0|  0.00%|    # NOTE [ Data Loader Multiprocessing Shutdown Logic ]
   571|         0|            0|            0|  0.00%|    #
   572|         0|            0|            0|  0.00%|    # Preliminary:
   573|         0|            0|            0|  0.00%|    #
   574|         0|            0|            0|  0.00%|    # Our data model looks like this (queues are indicated with curly brackets):
   575|         0|            0|            0|  0.00%|    #
   576|         0|            0|            0|  0.00%|    #                main process                              ||
   577|         0|            0|            0|  0.00%|    #                     |                                    ||
   578|         0|            0|            0|  0.00%|    #               {index_queue}                              ||
   579|         0|            0|            0|  0.00%|    #                     |                                    ||
   580|         0|            0|            0|  0.00%|    #              worker processes                            ||     DATA
   581|         0|            0|            0|  0.00%|    #                     |                                    ||
   582|         0|            0|            0|  0.00%|    #            {worker_result_queue}                         ||     FLOW
   583|         0|            0|            0|  0.00%|    #                     |                                    ||
   584|         0|            0|            0|  0.00%|    #      pin_memory_thread of main process                   ||   DIRECTION
   585|         0|            0|            0|  0.00%|    #                     |                                    ||
   586|         0|            0|            0|  0.00%|    #               {data_queue}                               ||
   587|         0|            0|            0|  0.00%|    #                     |                                    ||
   588|         0|            0|            0|  0.00%|    #                data output                               \/
   589|         0|            0|            0|  0.00%|    #
   590|         0|            0|            0|  0.00%|    # P.S. `worker_result_queue` and `pin_memory_thread` part may be omitted if
   591|         0|            0|            0|  0.00%|    #      `pin_memory=False`.
   592|         0|            0|            0|  0.00%|    #
   593|         0|            0|            0|  0.00%|    #
   594|         0|            0|            0|  0.00%|    # Terminating multiprocessing logic requires very careful design. In
   595|         0|            0|            0|  0.00%|    # particular, we need to make sure that
   596|         0|            0|            0|  0.00%|    #
   597|         0|            0|            0|  0.00%|    #   1. The iterator gracefully exits the workers when its last reference is
   598|         0|            0|            0|  0.00%|    #      gone or it is depleted.
   599|         0|            0|            0|  0.00%|    #
   600|         0|            0|            0|  0.00%|    #      In this case, the workers should be gracefully exited because the
   601|         0|            0|            0|  0.00%|    #      main process may still need to continue to run, and we want cleaning
   602|         0|            0|            0|  0.00%|    #      up code in the workers to be executed (e.g., releasing GPU memory).
   603|         0|            0|            0|  0.00%|    #      Naturally, we implement the shutdown logic in `__del__` of
   604|         0|            0|            0|  0.00%|    #      DataLoaderIterator.
   605|         0|            0|            0|  0.00%|    #
   606|         0|            0|            0|  0.00%|    #      We delay the discussion on the logic in this case until later.
   607|         0|            0|            0|  0.00%|    #
   608|         0|            0|            0|  0.00%|    #   2. The iterator exits the workers when the loader process and/or worker
   609|         0|            0|            0|  0.00%|    #      processes exits normally or with error.
   610|         0|            0|            0|  0.00%|    #
   611|         0|            0|            0|  0.00%|    #      We set all workers and `pin_memory_thread` to have `daemon=True`.
   612|         0|            0|            0|  0.00%|    #
   613|         0|            0|            0|  0.00%|    #      You may ask, why can't we make the workers non-daemonic, and
   614|         0|            0|            0|  0.00%|    #      gracefully exit using the same logic as we have in `__del__` when the
   615|         0|            0|            0|  0.00%|    #      iterator gets deleted (see 1 above)?
   616|         0|            0|            0|  0.00%|    #
   617|         0|            0|            0|  0.00%|    #      First of all, `__del__` is **not** guaranteed to be called when
   618|         0|            0|            0|  0.00%|    #      interpreter exits. Even if it is called, by the time it executes,
   619|         0|            0|            0|  0.00%|    #      many Python core library resources may alreay be freed, and even
   620|         0|            0|            0|  0.00%|    #      simple things like acquiring an internal lock of a queue may hang.
   621|         0|            0|            0|  0.00%|    #      Therefore, in this case, we actually need to prevent `__del__` from
   622|         0|            0|            0|  0.00%|    #      being executed, and rely on the automatic termination of daemonic
   623|         0|            0|            0|  0.00%|    #      children.
   624|         0|            0|            0|  0.00%|    #
   625|         0|            0|            0|  0.00%|    #      Thus, we register an `atexit` hook that sets a global flag
   626|         0|            0|            0|  0.00%|    #      `_utils.python_exit_status`. Since `atexit` hooks are executed in the
   627|         0|            0|            0|  0.00%|    #      reverse order of registration, we are guaranteed that this flag is
   628|         0|            0|            0|  0.00%|    #      set before library resources we use are freed (which, at least in
   629|         0|            0|            0|  0.00%|    #      CPython, is done via an `atexit` handler defined in
   630|         0|            0|            0|  0.00%|    #      `multiprocessing/util.py`
   631|         0|            0|            0|  0.00%|    #      https://github.com/python/cpython/blob/c606624af8d4cb3b4a052fb263bb983b3f87585b/Lib/multiprocessing/util.py#L320-L362
   632|         0|            0|            0|  0.00%|    #      registered when an object requiring this mechanism is first
   633|         0|            0|            0|  0.00%|    #      created, e.g., `mp.Queue`
   634|         0|            0|            0|  0.00%|    #      https://github.com/python/cpython/blob/c606624af8d4cb3b4a052fb263bb983b3f87585b/Lib/multiprocessing/context.py#L100-L103
   635|         0|            0|            0|  0.00%|    #      https://github.com/python/cpython/blob/c606624af8d4cb3b4a052fb263bb983b3f87585b/Lib/multiprocessing/queues.py#L29
   636|         0|            0|            0|  0.00%|    #      )
   637|         0|            0|            0|  0.00%|    #
   638|         0|            0|            0|  0.00%|    #      So in `__del__`, we check if `_utils.python_exit_status` is set or
   639|         0|            0|            0|  0.00%|    #      `None` (freed), and perform no-op if so.
   640|         0|            0|            0|  0.00%|    #
   641|         0|            0|            0|  0.00%|    #      However, simply letting library clean-up codes run can also be bad,
   642|         0|            0|            0|  0.00%|    #      because such codes (i.e., `multiprocessing.util._exit_function()`)
   643|         0|            0|            0|  0.00%|    #      include join putting threads for `mp.Queue`, which can be blocking.
   644|         0|            0|            0|  0.00%|    #      Hence, the main process putting threads are called with
   645|         0|            0|            0|  0.00%|    #      `cancel_join_thread` at creation.  See later section
   646|         0|            0|            0|  0.00%|    #      [ 3b. A process won't hang when putting into a queue; ]
   647|         0|            0|            0|  0.00%|    #      for more details.
   648|         0|            0|            0|  0.00%|    #
   649|         0|            0|            0|  0.00%|    #      Here are two example cases where library clean-up codes can run
   650|         0|            0|            0|  0.00%|    #      before `__del__` is called:
   651|         0|            0|            0|  0.00%|    #
   652|         0|            0|            0|  0.00%|    #        1. If we hold onto a reference to the iterator, it more often
   653|         0|            0|            0|  0.00%|    #           than not tries to do `multiprocessing` library cleaning before
   654|         0|            0|            0|  0.00%|    #           clearing the alive referenced objects (https://github.com/pytorch/pytorch/issues/48666)
   655|         0|            0|            0|  0.00%|    #           and thus prevents our cleaning-up code to run first.
   656|         0|            0|            0|  0.00%|    #
   657|         0|            0|            0|  0.00%|    #        2. A similar issue araises when a `DataLoader` is used in a subprocess.
   658|         0|            0|            0|  0.00%|    #           When a process ends, it shuts the all its daemonic children
   659|         0|            0|            0|  0.00%|    #           down with a SIGTERM (instead of joining them without a timeout).
   660|         0|            0|            0|  0.00%|    #           Simiarly for threads, but by a different mechanism. This fact,
   661|         0|            0|            0|  0.00%|    #           together with a few implementation details of multiprocessing, forces
   662|         0|            0|            0|  0.00%|    #           us to make workers daemonic. All of our problems arise when a
   663|         0|            0|            0|  0.00%|    #           DataLoader is used in a subprocess, and are caused by multiprocessing
   664|         0|            0|            0|  0.00%|    #           code which looks more or less like this:
   665|         0|            0|            0|  0.00%|    #
   666|         0|            0|            0|  0.00%|    #               try:
   667|         0|            0|            0|  0.00%|    #                   your_function_using_a_dataloader()
   668|         0|            0|            0|  0.00%|    #               finally:
   669|         0|            0|            0|  0.00%|    #                   multiprocessing.util._exit_function()
   670|         0|            0|            0|  0.00%|    #
   671|         0|            0|            0|  0.00%|    #           The joining/termination mentioned above happens inside
   672|         0|            0|            0|  0.00%|    #           `_exit_function()`. Now, if `your_function_using_a_dataloader()`
   673|         0|            0|            0|  0.00%|    #           throws, the stack trace stored in the exception will prevent the
   674|         0|            0|            0|  0.00%|    #           frame which uses `DataLoaderIter` to be freed. If the frame has any
   675|         0|            0|            0|  0.00%|    #           reference to the `DataLoaderIter` (e.g., in a method of the iter),
   676|         0|            0|            0|  0.00%|    #           its  `__del__`, which starts the shutdown procedure, will not be
   677|         0|            0|            0|  0.00%|    #           called. That, in turn, means that workers aren't notified. Attempting
   678|         0|            0|            0|  0.00%|    #           to join in `_exit_function` will then result in a hang.
   679|         0|            0|            0|  0.00%|    #
   680|         0|            0|            0|  0.00%|    #           For context, `_exit_function` is also registered as an `atexit` call.
   681|         0|            0|            0|  0.00%|    #           So it is unclear to me (@ssnl) why this is needed in a finally block.
   682|         0|            0|            0|  0.00%|    #           The code dates back to 2008 and there is no comment on the original
   683|         0|            0|            0|  0.00%|    #           PEP 371 or patch https://bugs.python.org/issue3050 (containing both
   684|         0|            0|            0|  0.00%|    #           the finally block and the `atexit` registration) that explains this.
   685|         0|            0|            0|  0.00%|    #
   686|         0|            0|            0|  0.00%|    #
   687|         0|            0|            0|  0.00%|    #      Finally, another choice is to just shutdown workers with logic in 1
   688|         0|            0|            0|  0.00%|    #      above whenever we see an error in `next`. This isn't ideal because
   689|         0|            0|            0|  0.00%|    #        a. It prevents users from using try-catch to resume data loading.
   690|         0|            0|            0|  0.00%|    #        b. It doesn't prevent hanging if users have references to the
   691|         0|            0|            0|  0.00%|    #           iterator.
   692|         0|            0|            0|  0.00%|    #
   693|         0|            0|            0|  0.00%|    #   3. All processes exit if any of them die unexpectedly by fatal signals.
   694|         0|            0|            0|  0.00%|    #
   695|         0|            0|            0|  0.00%|    #      As shown above, the workers are set as daemonic children of the main
   696|         0|            0|            0|  0.00%|    #      process. However, automatic cleaning-up of such child processes only
   697|         0|            0|            0|  0.00%|    #      happens if the parent process exits gracefully (e.g., not via fatal
   698|         0|            0|            0|  0.00%|    #      signals like SIGKILL). So we must ensure that each process will exit
   699|         0|            0|            0|  0.00%|    #      even the process that should send/receive data to/from it were
   700|         0|            0|            0|  0.00%|    #      killed, i.e.,
   701|         0|            0|            0|  0.00%|    #
   702|         0|            0|            0|  0.00%|    #        a. A process won't hang when getting from a queue.
   703|         0|            0|            0|  0.00%|    #
   704|         0|            0|            0|  0.00%|    #           Even with carefully designed data dependencies (i.e., a `put()`
   705|         0|            0|            0|  0.00%|    #           always corresponding to a `get()`), hanging on `get()` can still
   706|         0|            0|            0|  0.00%|    #           happen when data in queue is corrupted (e.g., due to
   707|         0|            0|            0|  0.00%|    #           `cancel_join_thread` or unexpected exit).
   708|         0|            0|            0|  0.00%|    #
   709|         0|            0|            0|  0.00%|    #           For child exit, we set a timeout whenever we try to get data
   710|         0|            0|            0|  0.00%|    #           from `data_queue`, and check the workers' status on each timeout
   711|         0|            0|            0|  0.00%|    #           and error.
   712|         0|            0|            0|  0.00%|    #           See `_DataLoaderiter._get_batch()` and
   713|         0|            0|            0|  0.00%|    #           `_DataLoaderiter._try_get_data()` for details.
   714|         0|            0|            0|  0.00%|    #
   715|         0|            0|            0|  0.00%|    #           Additionally, for child exit on non-Windows platforms, we also
   716|         0|            0|            0|  0.00%|    #           register a SIGCHLD handler (which is supported on Windows) on
   717|         0|            0|            0|  0.00%|    #           the main process, which checks if any of the workers fail in the
   718|         0|            0|            0|  0.00%|    #           (Python) handler. This is more efficient and faster in detecting
   719|         0|            0|            0|  0.00%|    #           worker failures, compared to only using the above mechanism.
   720|         0|            0|            0|  0.00%|    #           See `DataLoader.cpp` and `_utils/signal_handling.py` for details.
   721|         0|            0|            0|  0.00%|    #
   722|         0|            0|            0|  0.00%|    #           For `.get()` calls where the sender(s) is not the workers, we
   723|         0|            0|            0|  0.00%|    #           guard them with timeouts, and check the status of the sender
   724|         0|            0|            0|  0.00%|    #           when timeout happens:
   725|         0|            0|            0|  0.00%|    #             + in the workers, the `_utils.worker.ManagerWatchdog` class
   726|         0|            0|            0|  0.00%|    #               checks the status of the main process.
   727|         0|            0|            0|  0.00%|    #             + if `pin_memory=True`, when getting from `pin_memory_thread`,
   728|         0|            0|            0|  0.00%|    #               check `pin_memory_thread` status periodically until `.get()`
   729|         0|            0|            0|  0.00%|    #               returns or see that `pin_memory_thread` died.
   730|         0|            0|            0|  0.00%|    #
   731|         0|            0|            0|  0.00%|    #        b. A process won't hang when putting into a queue;
   732|         0|            0|            0|  0.00%|    #
   733|         0|            0|            0|  0.00%|    #           We use `mp.Queue` which has a separate background thread to put
   734|         0|            0|            0|  0.00%|    #           objects from an unbounded buffer array. The background thread is
   735|         0|            0|            0|  0.00%|    #           daemonic and usually automatically joined when the process
   736|         0|            0|            0|  0.00%|    #           *exits*.
   737|         0|            0|            0|  0.00%|    #
   738|         0|            0|            0|  0.00%|    #           In case that the receiver has ended abruptly while
   739|         0|            0|            0|  0.00%|    #           reading from the pipe, the join will hang forever.  The usual
   740|         0|            0|            0|  0.00%|    #           solution for this in Python is calling  `q.cancel_join_thread`,
   741|         0|            0|            0|  0.00%|    #           which prevents automatically joining it when finalizing
   742|         0|            0|            0|  0.00%|    #           (exiting).
   743|         0|            0|            0|  0.00%|    #
   744|         0|            0|            0|  0.00%|    #           Nonetheless, `cancel_join_thread` must only be called when the
   745|         0|            0|            0|  0.00%|    #           queue is **not** going to be read from or write into by another
   746|         0|            0|            0|  0.00%|    #           process, because it may hold onto a lock or leave corrupted data
   747|         0|            0|            0|  0.00%|    #           in the queue, leading other readers/writers to hang.
   748|         0|            0|            0|  0.00%|    #
   749|         0|            0|            0|  0.00%|    #           Hence,
   750|         0|            0|            0|  0.00%|    #             + For worker processes, we only do so (for their output
   751|         0|            0|            0|  0.00%|    #               queues, i.e., `worker_result_queue`) before exiting.
   752|         0|            0|            0|  0.00%|    #             + For `pin_memory_thread`, its output queue `data_queue` is a
   753|         0|            0|            0|  0.00%|    #               `queue.Queue` that does blocking `put` if the queue is full.
   754|         0|            0|            0|  0.00%|    #               So there is no above problem, but as a result, in
   755|         0|            0|            0|  0.00%|    #               `_pin_memory_loop`, we do need to  wrap the `put` in a loop
   756|         0|            0|            0|  0.00%|    #               that breaks not only upon success, but also when the main
   757|         0|            0|            0|  0.00%|    #               process stops reading, i.e., is shutting down.
   758|         0|            0|            0|  0.00%|    #             + For loader process, we `cancel_join_thread()` for all
   759|         0|            0|            0|  0.00%|    #               `_index_queues` because the whole purpose of workers and
   760|         0|            0|            0|  0.00%|    #               `pin_memory_thread` is to serve the loader process.  If
   761|         0|            0|            0|  0.00%|    #               loader process is already exiting, we don't really care if
   762|         0|            0|            0|  0.00%|    #               the queues are corrupted.
   763|         0|            0|            0|  0.00%|    #
   764|         0|            0|            0|  0.00%|    #
   765|         0|            0|            0|  0.00%|    # Now let's get back to 1:
   766|         0|            0|            0|  0.00%|    #   how we gracefully exit the workers when the last reference to the
   767|         0|            0|            0|  0.00%|    #   iterator is gone.
   768|         0|            0|            0|  0.00%|    #
   769|         0|            0|            0|  0.00%|    # To achieve this, we implement the following logic along with the design
   770|         0|            0|            0|  0.00%|    # choices mentioned above:
   771|         0|            0|            0|  0.00%|    #
   772|         0|            0|            0|  0.00%|    # `workers_done_event`:
   773|         0|            0|            0|  0.00%|    #   A `multiprocessing.Event` shared among the main process and all worker
   774|         0|            0|            0|  0.00%|    #   processes. This is used to signal the workers that the iterator is
   775|         0|            0|            0|  0.00%|    #   shutting down. After it is set, they will not send processed data to
   776|         0|            0|            0|  0.00%|    #   queues anymore, and only wait for the final `None` before exiting.
   777|         0|            0|            0|  0.00%|    #   `done_event` isn't strictly needed. I.e., we can just check for `None`
   778|         0|            0|            0|  0.00%|    #   from the input queue, but it allows us to skip wasting resources
   779|         0|            0|            0|  0.00%|    #   processing data if we are already shutting down.
   780|         0|            0|            0|  0.00%|    #
   781|         0|            0|            0|  0.00%|    # `pin_memory_thread_done_event`:
   782|         0|            0|            0|  0.00%|    #   A `threading.Event` for a similar purpose to that of
   783|         0|            0|            0|  0.00%|    #   `workers_done_event`, but is for the `pin_memory_thread`. The reason
   784|         0|            0|            0|  0.00%|    #   that separate events are needed is that `pin_memory_thread` reads from
   785|         0|            0|            0|  0.00%|    #   the output queue of the workers. But the workers, upon seeing that
   786|         0|            0|            0|  0.00%|    #   `workers_done_event` is set, only wants to see the final `None`, and is
   787|         0|            0|            0|  0.00%|    #   not required to flush all data in the output queue (e.g., it may call
   788|         0|            0|            0|  0.00%|    #   `cancel_join_thread` on that queue if its `IterableDataset` iterator
   789|         0|            0|            0|  0.00%|    #   happens to exhaust coincidentally, which is out of the control of the
   790|         0|            0|            0|  0.00%|    #   main process). Thus, since we will exit `pin_memory_thread` before the
   791|         0|            0|            0|  0.00%|    #   workers (see below), two separete events are used.
   792|         0|            0|            0|  0.00%|    #
   793|         0|            0|            0|  0.00%|    # NOTE: In short, the protocol is that the main process will set these
   794|         0|            0|            0|  0.00%|    #       `done_event`s and then the corresponding processes/threads a `None`,
   795|         0|            0|            0|  0.00%|    #       and that they may exit at any time after receiving the `None`.
   796|         0|            0|            0|  0.00%|    #
   797|         0|            0|            0|  0.00%|    # NOTE: Using `None` as the final signal is valid, since normal data will
   798|         0|            0|            0|  0.00%|    #       always be a 2-tuple with the 1st element being the index of the data
   799|         0|            0|            0|  0.00%|    #       transferred (different from dataset index/key), and the 2nd being
   800|         0|            0|            0|  0.00%|    #       either the dataset key or the data sample (depending on which part
   801|         0|            0|            0|  0.00%|    #       of the data model the queue is at).
   802|         0|            0|            0|  0.00%|    #
   803|         0|            0|            0|  0.00%|    # [ worker processes ]
   804|         0|            0|            0|  0.00%|    #   While loader process is alive:
   805|         0|            0|            0|  0.00%|    #     Get from `index_queue`.
   806|         0|            0|            0|  0.00%|    #       If get anything else,
   807|         0|            0|            0|  0.00%|    #          Check `workers_done_event`.
   808|         0|            0|            0|  0.00%|    #            If set, continue to next iteration
   809|         0|            0|            0|  0.00%|    #                    i.e., keep getting until see the `None`, then exit.
   810|         0|            0|            0|  0.00%|    #            Otherwise, process data:
   811|         0|            0|            0|  0.00%|    #                If is fetching from an `IterableDataset` and the iterator
   812|         0|            0|            0|  0.00%|    #                    is exhausted, send an `_IterableDatasetStopIteration`
   813|         0|            0|            0|  0.00%|    #                    object to signal iteration end. The main process, upon
   814|         0|            0|            0|  0.00%|    #                    receiving such an object, will send `None` to this
   815|         0|            0|            0|  0.00%|    #                    worker and not use the corresponding `index_queue`
   816|         0|            0|            0|  0.00%|    #                    anymore.
   817|         0|            0|            0|  0.00%|    #       If timed out,
   818|         0|            0|            0|  0.00%|    #          No matter `workers_done_event` is set (still need to see `None`)
   819|         0|            0|            0|  0.00%|    #          or not, must continue to next iteration.
   820|         0|            0|            0|  0.00%|    #   (outside loop)
   821|         0|            0|            0|  0.00%|    #   If `workers_done_event` is set,  (this can be False with `IterableDataset`)
   822|         0|            0|            0|  0.00%|    #     `data_queue.cancel_join_thread()`.  (Everything is ending here:
   823|         0|            0|            0|  0.00%|    #                                          main process won't read from it;
   824|         0|            0|            0|  0.00%|    #                                          other workers will also call
   825|         0|            0|            0|  0.00%|    #                                          `cancel_join_thread`.)
   826|         0|            0|            0|  0.00%|    #
   827|         0|            0|            0|  0.00%|    # [ pin_memory_thread ]
   828|         0|            0|            0|  0.00%|    #   # No need to check main thread. If this thread is alive, the main loader
   829|         0|            0|            0|  0.00%|    #   # thread must be alive, because this thread is set as daemonic.
   830|         0|            0|            0|  0.00%|    #   While `pin_memory_thread_done_event` is not set:
   831|         0|            0|            0|  0.00%|    #     Get from `index_queue`.
   832|         0|            0|            0|  0.00%|    #       If timed out, continue to get in the next iteration.
   833|         0|            0|            0|  0.00%|    #       Otherwise, process data.
   834|         0|            0|            0|  0.00%|    #       While `pin_memory_thread_done_event` is not set:
   835|         0|            0|            0|  0.00%|    #         Put processed data to `data_queue` (a `queue.Queue` with blocking put)
   836|         0|            0|            0|  0.00%|    #         If timed out, continue to put in the next iteration.
   837|         0|            0|            0|  0.00%|    #         Otherwise, break, i.e., continuing to the out loop.
   838|         0|            0|            0|  0.00%|    #
   839|         0|            0|            0|  0.00%|    #   NOTE: we don't check the status of the main thread because
   840|         0|            0|            0|  0.00%|    #           1. if the process is killed by fatal signal, `pin_memory_thread`
   841|         0|            0|            0|  0.00%|    #              ends.
   842|         0|            0|            0|  0.00%|    #           2. in other cases, either the cleaning-up in __del__ or the
   843|         0|            0|            0|  0.00%|    #              automatic exit of daemonic thread will take care of it.
   844|         0|            0|            0|  0.00%|    #              This won't busy-wait either because `.get(timeout)` does not
   845|         0|            0|            0|  0.00%|    #              busy-wait.
   846|         0|            0|            0|  0.00%|    #
   847|         0|            0|            0|  0.00%|    # [ main process ]
   848|         0|            0|            0|  0.00%|    #   In the DataLoader Iter's `__del__`
   849|         0|            0|            0|  0.00%|    #     b. Exit `pin_memory_thread`
   850|         0|            0|            0|  0.00%|    #          i.   Set `pin_memory_thread_done_event`.
   851|         0|            0|            0|  0.00%|    #          ii   Put `None` in `worker_result_queue`.
   852|         0|            0|            0|  0.00%|    #          iii. Join the `pin_memory_thread`.
   853|         0|            0|            0|  0.00%|    #          iv.  `worker_result_queue.cancel_join_thread()`.
   854|         0|            0|            0|  0.00%|    #
   855|         0|            0|            0|  0.00%|    #     c. Exit the workers.
   856|         0|            0|            0|  0.00%|    #          i.   Set `workers_done_event`.
   857|         0|            0|            0|  0.00%|    #          ii.  Put `None` in each worker's `index_queue`.
   858|         0|            0|            0|  0.00%|    #          iii. Join the workers.
   859|         0|            0|            0|  0.00%|    #          iv.  Call `.cancel_join_thread()` on each worker's `index_queue`.
   860|         0|            0|            0|  0.00%|    #
   861|         0|            0|            0|  0.00%|    #        NOTE: (c) is better placed after (b) because it may leave corrupted
   862|         0|            0|            0|  0.00%|    #              data in `worker_result_queue`, which `pin_memory_thread`
   863|         0|            0|            0|  0.00%|    #              reads from, in which case the `pin_memory_thread` can only
   864|         0|            0|            0|  0.00%|    #              happen at timeing out, which is slow. Nonetheless, same thing
   865|         0|            0|            0|  0.00%|    #              happens if a worker is killed by signal at unfortunate times,
   866|         0|            0|            0|  0.00%|    #              but in other cases, we are better off having a non-corrupted
   867|         0|            0|            0|  0.00%|    #              `worker_result_queue` for `pin_memory_thread`.
   868|         0|            0|            0|  0.00%|    #
   869|         0|            0|            0|  0.00%|    #   NOTE: If `pin_memory=False`, there is no `pin_memory_thread` and (b)
   870|         0|            0|            0|  0.00%|    #         can be omitted
   871|         0|            0|            0|  0.00%|    #
   872|         0|            0|            0|  0.00%|    # NB: `done_event`s isn't strictly needed. E.g., we can just check for
   873|         0|            0|            0|  0.00%|    #     `None` from `index_queue`, but it allows us to skip wasting resources
   874|         0|            0|            0|  0.00%|    #     processing indices already in `index_queue` if we are already shutting
   875|         0|            0|            0|  0.00%|    #     down.
   876|         0|            0|            0|  0.00%|
   877|         2|  2.93255e-05|  1.46627e-05|  0.00%|    def __init__(self, loader):
   878|         2|  5.14984e-05|  2.57492e-05|  0.00%|        super(_MultiProcessingDataLoaderIter, self).__init__(loader)
(call)|         2|  0.000554085|  0.000277042|  0.00%|# /opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py:485 __init__
   879|         0|            0|            0|  0.00%|
   880|         2|  1.38283e-05|  6.91414e-06|  0.00%|        assert self._num_workers > 0
   881|         2|  1.28746e-05|   6.4373e-06|  0.00%|        assert self._prefetch_factor > 0
   882|         0|            0|            0|  0.00%|
   883|         2|  3.83854e-05|  1.91927e-05|  0.00%|        if loader.multiprocessing_context is None:
(call)|         2|  2.24113e-05|  1.12057e-05|  0.00%|# /opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py:307 multiprocessing_context
   884|         2|  1.33514e-05|  6.67572e-06|  0.00%|            multiprocessing_context = multiprocessing
   885|         0|            0|            0|  0.00%|        else:
   886|         0|            0|            0|  0.00%|            multiprocessing_context = loader.multiprocessing_context
   887|         0|            0|            0|  0.00%|
   888|         2|  2.45571e-05|  1.22786e-05|  0.00%|        self._worker_init_fn = loader.worker_init_fn
   889|         2|  3.09944e-05|  1.54972e-05|  0.00%|        self._worker_queue_idx_cycle = itertools.cycle(range(self._num_workers))
   890|         0|            0|            0|  0.00%|        # No certainty which module multiprocessing_context is
   891|         2|  7.34329e-05|  3.67165e-05|  0.00%|        self._worker_result_queue = multiprocessing_context.Queue()  # type: ignore[var-annotated]
(call)|         2|   0.00950193|   0.00475097|  0.02%|# /opt/conda/lib/python3.8/multiprocessing/context.py:100 Queue
   892|         2|  1.38283e-05|  6.91414e-06|  0.00%|        self._worker_pids_set = False
   893|         2|  1.28746e-05|   6.4373e-06|  0.00%|        self._shutdown = False
   894|         2|  4.22001e-05|     2.11e-05|  0.00%|        self._workers_done_event = multiprocessing_context.Event()
(call)|         2|    0.0116761|   0.00583804|  0.02%|# /opt/conda/lib/python3.8/multiprocessing/context.py:90 Event
   895|         0|            0|            0|  0.00%|
   896|         2|  1.40667e-05|  7.03335e-06|  0.00%|        self._index_queues = []
   897|         2|  1.23978e-05|  6.19888e-06|  0.00%|        self._workers = []
   898|        10|  9.08375e-05|  9.08375e-06|  0.00%|        for i in range(self._num_workers):
   899|         0|            0|            0|  0.00%|            # No certainty which module multiprocessing_context is
   900|         8|  0.000649214|  8.11517e-05|  0.00%|            index_queue = multiprocessing_context.Queue()  # type: ignore[var-annotated]
(call)|         8|      0.04422|    0.0055275|  0.08%|# /opt/conda/lib/python3.8/multiprocessing/context.py:100 Queue
   901|         0|            0|            0|  0.00%|            # Need to `cancel_join_thread` here!
   902|         0|            0|            0|  0.00%|            # See sections (2) and (3b) above.
   903|         8|  0.000141144|   1.7643e-05|  0.00%|            index_queue.cancel_join_thread()
(call)|         8|  0.000656366|  8.20458e-05|  0.00%|# /opt/conda/lib/python3.8/multiprocessing/queues.py:150 cancel_join_thread
   904|        16|  0.000411034|  2.56896e-05|  0.00%|            w = multiprocessing_context.Process(
(call)|         8|   0.00235105|  0.000293881|  0.00%|# /opt/conda/lib/python3.8/multiprocessing/process.py:80 __init__
   905|         8|  0.000119686|  1.49608e-05|  0.00%|                target=_utils.worker._worker_loop,
   906|        16|  0.000151157|  9.44734e-06|  0.00%|                args=(self._dataset_kind, self._dataset, index_queue,
   907|         8|  5.36442e-05|  6.70552e-06|  0.00%|                      self._worker_result_queue, self._workers_done_event,
   908|         8|  4.60148e-05|  5.75185e-06|  0.00%|                      self._auto_collation, self._collate_fn, self._drop_last,
   909|         8|  0.000106096|   1.3262e-05|  0.00%|                      self._base_seed, self._worker_init_fn, i, self._num_workers,
   910|         8|  4.88758e-05|  6.10948e-06|  0.00%|                      self._persistent_workers))
   911|         8|  0.000170708|  2.13385e-05|  0.00%|            w.daemon = True
(call)|         8|  0.000121832|   1.5229e-05|  0.00%|# /opt/conda/lib/python3.8/multiprocessing/process.py:205 daemon
   912|         0|            0|            0|  0.00%|            # NB: Process.start() actually take some time as it needs to
   913|         0|            0|            0|  0.00%|            #     start a process and pass the arguments over via a pipe.
   914|         0|            0|            0|  0.00%|            #     Therefore, we only add a worker to self._workers list after
   915|         0|            0|            0|  0.00%|            #     it started, so that we do not call .join() if program dies
   916|         0|            0|            0|  0.00%|            #     before it starts, and __del__ tries to join but will get:
   917|         0|            0|            0|  0.00%|            #     AssertionError: can only join a started process.
   918|         8|   0.00130129|  0.000162661|  0.00%|            w.start()
(call)|         8|      1.02099|     0.127624|  1.91%|# /opt/conda/lib/python3.8/multiprocessing/process.py:110 start
   919|         8|  0.000374079|  4.67598e-05|  0.00%|            self._index_queues.append(index_queue)
   920|         8|    0.0001266|   1.5825e-05|  0.00%|            self._workers.append(w)
   921|         0|            0|            0|  0.00%|
   922|         2|  4.22001e-05|     2.11e-05|  0.00%|        if self._pin_memory:
   923|         0|            0|            0|  0.00%|            self._pin_memory_thread_done_event = threading.Event()
   924|         0|            0|            0|  0.00%|
   925|         0|            0|            0|  0.00%|            # Queue is not type-annotated
   926|         0|            0|            0|  0.00%|            self._data_queue = queue.Queue()  # type: ignore[var-annotated]
   927|         0|            0|            0|  0.00%|            pin_memory_thread = threading.Thread(
   928|         0|            0|            0|  0.00%|                target=_utils.pin_memory._pin_memory_loop,
   929|         0|            0|            0|  0.00%|                args=(self._worker_result_queue, self._data_queue,
   930|         0|            0|            0|  0.00%|                      torch.cuda.current_device(),
   931|         0|            0|            0|  0.00%|                      self._pin_memory_thread_done_event))
   932|         0|            0|            0|  0.00%|            pin_memory_thread.daemon = True
   933|         0|            0|            0|  0.00%|            pin_memory_thread.start()
   934|         0|            0|            0|  0.00%|            # Similar to workers (see comment above), we only register
   935|         0|            0|            0|  0.00%|            # pin_memory_thread once it is started.
   936|         0|            0|            0|  0.00%|            self._pin_memory_thread = pin_memory_thread
   937|         0|            0|            0|  0.00%|        else:
   938|         2|  9.34601e-05|    4.673e-05|  0.00%|            self._data_queue = self._worker_result_queue
   939|         0|            0|            0|  0.00%|
   940|         0|            0|            0|  0.00%|        # .pid can be None only before process is spawned (not the case, so ignore)
   941|        22|  0.000905275|  4.11489e-05|  0.00%|        _utils.signal_handling._set_worker_pids(id(self), tuple(w.pid for w in self._workers))  # type: ignore[misc]
(call)|         8|  0.000403404|  5.04255e-05|  0.00%|# /opt/conda/lib/python3.8/multiprocessing/process.py:234 ident
(call)|        10|  0.000716686|  7.16686e-05|  0.00%|# /opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py:941 <genexpr>
   942|         2|  0.000102997|  5.14984e-05|  0.00%|        _utils.signal_handling._set_SIGCHLD_handler()
(call)|         2|  0.000413656|  0.000206828|  0.00%|# /opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/signal_handling.py:47 _set_SIGCHLD_handler
   943|         2|  1.43051e-05|  7.15256e-06|  0.00%|        self._worker_pids_set = True
   944|         2|  0.000174522|  8.72612e-05|  0.00%|        self._reset(loader, first_iter=True)
(call)|         2|    0.0233274|    0.0116637|  0.04%|# /opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py:946 _reset
   945|         0|            0|            0|  0.00%|
   946|         2|  3.26633e-05|  1.63317e-05|  0.00%|    def _reset(self, loader, first_iter=False):
   947|         2|  8.10623e-05|  4.05312e-05|  0.00%|        super()._reset(loader, first_iter)
(call)|         2|  0.000329256|  0.000164628|  0.00%|# /opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py:506 _reset
   948|         2|  3.88622e-05|  1.94311e-05|  0.00%|        self._send_idx = 0  # idx of the next task to be sent to workers
   949|         2|  2.19345e-05|  1.09673e-05|  0.00%|        self._rcvd_idx = 0  # idx of the next task to be returned in __next__
   950|         0|            0|            0|  0.00%|        # information about data not yet yielded, i.e., tasks w/ indices in range [rcvd_idx, send_idx).
   951|         0|            0|            0|  0.00%|        # map: task idx => - (worker_id,)        if data isn't fetched (outstanding)
   952|         0|            0|            0|  0.00%|        #                  \ (worker_id, data)   if data is already fetched (out-of-order)
   953|         2|  1.23978e-05|  6.19888e-06|  0.00%|        self._task_info = {}
   954|         2|  1.21593e-05|  6.07967e-06|  0.00%|        self._tasks_outstanding = 0  # always equal to count(v for v in task_info.values() if len(v) == 1)
   955|         0|            0|            0|  0.00%|        # A list of booleans representing whether each worker still has work to
   956|         0|            0|            0|  0.00%|        # do, i.e., not having exhausted its iterable dataset object. It always
   957|         0|            0|            0|  0.00%|        # contains all `True`s if not using an iterable-style dataset
   958|         0|            0|            0|  0.00%|        # (i.e., if kind != Iterable).
   959|         0|            0|            0|  0.00%|        # Not that this indicates that a worker still has work to do *for this epoch*.
   960|         0|            0|            0|  0.00%|        # It does not mean that a worker is dead. In case of `_persistent_workers`,
   961|         0|            0|            0|  0.00%|        # the worker will be reset to available in the next epoch.
   962|        14|  0.000124693|  8.90664e-06|  0.00%|        self._workers_status = [True for i in range(self._num_workers)]
(call)|         2|  6.17504e-05|  3.08752e-05|  0.00%|# /opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py:962 <listcomp>
   963|         0|            0|            0|  0.00%|        # We resume the prefetching in case it was enabled
   964|         2|  1.09673e-05|  5.48363e-06|  0.00%|        if not first_iter:
   965|         0|            0|            0|  0.00%|            for idx in range(self._num_workers):
   966|         0|            0|            0|  0.00%|                self._index_queues[idx].put(_utils.worker._ResumeIteration())
   967|         0|            0|            0|  0.00%|            resume_iteration_cnt = self._num_workers
   968|         0|            0|            0|  0.00%|            while resume_iteration_cnt > 0:
   969|         0|            0|            0|  0.00%|                return_idx, return_data = self._get_data()
   970|         0|            0|            0|  0.00%|                if isinstance(return_idx, _utils.worker._ResumeIteration):
   971|         0|            0|            0|  0.00%|                    assert return_data is None
   972|         0|            0|            0|  0.00%|                    resume_iteration_cnt -= 1
   973|         0|            0|            0|  0.00%|        # prime the prefetch loop
   974|        18|  0.000120401|  6.68897e-06|  0.00%|        for _ in range(self._prefetch_factor * self._num_workers):
   975|        16|  0.000312567|  1.95354e-05|  0.00%|            self._try_put_index()
(call)|        16|    0.0222304|    0.0013894|  0.04%|# /opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1205 _try_put_index
   976|         0|            0|            0|  0.00%|
   977|       100|  0.000986814|  9.86814e-06|  0.00%|    def _try_get_data(self, timeout=_utils.MP_STATUS_CHECK_INTERVAL):
   978|         0|            0|            0|  0.00%|        # Tries to fetch data from `self._data_queue` once for a given timeout.
   979|         0|            0|            0|  0.00%|        # This can also be used as inner loop of fetching without timeout, with
   980|         0|            0|            0|  0.00%|        # the sender status as the loop condition.
   981|         0|            0|            0|  0.00%|        #
   982|         0|            0|            0|  0.00%|        # This raises a `RuntimeError` if any worker died expectedly. This error
   983|         0|            0|            0|  0.00%|        # can come from either the SIGCHLD handler in `_utils/signal_handling.py`
   984|         0|            0|            0|  0.00%|        # (only for non-Windows platforms), or the manual check below on errors
   985|         0|            0|            0|  0.00%|        # and timeouts.
   986|         0|            0|            0|  0.00%|        #
   987|         0|            0|            0|  0.00%|        # Returns a 2-tuple:
   988|         0|            0|            0|  0.00%|        #   (bool: whether successfully get data, any: data if successful else None)
   989|       100|  0.000626564|  6.26564e-06|  0.00%|        try:
   990|       100|   0.00231981|  2.31981e-05|  0.00%|            data = self._data_queue.get(timeout=timeout)
(call)|       100|      1.24269|    0.0124269|  2.32%|# /opt/conda/lib/python3.8/multiprocessing/queues.py:92 get
   991|       100|  0.000686407|  6.86407e-06|  0.00%|            return (True, data)
   992|         0|            0|            0|  0.00%|        except Exception as e:
   993|         0|            0|            0|  0.00%|            # At timeout and error, we manually check whether any worker has
   994|         0|            0|            0|  0.00%|            # failed. Note that this is the only mechanism for Windows to detect
   995|         0|            0|            0|  0.00%|            # worker failures.
   996|         0|            0|            0|  0.00%|            failed_workers = []
   997|         0|            0|            0|  0.00%|            for worker_id, w in enumerate(self._workers):
   998|         0|            0|            0|  0.00%|                if self._workers_status[worker_id] and not w.is_alive():
   999|         0|            0|            0|  0.00%|                    failed_workers.append(w)
  1000|         0|            0|            0|  0.00%|                    self._mark_worker_as_unavailable(worker_id)
  1001|         0|            0|            0|  0.00%|            if len(failed_workers) > 0:
  1002|         0|            0|            0|  0.00%|                pids_str = ', '.join(str(w.pid) for w in failed_workers)
  1003|         0|            0|            0|  0.00%|                raise RuntimeError('DataLoader worker (pid(s) {}) exited unexpectedly'.format(pids_str)) from e
  1004|         0|            0|            0|  0.00%|            if isinstance(e, queue.Empty):
  1005|         0|            0|            0|  0.00%|                return (False, None)
  1006|         0|            0|            0|  0.00%|            import tempfile
  1007|         0|            0|            0|  0.00%|            import errno
  1008|         0|            0|            0|  0.00%|            try:
  1009|         0|            0|            0|  0.00%|                # Raise an exception if we are this close to the FDs limit.
  1010|         0|            0|            0|  0.00%|                # Apparently, trying to open only one file is not a sufficient
  1011|         0|            0|            0|  0.00%|                # test.
  1012|         0|            0|            0|  0.00%|                # See NOTE [ DataLoader on Linux and open files limit ]
  1013|         0|            0|            0|  0.00%|                fds_limit_margin = 10
  1014|         0|            0|            0|  0.00%|                fs = [tempfile.NamedTemporaryFile() for i in range(fds_limit_margin)]
  1015|         0|            0|            0|  0.00%|            except OSError as e:
  1016|         0|            0|            0|  0.00%|                if e.errno == errno.EMFILE:
  1017|         0|            0|            0|  0.00%|                    raise RuntimeError(
  1018|         0|            0|            0|  0.00%|                        "Too many open files. Communication with the"
  1019|         0|            0|            0|  0.00%|                        " workers is no longer possible. Please increase the"
  1020|         0|            0|            0|  0.00%|                        " limit using `ulimit -n` in the shell or change the"
  1021|         0|            0|            0|  0.00%|                        " sharing strategy by calling"
  1022|         0|            0|            0|  0.00%|                        " `torch.multiprocessing.set_sharing_strategy('file_system')`"
  1023|         0|            0|            0|  0.00%|                        " at the beginning of your code") from None
  1024|         0|            0|            0|  0.00%|            raise
  1025|         0|            0|            0|  0.00%|
  1026|         0|            0|            0|  0.00%|# NOTE [ DataLoader on Linux and open files limit ]
  1027|         0|            0|            0|  0.00%|#
  1028|         0|            0|            0|  0.00%|# On Linux when DataLoader is used with multiprocessing we pass the data between
  1029|         0|            0|            0|  0.00%|# the root process and the workers through SHM files. We remove those files from
  1030|         0|            0|            0|  0.00%|# the filesystem as soon as they are created and keep them alive by
  1031|         0|            0|            0|  0.00%|# passing around their file descriptors through AF_UNIX sockets. (See
  1032|         0|            0|            0|  0.00%|# docs/source/multiprocessing.rst and 'Multiprocessing Technical Notes` in
  1033|         0|            0|            0|  0.00%|# the wiki (https://github.com/pytorch/pytorch/wiki).)
  1034|         0|            0|            0|  0.00%|#
  1035|         0|            0|            0|  0.00%|# This sometimes leads us to exceeding the open files limit. When that happens,
  1036|         0|            0|            0|  0.00%|# and the offending file descriptor is coming over a socket, the `socket` Python
  1037|         0|            0|            0|  0.00%|# package silently strips the file descriptor from the message, setting only the
  1038|         0|            0|            0|  0.00%|# `MSG_CTRUNC` flag (which might be a bit misleading since the manpage says that
  1039|         0|            0|            0|  0.00%|# it _indicates that some control data were discarded due to lack of space in
  1040|         0|            0|            0|  0.00%|# the buffer for ancillary data_). This might reflect the C implementation of
  1041|         0|            0|            0|  0.00%|# AF_UNIX sockets.
  1042|         0|            0|            0|  0.00%|#
  1043|         0|            0|            0|  0.00%|# This behaviour can be reproduced with the script and instructions at the
  1044|         0|            0|            0|  0.00%|# bottom of this note.
  1045|         0|            0|            0|  0.00%|#
  1046|         0|            0|            0|  0.00%|# When that happens, the standard Python `multiprocessing` (and not
  1047|         0|            0|            0|  0.00%|# `torch.multiprocessing`) raises a `RuntimeError: received 0 items of ancdata`
  1048|         0|            0|            0|  0.00%|#
  1049|         0|            0|            0|  0.00%|# Sometimes, instead of the FD being stripped, you may get an `OSError:
  1050|         0|            0|            0|  0.00%|# Too many open files`, both in the script below and in DataLoader. However,
  1051|         0|            0|            0|  0.00%|# this is rare and seems to be nondeterministic.
  1052|         0|            0|            0|  0.00%|#
  1053|         0|            0|            0|  0.00%|#
  1054|         0|            0|            0|  0.00%|#   #!/usr/bin/env python3
  1055|         0|            0|            0|  0.00%|#   import sys
  1056|         0|            0|            0|  0.00%|#   import socket
  1057|         0|            0|            0|  0.00%|#   import os
  1058|         0|            0|            0|  0.00%|#   import array
  1059|         0|            0|            0|  0.00%|#   import shutil
  1060|         0|            0|            0|  0.00%|#   import socket
  1061|         0|            0|            0|  0.00%|#
  1062|         0|            0|            0|  0.00%|#
  1063|         0|            0|            0|  0.00%|#   if len(sys.argv) != 4:
  1064|         0|            0|            0|  0.00%|#       print("Usage: ", sys.argv[0], " tmp_dirname iteration (send|recv)")
  1065|         0|            0|            0|  0.00%|#       sys.exit(1)
  1066|         0|            0|            0|  0.00%|#
  1067|         0|            0|            0|  0.00%|#   if __name__ == '__main__':
  1068|         0|            0|            0|  0.00%|#       dirname = sys.argv[1]
  1069|         0|            0|            0|  0.00%|#       sock_path = dirname + "/sock"
  1070|         0|            0|            0|  0.00%|#       iterations = int(sys.argv[2])
  1071|         0|            0|            0|  0.00%|#       def dummy_path(i):
  1072|         0|            0|            0|  0.00%|#           return dirname + "/" + str(i) + ".dummy"
  1073|         0|            0|            0|  0.00%|#
  1074|         0|            0|            0|  0.00%|#
  1075|         0|            0|            0|  0.00%|#       if sys.argv[3] == 'send':
  1076|         0|            0|            0|  0.00%|#           while not os.path.exists(sock_path):
  1077|         0|            0|            0|  0.00%|#               pass
  1078|         0|            0|            0|  0.00%|#           client = socket.socket(socket.AF_UNIX, socket.SOCK_DGRAM)
  1079|         0|            0|            0|  0.00%|#           client.connect(sock_path)
  1080|         0|            0|            0|  0.00%|#           for i in range(iterations):
  1081|         0|            0|            0|  0.00%|#               fd = os.open(dummy_path(i), os.O_WRONLY | os.O_CREAT)
  1082|         0|            0|            0|  0.00%|#               ancdata = array.array('i', [fd])
  1083|         0|            0|            0|  0.00%|#               msg = bytes([i % 256])
  1084|         0|            0|            0|  0.00%|#               print("Sending fd ", fd, " (iteration #", i, ")")
  1085|         0|            0|            0|  0.00%|#               client.sendmsg([msg], [(socket.SOL_SOCKET, socket.SCM_RIGHTS, ancdata)])
  1086|         0|            0|            0|  0.00%|#
  1087|         0|            0|            0|  0.00%|#
  1088|         0|            0|            0|  0.00%|#       else:
  1089|         0|            0|            0|  0.00%|#           assert sys.argv[3] == 'recv'
  1090|         0|            0|            0|  0.00%|#
  1091|         0|            0|            0|  0.00%|#           if os.path.exists(dirname):
  1092|         0|            0|            0|  0.00%|#               raise Exception("Directory exists")
  1093|         0|            0|            0|  0.00%|#
  1094|         0|            0|            0|  0.00%|#           os.mkdir(dirname)
  1095|         0|            0|            0|  0.00%|#
  1096|         0|            0|            0|  0.00%|#           print("Opening socket...")
  1097|         0|            0|            0|  0.00%|#           server = socket.socket(socket.AF_UNIX, socket.SOCK_DGRAM)
  1098|         0|            0|            0|  0.00%|#           server.bind(sock_path)
  1099|         0|            0|            0|  0.00%|#
  1100|         0|            0|            0|  0.00%|#           print("Listening...")
  1101|         0|            0|            0|  0.00%|#           for i in range(iterations):
  1102|         0|            0|            0|  0.00%|#               a = array.array('i')
  1103|         0|            0|            0|  0.00%|#               msg, ancdata, flags, addr = server.recvmsg(1, socket.CMSG_SPACE(a.itemsize))
  1104|         0|            0|            0|  0.00%|#               assert(len(ancdata) == 1)
  1105|         0|            0|            0|  0.00%|#               cmsg_level, cmsg_type, cmsg_data = ancdata[0]
  1106|         0|            0|            0|  0.00%|#               a.frombytes(cmsg_data)
  1107|         0|            0|            0|  0.00%|#               print("Received fd ", a[0], " (iteration #", i, ")")
  1108|         0|            0|            0|  0.00%|#
  1109|         0|            0|            0|  0.00%|#           shutil.rmtree(dirname)
  1110|         0|            0|            0|  0.00%|#
  1111|         0|            0|            0|  0.00%|# Steps to reproduce:
  1112|         0|            0|            0|  0.00%|#
  1113|         0|            0|            0|  0.00%|# 1. Run two shells and set lower file descriptor limit in the receiving one:
  1114|         0|            0|            0|  0.00%|# (shell1) ulimit -n 1020
  1115|         0|            0|            0|  0.00%|# (shell2) ulimit -n 1022
  1116|         0|            0|            0|  0.00%|#
  1117|         0|            0|            0|  0.00%|# 2. Run the script above with the `recv` option in the first shell
  1118|         0|            0|            0|  0.00%|# (shell1) ./test_socket.py sock_tmp 1017 recv
  1119|         0|            0|            0|  0.00%|#
  1120|         0|            0|            0|  0.00%|# 3. Run the script with the `send` option in the second shell:
  1121|         0|            0|            0|  0.00%|# (shell2) ./test_socket.py sock_tmp 1017 send
  1122|         0|            0|            0|  0.00%|
  1123|       100|  0.000617743|  6.17743e-06|  0.00%|    def _get_data(self):
  1124|         0|            0|            0|  0.00%|        # Fetches data from `self._data_queue`.
  1125|         0|            0|            0|  0.00%|        #
  1126|         0|            0|            0|  0.00%|        # We check workers' status every `MP_STATUS_CHECK_INTERVAL` seconds,
  1127|         0|            0|            0|  0.00%|        # which we achieve by running `self._try_get_data(timeout=MP_STATUS_CHECK_INTERVAL)`
  1128|         0|            0|            0|  0.00%|        # in a loop. This is the only mechanism to detect worker failures for
  1129|         0|            0|            0|  0.00%|        # Windows. For other platforms, a SIGCHLD handler is also used for
  1130|         0|            0|            0|  0.00%|        # worker failure detection.
  1131|         0|            0|            0|  0.00%|        #
  1132|         0|            0|            0|  0.00%|        # If `pin_memory=True`, we also need check if `pin_memory_thread` had
  1133|         0|            0|            0|  0.00%|        # died at timeouts.
  1134|       100|  0.000495195|  4.95195e-06|  0.00%|        if self._timeout > 0:
  1135|         0|            0|            0|  0.00%|            success, data = self._try_get_data(self._timeout)
  1136|         0|            0|            0|  0.00%|            if success:
  1137|         0|            0|            0|  0.00%|                return data
  1138|         0|            0|            0|  0.00%|            else:
  1139|         0|            0|            0|  0.00%|                raise RuntimeError('DataLoader timed out after {} seconds'.format(self._timeout))
  1140|       100|  0.000478745|  4.78745e-06|  0.00%|        elif self._pin_memory:
  1141|         0|            0|            0|  0.00%|            while self._pin_memory_thread.is_alive():
  1142|         0|            0|            0|  0.00%|                success, data = self._try_get_data()
  1143|         0|            0|            0|  0.00%|                if success:
  1144|         0|            0|            0|  0.00%|                    return data
  1145|         0|            0|            0|  0.00%|            else:
  1146|         0|            0|            0|  0.00%|                # while condition is false, i.e., pin_memory_thread died.
  1147|         0|            0|            0|  0.00%|                raise RuntimeError('Pin memory thread exited unexpectedly')
  1148|         0|            0|            0|  0.00%|            # In this case, `self._data_queue` is a `queue.Queue`,. But we don't
  1149|         0|            0|            0|  0.00%|            # need to call `.task_done()` because we don't use `.join()`.
  1150|         0|            0|            0|  0.00%|        else:
  1151|         0|            0|            0|  0.00%|            while True:
  1152|       100|   0.00206542|  2.06542e-05|  0.00%|                success, data = self._try_get_data()
(call)|       100|      1.24731|    0.0124731|  2.33%|# /opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py:977 _try_get_data
  1153|       100|  0.000463486|  4.63486e-06|  0.00%|                if success:
  1154|       100|  0.000430584|  4.30584e-06|  0.00%|                    return data
  1155|         0|            0|            0|  0.00%|
  1156|       102|   0.00074625|  7.31618e-06|  0.00%|    def _next_data(self):
  1157|         0|            0|            0|  0.00%|        while True:
  1158|         0|            0|            0|  0.00%|            # If the worker responsible for `self._rcvd_idx` has already ended
  1159|         0|            0|            0|  0.00%|            # and was unable to fulfill this task (due to exhausting an `IterableDataset`),
  1160|         0|            0|            0|  0.00%|            # we try to advance `self._rcvd_idx` to find the next valid index.
  1161|         0|            0|            0|  0.00%|            #
  1162|         0|            0|            0|  0.00%|            # This part needs to run in the loop because both the `self._get_data()`
  1163|         0|            0|            0|  0.00%|            # call and `_IterableDatasetStopIteration` check below can mark
  1164|         0|            0|            0|  0.00%|            # extra worker(s) as dead.
  1165|       109|  0.000699759|   6.4198e-06|  0.00%|            while self._rcvd_idx < self._send_idx:
  1166|       107|  0.000613689|  5.73542e-06|  0.00%|                info = self._task_info[self._rcvd_idx]
  1167|       107|  0.000541925|  5.06472e-06|  0.00%|                worker_id = info[0]
  1168|       107|  0.000905037|  8.45829e-06|  0.00%|                if len(info) == 2 or self._workers_status[worker_id]:  # has data or is still active
  1169|       100|  0.000486135|  4.86135e-06|  0.00%|                    break
  1170|         0|            0|            0|  0.00%|                del self._task_info[self._rcvd_idx]
  1171|         0|            0|            0|  0.00%|                self._rcvd_idx += 1
  1172|         0|            0|            0|  0.00%|            else:
  1173|         0|            0|            0|  0.00%|                # no valid `self._rcvd_idx` is found (i.e., didn't break)
  1174|         2|  1.07288e-05|  5.36442e-06|  0.00%|                if not self._persistent_workers:
  1175|         2|  5.05447e-05|  2.52724e-05|  0.00%|                    self._shutdown_workers()
(call)|         2|     0.135687|    0.0678436|  0.25%|# /opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1258 _shutdown_workers
  1176|         2|  1.97887e-05|  9.89437e-06|  0.00%|                raise StopIteration
  1177|         0|            0|            0|  0.00%|
  1178|         0|            0|            0|  0.00%|            # Now `self._rcvd_idx` is the batch index we want to fetch
  1179|         0|            0|            0|  0.00%|
  1180|         0|            0|            0|  0.00%|            # Check if the next sample has already been generated
  1181|       107|  0.000710011|  6.63561e-06|  0.00%|            if len(self._task_info[self._rcvd_idx]) == 2:
  1182|         7|  7.20024e-05|  1.02861e-05|  0.00%|                data = self._task_info.pop(self._rcvd_idx)[1]
  1183|         7|  0.000114918|  1.64168e-05|  0.00%|                return self._process_data(data)
(call)|         7|   0.00353408|  0.000504868|  0.01%|# /opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1225 _process_data
  1184|         0|            0|            0|  0.00%|
  1185|       100|  0.000533581|  5.33581e-06|  0.00%|            assert not self._shutdown and self._tasks_outstanding > 0
  1186|       100|   0.00158358|  1.58358e-05|  0.00%|            idx, data = self._get_data()
(call)|       100|      1.25186|    0.0125186|  2.34%|# /opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1123 _get_data
  1187|       100|  0.000541925|  5.41925e-06|  0.00%|            self._tasks_outstanding -= 1
  1188|       100|  0.000572681|  5.72681e-06|  0.00%|            if self._dataset_kind == _DatasetKind.Iterable:
  1189|         0|            0|            0|  0.00%|                # Check for _IterableDatasetStopIteration
  1190|         0|            0|            0|  0.00%|                if isinstance(data, _utils.worker._IterableDatasetStopIteration):
  1191|         0|            0|            0|  0.00%|                    if self._persistent_workers:
  1192|         0|            0|            0|  0.00%|                        self._workers_status[data.worker_id] = False
  1193|         0|            0|            0|  0.00%|                    else:
  1194|         0|            0|            0|  0.00%|                        self._mark_worker_as_unavailable(data.worker_id)
  1195|         0|            0|            0|  0.00%|                    self._try_put_index()
  1196|         0|            0|            0|  0.00%|                    continue
  1197|         0|            0|            0|  0.00%|
  1198|       100|  0.000452995|  4.52995e-06|  0.00%|            if idx != self._rcvd_idx:
  1199|         0|            0|            0|  0.00%|                # store out-of-order samples
  1200|         7|  4.17233e-05|  5.96046e-06|  0.00%|                self._task_info[idx] += (data,)
  1201|         0|            0|            0|  0.00%|            else:
  1202|        93|  0.000442266|  4.75555e-06|  0.00%|                del self._task_info[idx]
  1203|        93|   0.00138474|  1.48896e-05|  0.00%|                return self._process_data(data)
(call)|        93|    0.0385809|  0.000414848|  0.07%|# /opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1225 _process_data
  1204|         0|            0|            0|  0.00%|
  1205|       116|  0.000669956|  5.77548e-06|  0.00%|    def _try_put_index(self):
  1206|       116|  0.000653982|  5.63778e-06|  0.00%|        assert self._tasks_outstanding < self._prefetch_factor * self._num_workers
  1207|         0|            0|            0|  0.00%|
  1208|       116|  0.000540257|  4.65738e-06|  0.00%|        try:
  1209|       116|    0.0017004|  1.46586e-05|  0.00%|            index = self._next_index()
(call)|       116|     0.017591|  0.000151647|  0.03%|# /opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py:511 _next_index
  1210|        16|  9.58443e-05|  5.99027e-06|  0.00%|        except StopIteration:
  1211|        16|  0.000101089|  6.31809e-06|  0.00%|            return
  1212|       100|  0.000783682|  7.83682e-06|  0.00%|        for _ in range(self._num_workers):  # find the next active worker, if any
  1213|       100|  0.000711679|  7.11679e-06|  0.00%|            worker_queue_idx = next(self._worker_queue_idx_cycle)
  1214|       100|  0.000460863|  4.60863e-06|  0.00%|            if self._workers_status[worker_queue_idx]:
  1215|       100|    0.0004282|    4.282e-06|  0.00%|                break
  1216|         0|            0|            0|  0.00%|        else:
  1217|         0|            0|            0|  0.00%|            # not found (i.e., didn't break)
  1218|         0|            0|            0|  0.00%|            return
  1219|         0|            0|            0|  0.00%|
  1220|       100|   0.00181198|  1.81198e-05|  0.00%|        self._index_queues[worker_queue_idx].put((self._send_idx, index))
(call)|       100|    0.0336649|  0.000336649|  0.06%|# /opt/conda/lib/python3.8/multiprocessing/queues.py:80 put
  1221|       100|   0.00051713|   5.1713e-06|  0.00%|        self._task_info[self._send_idx] = (worker_queue_idx,)
  1222|       100|  0.000458717|  4.58717e-06|  0.00%|        self._tasks_outstanding += 1
  1223|       100|   0.00044322|   4.4322e-06|  0.00%|        self._send_idx += 1
  1224|         0|            0|            0|  0.00%|
  1225|       100|  0.000598431|  5.98431e-06|  0.00%|    def _process_data(self, data):
  1226|       100|  0.000480652|  4.80652e-06|  0.00%|        self._rcvd_idx += 1
  1227|       100|   0.00142622|  1.42622e-05|  0.00%|        self._try_put_index()
(call)|       100|    0.0384026|  0.000384026|  0.07%|# /opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1205 _try_put_index
  1228|       100|  0.000784397|  7.84397e-06|  0.00%|        if isinstance(data, ExceptionWrapper):
  1229|         0|            0|            0|  0.00%|            data.reraise()
  1230|       100|  0.000422716|  4.22716e-06|  0.00%|        return data
  1231|         0|            0|            0|  0.00%|
  1232|         8|  4.33922e-05|  5.42402e-06|  0.00%|    def _mark_worker_as_unavailable(self, worker_id, shutdown=False):
  1233|         0|            0|            0|  0.00%|        # Mark a worker as having finished its work e.g., due to
  1234|         0|            0|            0|  0.00%|        # exhausting an `IterableDataset`. This should be used only when this
  1235|         0|            0|            0|  0.00%|        # `_MultiProcessingDataLoaderIter` is going to continue running.
  1236|         0|            0|            0|  0.00%|
  1237|         8|   3.8147e-05|  4.76837e-06|  0.00%|        assert self._workers_status[worker_id] or (self._persistent_workers and shutdown)
  1238|         0|            0|            0|  0.00%|
  1239|         0|            0|            0|  0.00%|        # Signal termination to that specific worker.
  1240|         8|   3.6478e-05|  4.55976e-06|  0.00%|        q = self._index_queues[worker_id]
  1241|         0|            0|            0|  0.00%|        # Indicate that no more data will be put on this queue by the current
  1242|         0|            0|            0|  0.00%|        # process.
  1243|         8|  0.000112534|  1.40667e-05|  0.00%|        q.put(None)
(call)|         8|   0.00142455|  0.000178069|  0.00%|# /opt/conda/lib/python3.8/multiprocessing/queues.py:80 put
  1244|         0|            0|            0|  0.00%|
  1245|         0|            0|            0|  0.00%|        # Note that we don't actually join the worker here, nor do we remove the
  1246|         0|            0|            0|  0.00%|        # worker's pid from C side struct because (1) joining may be slow, and
  1247|         0|            0|            0|  0.00%|        # (2) since we don't join, the worker may still raise error, and we
  1248|         0|            0|            0|  0.00%|        # prefer capturing those, rather than ignoring them, even though they
  1249|         0|            0|            0|  0.00%|        # are raised after the worker has finished its job.
  1250|         0|            0|            0|  0.00%|        # Joinning is deferred to `_shutdown_workers`, which it is called when
  1251|         0|            0|            0|  0.00%|        # all workers finish their jobs (e.g., `IterableDataset` replicas) or
  1252|         0|            0|            0|  0.00%|        # when this iterator is garbage collected.
  1253|         0|            0|            0|  0.00%|
  1254|         8|  3.71933e-05|  4.64916e-06|  0.00%|        self._workers_status[worker_id] = False
  1255|         0|            0|            0|  0.00%|
  1256|         8|  0.000106096|   1.3262e-05|  0.00%|        assert self._workers_done_event.is_set() == shutdown
(call)|         8|  0.000763655|  9.54568e-05|  0.00%|# /opt/conda/lib/python3.8/multiprocessing/synchronize.py:327 is_set
  1257|         0|            0|            0|  0.00%|
  1258|         4|  9.03606e-05|  2.25902e-05|  0.00%|    def _shutdown_workers(self):
  1259|         0|            0|            0|  0.00%|        # Called when shutting down this `_MultiProcessingDataLoaderIter`.
  1260|         0|            0|            0|  0.00%|        # See NOTE [ Data Loader Multiprocessing Shutdown Logic ] for details on
  1261|         0|            0|            0|  0.00%|        # the logic of this function.
  1262|         4|  2.88486e-05|  7.21216e-06|  0.00%|        python_exit_status = _utils.python_exit_status
  1263|         4|  2.26498e-05|  5.66244e-06|  0.00%|        if python_exit_status is True or python_exit_status is None:
  1264|         0|            0|            0|  0.00%|            # See (2) of the note. If Python is shutting down, do no-op.
  1265|         0|            0|            0|  0.00%|            return
  1266|         0|            0|            0|  0.00%|        # Normal exit when last reference is gone / iterator is depleted.
  1267|         0|            0|            0|  0.00%|        # See (1) and the second half of the note.
  1268|         4|  2.26498e-05|  5.66244e-06|  0.00%|        if not self._shutdown:
  1269|         2|  1.23978e-05|  6.19888e-06|  0.00%|            self._shutdown = True
  1270|         2|  1.19209e-05|  5.96046e-06|  0.00%|            try:
  1271|         0|            0|            0|  0.00%|                # Normal exit when last reference is gone / iterator is depleted.
  1272|         0|            0|            0|  0.00%|                # See (1) and the second half of the note.
  1273|         0|            0|            0|  0.00%|
  1274|         0|            0|            0|  0.00%|                # Exit `pin_memory_thread` first because exiting workers may leave
  1275|         0|            0|            0|  0.00%|                # corrupted data in `worker_result_queue` which `pin_memory_thread`
  1276|         0|            0|            0|  0.00%|                # reads from.
  1277|         2|  1.88351e-05|  9.41753e-06|  0.00%|                if hasattr(self, '_pin_memory_thread'):
  1278|         0|            0|            0|  0.00%|                    # Use hasattr in case error happens before we set the attribute.
  1279|         0|            0|            0|  0.00%|                    self._pin_memory_thread_done_event.set()
  1280|         0|            0|            0|  0.00%|                    # Send something to pin_memory_thread in case it is waiting
  1281|         0|            0|            0|  0.00%|                    # so that it can wake up and check `pin_memory_thread_done_event`
  1282|         0|            0|            0|  0.00%|                    self._worker_result_queue.put((None, None))
  1283|         0|            0|            0|  0.00%|                    self._pin_memory_thread.join()
  1284|         0|            0|            0|  0.00%|                    self._worker_result_queue.cancel_join_thread()
  1285|         0|            0|            0|  0.00%|                    self._worker_result_queue.close()
  1286|         0|            0|            0|  0.00%|
  1287|         0|            0|            0|  0.00%|                # Exit workers now.
  1288|         2|   6.8903e-05|  3.44515e-05|  0.00%|                self._workers_done_event.set()
(call)|         2|  0.000717878|  0.000358939|  0.00%|# /opt/conda/lib/python3.8/multiprocessing/synchronize.py:334 set
  1289|        10|  6.74725e-05|  6.74725e-06|  0.00%|                for worker_id in range(len(self._workers)):
  1290|         0|            0|            0|  0.00%|                    # Get number of workers from `len(self._workers)` instead of
  1291|         0|            0|            0|  0.00%|                    # `self._num_workers` in case we error before starting all
  1292|         0|            0|            0|  0.00%|                    # workers.
  1293|         0|            0|            0|  0.00%|                    # If we are using workers_status with persistent_workers
  1294|         0|            0|            0|  0.00%|                    # we have to shut it down because the worker is paused
  1295|         8|  4.07696e-05|   5.0962e-06|  0.00%|                    if self._persistent_workers or self._workers_status[worker_id]:
  1296|         8|   0.00012064|    1.508e-05|  0.00%|                        self._mark_worker_as_unavailable(worker_id, shutdown=True)
(call)|         8|   0.00256205|  0.000320256|  0.00%|# /opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1232 _mark_worker_as_unavailable
  1297|        10|  5.22137e-05|  5.22137e-06|  0.00%|                for w in self._workers:
  1298|         0|            0|            0|  0.00%|                    # We should be able to join here, but in case anything went
  1299|         0|            0|            0|  0.00%|                    # wrong, we set a timeout and if the workers fail to join,
  1300|         0|            0|            0|  0.00%|                    # they are killed in the `finally` block.
  1301|         8|  0.000157356|  1.96695e-05|  0.00%|                    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)
(call)|         8|     0.126938|    0.0158673|  0.24%|# /opt/conda/lib/python3.8/multiprocessing/process.py:142 join
  1302|        10|  4.95911e-05|  4.95911e-06|  0.00%|                for q in self._index_queues:
  1303|         8|  0.000118971|  1.48714e-05|  0.00%|                    q.cancel_join_thread()
(call)|         8|  0.000419855|  5.24819e-05|  0.00%|# /opt/conda/lib/python3.8/multiprocessing/queues.py:150 cancel_join_thread
  1304|         8|  0.000130653|  1.63317e-05|  0.00%|                    q.close()
(call)|         8|   0.00313306|  0.000391632|  0.01%|# /opt/conda/lib/python3.8/multiprocessing/queues.py:134 close
  1305|         0|            0|            0|  0.00%|            finally:
  1306|         0|            0|            0|  0.00%|                # Even though all this function does is putting into queues that
  1307|         0|            0|            0|  0.00%|                # we have called `cancel_join_thread` on, weird things can
  1308|         0|            0|            0|  0.00%|                # happen when a worker is killed by a signal, e.g., hanging in
  1309|         0|            0|            0|  0.00%|                # `Event.set()`. So we need to guard this with SIGCHLD handler,
  1310|         0|            0|            0|  0.00%|                # and remove pids from the C side data structure only at the
  1311|         0|            0|            0|  0.00%|                # end.
  1312|         0|            0|            0|  0.00%|                #
  1313|         0|            0|            0|  0.00%|                # FIXME: Unfortunately, for Windows, we are missing a worker
  1314|         0|            0|            0|  0.00%|                #        error detection mechanism here in this function, as it
  1315|         0|            0|            0|  0.00%|                #        doesn't provide a SIGCHLD handler.
  1316|         2|  8.58307e-06|  4.29153e-06|  0.00%|                if self._worker_pids_set:
  1317|         2|  6.17504e-05|  3.08752e-05|  0.00%|                    _utils.signal_handling._remove_worker_pids(id(self))
  1318|         2|  2.09808e-05|  1.04904e-05|  0.00%|                    self._worker_pids_set = False
  1319|        10|    5.126e-05|    5.126e-06|  0.00%|                for w in self._workers:
  1320|         8|  0.000122547|  1.53184e-05|  0.00%|                    if w.is_alive():
(call)|         8|  0.000682831|  8.53539e-05|  0.00%|# /opt/conda/lib/python3.8/multiprocessing/process.py:153 is_alive
  1321|         0|            0|            0|  0.00%|                        # Existing mechanisms try to make the workers exit
  1322|         0|            0|            0|  0.00%|                        # peacefully, but in case that we unfortunately reach
  1323|         0|            0|            0|  0.00%|                        # here, which we shouldn't, (e.g., pytorch/pytorch#39570),
  1324|         0|            0|            0|  0.00%|                        # we kill the worker.
  1325|         0|            0|            0|  0.00%|                        w.terminate()
  1326|         0|            0|            0|  0.00%|
  1327|         2|  1.74046e-05|  8.70228e-06|  0.00%|    def __del__(self):
  1328|         2|  4.17233e-05|  2.08616e-05|  0.00%|        self._shutdown_workers()
(call)|         2|  4.60148e-05|  2.30074e-05|  0.00%|# /opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1258 _shutdown_workers
File: /opt/conda/lib/python3.8/multiprocessing/reduction.py
File duration: 0.0537171s (0.10%)
Line #|      Hits|         Time| Time per hit|      %|Source code
------+----------+-------------+-------------+-------+-----------
     1|         0|            0|            0|  0.00%|#
     2|         0|            0|            0|  0.00%|# Module which deals with pickling of objects.
     3|         0|            0|            0|  0.00%|#
     4|         0|            0|            0|  0.00%|# multiprocessing/reduction.py
     5|         0|            0|            0|  0.00%|#
     6|         0|            0|            0|  0.00%|# Copyright (c) 2006-2008, R Oudkerk
     7|         0|            0|            0|  0.00%|# Licensed to PSF under a Contributor Agreement.
     8|         0|            0|            0|  0.00%|#
     9|         0|            0|            0|  0.00%|
    10|         0|            0|            0|  0.00%|from abc import ABCMeta
    11|         0|            0|            0|  0.00%|import copyreg
    12|         0|            0|            0|  0.00%|import functools
    13|         0|            0|            0|  0.00%|import io
    14|         0|            0|            0|  0.00%|import os
    15|         0|            0|            0|  0.00%|import pickle
    16|         0|            0|            0|  0.00%|import socket
    17|         0|            0|            0|  0.00%|import sys
    18|         0|            0|            0|  0.00%|
    19|         0|            0|            0|  0.00%|from . import context
    20|         0|            0|            0|  0.00%|
    21|         0|            0|            0|  0.00%|__all__ = ['send_handle', 'recv_handle', 'ForkingPickler', 'register', 'dump']
    22|         0|            0|            0|  0.00%|
    23|         0|            0|            0|  0.00%|
    24|         0|            0|            0|  0.00%|HAVE_SEND_HANDLE = (sys.platform == 'win32' or
    25|         0|            0|            0|  0.00%|                    (hasattr(socket, 'CMSG_LEN') and
    26|         0|            0|            0|  0.00%|                     hasattr(socket, 'SCM_RIGHTS') and
    27|         0|            0|            0|  0.00%|                     hasattr(socket.socket, 'sendmsg')))
    28|         0|            0|            0|  0.00%|
    29|         0|            0|            0|  0.00%|#
    30|         0|            0|            0|  0.00%|# Pickler subclass
    31|         0|            0|            0|  0.00%|#
    32|         0|            0|            0|  0.00%|
    33|         0|            0|            0|  0.00%|class ForkingPickler(pickle.Pickler):
    34|         0|            0|            0|  0.00%|    '''Pickler subclass used by multiprocessing.'''
    35|         0|            0|            0|  0.00%|    _extra_reducers = {}
    36|         0|            0|            0|  0.00%|    _copyreg_dispatch_table = copyreg.dispatch_table
    37|         0|            0|            0|  0.00%|
    38|       200|   0.00100708|   5.0354e-06|  0.00%|    def __init__(self, *args):
    39|       200|   0.00292587|  1.46294e-05|  0.01%|        super().__init__(*args)
    40|       200|   0.00181198|  9.05991e-06|  0.00%|        self.dispatch_table = self._copyreg_dispatch_table.copy()
    41|       200|   0.00283432|  1.41716e-05|  0.01%|        self.dispatch_table.update(self._extra_reducers)
    42|         0|            0|            0|  0.00%|
    43|         0|            0|            0|  0.00%|    @classmethod
    44|         0|            0|            0|  0.00%|    def register(cls, type, reduce):
    45|         0|            0|            0|  0.00%|        '''Register a reduce function for a type.'''
    46|         0|            0|            0|  0.00%|        cls._extra_reducers[type] = reduce
    47|         0|            0|            0|  0.00%|
    48|       200|   0.00101781|  5.08904e-06|  0.00%|    @classmethod
    49|         0|            0|            0|  0.00%|    def dumps(cls, obj, protocol=None):
    50|       200|   0.00114918|  5.74589e-06|  0.00%|        buf = io.BytesIO()
    51|       200|     0.004879|   2.4395e-05|  0.01%|        cls(buf, protocol).dump(obj)
(call)|       200|   0.00857925|  4.28963e-05|  0.02%|# /opt/conda/lib/python3.8/multiprocessing/reduction.py:38 __init__
    52|       200|   0.00166059|  8.30293e-06|  0.00%|        return buf.getbuffer()
    53|         0|            0|            0|  0.00%|
    54|         0|            0|            0|  0.00%|    loads = pickle.loads
    55|         0|            0|            0|  0.00%|
    56|         0|            0|            0|  0.00%|register = ForkingPickler.register
    57|         0|            0|            0|  0.00%|
    58|         0|            0|            0|  0.00%|def dump(obj, file, protocol=None):
    59|         0|            0|            0|  0.00%|    '''Replacement for pickle.dump() using ForkingPickler.'''
    60|         0|            0|            0|  0.00%|    ForkingPickler(file, protocol).dump(obj)
    61|         0|            0|            0|  0.00%|
    62|         0|            0|            0|  0.00%|#
    63|         0|            0|            0|  0.00%|# Platform specific definitions
    64|         0|            0|            0|  0.00%|#
    65|         0|            0|            0|  0.00%|
    66|         0|            0|            0|  0.00%|if sys.platform == 'win32':
    67|         0|            0|            0|  0.00%|    # Windows
    68|         0|            0|            0|  0.00%|    __all__ += ['DupHandle', 'duplicate', 'steal_handle']
    69|         0|            0|            0|  0.00%|    import _winapi
    70|         0|            0|            0|  0.00%|
    71|         0|            0|            0|  0.00%|    def duplicate(handle, target_process=None, inheritable=False,
    72|         0|            0|            0|  0.00%|                  *, source_process=None):
    73|         0|            0|            0|  0.00%|        '''Duplicate a handle.  (target_process is a handle not a pid!)'''
    74|         0|            0|            0|  0.00%|        current_process = _winapi.GetCurrentProcess()
    75|         0|            0|            0|  0.00%|        if source_process is None:
    76|         0|            0|            0|  0.00%|            source_process = current_process
    77|         0|            0|            0|  0.00%|        if target_process is None:
    78|         0|            0|            0|  0.00%|            target_process = current_process
    79|         0|            0|            0|  0.00%|        return _winapi.DuplicateHandle(
    80|         0|            0|            0|  0.00%|            source_process, handle, target_process,
    81|         0|            0|            0|  0.00%|            0, inheritable, _winapi.DUPLICATE_SAME_ACCESS)
    82|         0|            0|            0|  0.00%|
    83|         0|            0|            0|  0.00%|    def steal_handle(source_pid, handle):
    84|         0|            0|            0|  0.00%|        '''Steal a handle from process identified by source_pid.'''
    85|         0|            0|            0|  0.00%|        source_process_handle = _winapi.OpenProcess(
    86|         0|            0|            0|  0.00%|            _winapi.PROCESS_DUP_HANDLE, False, source_pid)
    87|         0|            0|            0|  0.00%|        try:
    88|         0|            0|            0|  0.00%|            return _winapi.DuplicateHandle(
    89|         0|            0|            0|  0.00%|                source_process_handle, handle,
    90|         0|            0|            0|  0.00%|                _winapi.GetCurrentProcess(), 0, False,
    91|         0|            0|            0|  0.00%|                _winapi.DUPLICATE_SAME_ACCESS | _winapi.DUPLICATE_CLOSE_SOURCE)
    92|         0|            0|            0|  0.00%|        finally:
    93|         0|            0|            0|  0.00%|            _winapi.CloseHandle(source_process_handle)
    94|         0|            0|            0|  0.00%|
    95|         0|            0|            0|  0.00%|    def send_handle(conn, handle, destination_pid):
    96|         0|            0|            0|  0.00%|        '''Send a handle over a local connection.'''
    97|         0|            0|            0|  0.00%|        dh = DupHandle(handle, _winapi.DUPLICATE_SAME_ACCESS, destination_pid)
    98|         0|            0|            0|  0.00%|        conn.send(dh)
    99|         0|            0|            0|  0.00%|
   100|         0|            0|            0|  0.00%|    def recv_handle(conn):
   101|         0|            0|            0|  0.00%|        '''Receive a handle over a local connection.'''
   102|         0|            0|            0|  0.00%|        return conn.recv().detach()
   103|         0|            0|            0|  0.00%|
   104|         0|            0|            0|  0.00%|    class DupHandle(object):
   105|         0|            0|            0|  0.00%|        '''Picklable wrapper for a handle.'''
   106|         0|            0|            0|  0.00%|        def __init__(self, handle, access, pid=None):
   107|         0|            0|            0|  0.00%|            if pid is None:
   108|         0|            0|            0|  0.00%|                # We just duplicate the handle in the current process and
   109|         0|            0|            0|  0.00%|                # let the receiving process steal the handle.
   110|         0|            0|            0|  0.00%|                pid = os.getpid()
   111|         0|            0|            0|  0.00%|            proc = _winapi.OpenProcess(_winapi.PROCESS_DUP_HANDLE, False, pid)
   112|         0|            0|            0|  0.00%|            try:
   113|         0|            0|            0|  0.00%|                self._handle = _winapi.DuplicateHandle(
   114|         0|            0|            0|  0.00%|                    _winapi.GetCurrentProcess(),
   115|         0|            0|            0|  0.00%|                    handle, proc, access, False, 0)
   116|         0|            0|            0|  0.00%|            finally:
   117|         0|            0|            0|  0.00%|                _winapi.CloseHandle(proc)
   118|         0|            0|            0|  0.00%|            self._access = access
   119|         0|            0|            0|  0.00%|            self._pid = pid
   120|         0|            0|            0|  0.00%|
   121|         0|            0|            0|  0.00%|        def detach(self):
   122|         0|            0|            0|  0.00%|            '''Get the handle.  This should only be called once.'''
   123|         0|            0|            0|  0.00%|            # retrieve handle from process which currently owns it
   124|         0|            0|            0|  0.00%|            if self._pid == os.getpid():
   125|         0|            0|            0|  0.00%|                # The handle has already been duplicated for this process.
   126|         0|            0|            0|  0.00%|                return self._handle
   127|         0|            0|            0|  0.00%|            # We must steal the handle from the process whose pid is self._pid.
   128|         0|            0|            0|  0.00%|            proc = _winapi.OpenProcess(_winapi.PROCESS_DUP_HANDLE, False,
   129|         0|            0|            0|  0.00%|                                       self._pid)
   130|         0|            0|            0|  0.00%|            try:
   131|         0|            0|            0|  0.00%|                return _winapi.DuplicateHandle(
   132|         0|            0|            0|  0.00%|                    proc, self._handle, _winapi.GetCurrentProcess(),
   133|         0|            0|            0|  0.00%|                    self._access, False, _winapi.DUPLICATE_CLOSE_SOURCE)
   134|         0|            0|            0|  0.00%|            finally:
   135|         0|            0|            0|  0.00%|                _winapi.CloseHandle(proc)
   136|         0|            0|            0|  0.00%|
   137|         0|            0|            0|  0.00%|else:
   138|         0|            0|            0|  0.00%|    # Unix
   139|         0|            0|            0|  0.00%|    __all__ += ['DupFd', 'sendfds', 'recvfds']
   140|         0|            0|            0|  0.00%|    import array
   141|         0|            0|            0|  0.00%|
   142|         0|            0|            0|  0.00%|    # On MacOSX we should acknowledge receipt of fds -- see Issue14669
   143|         0|            0|            0|  0.00%|    ACKNOWLEDGE = sys.platform == 'darwin'
   144|         0|            0|            0|  0.00%|
   145|         0|            0|            0|  0.00%|    def sendfds(sock, fds):
   146|         0|            0|            0|  0.00%|        '''Send an array of fds over an AF_UNIX socket.'''
   147|         0|            0|            0|  0.00%|        fds = array.array('i', fds)
   148|         0|            0|            0|  0.00%|        msg = bytes([len(fds) % 256])
   149|         0|            0|            0|  0.00%|        sock.sendmsg([msg], [(socket.SOL_SOCKET, socket.SCM_RIGHTS, fds)])
   150|         0|            0|            0|  0.00%|        if ACKNOWLEDGE and sock.recv(1) != b'A':
   151|         0|            0|            0|  0.00%|            raise RuntimeError('did not receive acknowledgement of fd')
   152|         0|            0|            0|  0.00%|
   153|       200|   0.00130296|  6.51479e-06|  0.00%|    def recvfds(sock, size):
   154|         0|            0|            0|  0.00%|        '''Receive an array of fds over an AF_UNIX socket.'''
   155|       200|   0.00160909|  8.04543e-06|  0.00%|        a = array.array('i')
   156|       200|   0.00129247|  6.46234e-06|  0.00%|        bytes_size = a.itemsize * size
   157|       200|   0.00608778|  3.04389e-05|  0.01%|        msg, ancdata, flags, addr = sock.recvmsg(1, socket.CMSG_SPACE(bytes_size))
   158|       200|   0.00125861|  6.29306e-06|  0.00%|        if not msg and not ancdata:
   159|         0|            0|            0|  0.00%|            raise EOFError
   160|       200|  0.000962496|  4.81248e-06|  0.00%|        try:
   161|       200|  0.000924349|  4.62174e-06|  0.00%|            if ACKNOWLEDGE:
   162|         0|            0|            0|  0.00%|                sock.send(b'A')
   163|       200|   0.00137043|  6.85215e-06|  0.00%|            if len(ancdata) != 1:
   164|         0|            0|            0|  0.00%|                raise RuntimeError('received %d items of ancdata' %
   165|         0|            0|            0|  0.00%|                                   len(ancdata))
   166|       200|   0.00111818|  5.59092e-06|  0.00%|            cmsg_level, cmsg_type, cmsg_data = ancdata[0]
   167|       400|   0.00179243|  4.48108e-06|  0.00%|            if (cmsg_level == socket.SOL_SOCKET and
   168|       200|  0.000952959|   4.7648e-06|  0.00%|                cmsg_type == socket.SCM_RIGHTS):
   169|       200|   0.00132942|  6.64711e-06|  0.00%|                if len(cmsg_data) % a.itemsize != 0:
   170|         0|            0|            0|  0.00%|                    raise ValueError
   171|       200|   0.00149536|  7.47681e-06|  0.00%|                a.frombytes(cmsg_data)
   172|       200|   0.00136018|  6.80089e-06|  0.00%|                if len(a) % 256 != msg[0]:
   173|         0|            0|            0|  0.00%|                    raise AssertionError(
   174|         0|            0|            0|  0.00%|                        "Len is {0:n} but msg[0] is {1!r}".format(
   175|         0|            0|            0|  0.00%|                            len(a), msg[0]))
   176|       200|   0.00129533|  6.47664e-06|  0.00%|                return list(a)
   177|         0|            0|            0|  0.00%|        except (ValueError, IndexError):
   178|         0|            0|            0|  0.00%|            pass
   179|         0|            0|            0|  0.00%|        raise RuntimeError('Invalid data received')
   180|         0|            0|            0|  0.00%|
   181|         0|            0|            0|  0.00%|    def send_handle(conn, handle, destination_pid):
   182|         0|            0|            0|  0.00%|        '''Send a handle over a local connection.'''
   183|         0|            0|            0|  0.00%|        with socket.fromfd(conn.fileno(), socket.AF_UNIX, socket.SOCK_STREAM) as s:
   184|         0|            0|            0|  0.00%|            sendfds(s, [handle])
   185|         0|            0|            0|  0.00%|
   186|       200|   0.00100446|  5.02229e-06|  0.00%|    def recv_handle(conn):
   187|         0|            0|            0|  0.00%|        '''Receive a handle over a local connection.'''
   188|       200|   0.00611067|  3.05533e-05|  0.01%|        with socket.fromfd(conn.fileno(), socket.AF_UNIX, socket.SOCK_STREAM) as s:
(call)|       200|   0.00550675|  2.75338e-05|  0.01%|# /opt/conda/lib/python3.8/multiprocessing/connection.py:168 fileno
(call)|       200|    0.0108397|  5.41985e-05|  0.02%|# /opt/conda/lib/python3.8/socket.py:537 fromfd
(call)|       200|   0.00159311|  7.96556e-06|  0.00%|# /opt/conda/lib/python3.8/socket.py:235 __enter__
   189|       200|   0.00516415|  2.58207e-05|  0.01%|            return recvfds(s, 1)[0]
(call)|       200|     0.024152|   0.00012076|  0.05%|# /opt/conda/lib/python3.8/multiprocessing/reduction.py:153 recvfds
(call)|       200|    0.0118408|  5.92041e-05|  0.02%|# /opt/conda/lib/python3.8/socket.py:238 __exit__
   190|         0|            0|            0|  0.00%|
   191|         0|            0|            0|  0.00%|    def DupFd(fd):
   192|         0|            0|            0|  0.00%|        '''Return a wrapper for an fd.'''
   193|         0|            0|            0|  0.00%|        popen_obj = context.get_spawning_popen()
   194|         0|            0|            0|  0.00%|        if popen_obj is not None:
   195|         0|            0|            0|  0.00%|            return popen_obj.DupFd(popen_obj.duplicate_for_child(fd))
   196|         0|            0|            0|  0.00%|        elif HAVE_SEND_HANDLE:
   197|         0|            0|            0|  0.00%|            from . import resource_sharer
   198|         0|            0|            0|  0.00%|            return resource_sharer.DupFd(fd)
   199|         0|            0|            0|  0.00%|        else:
   200|         0|            0|            0|  0.00%|            raise ValueError('SCM_RIGHTS appears not to be available')
   201|         0|            0|            0|  0.00%|
   202|         0|            0|            0|  0.00%|#
   203|         0|            0|            0|  0.00%|# Try making some callable types picklable
   204|         0|            0|            0|  0.00%|#
   205|         0|            0|            0|  0.00%|
   206|         0|            0|            0|  0.00%|def _reduce_method(m):
   207|         0|            0|            0|  0.00%|    if m.__self__ is None:
   208|         0|            0|            0|  0.00%|        return getattr, (m.__class__, m.__func__.__name__)
   209|         0|            0|            0|  0.00%|    else:
   210|         0|            0|            0|  0.00%|        return getattr, (m.__self__, m.__func__.__name__)
   211|         0|            0|            0|  0.00%|class _C:
   212|         0|            0|            0|  0.00%|    def f(self):
   213|         0|            0|            0|  0.00%|        pass
   214|         0|            0|            0|  0.00%|register(type(_C().f), _reduce_method)
   215|         0|            0|            0|  0.00%|
   216|         0|            0|            0|  0.00%|
   217|         0|            0|            0|  0.00%|def _reduce_method_descriptor(m):
   218|         0|            0|            0|  0.00%|    return getattr, (m.__objclass__, m.__name__)
   219|         0|            0|            0|  0.00%|register(type(list.append), _reduce_method_descriptor)
   220|         0|            0|            0|  0.00%|register(type(int.__add__), _reduce_method_descriptor)
   221|         0|            0|            0|  0.00%|
   222|         0|            0|            0|  0.00%|
   223|         0|            0|            0|  0.00%|def _reduce_partial(p):
   224|         0|            0|            0|  0.00%|    return _rebuild_partial, (p.func, p.args, p.keywords or {})
   225|         0|            0|            0|  0.00%|def _rebuild_partial(func, args, keywords):
   226|         0|            0|            0|  0.00%|    return functools.partial(func, *args, **keywords)
   227|         0|            0|            0|  0.00%|register(functools.partial, _reduce_partial)
   228|         0|            0|            0|  0.00%|
   229|         0|            0|            0|  0.00%|#
   230|         0|            0|            0|  0.00%|# Make sockets picklable
   231|         0|            0|            0|  0.00%|#
   232|         0|            0|            0|  0.00%|
   233|         0|            0|            0|  0.00%|if sys.platform == 'win32':
   234|         0|            0|            0|  0.00%|    def _reduce_socket(s):
   235|         0|            0|            0|  0.00%|        from .resource_sharer import DupSocket
   236|         0|            0|            0|  0.00%|        return _rebuild_socket, (DupSocket(s),)
   237|         0|            0|            0|  0.00%|    def _rebuild_socket(ds):
   238|         0|            0|            0|  0.00%|        return ds.detach()
   239|         0|            0|            0|  0.00%|    register(socket.socket, _reduce_socket)
   240|         0|            0|            0|  0.00%|
   241|         0|            0|            0|  0.00%|else:
   242|         0|            0|            0|  0.00%|    def _reduce_socket(s):
   243|         0|            0|            0|  0.00%|        df = DupFd(s.fileno())
   244|         0|            0|            0|  0.00%|        return _rebuild_socket, (df, s.family, s.type, s.proto)
   245|         0|            0|            0|  0.00%|    def _rebuild_socket(df, family, type, proto):
   246|         0|            0|            0|  0.00%|        fd = df.detach()
   247|         0|            0|            0|  0.00%|        return socket.socket(family, type, proto, fileno=fd)
   248|         0|            0|            0|  0.00%|    register(socket.socket, _reduce_socket)
   249|         0|            0|            0|  0.00%|
   250|         0|            0|            0|  0.00%|
   251|         0|            0|            0|  0.00%|class AbstractReducer(metaclass=ABCMeta):
   252|         0|            0|            0|  0.00%|    '''Abstract base class for use in implementing a Reduction class
   253|         0|            0|            0|  0.00%|    suitable for use in replacing the standard reduction mechanism
   254|         0|            0|            0|  0.00%|    used in multiprocessing.'''
   255|         0|            0|            0|  0.00%|    ForkingPickler = ForkingPickler
   256|         0|            0|            0|  0.00%|    register = register
   257|         0|            0|            0|  0.00%|    dump = dump
   258|         0|            0|            0|  0.00%|    send_handle = send_handle
   259|         0|            0|            0|  0.00%|    recv_handle = recv_handle
   260|         0|            0|            0|  0.00%|
   261|         0|            0|            0|  0.00%|    if sys.platform == 'win32':
   262|         0|            0|            0|  0.00%|        steal_handle = steal_handle
   263|         0|            0|            0|  0.00%|        duplicate = duplicate
   264|         0|            0|            0|  0.00%|        DupHandle = DupHandle
   265|         0|            0|            0|  0.00%|    else:
   266|         0|            0|            0|  0.00%|        sendfds = sendfds
   267|         0|            0|            0|  0.00%|        recvfds = recvfds
   268|         0|            0|            0|  0.00%|        DupFd = DupFd
   269|         0|            0|            0|  0.00%|
   270|         0|            0|            0|  0.00%|    _reduce_method = _reduce_method
   271|         0|            0|            0|  0.00%|    _reduce_method_descriptor = _reduce_method_descriptor
   272|         0|            0|            0|  0.00%|    _rebuild_partial = _rebuild_partial
   273|         0|            0|            0|  0.00%|    _reduce_socket = _reduce_socket
   274|         0|            0|            0|  0.00%|    _rebuild_socket = _rebuild_socket
   275|         0|            0|            0|  0.00%|
   276|         0|            0|            0|  0.00%|    def __init__(self, *args):
   277|         0|            0|            0|  0.00%|        register(type(_C().f), _reduce_method)
   278|         0|            0|            0|  0.00%|        register(type(list.append), _reduce_method_descriptor)
   279|         0|            0|            0|  0.00%|        register(type(int.__add__), _reduce_method_descriptor)
   280|         0|            0|            0|  0.00%|        register(functools.partial, _reduce_partial)
   281|         0|            0|            0|  0.00%|        register(socket.socket, _reduce_socket)
File: /opt/conda/lib/python3.8/site-packages/torch/autograd/profiler.py
File duration: 0.046592s (0.09%)
Line #|      Hits|         Time| Time per hit|      %|Source code
------+----------+-------------+-------------+-------+-----------
     1|         0|            0|            0|  0.00%|import itertools
     2|         0|            0|            0|  0.00%|from typing import Any
     3|         0|            0|            0|  0.00%|import torch
     4|         0|            0|            0|  0.00%|from torch.autograd import DeviceType, ProfilerActivity, ProfilerState
     5|         0|            0|            0|  0.00%|from torch.futures import Future
     6|         0|            0|            0|  0.00%|
     7|         0|            0|            0|  0.00%|from collections import defaultdict, namedtuple
     8|         0|            0|            0|  0.00%|from operator import attrgetter
     9|         0|            0|            0|  0.00%|
    10|         0|            0|            0|  0.00%|from typing import Dict, List, Tuple, Optional
    11|         0|            0|            0|  0.00%|from warnings import warn
    12|         0|            0|            0|  0.00%|
    13|         0|            0|            0|  0.00%|import math
    14|         0|            0|            0|  0.00%|
    15|         0|            0|            0|  0.00%|try:
    16|         0|            0|            0|  0.00%|    # Available in Python >= 3.2
    17|         0|            0|            0|  0.00%|    from contextlib import ContextDecorator
    18|         0|            0|            0|  0.00%|except ImportError:
    19|         0|            0|            0|  0.00%|    import functools
    20|         0|            0|            0|  0.00%|
    21|         0|            0|            0|  0.00%|    class ContextDecorator(object):  # type: ignore[no-redef]
    22|         0|            0|            0|  0.00%|
    23|         0|            0|            0|  0.00%|        def __enter__(self):
    24|         0|            0|            0|  0.00%|            raise NotImplementedError
    25|         0|            0|            0|  0.00%|
    26|         0|            0|            0|  0.00%|        def __exit__(self, exc_type, exc_val, exc_tb):
    27|         0|            0|            0|  0.00%|            raise NotImplementedError
    28|         0|            0|            0|  0.00%|
    29|         0|            0|            0|  0.00%|        def __call__(self, func):
    30|         0|            0|            0|  0.00%|            @functools.wraps(func)
    31|         0|            0|            0|  0.00%|            def wrapped(*args, **kwargs):
    32|         0|            0|            0|  0.00%|                with self:
    33|         0|            0|            0|  0.00%|                    return func(*args, **kwargs)
    34|         0|            0|            0|  0.00%|
    35|         0|            0|            0|  0.00%|            return wrapped
    36|         0|            0|            0|  0.00%|
    37|         0|            0|            0|  0.00%|
    38|         0|            0|            0|  0.00%|class EventList(list):
    39|         0|            0|            0|  0.00%|    """A list of Events (for pretty printing)"""
    40|         0|            0|            0|  0.00%|    def __init__(self, *args, **kwargs):
    41|         0|            0|            0|  0.00%|        use_cuda = kwargs.pop('use_cuda', True)
    42|         0|            0|            0|  0.00%|        profile_memory = kwargs.pop('profile_memory', False)
    43|         0|            0|            0|  0.00%|        with_flops = kwargs.pop('with_flops', False)
    44|         0|            0|            0|  0.00%|        super(EventList, self).__init__(*args, **kwargs)
    45|         0|            0|            0|  0.00%|        self._use_cuda = use_cuda
    46|         0|            0|            0|  0.00%|        self._profile_memory = profile_memory
    47|         0|            0|            0|  0.00%|        self._tree_built = False
    48|         0|            0|            0|  0.00%|        self._with_flops = with_flops
    49|         0|            0|            0|  0.00%|
    50|         0|            0|            0|  0.00%|    def _build_tree(self):
    51|         0|            0|            0|  0.00%|        self._populate_cpu_children()
    52|         0|            0|            0|  0.00%|        self._remove_dup_nodes()
    53|         0|            0|            0|  0.00%|        self._set_backward_stacktraces()
    54|         0|            0|            0|  0.00%|        self._tree_built = True
    55|         0|            0|            0|  0.00%|
    56|         0|            0|            0|  0.00%|    def __str__(self):
    57|         0|            0|            0|  0.00%|        return self.table()
    58|         0|            0|            0|  0.00%|
    59|         0|            0|            0|  0.00%|    def _remove_dup_nodes(self):
    60|         0|            0|            0|  0.00%|        while True:
    61|         0|            0|            0|  0.00%|            to_delete = set()
    62|         0|            0|            0|  0.00%|            for idx in range(len(self)):
    63|         0|            0|            0|  0.00%|                if (self[idx].cpu_parent is not None and
    64|         0|            0|            0|  0.00%|                        self[idx].cpu_parent.name == self[idx].name and
    65|         0|            0|            0|  0.00%|                        len(self[idx].cpu_parent.cpu_children) == 1):
    66|         0|            0|            0|  0.00%|                    self[idx].cpu_parent.cpu_children = self[idx].cpu_children
    67|         0|            0|            0|  0.00%|                    self[idx].cpu_parent.kernels = self[idx].kernels  # lift kernels up
    68|         0|            0|            0|  0.00%|                    for ch in self[idx].cpu_children:
    69|         0|            0|            0|  0.00%|                        ch.cpu_parent = self[idx].cpu_parent
    70|         0|            0|            0|  0.00%|                    to_delete.add(idx)
    71|         0|            0|            0|  0.00%|            if len(to_delete) == 0:
    72|         0|            0|            0|  0.00%|                break
    73|         0|            0|            0|  0.00%|            new_evts = [ev for ind, ev in enumerate(self) if ind not in to_delete]
    74|         0|            0|            0|  0.00%|            self.clear()
    75|         0|            0|            0|  0.00%|            self.extend(new_evts)
    76|         0|            0|            0|  0.00%|
    77|         0|            0|            0|  0.00%|    def _populate_cpu_children(self):
    78|         0|            0|            0|  0.00%|        """Populates child events into each underlying FunctionEvent object.
    79|         0|            0|            0|  0.00%|        One event is a child of another if [s1, e1) is inside [s2, e2). Where
    80|         0|            0|            0|  0.00%|        s1 and e1 would be start and end of the child event's interval. And
    81|         0|            0|            0|  0.00%|        s2 and e2 start and end of the parent event's interval
    82|         0|            0|            0|  0.00%|
    83|         0|            0|            0|  0.00%|        Example: In event list [[0, 10], [1, 3], [3, 4]] would have make [0, 10]
    84|         0|            0|            0|  0.00%|        be a parent of two other intervals.
    85|         0|            0|            0|  0.00%|
    86|         0|            0|            0|  0.00%|        If for any reason two intervals intersect only partially, this function
    87|         0|            0|            0|  0.00%|        will not record a parent child relationship between then.
    88|         0|            0|            0|  0.00%|        """
    89|         0|            0|            0|  0.00%|
    90|         0|            0|            0|  0.00%|        # Some events can be async (i.e. start and end on different threads),
    91|         0|            0|            0|  0.00%|        # since it's generally undefined how to attribute children ranges to
    92|         0|            0|            0|  0.00%|        # async ranges, we do not use them when calculating nested ranges and stats
    93|         0|            0|            0|  0.00%|        sync_events = [evt for evt in self if not evt.is_async and evt.device_type == DeviceType.CPU]
    94|         0|            0|            0|  0.00%|        events = sorted(
    95|         0|            0|            0|  0.00%|            sync_events,
    96|         0|            0|            0|  0.00%|            key=attrgetter("thread"),
    97|         0|            0|            0|  0.00%|        )
    98|         0|            0|            0|  0.00%|        # Group by both thread and node_id, so that events that happen to have
    99|         0|            0|            0|  0.00%|        # the same thread_id but are from different nodes aren't incorrectly
   100|         0|            0|            0|  0.00%|        # grouped together.
   101|         0|            0|            0|  0.00%|        threads = itertools.groupby(
   102|         0|            0|            0|  0.00%|            events, key=lambda event: (event.thread, event.node_id)
   103|         0|            0|            0|  0.00%|        )
   104|         0|            0|            0|  0.00%|
   105|         0|            0|            0|  0.00%|        # For each thread we keep a stack of current nested parents.
   106|         0|            0|            0|  0.00%|        # We maintain the invariant that each interval is a subset of all other
   107|         0|            0|            0|  0.00%|        # intervals lower in the stack.
   108|         0|            0|            0|  0.00%|        #
   109|         0|            0|            0|  0.00%|        # First we sort the intervals by their start time. Then we iterate over them.
   110|         0|            0|            0|  0.00%|        # Every time we see a new interval we remove several parents from
   111|         0|            0|            0|  0.00%|        # the top until we restore the invariant. Then parent child relationship
   112|         0|            0|            0|  0.00%|        # if recorded if the stack is not empty.
   113|         0|            0|            0|  0.00%|        # Finally we add new interval to the list
   114|         0|            0|            0|  0.00%|        #
   115|         0|            0|            0|  0.00%|        # Algorithm has O(N * log(N)) complexity where N is number of
   116|         0|            0|            0|  0.00%|        # intervals
   117|         0|            0|            0|  0.00%|        for thread_id, thread_events in threads:
   118|         0|            0|            0|  0.00%|            thread_events_ = sorted(
   119|         0|            0|            0|  0.00%|                thread_events,
   120|         0|            0|            0|  0.00%|                key=lambda event: [event.time_range.start, -event.time_range.end],
   121|         0|            0|            0|  0.00%|            )
   122|         0|            0|            0|  0.00%|            current_events: List[FunctionEvent] = []
   123|         0|            0|            0|  0.00%|            cur_end = 0
   124|         0|            0|            0|  0.00%|            for event in thread_events_:
   125|         0|            0|            0|  0.00%|                while len(current_events) > 0:
   126|         0|            0|            0|  0.00%|                    parent = current_events[-1]
   127|         0|            0|            0|  0.00%|                    if event.time_range.start >= parent.time_range.end or \
   128|         0|            0|            0|  0.00%|                            event.time_range.end > parent.time_range.end:
   129|         0|            0|            0|  0.00%|                        # this can't be a parent
   130|         0|            0|            0|  0.00%|                        current_events.pop()
   131|         0|            0|            0|  0.00%|                    else:
   132|         0|            0|            0|  0.00%|                        parent.append_cpu_child(event)
   133|         0|            0|            0|  0.00%|                        assert (
   134|         0|            0|            0|  0.00%|                            event.cpu_parent is None
   135|         0|            0|            0|  0.00%|                        ), "There is already a CPU parent event for {}".format(
   136|         0|            0|            0|  0.00%|                            event.key
   137|         0|            0|            0|  0.00%|                        )
   138|         0|            0|            0|  0.00%|                        event.set_cpu_parent(parent)
   139|         0|            0|            0|  0.00%|                        break
   140|         0|            0|            0|  0.00%|
   141|         0|            0|            0|  0.00%|                current_events.append(event)
   142|         0|            0|            0|  0.00%|
   143|         0|            0|            0|  0.00%|    def _set_backward_stacktraces(self):
   144|         0|            0|            0|  0.00%|        def bw_parent(evt):
   145|         0|            0|            0|  0.00%|            if evt is None:
   146|         0|            0|            0|  0.00%|                return None
   147|         0|            0|            0|  0.00%|            elif evt.scope == 1:  # BACKWARD_FUNCTION
   148|         0|            0|            0|  0.00%|                return evt
   149|         0|            0|            0|  0.00%|            else:
   150|         0|            0|            0|  0.00%|                return bw_parent(evt.cpu_parent)
   151|         0|            0|            0|  0.00%|
   152|         0|            0|            0|  0.00%|        fwd_stacks = {}
   153|         0|            0|            0|  0.00%|        for evt in self:
   154|         0|            0|            0|  0.00%|            if bw_parent(evt) is None and evt.stack is not None:
   155|         0|            0|            0|  0.00%|                t = (evt.sequence_nr, evt.thread)
   156|         0|            0|            0|  0.00%|                if t not in fwd_stacks:
   157|         0|            0|            0|  0.00%|                    fwd_stacks[t] = evt.stack
   158|         0|            0|            0|  0.00%|
   159|         0|            0|            0|  0.00%|        for evt in self:
   160|         0|            0|            0|  0.00%|            p = bw_parent(evt)
   161|         0|            0|            0|  0.00%|            if p is not None:
   162|         0|            0|            0|  0.00%|                assert p.fwd_thread is not None
   163|         0|            0|            0|  0.00%|                t = (p.sequence_nr, p.fwd_thread)
   164|         0|            0|            0|  0.00%|                if t in fwd_stacks:
   165|         0|            0|            0|  0.00%|                    evt.stack = fwd_stacks[t]
   166|         0|            0|            0|  0.00%|                else:
   167|         0|            0|            0|  0.00%|                    evt.stack = []
   168|         0|            0|            0|  0.00%|
   169|         0|            0|            0|  0.00%|    @property
   170|         0|            0|            0|  0.00%|    def self_cpu_time_total(self):
   171|         0|            0|            0|  0.00%|        return sum([event.self_cpu_time_total for event in self])
   172|         0|            0|            0|  0.00%|
   173|         0|            0|            0|  0.00%|    def table(self, sort_by=None, row_limit=100, max_src_column_width=75, header=None, top_level_events_only=False):
   174|         0|            0|            0|  0.00%|        """Prints an EventList as a nicely formatted table.
   175|         0|            0|            0|  0.00%|
   176|         0|            0|            0|  0.00%|        Args:
   177|         0|            0|            0|  0.00%|            sort_by (str, optional): Attribute used to sort entries. By default
   178|         0|            0|            0|  0.00%|                they are printed in the same order as they were registered.
   179|         0|            0|            0|  0.00%|                Valid keys include: ``cpu_time``, ``cuda_time``, ``cpu_time_total``,
   180|         0|            0|            0|  0.00%|                ``cuda_time_total``, ``cpu_memory_usage``, ``cuda_memory_usage``,
   181|         0|            0|            0|  0.00%|                ``self_cpu_memory_usage``, ``self_cuda_memory_usage``, ``count``.
   182|         0|            0|            0|  0.00%|            top_level_events_only(bool, optional): Boolean flag to determine the
   183|         0|            0|            0|  0.00%|                selection of events to display. If true, the profiler will only
   184|         0|            0|            0|  0.00%|                display events at top level like top-level invocation of python
   185|         0|            0|            0|  0.00%|                `lstm`, python `add` or other functions, nested events like low-level
   186|         0|            0|            0|  0.00%|                cpu/cuda ops events are omitted for profiler result readability.
   187|         0|            0|            0|  0.00%|
   188|         0|            0|            0|  0.00%|        Returns:
   189|         0|            0|            0|  0.00%|            A string containing the table.
   190|         0|            0|            0|  0.00%|        """
   191|         0|            0|            0|  0.00%|        return build_table(
   192|         0|            0|            0|  0.00%|            self,
   193|         0|            0|            0|  0.00%|            sort_by=sort_by,
   194|         0|            0|            0|  0.00%|            row_limit=row_limit,
   195|         0|            0|            0|  0.00%|            max_src_column_width=max_src_column_width,
   196|         0|            0|            0|  0.00%|            header=header,
   197|         0|            0|            0|  0.00%|            profile_memory=self._profile_memory,
   198|         0|            0|            0|  0.00%|            with_flops=self._with_flops,
   199|         0|            0|            0|  0.00%|            top_level_events_only=top_level_events_only)
   200|         0|            0|            0|  0.00%|
   201|         0|            0|            0|  0.00%|    def export_chrome_trace(self, path):
   202|         0|            0|            0|  0.00%|        """Exports an EventList as a Chrome tracing tools file.
   203|         0|            0|            0|  0.00%|
   204|         0|            0|            0|  0.00%|        The checkpoint can be later loaded and inspected under ``chrome://tracing`` URL.
   205|         0|            0|            0|  0.00%|
   206|         0|            0|            0|  0.00%|        Args:
   207|         0|            0|            0|  0.00%|            path (str): Path where the trace will be written.
   208|         0|            0|            0|  0.00%|        """
   209|         0|            0|            0|  0.00%|        import os
   210|         0|            0|            0|  0.00%|        with open(path, 'w') as f:
   211|         0|            0|            0|  0.00%|            chrome_events = []
   212|         0|            0|            0|  0.00%|            next_id = 0
   213|         0|            0|            0|  0.00%|            # Use file IO over using json.dump since JSON dumping is very slow and
   214|         0|            0|            0|  0.00%|            # this technique is proven to give a 4x speedup.
   215|         0|            0|            0|  0.00%|            f.write("[")
   216|         0|            0|            0|  0.00%|            for evt in self:
   217|         0|            0|            0|  0.00%|                if evt.trace_name is None:
   218|         0|            0|            0|  0.00%|                    continue
   219|         0|            0|            0|  0.00%|                f.write(
   220|         0|            0|            0|  0.00%|                    '{"name": "%s", '
   221|         0|            0|            0|  0.00%|                    '"ph": "X", '
   222|         0|            0|            0|  0.00%|                    '"ts": %s, '
   223|         0|            0|            0|  0.00%|                    '"dur": %s, '
   224|         0|            0|            0|  0.00%|                    '"tid": %s, '
   225|         0|            0|            0|  0.00%|                    '"pid": "CPU functions", '
   226|         0|            0|            0|  0.00%|                    '"args": {}}, '
   227|         0|            0|            0|  0.00%|                    % (
   228|         0|            0|            0|  0.00%|                        evt.trace_name,
   229|         0|            0|            0|  0.00%|                        evt.time_range.start,
   230|         0|            0|            0|  0.00%|                        evt.time_range.elapsed_us(),
   231|         0|            0|            0|  0.00%|                        evt.thread
   232|         0|            0|            0|  0.00%|                        if not evt.is_remote
   233|         0|            0|            0|  0.00%|                        else f'" node_id:{evt.node_id}, thread_id:{evt.thread} "',
   234|         0|            0|            0|  0.00%|                    )
   235|         0|            0|            0|  0.00%|                )
   236|         0|            0|            0|  0.00%|                for k in evt.kernels:
   237|         0|            0|            0|  0.00%|                    # 's' and 'f' draw Flow arrows from
   238|         0|            0|            0|  0.00%|                    # the CPU launch to the GPU kernel
   239|         0|            0|            0|  0.00%|                    f.write('{"name": "%s", '
   240|         0|            0|            0|  0.00%|                            '"ph": "s", '
   241|         0|            0|            0|  0.00%|                            '"ts": %s, '
   242|         0|            0|            0|  0.00%|                            '"tid": %s, '
   243|         0|            0|            0|  0.00%|                            '"pid": "CPU functions", '
   244|         0|            0|            0|  0.00%|                            '"id": %s, '
   245|         0|            0|            0|  0.00%|                            '"cat": "cpu_to_cuda", '
   246|         0|            0|            0|  0.00%|                            '"args": {}}, ' % (evt.trace_name, evt.time_range.start,
   247|         0|            0|            0|  0.00%|                                               evt.thread, next_id))
   248|         0|            0|            0|  0.00%|                    # Note: use torch.profiler to get device kernel trace
   249|         0|            0|            0|  0.00%|                    next_id += 1
   250|         0|            0|            0|  0.00%|            if len(self) > 0:
   251|         0|            0|            0|  0.00%|                # remove trailing whitespace and comma
   252|         0|            0|            0|  0.00%|                f.seek(f.tell() - 2, os.SEEK_SET)
   253|         0|            0|            0|  0.00%|                f.truncate()
   254|         0|            0|            0|  0.00%|            f.write("]")
   255|         0|            0|            0|  0.00%|
   256|         0|            0|            0|  0.00%|    def supported_export_stacks_metrics(self):
   257|         0|            0|            0|  0.00%|        return ["self_cpu_time_total", "self_cuda_time_total"]
   258|         0|            0|            0|  0.00%|
   259|         0|            0|            0|  0.00%|    def export_stacks(self, path: str, metric: str):
   260|         0|            0|            0|  0.00%|        if metric not in self.supported_export_stacks_metrics():
   261|         0|            0|            0|  0.00%|            raise ValueError("metric should be one of: " + str(self.supported_export_stacks_metrics()))
   262|         0|            0|            0|  0.00%|        translate_table = str.maketrans(" ;\t\n", "____")
   263|         0|            0|            0|  0.00%|        with open(path, 'w') as f:
   264|         0|            0|            0|  0.00%|            for evt in self:
   265|         0|            0|            0|  0.00%|                if evt.stack and len(evt.stack) > 0:
   266|         0|            0|            0|  0.00%|                    metric_value = getattr(evt, metric)
   267|         0|            0|            0|  0.00%|                    if int(metric_value) > 0:
   268|         0|            0|            0|  0.00%|                        stack_str = ""
   269|         0|            0|            0|  0.00%|                        for entry in reversed(evt.stack):
   270|         0|            0|            0|  0.00%|                            stack_str += entry.translate(translate_table)
   271|         0|            0|            0|  0.00%|                            stack_str += ";"
   272|         0|            0|            0|  0.00%|                        stack_str = stack_str[:-1] + " " + str(int(metric_value))
   273|         0|            0|            0|  0.00%|                        f.write(stack_str + "\n")
   274|         0|            0|            0|  0.00%|
   275|         0|            0|            0|  0.00%|    def key_averages(self, group_by_input_shapes=False, group_by_stack_n=0):
   276|         0|            0|            0|  0.00%|        """Averages all function events over their keys.
   277|         0|            0|            0|  0.00%|
   278|         0|            0|            0|  0.00%|        Args:
   279|         0|            0|            0|  0.00%|            group_by_input_shapes: group entries by
   280|         0|            0|            0|  0.00%|                (event name, input shapes) rather than just event name.
   281|         0|            0|            0|  0.00%|                This is useful to see which input shapes contribute to the runtime
   282|         0|            0|            0|  0.00%|                the most and may help with size-specific optimizations or
   283|         0|            0|            0|  0.00%|                choosing the best candidates for quantization (aka fitting a roof line)
   284|         0|            0|            0|  0.00%|
   285|         0|            0|            0|  0.00%|            group_by_stack_n: group by top n stack trace entries
   286|         0|            0|            0|  0.00%|
   287|         0|            0|            0|  0.00%|        Returns:
   288|         0|            0|            0|  0.00%|            An EventList containing FunctionEventAvg objects.
   289|         0|            0|            0|  0.00%|        """
   290|         0|            0|            0|  0.00%|        assert self._tree_built
   291|         0|            0|            0|  0.00%|        stats: Dict[Tuple[str, ...], FunctionEventAvg] = defaultdict(FunctionEventAvg)
   292|         0|            0|            0|  0.00%|
   293|         0|            0|            0|  0.00%|        def get_key(event, group_by_input_shapes, group_by_stack_n) -> Tuple[str, ...]:
   294|         0|            0|            0|  0.00%|            key = [str(event.key), str(event.node_id), str(event.device_type), str(event.is_legacy)]
   295|         0|            0|            0|  0.00%|            if group_by_input_shapes:
   296|         0|            0|            0|  0.00%|                key.append(str(event.input_shapes))
   297|         0|            0|            0|  0.00%|            if group_by_stack_n > 0:
   298|         0|            0|            0|  0.00%|                key += event.stack[:group_by_stack_n]
   299|         0|            0|            0|  0.00%|            return tuple(key)
   300|         0|            0|            0|  0.00%|        for evt in self:
   301|         0|            0|            0|  0.00%|            stats[get_key(evt, group_by_input_shapes, group_by_stack_n)].add(evt)
   302|         0|            0|            0|  0.00%|
   303|         0|            0|            0|  0.00%|        avg_list = EventList(
   304|         0|            0|            0|  0.00%|            stats.values(),
   305|         0|            0|            0|  0.00%|            use_cuda=self._use_cuda,
   306|         0|            0|            0|  0.00%|            profile_memory=self._profile_memory,
   307|         0|            0|            0|  0.00%|            with_flops=self._with_flops)
   308|         0|            0|            0|  0.00%|        for evt in avg_list:
   309|         0|            0|            0|  0.00%|            evt.stack = evt.stack[:group_by_stack_n]
   310|         0|            0|            0|  0.00%|            if not group_by_input_shapes:
   311|         0|            0|            0|  0.00%|                evt.input_shapes = ""
   312|         0|            0|            0|  0.00%|        return avg_list
   313|         0|            0|            0|  0.00%|
   314|         0|            0|            0|  0.00%|    def total_average(self):
   315|         0|            0|            0|  0.00%|        """Averages all events.
   316|         0|            0|            0|  0.00%|
   317|         0|            0|            0|  0.00%|        Returns:
   318|         0|            0|            0|  0.00%|            A FunctionEventAvg object.
   319|         0|            0|            0|  0.00%|        """
   320|         0|            0|            0|  0.00%|        total_stat = FunctionEventAvg()
   321|         0|            0|            0|  0.00%|        for evt in self:
   322|         0|            0|            0|  0.00%|            total_stat += evt
   323|         0|            0|            0|  0.00%|            total_stat.key = None
   324|         0|            0|            0|  0.00%|        total_stat.key = 'Total'
   325|         0|            0|            0|  0.00%|        return total_stat
   326|         0|            0|            0|  0.00%|
   327|         0|            0|            0|  0.00%|
   328|         0|            0|            0|  0.00%|class profile(object):
   329|         0|            0|            0|  0.00%|    """Context manager that manages autograd profiler state and holds a summary of results.
   330|         0|            0|            0|  0.00%|    Under the hood it just records events of functions being executed in C++ and
   331|         0|            0|            0|  0.00%|    exposes those events to Python. You can wrap any code into it and it will
   332|         0|            0|            0|  0.00%|    only report runtime of PyTorch functions.
   333|         0|            0|            0|  0.00%|    Note: profiler is thread local and is automatically propagated into the async tasks
   334|         0|            0|            0|  0.00%|
   335|         0|            0|            0|  0.00%|    Args:
   336|         0|            0|            0|  0.00%|        enabled (bool, optional): Setting this to False makes this context manager a no-op.
   337|         0|            0|            0|  0.00%|
   338|         0|            0|            0|  0.00%|        use_cuda (bool, optional): Enables timing of CUDA events as well using the cudaEvent API.
   339|         0|            0|            0|  0.00%|            Adds approximately 4us of overhead to each tensor operation.
   340|         0|            0|            0|  0.00%|
   341|         0|            0|            0|  0.00%|        record_shapes (bool, optional): If shapes recording is set, information
   342|         0|            0|            0|  0.00%|            about input dimensions will be collected. This allows one to see which
   343|         0|            0|            0|  0.00%|            dimensions have been used under the hood and further group by them
   344|         0|            0|            0|  0.00%|            using prof.key_averages(group_by_input_shape=True). Please note that
   345|         0|            0|            0|  0.00%|            shape recording might skew your profiling data. It is recommended to
   346|         0|            0|            0|  0.00%|            use separate runs with and without shape recording to validate the timing.
   347|         0|            0|            0|  0.00%|            Most likely the skew will be negligible for bottom most events (in a case
   348|         0|            0|            0|  0.00%|            of nested function calls). But for higher level functions the total
   349|         0|            0|            0|  0.00%|            self cpu time might be artificially increased because of the shape
   350|         0|            0|            0|  0.00%|            collection.
   351|         0|            0|            0|  0.00%|
   352|         0|            0|            0|  0.00%|        with_flops (bool, optional): If with_flops is set, the profiler will estimate
   353|         0|            0|            0|  0.00%|            the FLOPS (floating pointer operations per second) value using the operator's input shape
   354|         0|            0|            0|  0.00%|            and total time. This allows one to estimate the hardware performance. Currently,
   355|         0|            0|            0|  0.00%|            this option only works for the matrix multiplication and 2D convolution operators.
   356|         0|            0|            0|  0.00%|
   357|         0|            0|            0|  0.00%|        profile_memory (bool, optional): track tensor memory allocation/deallocation.
   358|         0|            0|            0|  0.00%|
   359|         0|            0|            0|  0.00%|        with_stack (bool, optional): record source information (file and line number) for the ops.
   360|         0|            0|            0|  0.00%|
   361|         0|            0|            0|  0.00%|        use_kineto (bool, optional): experimental, enable profiling with Kineto profiler.
   362|         0|            0|            0|  0.00%|
   363|         0|            0|            0|  0.00%|        use_cpu (bool, optional): profile CPU events; setting to ``False`` requires
   364|         0|            0|            0|  0.00%|            ``use_kineto=True`` and can be used to lower the overhead for GPU-only profiling.
   365|         0|            0|            0|  0.00%|
   366|         0|            0|            0|  0.00%|    .. warning:
   367|         0|            0|            0|  0.00%|        Enabling memory profiling or source attribution incurs additional profiler
   368|         0|            0|            0|  0.00%|        overhead
   369|         0|            0|            0|  0.00%|
   370|         0|            0|            0|  0.00%|    .. warning:
   371|         0|            0|            0|  0.00%|        This context managers should not be called recursively, i.e. no nested
   372|         0|            0|            0|  0.00%|        instances are allowed
   373|         0|            0|            0|  0.00%|
   374|         0|            0|            0|  0.00%|    .. warning:
   375|         0|            0|            0|  0.00%|        Due to some CUDA multiprocessing limitations (multiprocessing-cuda-note_),
   376|         0|            0|            0|  0.00%|        one cannot use the profiler with ``use_cuda = True`` to benchmark
   377|         0|            0|            0|  0.00%|        DataLoaders with ``num_workers > 0``. If you wish to benchmark data loading,
   378|         0|            0|            0|  0.00%|        please use ``use_cuda = False`` or ``num_workers = 0``.
   379|         0|            0|            0|  0.00%|
   380|         0|            0|            0|  0.00%|    Example:
   381|         0|            0|            0|  0.00%|        >>> x = torch.randn((1, 1), requires_grad=True)
   382|         0|            0|            0|  0.00%|        >>> with torch.autograd.profiler.profile() as prof:
   383|         0|            0|            0|  0.00%|        >>>     for _ in range(100):  # any normal python code, really!
   384|         0|            0|            0|  0.00%|        >>>         y = x ** 2
   385|         0|            0|            0|  0.00%|        >>          y.backward()
   386|         0|            0|            0|  0.00%|        >>> # NOTE: some columns were removed for brevity
   387|         0|            0|            0|  0.00%|        >>> print(prof.key_averages().table(sort_by="self_cpu_time_total"))
   388|         0|            0|            0|  0.00%|        -----------------------------------  ---------------  ---------------  ---------------
   389|         0|            0|            0|  0.00%|        Name                                 Self CPU total   CPU time avg     Number of Calls
   390|         0|            0|            0|  0.00%|        -----------------------------------  ---------------  ---------------  ---------------
   391|         0|            0|            0|  0.00%|        mul                                  32.048ms         32.048ms         200
   392|         0|            0|            0|  0.00%|        pow                                  27.041ms         27.041ms         200
   393|         0|            0|            0|  0.00%|        PowBackward0                         9.727ms          55.483ms         100
   394|         0|            0|            0|  0.00%|        torch::autograd::AccumulateGrad      9.148ms          9.148ms          100
   395|         0|            0|            0|  0.00%|        torch::autograd::GraphRoot           691.816us        691.816us        100
   396|         0|            0|            0|  0.00%|        -----------------------------------  ---------------  ---------------  ---------------
   397|         0|            0|            0|  0.00%|
   398|         0|            0|            0|  0.00%|    """
   399|         0|            0|            0|  0.00%|    def __init__(
   400|         0|            0|            0|  0.00%|            self,
   401|         0|            0|            0|  0.00%|            enabled=True,
   402|         0|            0|            0|  0.00%|            *,
   403|         0|            0|            0|  0.00%|            use_cuda=False,
   404|         0|            0|            0|  0.00%|            record_shapes=False,
   405|         0|            0|            0|  0.00%|            with_flops=False,
   406|         0|            0|            0|  0.00%|            profile_memory=False,
   407|         0|            0|            0|  0.00%|            with_stack=False,
   408|         0|            0|            0|  0.00%|            use_kineto=False,
   409|         0|            0|            0|  0.00%|            use_cpu=True):
   410|         0|            0|            0|  0.00%|        self.enabled: bool = enabled
   411|         0|            0|            0|  0.00%|        if not self.enabled:
   412|         0|            0|            0|  0.00%|            return
   413|         0|            0|            0|  0.00%|        self.use_cuda = use_cuda
   414|         0|            0|            0|  0.00%|        self.function_events = None
   415|         0|            0|            0|  0.00%|        self.entered = False
   416|         0|            0|            0|  0.00%|        self.record_shapes = record_shapes
   417|         0|            0|            0|  0.00%|        self.with_flops = with_flops
   418|         0|            0|            0|  0.00%|        self.record_shapes |= self.with_flops
   419|         0|            0|            0|  0.00%|        self.profile_memory = profile_memory
   420|         0|            0|            0|  0.00%|        self.with_stack = with_stack
   421|         0|            0|            0|  0.00%|        self.use_cpu = use_cpu
   422|         0|            0|            0|  0.00%|        self.kineto_results = None
   423|         0|            0|            0|  0.00%|        if not self.use_cpu:
   424|         0|            0|            0|  0.00%|            assert use_kineto, \
   425|         0|            0|            0|  0.00%|                "Device-only events supported only with Kineto (use_kineto=True)"
   426|         0|            0|            0|  0.00%|        if self.use_cuda and not torch.cuda.is_available():
   427|         0|            0|            0|  0.00%|            warn("CUDA is not available, disabling CUDA profiling")
   428|         0|            0|            0|  0.00%|            self.use_cuda = False
   429|         0|            0|            0|  0.00%|
   430|         0|            0|            0|  0.00%|        self.profiler_kind = None
   431|         0|            0|            0|  0.00%|        self.kineto_activities = set()
   432|         0|            0|            0|  0.00%|        if use_kineto:
   433|         0|            0|            0|  0.00%|            if torch.autograd.kineto_available():
   434|         0|            0|            0|  0.00%|                if self.use_cpu:
   435|         0|            0|            0|  0.00%|                    self.kineto_activities.add(ProfilerActivity.CPU)
   436|         0|            0|            0|  0.00%|                use_gpu_fallback = False
   437|         0|            0|            0|  0.00%|                if self.use_cuda:
   438|         0|            0|            0|  0.00%|                    if (ProfilerActivity.CUDA not in
   439|         0|            0|            0|  0.00%|                            torch.autograd._supported_kineto_activities()):
   440|         0|            0|            0|  0.00%|                        warn("CUPTI tracing is not available, falling back to legacy CUDA profiling")
   441|         0|            0|            0|  0.00%|                        use_gpu_fallback = True
   442|         0|            0|            0|  0.00%|                    else:
   443|         0|            0|            0|  0.00%|                        self.kineto_activities.add(ProfilerActivity.CUDA)
   444|         0|            0|            0|  0.00%|                self.profiler_kind = ProfilerState.KINETO if not use_gpu_fallback else \
   445|         0|            0|            0|  0.00%|                    ProfilerState.KINETO_GPU_FALLBACK
   446|         0|            0|            0|  0.00%|
   447|         0|            0|            0|  0.00%|                assert len(self.kineto_activities) > 0, \
   448|         0|            0|            0|  0.00%|                    "No activities specified for Kineto profiler"
   449|         0|            0|            0|  0.00%|            else:
   450|         0|            0|            0|  0.00%|                warn("Kineto is not available, falling back to legacy profiler")
   451|         0|            0|            0|  0.00%|
   452|         0|            0|            0|  0.00%|        if not self.kineto_activities:
   453|         0|            0|            0|  0.00%|            if self.use_cuda:
   454|         0|            0|            0|  0.00%|                # legacy CUDA mode
   455|         0|            0|            0|  0.00%|                self.profiler_kind = ProfilerState.CUDA
   456|         0|            0|            0|  0.00%|            else:
   457|         0|            0|            0|  0.00%|                self.profiler_kind = ProfilerState.CPU
   458|         0|            0|            0|  0.00%|
   459|         0|            0|            0|  0.00%|    def config(self):
   460|         0|            0|            0|  0.00%|        assert self.profiler_kind is not None
   461|         0|            0|            0|  0.00%|        return torch.autograd.ProfilerConfig(
   462|         0|            0|            0|  0.00%|            self.profiler_kind,
   463|         0|            0|            0|  0.00%|            self.record_shapes,
   464|         0|            0|            0|  0.00%|            self.profile_memory,
   465|         0|            0|            0|  0.00%|            self.with_stack,
   466|         0|            0|            0|  0.00%|            self.with_flops)
   467|         0|            0|            0|  0.00%|
   468|         0|            0|            0|  0.00%|    def __enter__(self):
   469|         0|            0|            0|  0.00%|        if not self.enabled:
   470|         0|            0|            0|  0.00%|            return
   471|         0|            0|            0|  0.00%|        if self.entered:
   472|         0|            0|            0|  0.00%|            raise RuntimeError("profiler context manager is not reentrant")
   473|         0|            0|            0|  0.00%|        self._prepare_trace()
   474|         0|            0|            0|  0.00%|        self._start_trace()
   475|         0|            0|            0|  0.00%|        return self
   476|         0|            0|            0|  0.00%|
   477|         0|            0|            0|  0.00%|    def _prepare_trace(self):
   478|         0|            0|            0|  0.00%|        self.entered = True
   479|         0|            0|            0|  0.00%|        if self.kineto_activities:
   480|         0|            0|            0|  0.00%|            torch.autograd._prepare_profiler(self.config(), self.kineto_activities)
   481|         0|            0|            0|  0.00%|        else:
   482|         0|            0|            0|  0.00%|            # no-op in case of legacy profiler
   483|         0|            0|            0|  0.00%|            pass
   484|         0|            0|            0|  0.00%|
   485|         0|            0|            0|  0.00%|    def _start_trace(self):
   486|         0|            0|            0|  0.00%|        self.entered = True
   487|         0|            0|            0|  0.00%|        if self.kineto_activities:
   488|         0|            0|            0|  0.00%|            torch.autograd._enable_profiler(self.config(), self.kineto_activities)
   489|         0|            0|            0|  0.00%|        else:
   490|         0|            0|            0|  0.00%|            torch.autograd._enable_profiler_legacy(self.config())
   491|         0|            0|            0|  0.00%|
   492|         0|            0|            0|  0.00%|    def __exit__(self, exc_type, exc_val, exc_tb):
   493|         0|            0|            0|  0.00%|        if not self.enabled:
   494|         0|            0|            0|  0.00%|            return
   495|         0|            0|            0|  0.00%|        if self.use_cuda:
   496|         0|            0|            0|  0.00%|            torch.cuda.synchronize()
   497|         0|            0|            0|  0.00%|        if self.kineto_activities:
   498|         0|            0|            0|  0.00%|            self.kineto_results = torch.autograd._disable_profiler()
   499|         0|            0|            0|  0.00%|            parsed_results = parse_kineto_results(self.kineto_results)
   500|         0|            0|            0|  0.00%|        else:
   501|         0|            0|            0|  0.00%|            records = torch.autograd._disable_profiler_legacy()
   502|         0|            0|            0|  0.00%|            parsed_results = parse_legacy_records(records)
   503|         0|            0|            0|  0.00%|        self.function_events = EventList(
   504|         0|            0|            0|  0.00%|            parsed_results,
   505|         0|            0|            0|  0.00%|            use_cuda=self.use_cuda,
   506|         0|            0|            0|  0.00%|            profile_memory=self.profile_memory,
   507|         0|            0|            0|  0.00%|            with_flops=self.with_flops)
   508|         0|            0|            0|  0.00%|        self.function_events._build_tree()
   509|         0|            0|            0|  0.00%|        return False
   510|         0|            0|            0|  0.00%|
   511|         0|            0|            0|  0.00%|    def __repr__(self):
   512|         0|            0|            0|  0.00%|        if self.function_events is None:
   513|         0|            0|            0|  0.00%|            return '<unfinished torch.autograd.profile>'
   514|         0|            0|            0|  0.00%|        return repr(self.function_events)
   515|         0|            0|            0|  0.00%|
   516|         0|            0|            0|  0.00%|    def __str__(self):
   517|         0|            0|            0|  0.00%|        if self.function_events is None:
   518|         0|            0|            0|  0.00%|            return '<unfinished torch.autograd.profile>'
   519|         0|            0|            0|  0.00%|        return str(self.function_events)
   520|         0|            0|            0|  0.00%|
   521|         0|            0|            0|  0.00%|    def _check_finish(self):
   522|         0|            0|            0|  0.00%|        if self.function_events is None:
   523|         0|            0|            0|  0.00%|            raise RuntimeError("can't export a trace that didn't finish running")
   524|         0|            0|            0|  0.00%|
   525|         0|            0|            0|  0.00%|    def table(self, sort_by=None, row_limit=100, max_src_column_width=75, header=None, top_level_events_only=False):
   526|         0|            0|            0|  0.00%|        self._check_finish()
   527|         0|            0|            0|  0.00%|        assert self.function_events is not None
   528|         0|            0|            0|  0.00%|        return self.function_events.table(
   529|         0|            0|            0|  0.00%|            sort_by=sort_by, row_limit=row_limit, max_src_column_width=max_src_column_width, header=header,
   530|         0|            0|            0|  0.00%|            top_level_events_only=top_level_events_only
   531|         0|            0|            0|  0.00%|        )
   532|         0|            0|            0|  0.00%|    table.__doc__ = EventList.table.__doc__
   533|         0|            0|            0|  0.00%|
   534|         0|            0|            0|  0.00%|    def export_chrome_trace(self, path):
   535|         0|            0|            0|  0.00%|        self._check_finish()
   536|         0|            0|            0|  0.00%|        if self.kineto_results is not None:
   537|         0|            0|            0|  0.00%|            self.kineto_results.save(path)
   538|         0|            0|            0|  0.00%|        else:
   539|         0|            0|            0|  0.00%|            assert self.function_events is not None
   540|         0|            0|            0|  0.00%|            return self.function_events.export_chrome_trace(path)
   541|         0|            0|            0|  0.00%|    export_chrome_trace.__doc__ = EventList.export_chrome_trace.__doc__
   542|         0|            0|            0|  0.00%|
   543|         0|            0|            0|  0.00%|    def export_stacks(self, path: str, metric: str = "self_cpu_time_total"):
   544|         0|            0|            0|  0.00%|        self._check_finish()
   545|         0|            0|            0|  0.00%|        assert self.function_events is not None, "Expected profiling results"
   546|         0|            0|            0|  0.00%|        assert self.with_stack, "export_stacks() requires with_stack=True"
   547|         0|            0|            0|  0.00%|        return self.function_events.export_stacks(path, metric)
   548|         0|            0|            0|  0.00%|
   549|         0|            0|            0|  0.00%|    def key_averages(self, group_by_input_shape=False, group_by_stack_n=0):
   550|         0|            0|            0|  0.00%|        self._check_finish()
   551|         0|            0|            0|  0.00%|        assert self.function_events is not None, "Expected profiling results"
   552|         0|            0|            0|  0.00%|        return self.function_events.key_averages(group_by_input_shape, group_by_stack_n)
   553|         0|            0|            0|  0.00%|    key_averages.__doc__ = EventList.key_averages.__doc__
   554|         0|            0|            0|  0.00%|
   555|         0|            0|            0|  0.00%|    def total_average(self):
   556|         0|            0|            0|  0.00%|        self._check_finish()
   557|         0|            0|            0|  0.00%|        assert self.function_events is not None, "Expected profiling results"
   558|         0|            0|            0|  0.00%|        return self.function_events.total_average()
   559|         0|            0|            0|  0.00%|    total_average.__doc__ = EventList.total_average.__doc__
   560|         0|            0|            0|  0.00%|
   561|         0|            0|            0|  0.00%|    @property
   562|         0|            0|            0|  0.00%|    def self_cpu_time_total(self):
   563|         0|            0|            0|  0.00%|        """ Returns total time spent on CPU obtained as a sum of
   564|         0|            0|            0|  0.00%|        all self times across all the events.
   565|         0|            0|            0|  0.00%|        """
   566|         0|            0|            0|  0.00%|        self._check_finish()
   567|         0|            0|            0|  0.00%|        assert self.function_events is not None
   568|         0|            0|            0|  0.00%|        return self.function_events.self_cpu_time_total
   569|         0|            0|            0|  0.00%|
   570|         0|            0|            0|  0.00%|
   571|         0|            0|            0|  0.00%|class record_function(ContextDecorator):
   572|         0|            0|            0|  0.00%|    """Context manager/function decorator that adds a label to a block of
   573|         0|            0|            0|  0.00%|    Python code (or function) when running autograd profiler. It is
   574|         0|            0|            0|  0.00%|    useful when tracing the code profile.
   575|         0|            0|            0|  0.00%|
   576|         0|            0|            0|  0.00%|    Args:
   577|         0|            0|            0|  0.00%|        name (str): Label assigned to the block of code.
   578|         0|            0|            0|  0.00%|        node_id (int): ID of node, for distributed profiling. Unset in
   579|         0|            0|            0|  0.00%|        non-distributed cases.
   580|         0|            0|            0|  0.00%|
   581|         0|            0|            0|  0.00%|    Example:
   582|         0|            0|            0|  0.00%|        >>> x = torch.randn((1, 1), requires_grad=True)
   583|         0|            0|            0|  0.00%|        >>> with torch.autograd.profiler.profile() as prof:
   584|         0|            0|            0|  0.00%|        ...     y = x ** 2
   585|         0|            0|            0|  0.00%|        ...     with torch.autograd.profiler.record_function("label-z"): # label the block
   586|         0|            0|            0|  0.00%|        ...         z = y ** 3
   587|         0|            0|            0|  0.00%|        ...     y.backward()
   588|         0|            0|            0|  0.00%|        ...
   589|         0|            0|            0|  0.00%|        >>> # NOTE: some columns were removed for brevity
   590|         0|            0|            0|  0.00%|        >>> print(prof.key_averages().table(sort_by="self_cpu_time_total"))
   591|         0|            0|            0|  0.00%|        -----------------------------------  ---------------  ---------------  ---------------
   592|         0|            0|            0|  0.00%|        Name                                 Self CPU total %  CPU time avg     Number of Calls
   593|         0|            0|            0|  0.00%|        -----------------------------------  ---------------  ---------------  ---------------
   594|         0|            0|            0|  0.00%|        pow                                  60.77%           47.470us         3
   595|         0|            0|            0|  0.00%|        mul                                  21.73%           25.465us         2
   596|         0|            0|            0|  0.00%|        PowBackward0                         12.03%           121.891us        1
   597|         0|            0|            0|  0.00%|        torch::autograd::AccumulateGrad      2.70%            6.324us          1
   598|         0|            0|            0|  0.00%|        label-z                              2.13%            12.421us         1
   599|         0|            0|            0|  0.00%|        torch::autograd::GraphRoot           0.64%            1.503us          1
   600|         0|            0|            0|  0.00%|        -----------------------------------  ---------------  ---------------  ---------------
   601|         0|            0|            0|  0.00%|        Self CPU time total: 234.344us
   602|         0|            0|            0|  0.00%|        CUDA time total: 0.000us
   603|         0|            0|            0|  0.00%|
   604|         0|            0|            0|  0.00%|    """
   605|       263|   0.00150251|  5.71298e-06|  0.00%|    def __init__(self, name: str):
   606|       263|   0.00157309|  5.98131e-06|  0.00%|        self.name: str = name
   607|         0|            0|            0|  0.00%|        # Whether or not we should run record function's end callbacks when exiting.
   608|       263|   0.00117087|  4.45199e-06|  0.00%|        self.run_callbacks_on_exit: bool = True
   609|         0|            0|            0|  0.00%|        # Stores underlying RecordFunction as a tensor. TODO: move to custom
   610|         0|            0|            0|  0.00%|        # class (https://github.com/pytorch/pytorch/issues/35026).
   611|       263|    0.0135996|  5.17096e-05|  0.03%|        self.handle: torch.Tensor = torch.zeros(1)
   612|         0|            0|            0|  0.00%|
   613|       263|   0.00144315|  5.48725e-06|  0.00%|    def __enter__(self):
   614|       263|    0.0128641|   4.8913e-05|  0.02%|        self.handle = torch.ops.profiler._record_function_enter(self.name)
   615|       263|   0.00186372|  7.08638e-06|  0.00%|        return self
   616|         0|            0|            0|  0.00%|
   617|       263|   0.00166178|  6.31855e-06|  0.00%|    def __exit__(self, exc_type: Any, exc_value: Any, traceback: Any):
   618|       263|   0.00149179|  5.67219e-06|  0.00%|        if self.run_callbacks_on_exit:
   619|       263|   0.00942135|  3.58226e-05|  0.02%|            torch.ops.profiler._record_function_exit(self.handle)
   620|         0|            0|            0|  0.00%|
   621|         0|            0|            0|  0.00%|    def _call_end_callbacks_on_future(self, fut: Future[Any]) -> Future[Any]:
   622|         0|            0|            0|  0.00%|        """
   623|         0|            0|            0|  0.00%|        _call_end_callbacks_on_future is meant to be used for profiling async
   624|         0|            0|            0|  0.00%|        calls that return a future. Calling this function will extend recording
   625|         0|            0|            0|  0.00%|        beyond this scope, until the future is satisfied. It is useful for profiling
   626|         0|            0|            0|  0.00%|        the end to end time of asynchronous calls. This function should only be called
   627|         0|            0|            0|  0.00%|        once to attach the callback onto the future, and will throw if called multiple
   628|         0|            0|            0|  0.00%|        times.
   629|         0|            0|            0|  0.00%|
   630|         0|            0|            0|  0.00%|        Args:
   631|         0|            0|            0|  0.00%|            fut: (torch._C.Future): future for which to schedule
   632|         0|            0|            0|  0.00%|            callback for.
   633|         0|            0|            0|  0.00%|
   634|         0|            0|            0|  0.00%|        Returns:
   635|         0|            0|            0|  0.00%|            A future that completes with the value of the passed in future when
   636|         0|            0|            0|  0.00%|            the profiling callbacks have ran.
   637|         0|            0|            0|  0.00%|
   638|         0|            0|            0|  0.00%|        """
   639|         0|            0|            0|  0.00%|        # Throw if we have already attached a callback onto the future.
   640|         0|            0|            0|  0.00%|        if not self.run_callbacks_on_exit:
   641|         0|            0|            0|  0.00%|            raise RuntimeError("_call_end_callbacks_on_future can only be called once.")
   642|         0|            0|            0|  0.00%|
   643|         0|            0|            0|  0.00%|        # We are scheduling to run this RecordFunction's end callbacks when the
   644|         0|            0|            0|  0.00%|        # passed in future completes, so don't run end callbacks on exit.
   645|         0|            0|            0|  0.00%|        self.run_callbacks_on_exit = False
   646|         0|            0|            0|  0.00%|        profiled_future = torch.ops.profiler._call_end_callbacks_on_jit_fut(self.handle, fut)
   647|         0|            0|            0|  0.00%|        return profiled_future
   648|         0|            0|            0|  0.00%|
   649|         0|            0|            0|  0.00%|
   650|         0|            0|            0|  0.00%|class emit_nvtx(object):
   651|         0|            0|            0|  0.00%|    """Context manager that makes every autograd operation emit an NVTX range.
   652|         0|            0|            0|  0.00%|
   653|         0|            0|            0|  0.00%|    It is useful when running the program under nvprof::
   654|         0|            0|            0|  0.00%|
   655|         0|            0|            0|  0.00%|        nvprof --profile-from-start off -o trace_name.prof -- <regular command here>
   656|         0|            0|            0|  0.00%|
   657|         0|            0|            0|  0.00%|    Unfortunately, there's no way to force nvprof to flush the data it collected
   658|         0|            0|            0|  0.00%|    to disk, so for CUDA profiling one has to use this context manager to annotate
   659|         0|            0|            0|  0.00%|    nvprof traces and wait for the process to exit before inspecting them.
   660|         0|            0|            0|  0.00%|    Then, either NVIDIA Visual Profiler (nvvp) can be used to visualize the timeline, or
   661|         0|            0|            0|  0.00%|    :func:`torch.autograd.profiler.load_nvprof` can load the results for inspection
   662|         0|            0|            0|  0.00%|    e.g. in Python REPL.
   663|         0|            0|            0|  0.00%|
   664|         0|            0|            0|  0.00%|    .. warning:
   665|         0|            0|            0|  0.00%|        This context manager should not be called recursively, i.e. at most one
   666|         0|            0|            0|  0.00%|        instance should be enabled at any given time.
   667|         0|            0|            0|  0.00%|
   668|         0|            0|            0|  0.00%|    Args:
   669|         0|            0|            0|  0.00%|        enabled (bool, optional, default=True): Setting ``enabled=False`` makes this context manager a no-op.
   670|         0|            0|            0|  0.00%|            Default: ``True``.
   671|         0|            0|            0|  0.00%|        record_shapes (bool, optional, default=False): If ``record_shapes=True``, the nvtx range wrapping
   672|         0|            0|            0|  0.00%|            each autograd op will append information about the sizes of Tensor arguments received
   673|         0|            0|            0|  0.00%|            by that op, in the following format:
   674|         0|            0|            0|  0.00%|            ``[[arg0.size(0), arg0.size(1), ...], [arg1.size(0), arg1.size(1), ...], ...]``
   675|         0|            0|            0|  0.00%|            Non-tensor arguments will be represented by ``[]``.
   676|         0|            0|            0|  0.00%|            Arguments will be listed in the order they are received by the backend op.
   677|         0|            0|            0|  0.00%|            Please note that this order may not match the order in which those arguments were passed
   678|         0|            0|            0|  0.00%|            on the Python side.  Also note that shape recording may increase the overhead of nvtx range creation.
   679|         0|            0|            0|  0.00%|
   680|         0|            0|            0|  0.00%|    Example:
   681|         0|            0|            0|  0.00%|        >>> with torch.cuda.profiler.profile():
   682|         0|            0|            0|  0.00%|        ...     model(x) # Warmup CUDA memory allocator and profiler
   683|         0|            0|            0|  0.00%|        ...     with torch.autograd.profiler.emit_nvtx():
   684|         0|            0|            0|  0.00%|        ...         model(x)
   685|         0|            0|            0|  0.00%|
   686|         0|            0|            0|  0.00%|    **Forward-backward correlation**
   687|         0|            0|            0|  0.00%|
   688|         0|            0|            0|  0.00%|    When viewing a profile created using :class:`emit_nvtx` in the Nvidia Visual Profiler,
   689|         0|            0|            0|  0.00%|    correlating each backward-pass op with the corresponding forward-pass op can be difficult.
   690|         0|            0|            0|  0.00%|    To ease this task, :class:`emit_nvtx` appends sequence number information to the ranges it
   691|         0|            0|            0|  0.00%|    generates.
   692|         0|            0|            0|  0.00%|
   693|         0|            0|            0|  0.00%|    During the forward pass, each function range is decorated with ``seq=<N>``.  ``seq`` is a running
   694|         0|            0|            0|  0.00%|    counter, incremented each time a new backward Function object is created and stashed for backward.
   695|         0|            0|            0|  0.00%|    Thus, the ``seq=<N>`` annotation associated with each forward function range tells you that
   696|         0|            0|            0|  0.00%|    if a backward Function object is created by this forward function,
   697|         0|            0|            0|  0.00%|    the backward object will receive sequence number N.
   698|         0|            0|            0|  0.00%|    During the backward pass, the top-level range wrapping each C++ backward Function's
   699|         0|            0|            0|  0.00%|    ``apply()`` call is decorated with ``stashed seq=<M>``.  ``M`` is the sequence number that
   700|         0|            0|            0|  0.00%|    the backward object was created with.  By comparing ``stashed seq`` numbers in backward with ``seq``
   701|         0|            0|            0|  0.00%|    numbers in forward, you can track down which forward op created each backward Function.
   702|         0|            0|            0|  0.00%|
   703|         0|            0|            0|  0.00%|    Any functions executed during the backward pass are also decorated with ``seq=<N>``.  During
   704|         0|            0|            0|  0.00%|    default backward (with ``create_graph=False``) this information is irrelevant, and in fact,
   705|         0|            0|            0|  0.00%|    ``N`` may simply be 0 for all such functions.  Only the top-level ranges associated with
   706|         0|            0|            0|  0.00%|    backward Function objects' ``apply()`` methods are useful, as a way to correlate these Function
   707|         0|            0|            0|  0.00%|    objects with the earlier forward pass.
   708|         0|            0|            0|  0.00%|
   709|         0|            0|            0|  0.00%|    **Double-backward**
   710|         0|            0|            0|  0.00%|
   711|         0|            0|            0|  0.00%|    If, on the other hand, a backward pass with ``create_graph=True`` is underway (in other words,
   712|         0|            0|            0|  0.00%|    if you are setting up for a double-backward), each function's execution during backward
   713|         0|            0|            0|  0.00%|    is given a nonzero, useful ``seq=<N>``.  Those functions may themselves create Function objects
   714|         0|            0|            0|  0.00%|    to be executed later during double-backward, just as the original functions in the forward pass did.
   715|         0|            0|            0|  0.00%|    The relationship between backward and double-backward is conceptually the same as the relationship
   716|         0|            0|            0|  0.00%|    between forward and backward: The functions still emit current-sequence-number-tagged ranges,
   717|         0|            0|            0|  0.00%|    the Function objects they create still stash those sequence numbers, and during the eventual
   718|         0|            0|            0|  0.00%|    double-backward, the Function objects' ``apply()`` ranges are still tagged with ``stashed seq``
   719|         0|            0|            0|  0.00%|    numbers, which can be compared to `seq` numbers from the backward pass.
   720|         0|            0|            0|  0.00%|
   721|         0|            0|            0|  0.00%|    .. warning:
   722|         0|            0|            0|  0.00%|        The sequence number is thread-local, and some forward functions don't create an associated
   723|         0|            0|            0|  0.00%|        backward Function object (instead delegating that to sub-functions further down the call chain).
   724|         0|            0|            0|  0.00%|        For these reasons, the correspondence of stashed sequence numbers in
   725|         0|            0|            0|  0.00%|        backward Function ``apply()`` ranges with `seq` numbers in forward-pass ranges is
   726|         0|            0|            0|  0.00%|        not guaranteed to be 1 to 1.  The sequence numbers alone may not be enough to fully
   727|         0|            0|            0|  0.00%|        disambiguate which forward function created which
   728|         0|            0|            0|  0.00%|        backward Function object.  You may need to make a judgment based on analytic knowledge of what
   729|         0|            0|            0|  0.00%|        the expected correspondence should be.
   730|         0|            0|            0|  0.00%|    """
   731|         0|            0|            0|  0.00%|    def __init__(self, enabled=True, record_shapes=False):
   732|         0|            0|            0|  0.00%|        self.enabled = enabled
   733|         0|            0|            0|  0.00%|        self.entered = False
   734|         0|            0|            0|  0.00%|        self.record_shapes = record_shapes
   735|         0|            0|            0|  0.00%|
   736|         0|            0|            0|  0.00%|    def __enter__(self):
   737|         0|            0|            0|  0.00%|        if not self.enabled:
   738|         0|            0|            0|  0.00%|            return
   739|         0|            0|            0|  0.00%|        if self.entered:
   740|         0|            0|            0|  0.00%|            raise RuntimeError("NVTX annotation context manager is not reentrant")
   741|         0|            0|            0|  0.00%|        self.entered = True
   742|         0|            0|            0|  0.00%|        torch.cuda.synchronize()
   743|         0|            0|            0|  0.00%|        torch.autograd._enable_profiler_legacy(
   744|         0|            0|            0|  0.00%|            torch.autograd.ProfilerConfig(
   745|         0|            0|            0|  0.00%|                ProfilerState.NVTX,
   746|         0|            0|            0|  0.00%|                self.record_shapes,
   747|         0|            0|            0|  0.00%|                False,
   748|         0|            0|            0|  0.00%|                False,
   749|         0|            0|            0|  0.00%|                False)
   750|         0|            0|            0|  0.00%|        )
   751|         0|            0|            0|  0.00%|        return self
   752|         0|            0|            0|  0.00%|
   753|         0|            0|            0|  0.00%|    def __exit__(self, exc_type, exc_val, exc_tb):
   754|         0|            0|            0|  0.00%|        if not self.enabled:
   755|         0|            0|            0|  0.00%|            return
   756|         0|            0|            0|  0.00%|        torch.cuda.synchronize()
   757|         0|            0|            0|  0.00%|        torch.autograd._disable_profiler_legacy()
   758|         0|            0|            0|  0.00%|        return False
   759|         0|            0|            0|  0.00%|
   760|         0|            0|            0|  0.00%|
   761|         0|            0|            0|  0.00%|def load_nvprof(path):
   762|         0|            0|            0|  0.00%|    """Opens an nvprof trace file and parses autograd annotations.
   763|         0|            0|            0|  0.00%|
   764|         0|            0|            0|  0.00%|    Args:
   765|         0|            0|            0|  0.00%|        path (str): path to nvprof trace
   766|         0|            0|            0|  0.00%|    """
   767|         0|            0|            0|  0.00%|    return EventList(parse_nvprof_trace(path))
   768|         0|            0|            0|  0.00%|
   769|         0|            0|            0|  0.00%|
   770|         0|            0|            0|  0.00%|################################################################################
   771|         0|            0|            0|  0.00%|# FunctionEvent
   772|         0|            0|            0|  0.00%|
   773|         0|            0|            0|  0.00%|def format_time(time_us):
   774|         0|            0|            0|  0.00%|    """Defines how to format time in FunctionEvent"""
   775|         0|            0|            0|  0.00%|    US_IN_SECOND = 1000.0 * 1000.0
   776|         0|            0|            0|  0.00%|    US_IN_MS = 1000.0
   777|         0|            0|            0|  0.00%|    if time_us >= US_IN_SECOND:
   778|         0|            0|            0|  0.00%|        return '{:.3f}s'.format(time_us / US_IN_SECOND)
   779|         0|            0|            0|  0.00%|    if time_us >= US_IN_MS:
   780|         0|            0|            0|  0.00%|        return '{:.3f}ms'.format(time_us / US_IN_MS)
   781|         0|            0|            0|  0.00%|    return '{:.3f}us'.format(time_us)
   782|         0|            0|            0|  0.00%|
   783|         0|            0|            0|  0.00%|
   784|         0|            0|            0|  0.00%|def format_time_share(time_us, total_time_us):
   785|         0|            0|            0|  0.00%|    """Defines how to format time in FunctionEvent"""
   786|         0|            0|            0|  0.00%|    if total_time_us == 0:
   787|         0|            0|            0|  0.00%|        assert time_us == 0, "Expected time_us == 0 but got {}".format(time_us)
   788|         0|            0|            0|  0.00%|        return "NaN"
   789|         0|            0|            0|  0.00%|    return '{:.2f}%'.format(time_us * 100.0 / total_time_us)
   790|         0|            0|            0|  0.00%|
   791|         0|            0|            0|  0.00%|def format_memory(nbytes):
   792|         0|            0|            0|  0.00%|    """Returns a formatted memory size string"""
   793|         0|            0|            0|  0.00%|    KB = 1024
   794|         0|            0|            0|  0.00%|    MB = 1024 * KB
   795|         0|            0|            0|  0.00%|    GB = 1024 * MB
   796|         0|            0|            0|  0.00%|    if (abs(nbytes) >= GB):
   797|         0|            0|            0|  0.00%|        return '{:.2f} Gb'.format(nbytes * 1.0 / GB)
   798|         0|            0|            0|  0.00%|    elif (abs(nbytes) >= MB):
   799|         0|            0|            0|  0.00%|        return '{:.2f} Mb'.format(nbytes * 1.0 / MB)
   800|         0|            0|            0|  0.00%|    elif (abs(nbytes) >= KB):
   801|         0|            0|            0|  0.00%|        return '{:.2f} Kb'.format(nbytes * 1.0 / KB)
   802|         0|            0|            0|  0.00%|    else:
   803|         0|            0|            0|  0.00%|        return str(nbytes) + ' b'
   804|         0|            0|            0|  0.00%|
   805|         0|            0|            0|  0.00%|def attr_formatter(name):
   806|         0|            0|            0|  0.00%|    return property(lambda self: format_time(getattr(self, name)))
   807|         0|            0|            0|  0.00%|
   808|         0|            0|            0|  0.00%|
   809|         0|            0|            0|  0.00%|class FormattedTimesMixin(object):
   810|         0|            0|            0|  0.00%|    """Helpers for FunctionEvent and FunctionEventAvg.
   811|         0|            0|            0|  0.00%|
   812|         0|            0|            0|  0.00%|    The subclass should define `*_time_total` and `count` attributes.
   813|         0|            0|            0|  0.00%|    """
   814|         0|            0|            0|  0.00%|    cpu_time_str = attr_formatter('cpu_time')
   815|         0|            0|            0|  0.00%|    cuda_time_str = attr_formatter('cuda_time')
   816|         0|            0|            0|  0.00%|    cpu_time_total_str = attr_formatter('cpu_time_total')
   817|         0|            0|            0|  0.00%|    cuda_time_total_str = attr_formatter('cuda_time_total')
   818|         0|            0|            0|  0.00%|    self_cpu_time_total_str = attr_formatter('self_cpu_time_total')
   819|         0|            0|            0|  0.00%|    self_cuda_time_total_str = attr_formatter('self_cuda_time_total')
   820|         0|            0|            0|  0.00%|
   821|         0|            0|            0|  0.00%|    @property
   822|         0|            0|            0|  0.00%|    def cpu_time(self):
   823|         0|            0|            0|  0.00%|        return 0.0 if self.count == 0 else 1.0 * self.cpu_time_total / self.count  # type: ignore[attr-defined]
   824|         0|            0|            0|  0.00%|
   825|         0|            0|            0|  0.00%|    @property
   826|         0|            0|            0|  0.00%|    def cuda_time(self):
   827|         0|            0|            0|  0.00%|        return 0.0 if self.count == 0 else 1.0 * self.cuda_time_total / self.count  # type: ignore[attr-defined]
   828|         0|            0|            0|  0.00%|
   829|         0|            0|            0|  0.00%|
   830|         0|            0|            0|  0.00%|class Interval(object):
   831|         0|            0|            0|  0.00%|    def __init__(self, start, end):
   832|         0|            0|            0|  0.00%|        self.start = start
   833|         0|            0|            0|  0.00%|        self.end = end
   834|         0|            0|            0|  0.00%|
   835|         0|            0|            0|  0.00%|    def elapsed_us(self):
   836|         0|            0|            0|  0.00%|        return self.end - self.start
   837|         0|            0|            0|  0.00%|
   838|         0|            0|            0|  0.00%|
   839|         0|            0|            0|  0.00%|Kernel = namedtuple('Kernel', ['name', 'device', 'duration'])
   840|         0|            0|            0|  0.00%|
   841|         0|            0|            0|  0.00%|
   842|         0|            0|            0|  0.00%|class FunctionEvent(FormattedTimesMixin):
   843|         0|            0|            0|  0.00%|    """Profiling information about a single function."""
   844|         0|            0|            0|  0.00%|    def __init__(
   845|         0|            0|            0|  0.00%|            self, id, name, thread, start_us, end_us, fwd_thread=None, input_shapes=None,
   846|         0|            0|            0|  0.00%|            stack=None, scope=0, cpu_memory_usage=0, cuda_memory_usage=0, is_async=False,
   847|         0|            0|            0|  0.00%|            is_remote=False, sequence_nr=-1, node_id=-1, device_type=DeviceType.CPU, device_index=0,
   848|         0|            0|            0|  0.00%|            is_legacy=False, flops=None, trace_name=None):
   849|         0|            0|            0|  0.00%|        self.id: int = id
   850|         0|            0|            0|  0.00%|        self.node_id: int = node_id
   851|         0|            0|            0|  0.00%|        self.name: str = name
   852|         0|            0|            0|  0.00%|        self.trace_name: str = trace_name
   853|         0|            0|            0|  0.00%|        self.time_range: Interval = Interval(start_us, end_us)
   854|         0|            0|            0|  0.00%|        self.thread: int = thread
   855|         0|            0|            0|  0.00%|        self.fwd_thread: Optional[int] = fwd_thread
   856|         0|            0|            0|  0.00%|        self.kernels: List[Kernel] = []
   857|         0|            0|            0|  0.00%|        self.count: int = 1
   858|         0|            0|            0|  0.00%|        self.cpu_children: List[FunctionEvent] = []
   859|         0|            0|            0|  0.00%|        self.cpu_parent: Optional[FunctionEvent] = None
   860|         0|            0|            0|  0.00%|        self.input_shapes: Tuple[int, ...] = input_shapes
   861|         0|            0|            0|  0.00%|        self.stack: List = stack
   862|         0|            0|            0|  0.00%|        self.scope: int = scope
   863|         0|            0|            0|  0.00%|        self.cpu_memory_usage: int = cpu_memory_usage
   864|         0|            0|            0|  0.00%|        self.cuda_memory_usage: int = cuda_memory_usage
   865|         0|            0|            0|  0.00%|        self.is_async: bool = is_async
   866|         0|            0|            0|  0.00%|        self.is_remote: bool = is_remote
   867|         0|            0|            0|  0.00%|        self.sequence_nr: int = sequence_nr
   868|         0|            0|            0|  0.00%|        self.device_type: DeviceType = device_type
   869|         0|            0|            0|  0.00%|        self.device_index: int = device_index
   870|         0|            0|            0|  0.00%|        self.is_legacy: bool = is_legacy
   871|         0|            0|            0|  0.00%|        self.flops: Optional[float] = flops
   872|         0|            0|            0|  0.00%|
   873|         0|            0|            0|  0.00%|    def append_kernel(self, name, device, duration):
   874|         0|            0|            0|  0.00%|        assert self.device_type == DeviceType.CPU
   875|         0|            0|            0|  0.00%|        self.kernels.append(Kernel(name, device, duration))
   876|         0|            0|            0|  0.00%|
   877|         0|            0|            0|  0.00%|    def append_cpu_child(self, child):
   878|         0|            0|            0|  0.00%|        """Append a CPU child of type FunctionEvent.
   879|         0|            0|            0|  0.00%|
   880|         0|            0|            0|  0.00%|        One is supposed to append only direct children to the event to have
   881|         0|            0|            0|  0.00%|        correct self cpu time being reported.
   882|         0|            0|            0|  0.00%|        """
   883|         0|            0|            0|  0.00%|        assert(self.device_type == DeviceType.CPU)
   884|         0|            0|            0|  0.00%|        assert(isinstance(child, FunctionEvent))
   885|         0|            0|            0|  0.00%|        assert(child.device_type == DeviceType.CPU)
   886|         0|            0|            0|  0.00%|        self.cpu_children.append(child)
   887|         0|            0|            0|  0.00%|
   888|         0|            0|            0|  0.00%|    def set_cpu_parent(self, parent):
   889|         0|            0|            0|  0.00%|        """Set the immediate CPU parent of type FunctionEvent
   890|         0|            0|            0|  0.00%|
   891|         0|            0|            0|  0.00%|        One profiling FunctionEvent should have only one CPU parent such that
   892|         0|            0|            0|  0.00%|        the child's range interval is completely inside the parent's. We use
   893|         0|            0|            0|  0.00%|        this connection to determine the event is from top-level op or not.
   894|         0|            0|            0|  0.00%|        """
   895|         0|            0|            0|  0.00%|        assert(self.device_type == DeviceType.CPU)
   896|         0|            0|            0|  0.00%|        assert(isinstance(parent, FunctionEvent))
   897|         0|            0|            0|  0.00%|        assert(parent.device_type == DeviceType.CPU)
   898|         0|            0|            0|  0.00%|        self.cpu_parent = parent
   899|         0|            0|            0|  0.00%|
   900|         0|            0|            0|  0.00%|    # Note: async events don't have children, are not used when computing 'self'
   901|         0|            0|            0|  0.00%|    # metrics of other events, have only total cpu time
   902|         0|            0|            0|  0.00%|    @property
   903|         0|            0|            0|  0.00%|    def self_cpu_memory_usage(self):
   904|         0|            0|            0|  0.00%|        if self.is_async or self.device_type != DeviceType.CPU:
   905|         0|            0|            0|  0.00%|            return 0
   906|         0|            0|            0|  0.00%|        return self.cpu_memory_usage - sum(
   907|         0|            0|            0|  0.00%|            [child.cpu_memory_usage for child in self.cpu_children]
   908|         0|            0|            0|  0.00%|        )
   909|         0|            0|            0|  0.00%|
   910|         0|            0|            0|  0.00%|    @property
   911|         0|            0|            0|  0.00%|    def self_cuda_memory_usage(self):
   912|         0|            0|            0|  0.00%|        if self.is_async or self.device_type != DeviceType.CPU:
   913|         0|            0|            0|  0.00%|            return 0
   914|         0|            0|            0|  0.00%|        return self.cuda_memory_usage - sum(
   915|         0|            0|            0|  0.00%|            [child.cuda_memory_usage for child in self.cpu_children]
   916|         0|            0|            0|  0.00%|        )
   917|         0|            0|            0|  0.00%|
   918|         0|            0|            0|  0.00%|    @property
   919|         0|            0|            0|  0.00%|    def self_cpu_time_total(self):
   920|         0|            0|            0|  0.00%|        if self.is_async or self.device_type != DeviceType.CPU:
   921|         0|            0|            0|  0.00%|            return 0
   922|         0|            0|            0|  0.00%|        return self.cpu_time_total - sum(
   923|         0|            0|            0|  0.00%|            [child.cpu_time_total for child in self.cpu_children]
   924|         0|            0|            0|  0.00%|        )
   925|         0|            0|            0|  0.00%|
   926|         0|            0|            0|  0.00%|    @property
   927|         0|            0|            0|  0.00%|    def cuda_time_total(self):
   928|         0|            0|            0|  0.00%|        if self.is_async:
   929|         0|            0|            0|  0.00%|            return 0
   930|         0|            0|            0|  0.00%|        if self.device_type == DeviceType.CPU:
   931|         0|            0|            0|  0.00%|            if not self.is_legacy:
   932|         0|            0|            0|  0.00%|                # account for the kernels in the children ops
   933|         0|            0|            0|  0.00%|                return (sum(kinfo.duration for kinfo in self.kernels) +
   934|         0|            0|            0|  0.00%|                        sum(ch.cuda_time_total for ch in self.cpu_children))
   935|         0|            0|            0|  0.00%|            else:
   936|         0|            0|            0|  0.00%|                # each legacy cpu events has a single (fake) kernel
   937|         0|            0|            0|  0.00%|                return sum(kinfo.duration for kinfo in self.kernels)
   938|         0|            0|            0|  0.00%|        else:
   939|         0|            0|            0|  0.00%|            assert self.device_type == DeviceType.CUDA
   940|         0|            0|            0|  0.00%|            return self.time_range.elapsed_us()
   941|         0|            0|            0|  0.00%|
   942|         0|            0|            0|  0.00%|    @property
   943|         0|            0|            0|  0.00%|    def self_cuda_time_total(self):
   944|         0|            0|            0|  0.00%|        if self.is_async:
   945|         0|            0|            0|  0.00%|            return 0
   946|         0|            0|            0|  0.00%|        if self.device_type == DeviceType.CPU:
   947|         0|            0|            0|  0.00%|            return self.cuda_time_total - \
   948|         0|            0|            0|  0.00%|                sum([child.cuda_time_total for child in self.cpu_children])
   949|         0|            0|            0|  0.00%|        else:
   950|         0|            0|            0|  0.00%|            assert(self.device_type == DeviceType.CUDA)
   951|         0|            0|            0|  0.00%|            return self.cuda_time_total
   952|         0|            0|            0|  0.00%|
   953|         0|            0|            0|  0.00%|    @property
   954|         0|            0|            0|  0.00%|    def cpu_time_total(self):
   955|         0|            0|            0|  0.00%|        if self.device_type == DeviceType.CPU:
   956|         0|            0|            0|  0.00%|            return self.time_range.elapsed_us()
   957|         0|            0|            0|  0.00%|        else:
   958|         0|            0|            0|  0.00%|            return 0
   959|         0|            0|            0|  0.00%|
   960|         0|            0|            0|  0.00%|    @property
   961|         0|            0|            0|  0.00%|    def key(self):
   962|         0|            0|            0|  0.00%|        return self.name
   963|         0|            0|            0|  0.00%|
   964|         0|            0|            0|  0.00%|    def __repr__(self):
   965|         0|            0|            0|  0.00%|        return (
   966|         0|            0|            0|  0.00%|            '<FunctionEvent id={} name={} device_type={} node_id={} cpu_time={} start_us={} end_us={} '
   967|         0|            0|            0|  0.00%|            'cpu_children={} cuda_time={} name={} thread={} input_shapes={} '
   968|         0|            0|            0|  0.00%|            'cpu_memory_usage={} cuda_memory_usage={} is_async={} is_remote={} seq_nr={} is_legacy={}>'.format(
   969|         0|            0|            0|  0.00%|                self.id,
   970|         0|            0|            0|  0.00%|                self.name,
   971|         0|            0|            0|  0.00%|                self.device_type,
   972|         0|            0|            0|  0.00%|                self.node_id,
   973|         0|            0|            0|  0.00%|                self.cpu_time_str,
   974|         0|            0|            0|  0.00%|                self.time_range.start,
   975|         0|            0|            0|  0.00%|                self.time_range.end,
   976|         0|            0|            0|  0.00%|                str([child.id for child in self.cpu_children]),
   977|         0|            0|            0|  0.00%|                self.cuda_time_str,
   978|         0|            0|            0|  0.00%|                self.name,
   979|         0|            0|            0|  0.00%|                self.thread,
   980|         0|            0|            0|  0.00%|                str(self.input_shapes),
   981|         0|            0|            0|  0.00%|                self.cpu_memory_usage,
   982|         0|            0|            0|  0.00%|                self.cuda_memory_usage,
   983|         0|            0|            0|  0.00%|                self.is_async,
   984|         0|            0|            0|  0.00%|                self.is_remote,
   985|         0|            0|            0|  0.00%|                self.sequence_nr,
   986|         0|            0|            0|  0.00%|                self.is_legacy,
   987|         0|            0|            0|  0.00%|            )
   988|         0|            0|            0|  0.00%|        )
   989|         0|            0|            0|  0.00%|
   990|         0|            0|            0|  0.00%|
   991|         0|            0|            0|  0.00%|class FunctionEventAvg(FormattedTimesMixin):
   992|         0|            0|            0|  0.00%|    """Used to average stats over multiple FunctionEvent objects."""
   993|         0|            0|            0|  0.00%|    def __init__(self):
   994|         0|            0|            0|  0.00%|        self.key: Optional[str] = None
   995|         0|            0|            0|  0.00%|        self.count: int = 0
   996|         0|            0|            0|  0.00%|        self.node_id: int = 0
   997|         0|            0|            0|  0.00%|        self.is_async: bool = False
   998|         0|            0|            0|  0.00%|        self.is_remote: bool = False
   999|         0|            0|            0|  0.00%|        self.cpu_time_total: int = 0
  1000|         0|            0|            0|  0.00%|        self.cuda_time_total: int = 0
  1001|         0|            0|            0|  0.00%|        self.self_cpu_time_total: int = 0
  1002|         0|            0|            0|  0.00%|        self.self_cuda_time_total: int = 0
  1003|         0|            0|            0|  0.00%|        self.input_shapes: Optional[List[List[int]]] = None
  1004|         0|            0|            0|  0.00%|        self.stack: Optional[List] = None
  1005|         0|            0|            0|  0.00%|        self.scope: Optional[int] = None
  1006|         0|            0|            0|  0.00%|        self.cpu_memory_usage: int = 0
  1007|         0|            0|            0|  0.00%|        self.cuda_memory_usage: int = 0
  1008|         0|            0|            0|  0.00%|        self.self_cpu_memory_usage: int = 0
  1009|         0|            0|            0|  0.00%|        self.self_cuda_memory_usage: int = 0
  1010|         0|            0|            0|  0.00%|        self.cpu_children: Optional[List[FunctionEvent]] = None
  1011|         0|            0|            0|  0.00%|        self.cpu_parent: Optional[FunctionEvent] = None
  1012|         0|            0|            0|  0.00%|        self.device_type: DeviceType = DeviceType.CPU
  1013|         0|            0|            0|  0.00%|        self.is_legacy: bool = False
  1014|         0|            0|            0|  0.00%|        self.flops: float = 0.0
  1015|         0|            0|            0|  0.00%|
  1016|         0|            0|            0|  0.00%|    def add(self, other):
  1017|         0|            0|            0|  0.00%|        if self.key is None:
  1018|         0|            0|            0|  0.00%|            # First function being recorded as part of FunctionEventAvg, propagate
  1019|         0|            0|            0|  0.00%|            # fields.
  1020|         0|            0|            0|  0.00%|            self.key = other.key
  1021|         0|            0|            0|  0.00%|            self.node_id = other.node_id
  1022|         0|            0|            0|  0.00%|            self.is_async = other.is_async
  1023|         0|            0|            0|  0.00%|            self.is_remote = other.is_remote
  1024|         0|            0|            0|  0.00%|            self.cpu_parent = other.cpu_parent
  1025|         0|            0|            0|  0.00%|            self.cpu_children = other.cpu_children
  1026|         0|            0|            0|  0.00%|
  1027|         0|            0|            0|  0.00%|            self.input_shapes = other.input_shapes
  1028|         0|            0|            0|  0.00%|            self.stack = other.stack
  1029|         0|            0|            0|  0.00%|            self.scope = other.scope
  1030|         0|            0|            0|  0.00%|            self.device_type = other.device_type
  1031|         0|            0|            0|  0.00%|            self.is_legacy = other.is_legacy
  1032|         0|            0|            0|  0.00%|
  1033|         0|            0|            0|  0.00%|        assert isinstance(other, (FunctionEvent, FunctionEventAvg))
  1034|         0|            0|            0|  0.00%|        assert other.key == self.key
  1035|         0|            0|            0|  0.00%|        self.cpu_time_total += other.cpu_time_total
  1036|         0|            0|            0|  0.00%|        self.cuda_time_total += other.cuda_time_total
  1037|         0|            0|            0|  0.00%|        self.self_cpu_time_total += other.self_cpu_time_total
  1038|         0|            0|            0|  0.00%|        self.self_cuda_time_total += other.self_cuda_time_total
  1039|         0|            0|            0|  0.00%|        self.cpu_memory_usage += other.cpu_memory_usage
  1040|         0|            0|            0|  0.00%|        self.cuda_memory_usage += other.cuda_memory_usage
  1041|         0|            0|            0|  0.00%|        self.self_cpu_memory_usage += other.self_cpu_memory_usage
  1042|         0|            0|            0|  0.00%|        self.self_cuda_memory_usage += other.self_cuda_memory_usage
  1043|         0|            0|            0|  0.00%|        self.count += other.count
  1044|         0|            0|            0|  0.00%|        if self.flops is None:
  1045|         0|            0|            0|  0.00%|            self.flops = other.flops
  1046|         0|            0|            0|  0.00%|        elif other.flops is not None:
  1047|         0|            0|            0|  0.00%|            self.flops += other.flops
  1048|         0|            0|            0|  0.00%|        return self
  1049|         0|            0|            0|  0.00%|
  1050|         0|            0|            0|  0.00%|    def __iadd__(self, other):
  1051|         0|            0|            0|  0.00%|        return self.add(other)
  1052|         0|            0|            0|  0.00%|
  1053|         0|            0|            0|  0.00%|    def __repr__(self):
  1054|         0|            0|            0|  0.00%|        return (
  1055|         0|            0|            0|  0.00%|            '<FunctionEventAvg key={} self_cpu_time={} cpu_time={} '
  1056|         0|            0|            0|  0.00%|            ' self_cuda_time={} cuda_time={} input_shapes={} '
  1057|         0|            0|            0|  0.00%|            'cpu_memory_usage={} cuda_memory_usage={}>'.format(
  1058|         0|            0|            0|  0.00%|                self.key,
  1059|         0|            0|            0|  0.00%|                self.self_cpu_time_total_str,
  1060|         0|            0|            0|  0.00%|                self.cpu_time_str,
  1061|         0|            0|            0|  0.00%|                self.self_cuda_time_total_str,
  1062|         0|            0|            0|  0.00%|                self.cuda_time_str,
  1063|         0|            0|            0|  0.00%|                str(self.input_shapes),
  1064|         0|            0|            0|  0.00%|                self.cpu_memory_usage,
  1065|         0|            0|            0|  0.00%|                self.cuda_memory_usage,
  1066|         0|            0|            0|  0.00%|            )
  1067|         0|            0|            0|  0.00%|        )
  1068|         0|            0|            0|  0.00%|
  1069|         0|            0|            0|  0.00%|
  1070|         0|            0|            0|  0.00%|################################################################################
  1071|         0|            0|            0|  0.00%|# Utilities
  1072|         0|            0|            0|  0.00%|
  1073|         0|            0|            0|  0.00%|class StringTable(defaultdict):
  1074|         0|            0|            0|  0.00%|    def __missing__(self, key):
  1075|         0|            0|            0|  0.00%|        # manage cases like 't' (demangled to 'unsigned short') separately,
  1076|         0|            0|            0|  0.00%|        # for now simply check the length to avoid unexpected results for
  1077|         0|            0|            0|  0.00%|        # the short sequences
  1078|         0|            0|            0|  0.00%|        self[key] = torch._C._demangle(key) if len(key) > 1 else key
  1079|         0|            0|            0|  0.00%|        return self[key]
  1080|         0|            0|            0|  0.00%|
  1081|         0|            0|            0|  0.00%|def filter_stack_entry(entry):
  1082|         0|            0|            0|  0.00%|    filtered_entries = [
  1083|         0|            0|            0|  0.00%|        ("autograd/__init__", "_make_grads"),
  1084|         0|            0|            0|  0.00%|        ("autograd/__init__", "backward"),
  1085|         0|            0|            0|  0.00%|        ("torch/tensor", "backward"),
  1086|         0|            0|            0|  0.00%|        ("_internal/common_utils", "prof_callable"),
  1087|         0|            0|            0|  0.00%|        ("_internal/common_utils", "prof_func_call"),
  1088|         0|            0|            0|  0.00%|        ("_internal/common_utils", "prof_meth_call"),
  1089|         0|            0|            0|  0.00%|    ]
  1090|         0|            0|            0|  0.00%|    return all([not (f[0] in entry and f[1] in entry) for f in filtered_entries])
  1091|         0|            0|            0|  0.00%|
  1092|         0|            0|            0|  0.00%|def filter_name(name):
  1093|         0|            0|            0|  0.00%|    # ignoring the following utility ops
  1094|         0|            0|            0|  0.00%|    filtered_out_names = [
  1095|         0|            0|            0|  0.00%|        "profiler::_record_function_enter",
  1096|         0|            0|            0|  0.00%|        "profiler::_record_function_exit",
  1097|         0|            0|            0|  0.00%|        "aten::is_leaf",
  1098|         0|            0|            0|  0.00%|        "aten::output_nr",
  1099|         0|            0|            0|  0.00%|        "aten::_version",
  1100|         0|            0|            0|  0.00%|    ]
  1101|         0|            0|            0|  0.00%|    return name in filtered_out_names
  1102|         0|            0|            0|  0.00%|
  1103|         0|            0|            0|  0.00%|# Demangles and optionally rewrites the provided event name,
  1104|         0|            0|            0|  0.00%|# with_wildcard - whether to replace certain numbered event names
  1105|         0|            0|            0|  0.00%|# with a wildcard name to aggregate them together in the profiler table
  1106|         0|            0|            0|  0.00%|# output
  1107|         0|            0|            0|  0.00%|def rewrite_name(name, with_wildcard=False):
  1108|         0|            0|            0|  0.00%|    string_table = StringTable()
  1109|         0|            0|            0|  0.00%|    name = string_table[name]
  1110|         0|            0|            0|  0.00%|    if with_wildcard:
  1111|         0|            0|            0|  0.00%|        if name.startswith("ProfilerStep#"):
  1112|         0|            0|            0|  0.00%|            name = "ProfilerStep*"
  1113|         0|            0|            0|  0.00%|    return name
  1114|         0|            0|            0|  0.00%|
  1115|         0|            0|            0|  0.00%|# Parsing of kineto profiler events
  1116|         0|            0|            0|  0.00%|def parse_kineto_results(result):
  1117|         0|            0|            0|  0.00%|    # result.events() has most of the events - PyTorch op-level and device-level events
  1118|         0|            0|            0|  0.00%|    # result.legacy_events() has events not yet ported to kineto
  1119|         0|            0|            0|  0.00%|    # (e.g. start/stop marks, tensor memory allocator events)
  1120|         0|            0|            0|  0.00%|
  1121|         0|            0|            0|  0.00%|    # First, find __start_profile mark to get the absolute time of the start of the trace;
  1122|         0|            0|            0|  0.00%|    # save memory allocation records
  1123|         0|            0|            0|  0.00%|    start_record = None
  1124|         0|            0|            0|  0.00%|    mem_records = []
  1125|         0|            0|            0|  0.00%|    for record in itertools.chain(*result.legacy_events()):
  1126|         0|            0|            0|  0.00%|        if record.kind() == 'mark' and record.name() == '__start_profile':
  1127|         0|            0|            0|  0.00%|            assert start_record is None
  1128|         0|            0|            0|  0.00%|            start_record = record
  1129|         0|            0|            0|  0.00%|        if record.kind() == 'memory_alloc':
  1130|         0|            0|            0|  0.00%|            mem_records.append([record, False])
  1131|         0|            0|            0|  0.00%|    assert start_record is not None, "Invalid profiler output, __start_profile is missing"
  1132|         0|            0|            0|  0.00%|
  1133|         0|            0|            0|  0.00%|    # Create and return FunctionEvent list
  1134|         0|            0|            0|  0.00%|    function_events = []
  1135|         0|            0|            0|  0.00%|    cuda_corr_map: Dict[int, List[FunctionEvent]] = {}
  1136|         0|            0|            0|  0.00%|    for kineto_event in result.events():
  1137|         0|            0|            0|  0.00%|        if filter_name(kineto_event.name()):
  1138|         0|            0|            0|  0.00%|            continue
  1139|         0|            0|            0|  0.00%|        rel_start_us = kineto_event.start_us() - start_record.start_us()
  1140|         0|            0|            0|  0.00%|        rel_end_us = rel_start_us + kineto_event.duration_us()
  1141|         0|            0|            0|  0.00%|        abs_end_us = kineto_event.start_us() + kineto_event.duration_us()
  1142|         0|            0|            0|  0.00%|
  1143|         0|            0|            0|  0.00%|        cpu_memory_usage = 0
  1144|         0|            0|            0|  0.00%|        cuda_memory_usage = 0
  1145|         0|            0|            0|  0.00%|        if kineto_event.device_type() == DeviceType.CPU:
  1146|         0|            0|            0|  0.00%|            # find the corresponding memory allocation events
  1147|         0|            0|            0|  0.00%|            for mem_record in mem_records:
  1148|         0|            0|            0|  0.00%|                if (mem_record[0].start_us() >= kineto_event.start_us() and
  1149|         0|            0|            0|  0.00%|                        mem_record[0].start_us() <= abs_end_us):
  1150|         0|            0|            0|  0.00%|                    cpu_memory_usage += mem_record[0].cpu_memory_usage()
  1151|         0|            0|            0|  0.00%|                    cuda_memory_usage += mem_record[0].cuda_memory_usage()
  1152|         0|            0|            0|  0.00%|                    mem_record[1] = True
  1153|         0|            0|            0|  0.00%|
  1154|         0|            0|            0|  0.00%|        is_async = kineto_event.is_async() or (
  1155|         0|            0|            0|  0.00%|            kineto_event.start_thread_id() != kineto_event.end_thread_id()
  1156|         0|            0|            0|  0.00%|        )
  1157|         0|            0|            0|  0.00%|        fe = FunctionEvent(
  1158|         0|            0|            0|  0.00%|            id=kineto_event.correlation_id(),
  1159|         0|            0|            0|  0.00%|            name=rewrite_name(name=kineto_event.name(), with_wildcard=True),
  1160|         0|            0|            0|  0.00%|            trace_name=rewrite_name(name=kineto_event.name(), with_wildcard=False),
  1161|         0|            0|            0|  0.00%|            thread=kineto_event.start_thread_id(),
  1162|         0|            0|            0|  0.00%|            start_us=rel_start_us,
  1163|         0|            0|            0|  0.00%|            end_us=rel_end_us,
  1164|         0|            0|            0|  0.00%|            fwd_thread=kineto_event.fwd_thread_id(),
  1165|         0|            0|            0|  0.00%|            input_shapes=kineto_event.shapes(),
  1166|         0|            0|            0|  0.00%|            stack=[entry for entry in kineto_event.stack() if filter_stack_entry(entry)],
  1167|         0|            0|            0|  0.00%|            scope=kineto_event.scope(),
  1168|         0|            0|            0|  0.00%|            cpu_memory_usage=cpu_memory_usage,
  1169|         0|            0|            0|  0.00%|            cuda_memory_usage=cuda_memory_usage,
  1170|         0|            0|            0|  0.00%|            is_async=is_async,
  1171|         0|            0|            0|  0.00%|            sequence_nr=kineto_event.sequence_nr(),
  1172|         0|            0|            0|  0.00%|            device_type=kineto_event.device_type(),
  1173|         0|            0|            0|  0.00%|            device_index=kineto_event.device_index(),
  1174|         0|            0|            0|  0.00%|            flops=kineto_event.flops(),
  1175|         0|            0|            0|  0.00%|        )
  1176|         0|            0|            0|  0.00%|        if fe.device_type == DeviceType.CPU and not fe.is_async:
  1177|         0|            0|            0|  0.00%|            # Check if we have CUDA time as a fallback
  1178|         0|            0|            0|  0.00%|            cuda_time = kineto_event.cuda_elapsed_us()
  1179|         0|            0|            0|  0.00%|            if cuda_time > 0:
  1180|         0|            0|            0|  0.00%|                fe.append_kernel(
  1181|         0|            0|            0|  0.00%|                    fe.name,
  1182|         0|            0|            0|  0.00%|                    fe.device_index,
  1183|         0|            0|            0|  0.00%|                    cuda_time)
  1184|         0|            0|            0|  0.00%|                fe.is_legacy = True
  1185|         0|            0|            0|  0.00%|        function_events.append(fe)
  1186|         0|            0|            0|  0.00%|        corr_id = kineto_event.linked_correlation_id()
  1187|         0|            0|            0|  0.00%|        if corr_id > 0:
  1188|         0|            0|            0|  0.00%|            if corr_id not in cuda_corr_map:
  1189|         0|            0|            0|  0.00%|                cuda_corr_map[corr_id] = []
  1190|         0|            0|            0|  0.00%|            cuda_corr_map[corr_id].append(fe)
  1191|         0|            0|            0|  0.00%|
  1192|         0|            0|            0|  0.00%|    # associate CUDA kernels and CUDA runtime (CPU) with CPU events
  1193|         0|            0|            0|  0.00%|    for fe in function_events:
  1194|         0|            0|            0|  0.00%|        if (fe.device_type == DeviceType.CPU and not fe.is_async and
  1195|         0|            0|            0|  0.00%|                fe.id in cuda_corr_map):
  1196|         0|            0|            0|  0.00%|            for f_evt in cuda_corr_map[fe.id]:
  1197|         0|            0|            0|  0.00%|                if f_evt.device_type == DeviceType.CUDA:
  1198|         0|            0|            0|  0.00%|                    fe.append_kernel(
  1199|         0|            0|            0|  0.00%|                        f_evt.name,
  1200|         0|            0|            0|  0.00%|                        f_evt.device_index,
  1201|         0|            0|            0|  0.00%|                        f_evt.time_range.end - f_evt.time_range.start)
  1202|         0|            0|            0|  0.00%|                elif f_evt.device_type == DeviceType.CPU:
  1203|         0|            0|            0|  0.00%|                    # make sure that 'thread' of a CPU Kineto (e.g. CUDA Runtime) event is associated
  1204|         0|            0|            0|  0.00%|                    # with the 'thread' of the corresponding linked PyTorch event to properly track
  1205|         0|            0|            0|  0.00%|                    # parents and children
  1206|         0|            0|            0|  0.00%|                    f_evt.thread = fe.thread
  1207|         0|            0|            0|  0.00%|
  1208|         0|            0|            0|  0.00%|    # output top-level memory events
  1209|         0|            0|            0|  0.00%|    for mem_record in mem_records:
  1210|         0|            0|            0|  0.00%|        if not mem_record[1]:
  1211|         0|            0|            0|  0.00%|            rel_start_us = mem_record[0].start_us() - start_record.start_us()
  1212|         0|            0|            0|  0.00%|            fe = FunctionEvent(
  1213|         0|            0|            0|  0.00%|                id=mem_record[0].handle(),
  1214|         0|            0|            0|  0.00%|                name="[memory]",
  1215|         0|            0|            0|  0.00%|                trace_name=None,  # not outputting in the trace
  1216|         0|            0|            0|  0.00%|                thread=mem_record[0].thread_id(),
  1217|         0|            0|            0|  0.00%|                start_us=rel_start_us,
  1218|         0|            0|            0|  0.00%|                end_us=rel_start_us,  # no duration
  1219|         0|            0|            0|  0.00%|                fwd_thread=mem_record[0].fwd_thread_id(),
  1220|         0|            0|            0|  0.00%|                input_shapes=[],
  1221|         0|            0|            0|  0.00%|                stack=[],
  1222|         0|            0|            0|  0.00%|                scope=mem_record[0].scope(),
  1223|         0|            0|            0|  0.00%|                cpu_memory_usage=mem_record[0].cpu_memory_usage(),
  1224|         0|            0|            0|  0.00%|                cuda_memory_usage=mem_record[0].cuda_memory_usage(),
  1225|         0|            0|            0|  0.00%|                is_async=False,
  1226|         0|            0|            0|  0.00%|                sequence_nr=-1,
  1227|         0|            0|            0|  0.00%|                device_type=DeviceType.CPU,
  1228|         0|            0|            0|  0.00%|                device_index=0,
  1229|         0|            0|            0|  0.00%|            )
  1230|         0|            0|            0|  0.00%|            function_events.append(fe)
  1231|         0|            0|            0|  0.00%|
  1232|         0|            0|            0|  0.00%|    function_events.sort(key=lambda evt: [evt.time_range.start, -evt.time_range.end])
  1233|         0|            0|            0|  0.00%|    return function_events
  1234|         0|            0|            0|  0.00%|
  1235|         0|            0|            0|  0.00%|# Parsing of legacy profiler events
  1236|         0|            0|            0|  0.00%|def parse_legacy_records(thread_records):
  1237|         0|            0|            0|  0.00%|    def get_record_key(record):
  1238|         0|            0|            0|  0.00%|        """
  1239|         0|            0|            0|  0.00%|        Returns a tuple to be used by parse_legacy_records for correlating start and
  1240|         0|            0|            0|  0.00%|        end records.
  1241|         0|            0|            0|  0.00%|        """
  1242|         0|            0|            0|  0.00%|        return (record.handle(), record.node_id())
  1243|         0|            0|            0|  0.00%|
  1244|         0|            0|            0|  0.00%|    next_id = 0
  1245|         0|            0|            0|  0.00%|    start_record = None
  1246|         0|            0|            0|  0.00%|    functions = []
  1247|         0|            0|            0|  0.00%|    record_stack = []
  1248|         0|            0|            0|  0.00%|
  1249|         0|            0|            0|  0.00%|    # '__start_profile' is not guaranteed to be first, so we must find it here
  1250|         0|            0|            0|  0.00%|    for record in itertools.chain(*thread_records):
  1251|         0|            0|            0|  0.00%|        name = record.name()
  1252|         0|            0|            0|  0.00%|        if start_record is None and name == '__start_profile':
  1253|         0|            0|            0|  0.00%|            start_record = record
  1254|         0|            0|            0|  0.00%|
  1255|         0|            0|            0|  0.00%|    assert start_record is not None and not start_record.is_remote()
  1256|         0|            0|            0|  0.00%|
  1257|         0|            0|            0|  0.00%|    for thread_record_list in thread_records:
  1258|         0|            0|            0|  0.00%|        # accumulated memory allocations per handle
  1259|         0|            0|            0|  0.00%|        cpu_memory_allocs = {}
  1260|         0|            0|            0|  0.00%|        cuda_memory_allocs = {}
  1261|         0|            0|            0|  0.00%|        # ranges per handle
  1262|         0|            0|            0|  0.00%|        range_starts = {}
  1263|         0|            0|            0|  0.00%|
  1264|         0|            0|            0|  0.00%|        filtered_handles = set()
  1265|         0|            0|            0|  0.00%|        prev_record = None
  1266|         0|            0|            0|  0.00%|        for record in thread_record_list:
  1267|         0|            0|            0|  0.00%|            record_key = get_record_key(record)
  1268|         0|            0|            0|  0.00%|            if (filter_name(record.name()) or
  1269|         0|            0|            0|  0.00%|                    record_key in filtered_handles):
  1270|         0|            0|            0|  0.00%|                filtered_handles.add(record_key)
  1271|         0|            0|            0|  0.00%|                continue
  1272|         0|            0|            0|  0.00%|
  1273|         0|            0|            0|  0.00%|            if record.kind() == 'push':
  1274|         0|            0|            0|  0.00%|                # workaround to reduce double logging from operator
  1275|         0|            0|            0|  0.00%|                # wrappers and redispatch
  1276|         0|            0|            0|  0.00%|                if prev_record is not None:
  1277|         0|            0|            0|  0.00%|                    duplicate = (
  1278|         0|            0|            0|  0.00%|                        prev_record.name() == record.name()
  1279|         0|            0|            0|  0.00%|                        and prev_record.kind() == record.kind()
  1280|         0|            0|            0|  0.00%|                        and prev_record.node_id() == record.node_id()
  1281|         0|            0|            0|  0.00%|                    )
  1282|         0|            0|            0|  0.00%|                    if duplicate:
  1283|         0|            0|            0|  0.00%|                        filtered_handles.add(record_key)
  1284|         0|            0|            0|  0.00%|                        continue
  1285|         0|            0|            0|  0.00%|
  1286|         0|            0|            0|  0.00%|                range_starts[record_key] = record
  1287|         0|            0|            0|  0.00%|                cpu_memory_allocs[record_key] = 0
  1288|         0|            0|            0|  0.00%|                cuda_memory_allocs[record_key] = 0
  1289|         0|            0|            0|  0.00%|            elif record.kind() == 'pop':
  1290|         0|            0|            0|  0.00%|                assert (
  1291|         0|            0|            0|  0.00%|                    record_key in range_starts
  1292|         0|            0|            0|  0.00%|                ), """Expected record with key {} to exist in range_starts.
  1293|         0|            0|            0|  0.00%|                    This means that the pop event did not have a corresponding push.""".format(
  1294|         0|            0|            0|  0.00%|                    record_key
  1295|         0|            0|            0|  0.00%|                )
  1296|         0|            0|            0|  0.00%|
  1297|         0|            0|            0|  0.00%|                start = range_starts[record_key]
  1298|         0|            0|            0|  0.00%|
  1299|         0|            0|            0|  0.00%|                cpu_memory_usage = cpu_memory_allocs[record_key]
  1300|         0|            0|            0|  0.00%|                cuda_memory_usage = cuda_memory_allocs[record_key]
  1301|         0|            0|            0|  0.00%|                is_async = start.is_async() or (
  1302|         0|            0|            0|  0.00%|                    start.thread_id() != record.thread_id()
  1303|         0|            0|            0|  0.00%|                )
  1304|         0|            0|            0|  0.00%|                is_remote_event = record.is_remote()
  1305|         0|            0|            0|  0.00%|                start_flops = start.flops()
  1306|         0|            0|            0|  0.00%|
  1307|         0|            0|            0|  0.00%|                fe = FunctionEvent(
  1308|         0|            0|            0|  0.00%|                    id=record.handle(),
  1309|         0|            0|            0|  0.00%|                    node_id=record.node_id(),
  1310|         0|            0|            0|  0.00%|                    name=rewrite_name(name=start.name(), with_wildcard=True),
  1311|         0|            0|            0|  0.00%|                    trace_name=rewrite_name(name=start.name(), with_wildcard=False),
  1312|         0|            0|            0|  0.00%|                    thread=start.thread_id(),
  1313|         0|            0|            0|  0.00%|                    start_us=start_record.cpu_elapsed_us(start),
  1314|         0|            0|            0|  0.00%|                    end_us=start_record.cpu_elapsed_us(record),
  1315|         0|            0|            0|  0.00%|                    fwd_thread=start.fwd_thread_id(),
  1316|         0|            0|            0|  0.00%|                    input_shapes=start.shapes(),
  1317|         0|            0|            0|  0.00%|                    stack=[entry for entry in start.stack() if filter_stack_entry(entry)],
  1318|         0|            0|            0|  0.00%|                    scope=start.scope(),
  1319|         0|            0|            0|  0.00%|                    cpu_memory_usage=cpu_memory_usage,
  1320|         0|            0|            0|  0.00%|                    cuda_memory_usage=cuda_memory_usage,
  1321|         0|            0|            0|  0.00%|                    is_async=is_async,
  1322|         0|            0|            0|  0.00%|                    is_remote=is_remote_event,
  1323|         0|            0|            0|  0.00%|                    sequence_nr=start.sequence_nr(),
  1324|         0|            0|            0|  0.00%|                    device_type=DeviceType.CPU,
  1325|         0|            0|            0|  0.00%|                    is_legacy=True,
  1326|         0|            0|            0|  0.00%|                    flops=start_flops,
  1327|         0|            0|            0|  0.00%|                )
  1328|         0|            0|            0|  0.00%|                # note: async events have only cpu total time
  1329|         0|            0|            0|  0.00%|                if not is_async and start.has_cuda():
  1330|         0|            0|            0|  0.00%|                    duration = start.cuda_elapsed_us(record)
  1331|         0|            0|            0|  0.00%|                    if duration > 0:
  1332|         0|            0|            0|  0.00%|                        fe.append_kernel(
  1333|         0|            0|            0|  0.00%|                            start.name(),
  1334|         0|            0|            0|  0.00%|                            start.device(),
  1335|         0|            0|            0|  0.00%|                            duration)
  1336|         0|            0|            0|  0.00%|                functions.append(fe)
  1337|         0|            0|            0|  0.00%|                del range_starts[record_key]
  1338|         0|            0|            0|  0.00%|                del cpu_memory_allocs[record_key]
  1339|         0|            0|            0|  0.00%|                del cuda_memory_allocs[record_key]
  1340|         0|            0|            0|  0.00%|            elif record.kind() == 'memory_alloc':
  1341|         0|            0|            0|  0.00%|                num_open_handles_cpu = len(cpu_memory_allocs)
  1342|         0|            0|            0|  0.00%|                num_open_handles_cuda = len(cuda_memory_allocs)
  1343|         0|            0|            0|  0.00%|                assert num_open_handles_cpu == num_open_handles_cuda
  1344|         0|            0|            0|  0.00%|                for handle in cpu_memory_allocs.keys():
  1345|         0|            0|            0|  0.00%|                    cpu_memory_allocs[handle] += record.cpu_memory_usage()
  1346|         0|            0|            0|  0.00%|                for handle in cuda_memory_allocs.keys():
  1347|         0|            0|            0|  0.00%|                    cuda_memory_allocs[handle] += record.cuda_memory_usage()
  1348|         0|            0|            0|  0.00%|                if num_open_handles_cpu == 0:
  1349|         0|            0|            0|  0.00%|                    # output event as a top-level memory event
  1350|         0|            0|            0|  0.00%|                    fe = FunctionEvent(
  1351|         0|            0|            0|  0.00%|                        id=0,
  1352|         0|            0|            0|  0.00%|                        name="[memory]",
  1353|         0|            0|            0|  0.00%|                        trace_name=None,
  1354|         0|            0|            0|  0.00%|                        thread=0,
  1355|         0|            0|            0|  0.00%|                        start_us=0,
  1356|         0|            0|            0|  0.00%|                        end_us=0,
  1357|         0|            0|            0|  0.00%|                        stack=[],
  1358|         0|            0|            0|  0.00%|                        cpu_memory_usage=record.cpu_memory_usage(),
  1359|         0|            0|            0|  0.00%|                        cuda_memory_usage=record.cuda_memory_usage(),
  1360|         0|            0|            0|  0.00%|                        is_legacy=True,
  1361|         0|            0|            0|  0.00%|                    )
  1362|         0|            0|            0|  0.00%|                    functions.append(fe)
  1363|         0|            0|            0|  0.00%|            prev_record = record
  1364|         0|            0|            0|  0.00%|
  1365|         0|            0|            0|  0.00%|    # Sort functions by start time then by end time ascending.
  1366|         0|            0|            0|  0.00%|    # This ensures that--in the case of nested events which
  1367|         0|            0|            0|  0.00%|    # have the same start time (which may happen due to the
  1368|         0|            0|            0|  0.00%|    # granularity of the given clock tick)--we always show
  1369|         0|            0|            0|  0.00%|    # the outermost nested call first. This adds stability
  1370|         0|            0|            0|  0.00%|    # in how FunctionEvents appear
  1371|         0|            0|            0|  0.00%|    functions.sort(key=lambda evt: [evt.time_range.start, -evt.time_range.end])
  1372|         0|            0|            0|  0.00%|    return functions
  1373|         0|            0|            0|  0.00%|
  1374|         0|            0|            0|  0.00%|
  1375|         0|            0|            0|  0.00%|################################################################################
  1376|         0|            0|            0|  0.00%|# CUDA checkpoints
  1377|         0|            0|            0|  0.00%|
  1378|         0|            0|            0|  0.00%|class EnforceUnique(object):
  1379|         0|            0|            0|  0.00%|    """Raises an error if a key is seen more than once."""
  1380|         0|            0|            0|  0.00%|    def __init__(self):
  1381|         0|            0|            0|  0.00%|        self.seen = set()
  1382|         0|            0|            0|  0.00%|
  1383|         0|            0|            0|  0.00%|    def see(self, *key):
  1384|         0|            0|            0|  0.00%|        if key in self.seen:
  1385|         0|            0|            0|  0.00%|            raise RuntimeError('duplicate key: ' + str(key))
  1386|         0|            0|            0|  0.00%|        self.seen.add(key)
  1387|         0|            0|            0|  0.00%|
  1388|         0|            0|            0|  0.00%|
  1389|         0|            0|            0|  0.00%|def parse_nvprof_trace(path):
  1390|         0|            0|            0|  0.00%|    import sqlite3
  1391|         0|            0|            0|  0.00%|    conn = sqlite3.connect(path)
  1392|         0|            0|            0|  0.00%|    conn.row_factory = sqlite3.Row
  1393|         0|            0|            0|  0.00%|
  1394|         0|            0|            0|  0.00%|    # Parse strings table
  1395|         0|            0|            0|  0.00%|    strings = {}
  1396|         0|            0|            0|  0.00%|    for r in conn.execute("SELECT _id_ as id, value FROM StringTable"):
  1397|         0|            0|            0|  0.00%|        strings[r["id"]] = torch._C._demangle(r["value"])
  1398|         0|            0|            0|  0.00%|
  1399|         0|            0|            0|  0.00%|    # First, find all functions and create FunctionEvents for them
  1400|         0|            0|            0|  0.00%|    marker_query = """
  1401|         0|            0|            0|  0.00%|    SELECT
  1402|         0|            0|            0|  0.00%|        start.id AS marker_id, start.name, start.timestamp AS start_time, end.timestamp AS end_time
  1403|         0|            0|            0|  0.00%|    FROM
  1404|         0|            0|            0|  0.00%|        CUPTI_ACTIVITY_KIND_MARKER AS start INNER JOIN CUPTI_ACTIVITY_KIND_MARKER AS end
  1405|         0|            0|            0|  0.00%|        ON start.id = end.id
  1406|         0|            0|            0|  0.00%|    WHERE
  1407|         0|            0|            0|  0.00%|        start.name != 0 AND end.name = 0
  1408|         0|            0|            0|  0.00%|    """
  1409|         0|            0|            0|  0.00%|    functions = []
  1410|         0|            0|            0|  0.00%|    functions_map = {}
  1411|         0|            0|            0|  0.00%|    unique = EnforceUnique()
  1412|         0|            0|            0|  0.00%|    for row in conn.execute(marker_query):
  1413|         0|            0|            0|  0.00%|        unique.see(row['marker_id'])
  1414|         0|            0|            0|  0.00%|        evt = FunctionEvent(id=row['marker_id'],
  1415|         0|            0|            0|  0.00%|                            node_id=0,  # missing a node_id when calling FunctionEvent. This is just to ensure
  1416|         0|            0|            0|  0.00%|                                        # that pytorch doesn't crash when creating a FunctionEvent() object
  1417|         0|            0|            0|  0.00%|                            name=strings[row['name']],
  1418|         0|            0|            0|  0.00%|                            start_us=row['start_time'],
  1419|         0|            0|            0|  0.00%|                            end_us=row['end_time'],
  1420|         0|            0|            0|  0.00%|                            thread=0)  # TODO: find in sqlite database
  1421|         0|            0|            0|  0.00%|        functions.append(evt)
  1422|         0|            0|            0|  0.00%|        functions_map[evt.id] = evt
  1423|         0|            0|            0|  0.00%|
  1424|         0|            0|            0|  0.00%|    # Now, correlate all kernels with FunctionEvents
  1425|         0|            0|            0|  0.00%|    kernel_query = """
  1426|         0|            0|            0|  0.00%|    SELECT
  1427|         0|            0|            0|  0.00%|        start.id AS marker_id, start.name, start.timestamp, end.timestamp,
  1428|         0|            0|            0|  0.00%|        runtime._id_ AS runtime_id, runtime.cbid, runtime.start AS runtime_start, runtime.end AS runtime_end,
  1429|         0|            0|            0|  0.00%|        kernel.start AS kernel_start, kernel.end AS kernel_end, kernel.name AS kernel_name
  1430|         0|            0|            0|  0.00%|    FROM
  1431|         0|            0|            0|  0.00%|        CUPTI_ACTIVITY_KIND_MARKER AS start
  1432|         0|            0|            0|  0.00%|        INNER JOIN CUPTI_ACTIVITY_KIND_MARKER AS end
  1433|         0|            0|            0|  0.00%|            ON start.id = end.id
  1434|         0|            0|            0|  0.00%|        INNER JOIN CUPTI_ACTIVITY_KIND_RUNTIME as runtime
  1435|         0|            0|            0|  0.00%|            ON (start.timestamp < runtime.start AND runtime.end < end.timestamp)
  1436|         0|            0|            0|  0.00%|        INNER JOIN CUPTI_ACTIVITY_KIND_CONCURRENT_KERNEL AS kernel
  1437|         0|            0|            0|  0.00%|            ON kernel.correlationId = runtime.correlationId
  1438|         0|            0|            0|  0.00%|    """
  1439|         0|            0|            0|  0.00%|    unique = EnforceUnique()
  1440|         0|            0|            0|  0.00%|    for row in conn.execute(kernel_query):
  1441|         0|            0|            0|  0.00%|        unique.see(row['marker_id'], row['runtime_id'])
  1442|         0|            0|            0|  0.00%|        # 211 is cudaKernelLaunch for cuda >= 9.2; 13 is for older cuda versions
  1443|         0|            0|            0|  0.00%|        assert (row['cbid'] == 211) or (row['cbid'] == 13)
  1444|         0|            0|            0|  0.00%|        evt = functions_map[row['marker_id']]
  1445|         0|            0|            0|  0.00%|        evt.append_kernel(row['kernel_name'],
  1446|         0|            0|            0|  0.00%|                          0,
  1447|         0|            0|            0|  0.00%|                          row['kernel_end'] - row['kernel_start'])
  1448|         0|            0|            0|  0.00%|
  1449|         0|            0|            0|  0.00%|    functions.sort(key=lambda evt: evt.time_range.start)
  1450|         0|            0|            0|  0.00%|    return functions
  1451|         0|            0|            0|  0.00%|
  1452|         0|            0|            0|  0.00%|
  1453|         0|            0|            0|  0.00%|################################################################################
  1454|         0|            0|            0|  0.00%|# Pretty printer
  1455|         0|            0|            0|  0.00%|
  1456|         0|            0|            0|  0.00%|
  1457|         0|            0|            0|  0.00%|def build_table(
  1458|         0|            0|            0|  0.00%|        events,
  1459|         0|            0|            0|  0.00%|        sort_by=None,
  1460|         0|            0|            0|  0.00%|        header=None,
  1461|         0|            0|            0|  0.00%|        row_limit=100,
  1462|         0|            0|            0|  0.00%|        max_src_column_width=75,
  1463|         0|            0|            0|  0.00%|        with_flops=False,
  1464|         0|            0|            0|  0.00%|        profile_memory=False,
  1465|         0|            0|            0|  0.00%|        top_level_events_only=False):
  1466|         0|            0|            0|  0.00%|    """Prints a summary of events (which can be a list of FunctionEvent or FunctionEventAvg)."""
  1467|         0|            0|            0|  0.00%|    if len(events) == 0:
  1468|         0|            0|            0|  0.00%|        return ""
  1469|         0|            0|            0|  0.00%|
  1470|         0|            0|            0|  0.00%|    has_cuda_time = any([event.self_cuda_time_total > 0 for event in events])
  1471|         0|            0|            0|  0.00%|    has_cuda_mem = any([event.self_cuda_memory_usage > 0 for event in events])
  1472|         0|            0|            0|  0.00%|    has_input_shapes = any(
  1473|         0|            0|            0|  0.00%|        [(event.input_shapes is not None and len(event.input_shapes) > 0) for event in events])
  1474|         0|            0|            0|  0.00%|
  1475|         0|            0|            0|  0.00%|    if sort_by is not None:
  1476|         0|            0|            0|  0.00%|        events = EventList(sorted(
  1477|         0|            0|            0|  0.00%|            events, key=lambda evt: getattr(evt, sort_by), reverse=True
  1478|         0|            0|            0|  0.00%|        ), use_cuda=has_cuda_time, profile_memory=profile_memory, with_flops=with_flops)
  1479|         0|            0|            0|  0.00%|
  1480|         0|            0|            0|  0.00%|    MAX_NAME_COLUMN_WIDTH = 55
  1481|         0|            0|            0|  0.00%|    name_column_width = max([len(evt.key) for evt in events]) + 4
  1482|         0|            0|            0|  0.00%|    name_column_width = min(name_column_width, MAX_NAME_COLUMN_WIDTH)
  1483|         0|            0|            0|  0.00%|
  1484|         0|            0|            0|  0.00%|    MAX_SHAPES_COLUMN_WIDTH = 80
  1485|         0|            0|            0|  0.00%|    shapes_column_width = max([len(str(evt.input_shapes)) for evt in events]) + 4
  1486|         0|            0|            0|  0.00%|    shapes_column_width = min(shapes_column_width, MAX_SHAPES_COLUMN_WIDTH)
  1487|         0|            0|            0|  0.00%|
  1488|         0|            0|            0|  0.00%|    DEFAULT_COLUMN_WIDTH = 12
  1489|         0|            0|            0|  0.00%|    flops_column_width = DEFAULT_COLUMN_WIDTH
  1490|         0|            0|            0|  0.00%|
  1491|         0|            0|            0|  0.00%|    src_column_width = None
  1492|         0|            0|            0|  0.00%|    stacks = []
  1493|         0|            0|            0|  0.00%|    for evt in events:
  1494|         0|            0|            0|  0.00%|        if evt.stack is not None and len(evt.stack) > 0:
  1495|         0|            0|            0|  0.00%|            stacks.append(evt.stack)
  1496|         0|            0|            0|  0.00%|    has_stack = len(stacks) > 0
  1497|         0|            0|            0|  0.00%|    if has_stack:
  1498|         0|            0|            0|  0.00%|        src_column_width = max([max([len(entry) for entry in stack]) for stack in stacks]) + 4
  1499|         0|            0|            0|  0.00%|        src_column_width = min(src_column_width, max_src_column_width)
  1500|         0|            0|            0|  0.00%|
  1501|         0|            0|            0|  0.00%|    headers = [
  1502|         0|            0|            0|  0.00%|        'Name',
  1503|         0|            0|            0|  0.00%|        'Self CPU %',
  1504|         0|            0|            0|  0.00%|        'Self CPU',
  1505|         0|            0|            0|  0.00%|        'CPU total %',
  1506|         0|            0|            0|  0.00%|        'CPU total',
  1507|         0|            0|            0|  0.00%|        'CPU time avg',
  1508|         0|            0|            0|  0.00%|    ]
  1509|         0|            0|            0|  0.00%|    if has_cuda_time:
  1510|         0|            0|            0|  0.00%|        headers.extend([
  1511|         0|            0|            0|  0.00%|            'Self CUDA',
  1512|         0|            0|            0|  0.00%|            'Self CUDA %',
  1513|         0|            0|            0|  0.00%|            'CUDA total',
  1514|         0|            0|            0|  0.00%|            'CUDA time avg',
  1515|         0|            0|            0|  0.00%|        ])
  1516|         0|            0|            0|  0.00%|    if profile_memory:
  1517|         0|            0|            0|  0.00%|        headers.extend([
  1518|         0|            0|            0|  0.00%|            'CPU Mem',
  1519|         0|            0|            0|  0.00%|            'Self CPU Mem',
  1520|         0|            0|            0|  0.00%|        ])
  1521|         0|            0|            0|  0.00%|        if has_cuda_mem:
  1522|         0|            0|            0|  0.00%|            headers.extend([
  1523|         0|            0|            0|  0.00%|                'CUDA Mem',
  1524|         0|            0|            0|  0.00%|                'Self CUDA Mem',
  1525|         0|            0|            0|  0.00%|            ])
  1526|         0|            0|            0|  0.00%|    headers.append(
  1527|         0|            0|            0|  0.00%|        '# of Calls'
  1528|         0|            0|            0|  0.00%|    )
  1529|         0|            0|            0|  0.00%|    # Only append Node ID if any event has a valid (>= 0) Node ID
  1530|         0|            0|            0|  0.00%|    append_node_id = any([evt.node_id != -1 for evt in events])
  1531|         0|            0|            0|  0.00%|    if append_node_id:
  1532|         0|            0|            0|  0.00%|        headers.append('Node ID')
  1533|         0|            0|            0|  0.00%|
  1534|         0|            0|            0|  0.00%|    # Have to use a list because nonlocal is Py3 only...
  1535|         0|            0|            0|  0.00%|    SPACING_SIZE = 2
  1536|         0|            0|            0|  0.00%|    row_format_lst = [""]
  1537|         0|            0|            0|  0.00%|    header_sep_lst = [""]
  1538|         0|            0|            0|  0.00%|    line_length_lst = [-SPACING_SIZE]
  1539|         0|            0|            0|  0.00%|    MAX_STACK_ENTRY = 5
  1540|         0|            0|            0|  0.00%|
  1541|         0|            0|            0|  0.00%|    def add_column(padding, text_dir='>'):
  1542|         0|            0|            0|  0.00%|        row_format_lst[0] += '{: ' + text_dir + str(padding) + '}' + (' ' * SPACING_SIZE)
  1543|         0|            0|            0|  0.00%|        header_sep_lst[0] += '-' * padding + (' ' * SPACING_SIZE)
  1544|         0|            0|            0|  0.00%|        line_length_lst[0] += padding + SPACING_SIZE
  1545|         0|            0|            0|  0.00%|
  1546|         0|            0|            0|  0.00%|    def auto_scale_flops(flops):
  1547|         0|            0|            0|  0.00%|        flop_headers = [
  1548|         0|            0|            0|  0.00%|            'FLOPS',
  1549|         0|            0|            0|  0.00%|            'KFLOPS',
  1550|         0|            0|            0|  0.00%|            'MFLOPS',
  1551|         0|            0|            0|  0.00%|            'GFLOPS',
  1552|         0|            0|            0|  0.00%|            'TFLOPS',
  1553|         0|            0|            0|  0.00%|            'PFLOPS',
  1554|         0|            0|            0|  0.00%|        ]
  1555|         0|            0|            0|  0.00%|        assert flops > 0
  1556|         0|            0|            0|  0.00%|        log_flops = max(0, min(math.log10(flops) / 3, float(len(flop_headers) - 1)))
  1557|         0|            0|            0|  0.00%|        assert log_flops >= 0 and log_flops < len(flop_headers)
  1558|         0|            0|            0|  0.00%|        return (pow(10, (math.floor(log_flops) * -3.0)), flop_headers[int(log_flops)])
  1559|         0|            0|            0|  0.00%|
  1560|         0|            0|            0|  0.00%|    def flops_rate(evt):
  1561|         0|            0|            0|  0.00%|        US_IN_SECOND = 1000.0 * 1000.0
  1562|         0|            0|            0|  0.00%|        if evt.flops > 0:
  1563|         0|            0|            0|  0.00%|            if evt.cuda_time_total != 0:
  1564|         0|            0|            0|  0.00%|                return float(evt.flops) / evt.cuda_time_total * US_IN_SECOND
  1565|         0|            0|            0|  0.00%|            else:
  1566|         0|            0|            0|  0.00%|                return float(evt.flops) / evt.cpu_time_total * US_IN_SECOND
  1567|         0|            0|            0|  0.00%|        else:
  1568|         0|            0|            0|  0.00%|            return -1
  1569|         0|            0|            0|  0.00%|
  1570|         0|            0|            0|  0.00%|    add_column(name_column_width)
  1571|         0|            0|            0|  0.00%|    for _ in headers[1:]:
  1572|         0|            0|            0|  0.00%|        add_column(DEFAULT_COLUMN_WIDTH)
  1573|         0|            0|            0|  0.00%|
  1574|         0|            0|            0|  0.00%|    if has_input_shapes:
  1575|         0|            0|            0|  0.00%|        headers.append('Input Shapes')
  1576|         0|            0|            0|  0.00%|        add_column(shapes_column_width)
  1577|         0|            0|            0|  0.00%|
  1578|         0|            0|            0|  0.00%|    if has_stack:
  1579|         0|            0|            0|  0.00%|        headers.append('Source Location')
  1580|         0|            0|            0|  0.00%|        add_column(src_column_width, text_dir='<')
  1581|         0|            0|            0|  0.00%|
  1582|         0|            0|            0|  0.00%|    if with_flops:
  1583|         0|            0|            0|  0.00%|        # Auto-scaling of flops header
  1584|         0|            0|            0|  0.00%|        raw_flops = []
  1585|         0|            0|            0|  0.00%|        for evt in events:
  1586|         0|            0|            0|  0.00%|            rate = flops_rate(evt)
  1587|         0|            0|            0|  0.00%|            if rate > 0:
  1588|         0|            0|            0|  0.00%|                raw_flops.append(rate)
  1589|         0|            0|            0|  0.00%|        if len(raw_flops) != 0:
  1590|         0|            0|            0|  0.00%|            (flops_scale, flops_header) = auto_scale_flops(min(raw_flops))
  1591|         0|            0|            0|  0.00%|            headers.append(flops_header)
  1592|         0|            0|            0|  0.00%|            add_column(flops_column_width)
  1593|         0|            0|            0|  0.00%|        else:
  1594|         0|            0|            0|  0.00%|            with_flops = False  # can't find any valid flops
  1595|         0|            0|            0|  0.00%|
  1596|         0|            0|            0|  0.00%|    row_format = row_format_lst[0]
  1597|         0|            0|            0|  0.00%|    header_sep = header_sep_lst[0]
  1598|         0|            0|            0|  0.00%|    line_length = line_length_lst[0]
  1599|         0|            0|            0|  0.00%|    add_column = None  # type: ignore[assignment]
  1600|         0|            0|            0|  0.00%|
  1601|         0|            0|            0|  0.00%|    # Have to use a list because nonlocal is Py3 only...
  1602|         0|            0|            0|  0.00%|    result = []
  1603|         0|            0|            0|  0.00%|
  1604|         0|            0|            0|  0.00%|    def append(s):
  1605|         0|            0|            0|  0.00%|        result.append(s)
  1606|         0|            0|            0|  0.00%|        result.append('\n')  # Yes, newline after the end as well
  1607|         0|            0|            0|  0.00%|
  1608|         0|            0|            0|  0.00%|    sum_self_cpu_time_total = sum([event.self_cpu_time_total for event in events])
  1609|         0|            0|            0|  0.00%|    sum_self_cuda_time_total = 0
  1610|         0|            0|            0|  0.00%|    for evt in events:
  1611|         0|            0|            0|  0.00%|        if evt.device_type == DeviceType.CPU:
  1612|         0|            0|            0|  0.00%|            # in legacy profiler, kernel info is stored in cpu events
  1613|         0|            0|            0|  0.00%|            if evt.is_legacy:
  1614|         0|            0|            0|  0.00%|                sum_self_cuda_time_total += evt.self_cuda_time_total
  1615|         0|            0|            0|  0.00%|        elif evt.device_type == DeviceType.CUDA:
  1616|         0|            0|            0|  0.00%|            # in kineto profiler, there're events with the correct device type (e.g. CUDA)
  1617|         0|            0|            0|  0.00%|            sum_self_cuda_time_total += evt.self_cuda_time_total
  1618|         0|            0|            0|  0.00%|
  1619|         0|            0|            0|  0.00%|    # Actual printing
  1620|         0|            0|            0|  0.00%|    if header is not None:
  1621|         0|            0|            0|  0.00%|        append('=' * line_length)
  1622|         0|            0|            0|  0.00%|        append(header)
  1623|         0|            0|            0|  0.00%|    if top_level_events_only:
  1624|         0|            0|            0|  0.00%|        append('=' * line_length)
  1625|         0|            0|            0|  0.00%|        append('This report only display top-level ops statistics')
  1626|         0|            0|            0|  0.00%|    append(header_sep)
  1627|         0|            0|            0|  0.00%|    append(row_format.format(*headers))
  1628|         0|            0|            0|  0.00%|
  1629|         0|            0|            0|  0.00%|    append(header_sep)
  1630|         0|            0|            0|  0.00%|
  1631|         0|            0|            0|  0.00%|    def trim_path(path, src_column_width):
  1632|         0|            0|            0|  0.00%|        if len(path) > src_column_width:
  1633|         0|            0|            0|  0.00%|            offset = len(path) - src_column_width
  1634|         0|            0|            0|  0.00%|            path = path[offset:]
  1635|         0|            0|            0|  0.00%|            if len(path) > 3:
  1636|         0|            0|            0|  0.00%|                path = "..." + path[3:]
  1637|         0|            0|            0|  0.00%|        return path
  1638|         0|            0|            0|  0.00%|
  1639|         0|            0|            0|  0.00%|    event_limit = 0
  1640|         0|            0|            0|  0.00%|    for evt in events:
  1641|         0|            0|            0|  0.00%|        if event_limit == row_limit:
  1642|         0|            0|            0|  0.00%|            break
  1643|         0|            0|            0|  0.00%|        if top_level_events_only and evt.cpu_parent is not None:
  1644|         0|            0|            0|  0.00%|            continue
  1645|         0|            0|            0|  0.00%|        else:
  1646|         0|            0|            0|  0.00%|            event_limit += 1
  1647|         0|            0|            0|  0.00%|        name = evt.key
  1648|         0|            0|            0|  0.00%|        if len(name) >= MAX_NAME_COLUMN_WIDTH - 3:
  1649|         0|            0|            0|  0.00%|            name = name[:(MAX_NAME_COLUMN_WIDTH - 3)] + "..."
  1650|         0|            0|            0|  0.00%|        row_values = [
  1651|         0|            0|            0|  0.00%|            name,
  1652|         0|            0|            0|  0.00%|            # Self CPU total %, 0 for async events.
  1653|         0|            0|            0|  0.00%|            format_time_share(evt.self_cpu_time_total,
  1654|         0|            0|            0|  0.00%|                              sum_self_cpu_time_total),
  1655|         0|            0|            0|  0.00%|            evt.self_cpu_time_total_str,  # Self CPU total
  1656|         0|            0|            0|  0.00%|            # CPU total %, 0 for async events.
  1657|         0|            0|            0|  0.00%|            format_time_share(evt.cpu_time_total, sum_self_cpu_time_total) if not evt.is_async else 0,
  1658|         0|            0|            0|  0.00%|            evt.cpu_time_total_str,  # CPU total
  1659|         0|            0|            0|  0.00%|            evt.cpu_time_str,  # CPU time avg
  1660|         0|            0|            0|  0.00%|        ]
  1661|         0|            0|            0|  0.00%|        if has_cuda_time:
  1662|         0|            0|            0|  0.00%|            row_values.extend([
  1663|         0|            0|            0|  0.00%|                evt.self_cuda_time_total_str,
  1664|         0|            0|            0|  0.00%|                # CUDA time total %
  1665|         0|            0|            0|  0.00%|                format_time_share(evt.self_cuda_time_total, sum_self_cuda_time_total),
  1666|         0|            0|            0|  0.00%|                evt.cuda_time_total_str,
  1667|         0|            0|            0|  0.00%|                evt.cuda_time_str,  # Cuda time avg
  1668|         0|            0|            0|  0.00%|            ])
  1669|         0|            0|            0|  0.00%|        if profile_memory:
  1670|         0|            0|            0|  0.00%|            row_values.extend([
  1671|         0|            0|            0|  0.00%|                # CPU Mem Total
  1672|         0|            0|            0|  0.00%|                format_memory(evt.cpu_memory_usage),
  1673|         0|            0|            0|  0.00%|                # Self CPU Mem Total
  1674|         0|            0|            0|  0.00%|                format_memory(evt.self_cpu_memory_usage),
  1675|         0|            0|            0|  0.00%|            ])
  1676|         0|            0|            0|  0.00%|            if has_cuda_mem:
  1677|         0|            0|            0|  0.00%|                row_values.extend([
  1678|         0|            0|            0|  0.00%|                    # CUDA Mem Total
  1679|         0|            0|            0|  0.00%|                    format_memory(evt.cuda_memory_usage),
  1680|         0|            0|            0|  0.00%|                    # Self CUDA Mem Total
  1681|         0|            0|            0|  0.00%|                    format_memory(evt.self_cuda_memory_usage),
  1682|         0|            0|            0|  0.00%|                ])
  1683|         0|            0|            0|  0.00%|        row_values.append(
  1684|         0|            0|            0|  0.00%|            evt.count,  # Number of calls
  1685|         0|            0|            0|  0.00%|        )
  1686|         0|            0|            0|  0.00%|
  1687|         0|            0|            0|  0.00%|        if append_node_id:
  1688|         0|            0|            0|  0.00%|            row_values.append(evt.node_id)
  1689|         0|            0|            0|  0.00%|        if has_input_shapes:
  1690|         0|            0|            0|  0.00%|            row_values.append(str(evt.input_shapes)[:shapes_column_width])
  1691|         0|            0|            0|  0.00%|        if with_flops:
  1692|         0|            0|            0|  0.00%|            rate = flops_rate(evt)
  1693|         0|            0|            0|  0.00%|            if rate <= 0.0:
  1694|         0|            0|            0|  0.00%|                row_values.append("--")
  1695|         0|            0|            0|  0.00%|            else:
  1696|         0|            0|            0|  0.00%|                row_values.append('{0:8.3f}'.format(rate * flops_scale))
  1697|         0|            0|            0|  0.00%|        if has_stack:
  1698|         0|            0|            0|  0.00%|            src_field = ""
  1699|         0|            0|            0|  0.00%|            if len(evt.stack) > 0:
  1700|         0|            0|            0|  0.00%|                src_field = trim_path(evt.stack[0], src_column_width)
  1701|         0|            0|            0|  0.00%|            row_values.append(src_field)
  1702|         0|            0|            0|  0.00%|        append(row_format.format(*row_values))
  1703|         0|            0|            0|  0.00%|
  1704|         0|            0|            0|  0.00%|        if has_stack:
  1705|         0|            0|            0|  0.00%|            empty_headers = [""] * (len(headers) - 1)
  1706|         0|            0|            0|  0.00%|            for entry in evt.stack[1:MAX_STACK_ENTRY]:
  1707|         0|            0|            0|  0.00%|                append(row_format.format(*(empty_headers + [trim_path(entry, src_column_width)])))
  1708|         0|            0|            0|  0.00%|            empty_headers.append("")
  1709|         0|            0|            0|  0.00%|            append(row_format.format(*empty_headers))
  1710|         0|            0|            0|  0.00%|
  1711|         0|            0|            0|  0.00%|    append(header_sep)
  1712|         0|            0|            0|  0.00%|    append("Self CPU time total: {}".format(format_time(sum_self_cpu_time_total)))
  1713|         0|            0|            0|  0.00%|    if has_cuda_time:
  1714|         0|            0|            0|  0.00%|        append("Self CUDA time total: {}".format(format_time(sum_self_cuda_time_total)))
  1715|         0|            0|            0|  0.00%|    return ''.join(result)
File: /opt/conda/lib/python3.8/socket.py
File duration: 0.0439842s (0.08%)
Line #|      Hits|         Time| Time per hit|      %|Source code
------+----------+-------------+-------------+-------+-----------
     1|         0|            0|            0|  0.00%|# Wrapper module for _socket, providing some additional facilities
     2|         0|            0|            0|  0.00%|# implemented in Python.
     3|         0|            0|            0|  0.00%|
     4|         0|            0|            0|  0.00%|"""\
     5|         0|            0|            0|  0.00%|This module provides socket operations and some related functions.
     6|         0|            0|            0|  0.00%|On Unix, it supports IP (Internet Protocol) and Unix domain sockets.
     7|         0|            0|            0|  0.00%|On other systems, it only supports IP. Functions specific for a
     8|         0|            0|            0|  0.00%|socket are available as methods of the socket object.
     9|         0|            0|            0|  0.00%|
    10|         0|            0|            0|  0.00%|Functions:
    11|         0|            0|            0|  0.00%|
    12|         0|            0|            0|  0.00%|socket() -- create a new socket object
    13|         0|            0|            0|  0.00%|socketpair() -- create a pair of new socket objects [*]
    14|         0|            0|            0|  0.00%|fromfd() -- create a socket object from an open file descriptor [*]
    15|         0|            0|            0|  0.00%|fromshare() -- create a socket object from data received from socket.share() [*]
    16|         0|            0|            0|  0.00%|gethostname() -- return the current hostname
    17|         0|            0|            0|  0.00%|gethostbyname() -- map a hostname to its IP number
    18|         0|            0|            0|  0.00%|gethostbyaddr() -- map an IP number or hostname to DNS info
    19|         0|            0|            0|  0.00%|getservbyname() -- map a service name and a protocol name to a port number
    20|         0|            0|            0|  0.00%|getprotobyname() -- map a protocol name (e.g. 'tcp') to a number
    21|         0|            0|            0|  0.00%|ntohs(), ntohl() -- convert 16, 32 bit int from network to host byte order
    22|         0|            0|            0|  0.00%|htons(), htonl() -- convert 16, 32 bit int from host to network byte order
    23|         0|            0|            0|  0.00%|inet_aton() -- convert IP addr string (123.45.67.89) to 32-bit packed format
    24|         0|            0|            0|  0.00%|inet_ntoa() -- convert 32-bit packed format IP to string (123.45.67.89)
    25|         0|            0|            0|  0.00%|socket.getdefaulttimeout() -- get the default timeout value
    26|         0|            0|            0|  0.00%|socket.setdefaulttimeout() -- set the default timeout value
    27|         0|            0|            0|  0.00%|create_connection() -- connects to an address, with an optional timeout and
    28|         0|            0|            0|  0.00%|                       optional source address.
    29|         0|            0|            0|  0.00%|
    30|         0|            0|            0|  0.00%| [*] not available on all platforms!
    31|         0|            0|            0|  0.00%|
    32|         0|            0|            0|  0.00%|Special objects:
    33|         0|            0|            0|  0.00%|
    34|         0|            0|            0|  0.00%|SocketType -- type object for socket objects
    35|         0|            0|            0|  0.00%|error -- exception raised for I/O errors
    36|         0|            0|            0|  0.00%|has_ipv6 -- boolean value indicating if IPv6 is supported
    37|         0|            0|            0|  0.00%|
    38|         0|            0|            0|  0.00%|IntEnum constants:
    39|         0|            0|            0|  0.00%|
    40|         0|            0|            0|  0.00%|AF_INET, AF_UNIX -- socket domains (first argument to socket() call)
    41|         0|            0|            0|  0.00%|SOCK_STREAM, SOCK_DGRAM, SOCK_RAW -- socket types (second argument)
    42|         0|            0|            0|  0.00%|
    43|         0|            0|            0|  0.00%|Integer constants:
    44|         0|            0|            0|  0.00%|
    45|         0|            0|            0|  0.00%|Many other constants may be defined; these may be used in calls to
    46|         0|            0|            0|  0.00%|the setsockopt() and getsockopt() methods.
    47|         0|            0|            0|  0.00%|"""
    48|         0|            0|            0|  0.00%|
    49|         0|            0|            0|  0.00%|import _socket
    50|         0|            0|            0|  0.00%|from _socket import *
    51|         0|            0|            0|  0.00%|
    52|         0|            0|            0|  0.00%|import os, sys, io, selectors
    53|         0|            0|            0|  0.00%|from enum import IntEnum, IntFlag
    54|         0|            0|            0|  0.00%|
    55|         0|            0|            0|  0.00%|try:
    56|         0|            0|            0|  0.00%|    import errno
    57|         0|            0|            0|  0.00%|except ImportError:
    58|         0|            0|            0|  0.00%|    errno = None
    59|         0|            0|            0|  0.00%|EBADF = getattr(errno, 'EBADF', 9)
    60|         0|            0|            0|  0.00%|EAGAIN = getattr(errno, 'EAGAIN', 11)
    61|         0|            0|            0|  0.00%|EWOULDBLOCK = getattr(errno, 'EWOULDBLOCK', 11)
    62|         0|            0|            0|  0.00%|
    63|         0|            0|            0|  0.00%|__all__ = ["fromfd", "getfqdn", "create_connection", "create_server",
    64|         0|            0|            0|  0.00%|           "has_dualstack_ipv6", "AddressFamily", "SocketKind"]
    65|         0|            0|            0|  0.00%|__all__.extend(os._get_exports_list(_socket))
    66|         0|            0|            0|  0.00%|
    67|         0|            0|            0|  0.00%|# Set up the socket.AF_* socket.SOCK_* constants as members of IntEnums for
    68|         0|            0|            0|  0.00%|# nicer string representations.
    69|         0|            0|            0|  0.00%|# Note that _socket only knows about the integer values. The public interface
    70|         0|            0|            0|  0.00%|# in this module understands the enums and translates them back from integers
    71|         0|            0|            0|  0.00%|# where needed (e.g. .family property of a socket object).
    72|         0|            0|            0|  0.00%|
    73|         0|            0|            0|  0.00%|IntEnum._convert_(
    74|         0|            0|            0|  0.00%|        'AddressFamily',
    75|         0|            0|            0|  0.00%|        __name__,
    76|         0|            0|            0|  0.00%|        lambda C: C.isupper() and C.startswith('AF_'))
    77|         0|            0|            0|  0.00%|
    78|         0|            0|            0|  0.00%|IntEnum._convert_(
    79|         0|            0|            0|  0.00%|        'SocketKind',
    80|         0|            0|            0|  0.00%|        __name__,
    81|         0|            0|            0|  0.00%|        lambda C: C.isupper() and C.startswith('SOCK_'))
    82|         0|            0|            0|  0.00%|
    83|         0|            0|            0|  0.00%|IntFlag._convert_(
    84|         0|            0|            0|  0.00%|        'MsgFlag',
    85|         0|            0|            0|  0.00%|        __name__,
    86|         0|            0|            0|  0.00%|        lambda C: C.isupper() and C.startswith('MSG_'))
    87|         0|            0|            0|  0.00%|
    88|         0|            0|            0|  0.00%|IntFlag._convert_(
    89|         0|            0|            0|  0.00%|        'AddressInfo',
    90|         0|            0|            0|  0.00%|        __name__,
    91|         0|            0|            0|  0.00%|        lambda C: C.isupper() and C.startswith('AI_'))
    92|         0|            0|            0|  0.00%|
    93|         0|            0|            0|  0.00%|_LOCALHOST    = '127.0.0.1'
    94|         0|            0|            0|  0.00%|_LOCALHOST_V6 = '::1'
    95|         0|            0|            0|  0.00%|
    96|         0|            0|            0|  0.00%|
    97|         0|            0|            0|  0.00%|def _intenum_converter(value, enum_klass):
    98|         0|            0|            0|  0.00%|    """Convert a numeric family value to an IntEnum member.
    99|         0|            0|            0|  0.00%|
   100|         0|            0|            0|  0.00%|    If it's not a known member, return the numeric value itself.
   101|         0|            0|            0|  0.00%|    """
   102|         0|            0|            0|  0.00%|    try:
   103|         0|            0|            0|  0.00%|        return enum_klass(value)
   104|         0|            0|            0|  0.00%|    except ValueError:
   105|         0|            0|            0|  0.00%|        return value
   106|         0|            0|            0|  0.00%|
   107|         0|            0|            0|  0.00%|_realsocket = socket
   108|         0|            0|            0|  0.00%|
   109|         0|            0|            0|  0.00%|# WSA error codes
   110|         0|            0|            0|  0.00%|if sys.platform.lower().startswith("win"):
   111|         0|            0|            0|  0.00%|    errorTab = {}
   112|         0|            0|            0|  0.00%|    errorTab[6] = "Specified event object handle is invalid."
   113|         0|            0|            0|  0.00%|    errorTab[8] = "Insufficient memory available."
   114|         0|            0|            0|  0.00%|    errorTab[87] = "One or more parameters are invalid."
   115|         0|            0|            0|  0.00%|    errorTab[995] = "Overlapped operation aborted."
   116|         0|            0|            0|  0.00%|    errorTab[996] = "Overlapped I/O event object not in signaled state."
   117|         0|            0|            0|  0.00%|    errorTab[997] = "Overlapped operation will complete later."
   118|         0|            0|            0|  0.00%|    errorTab[10004] = "The operation was interrupted."
   119|         0|            0|            0|  0.00%|    errorTab[10009] = "A bad file handle was passed."
   120|         0|            0|            0|  0.00%|    errorTab[10013] = "Permission denied."
   121|         0|            0|            0|  0.00%|    errorTab[10014] = "A fault occurred on the network??"  # WSAEFAULT
   122|         0|            0|            0|  0.00%|    errorTab[10022] = "An invalid operation was attempted."
   123|         0|            0|            0|  0.00%|    errorTab[10024] = "Too many open files."
   124|         0|            0|            0|  0.00%|    errorTab[10035] = "The socket operation would block"
   125|         0|            0|            0|  0.00%|    errorTab[10036] = "A blocking operation is already in progress."
   126|         0|            0|            0|  0.00%|    errorTab[10037] = "Operation already in progress."
   127|         0|            0|            0|  0.00%|    errorTab[10038] = "Socket operation on nonsocket."
   128|         0|            0|            0|  0.00%|    errorTab[10039] = "Destination address required."
   129|         0|            0|            0|  0.00%|    errorTab[10040] = "Message too long."
   130|         0|            0|            0|  0.00%|    errorTab[10041] = "Protocol wrong type for socket."
   131|         0|            0|            0|  0.00%|    errorTab[10042] = "Bad protocol option."
   132|         0|            0|            0|  0.00%|    errorTab[10043] = "Protocol not supported."
   133|         0|            0|            0|  0.00%|    errorTab[10044] = "Socket type not supported."
   134|         0|            0|            0|  0.00%|    errorTab[10045] = "Operation not supported."
   135|         0|            0|            0|  0.00%|    errorTab[10046] = "Protocol family not supported."
   136|         0|            0|            0|  0.00%|    errorTab[10047] = "Address family not supported by protocol family."
   137|         0|            0|            0|  0.00%|    errorTab[10048] = "The network address is in use."
   138|         0|            0|            0|  0.00%|    errorTab[10049] = "Cannot assign requested address."
   139|         0|            0|            0|  0.00%|    errorTab[10050] = "Network is down."
   140|         0|            0|            0|  0.00%|    errorTab[10051] = "Network is unreachable."
   141|         0|            0|            0|  0.00%|    errorTab[10052] = "Network dropped connection on reset."
   142|         0|            0|            0|  0.00%|    errorTab[10053] = "Software caused connection abort."
   143|         0|            0|            0|  0.00%|    errorTab[10054] = "The connection has been reset."
   144|         0|            0|            0|  0.00%|    errorTab[10055] = "No buffer space available."
   145|         0|            0|            0|  0.00%|    errorTab[10056] = "Socket is already connected."
   146|         0|            0|            0|  0.00%|    errorTab[10057] = "Socket is not connected."
   147|         0|            0|            0|  0.00%|    errorTab[10058] = "The network has been shut down."
   148|         0|            0|            0|  0.00%|    errorTab[10059] = "Too many references."
   149|         0|            0|            0|  0.00%|    errorTab[10060] = "The operation timed out."
   150|         0|            0|            0|  0.00%|    errorTab[10061] = "Connection refused."
   151|         0|            0|            0|  0.00%|    errorTab[10062] = "Cannot translate name."
   152|         0|            0|            0|  0.00%|    errorTab[10063] = "The name is too long."
   153|         0|            0|            0|  0.00%|    errorTab[10064] = "The host is down."
   154|         0|            0|            0|  0.00%|    errorTab[10065] = "The host is unreachable."
   155|         0|            0|            0|  0.00%|    errorTab[10066] = "Directory not empty."
   156|         0|            0|            0|  0.00%|    errorTab[10067] = "Too many processes."
   157|         0|            0|            0|  0.00%|    errorTab[10068] = "User quota exceeded."
   158|         0|            0|            0|  0.00%|    errorTab[10069] = "Disk quota exceeded."
   159|         0|            0|            0|  0.00%|    errorTab[10070] = "Stale file handle reference."
   160|         0|            0|            0|  0.00%|    errorTab[10071] = "Item is remote."
   161|         0|            0|            0|  0.00%|    errorTab[10091] = "Network subsystem is unavailable."
   162|         0|            0|            0|  0.00%|    errorTab[10092] = "Winsock.dll version out of range."
   163|         0|            0|            0|  0.00%|    errorTab[10093] = "Successful WSAStartup not yet performed."
   164|         0|            0|            0|  0.00%|    errorTab[10101] = "Graceful shutdown in progress."
   165|         0|            0|            0|  0.00%|    errorTab[10102] = "No more results from WSALookupServiceNext."
   166|         0|            0|            0|  0.00%|    errorTab[10103] = "Call has been canceled."
   167|         0|            0|            0|  0.00%|    errorTab[10104] = "Procedure call table is invalid."
   168|         0|            0|            0|  0.00%|    errorTab[10105] = "Service provider is invalid."
   169|         0|            0|            0|  0.00%|    errorTab[10106] = "Service provider failed to initialize."
   170|         0|            0|            0|  0.00%|    errorTab[10107] = "System call failure."
   171|         0|            0|            0|  0.00%|    errorTab[10108] = "Service not found."
   172|         0|            0|            0|  0.00%|    errorTab[10109] = "Class type not found."
   173|         0|            0|            0|  0.00%|    errorTab[10110] = "No more results from WSALookupServiceNext."
   174|         0|            0|            0|  0.00%|    errorTab[10111] = "Call was canceled."
   175|         0|            0|            0|  0.00%|    errorTab[10112] = "Database query was refused."
   176|         0|            0|            0|  0.00%|    errorTab[11001] = "Host not found."
   177|         0|            0|            0|  0.00%|    errorTab[11002] = "Nonauthoritative host not found."
   178|         0|            0|            0|  0.00%|    errorTab[11003] = "This is a nonrecoverable error."
   179|         0|            0|            0|  0.00%|    errorTab[11004] = "Valid name, no data record requested type."
   180|         0|            0|            0|  0.00%|    errorTab[11005] = "QoS receivers."
   181|         0|            0|            0|  0.00%|    errorTab[11006] = "QoS senders."
   182|         0|            0|            0|  0.00%|    errorTab[11007] = "No QoS senders."
   183|         0|            0|            0|  0.00%|    errorTab[11008] = "QoS no receivers."
   184|         0|            0|            0|  0.00%|    errorTab[11009] = "QoS request confirmed."
   185|         0|            0|            0|  0.00%|    errorTab[11010] = "QoS admission error."
   186|         0|            0|            0|  0.00%|    errorTab[11011] = "QoS policy failure."
   187|         0|            0|            0|  0.00%|    errorTab[11012] = "QoS bad style."
   188|         0|            0|            0|  0.00%|    errorTab[11013] = "QoS bad object."
   189|         0|            0|            0|  0.00%|    errorTab[11014] = "QoS traffic control error."
   190|         0|            0|            0|  0.00%|    errorTab[11015] = "QoS generic error."
   191|         0|            0|            0|  0.00%|    errorTab[11016] = "QoS service type error."
   192|         0|            0|            0|  0.00%|    errorTab[11017] = "QoS flowspec error."
   193|         0|            0|            0|  0.00%|    errorTab[11018] = "Invalid QoS provider buffer."
   194|         0|            0|            0|  0.00%|    errorTab[11019] = "Invalid QoS filter style."
   195|         0|            0|            0|  0.00%|    errorTab[11020] = "Invalid QoS filter style."
   196|         0|            0|            0|  0.00%|    errorTab[11021] = "Incorrect QoS filter count."
   197|         0|            0|            0|  0.00%|    errorTab[11022] = "Invalid QoS object length."
   198|         0|            0|            0|  0.00%|    errorTab[11023] = "Incorrect QoS flow count."
   199|         0|            0|            0|  0.00%|    errorTab[11024] = "Unrecognized QoS object."
   200|         0|            0|            0|  0.00%|    errorTab[11025] = "Invalid QoS policy object."
   201|         0|            0|            0|  0.00%|    errorTab[11026] = "Invalid QoS flow descriptor."
   202|         0|            0|            0|  0.00%|    errorTab[11027] = "Invalid QoS provider-specific flowspec."
   203|         0|            0|            0|  0.00%|    errorTab[11028] = "Invalid QoS provider-specific filterspec."
   204|         0|            0|            0|  0.00%|    errorTab[11029] = "Invalid QoS shape discard mode object."
   205|         0|            0|            0|  0.00%|    errorTab[11030] = "Invalid QoS shaping rate object."
   206|         0|            0|            0|  0.00%|    errorTab[11031] = "Reserved policy QoS element type."
   207|         0|            0|            0|  0.00%|    __all__.append("errorTab")
   208|         0|            0|            0|  0.00%|
   209|         0|            0|            0|  0.00%|
   210|         0|            0|            0|  0.00%|class _GiveupOnSendfile(Exception): pass
   211|         0|            0|            0|  0.00%|
   212|         0|            0|            0|  0.00%|
   213|         0|            0|            0|  0.00%|class socket(_socket.socket):
   214|         0|            0|            0|  0.00%|
   215|         0|            0|            0|  0.00%|    """A subclass of _socket.socket adding the makefile() method."""
   216|         0|            0|            0|  0.00%|
   217|         0|            0|            0|  0.00%|    __slots__ = ["__weakref__", "_io_refs", "_closed"]
   218|         0|            0|            0|  0.00%|
   219|       400|   0.00203943|  5.09858e-06|  0.00%|    def __init__(self, family=-1, type=-1, proto=-1, fileno=None):
   220|         0|            0|            0|  0.00%|        # For user code address family and type values are IntEnum members, but
   221|         0|            0|            0|  0.00%|        # for the underlying _socket.socket they're just integers. The
   222|         0|            0|            0|  0.00%|        # constructor of _socket.socket converts the given argument to an
   223|         0|            0|            0|  0.00%|        # integer automatically.
   224|       400|   0.00184083|  4.60207e-06|  0.00%|        if fileno is None:
   225|       200|  0.000849485|  4.24743e-06|  0.00%|            if family == -1:
   226|         0|            0|            0|  0.00%|                family = AF_INET
   227|       200|  0.000796318|  3.98159e-06|  0.00%|            if type == -1:
   228|       200|  0.000866413|  4.33207e-06|  0.00%|                type = SOCK_STREAM
   229|       200|  0.000769615|  3.84808e-06|  0.00%|            if proto == -1:
   230|       200|    0.0007689|   3.8445e-06|  0.00%|                proto = 0
   231|       400|   0.00577736|  1.44434e-05|  0.01%|        _socket.socket.__init__(self, family, type, proto, fileno)
   232|       400|   0.00209451|  5.23627e-06|  0.00%|        self._io_refs = 0
   233|       400|   0.00166631|  4.16577e-06|  0.00%|        self._closed = False
   234|         0|            0|            0|  0.00%|
   235|       400|     0.001755|   4.3875e-06|  0.00%|    def __enter__(self):
   236|       400|   0.00157928|  3.94821e-06|  0.00%|        return self
   237|         0|            0|            0|  0.00%|
   238|       400|   0.00186801|  4.67002e-06|  0.00%|    def __exit__(self, *args):
   239|       400|    0.0017364|  4.34101e-06|  0.00%|        if not self._closed:
   240|       200|   0.00239611|  1.19805e-05|  0.00%|            self.close()
(call)|       200|   0.00763083|  3.81541e-05|  0.01%|# /opt/conda/lib/python3.8/socket.py:496 close
   241|         0|            0|            0|  0.00%|
   242|         0|            0|            0|  0.00%|    def __repr__(self):
   243|         0|            0|            0|  0.00%|        """Wrap __repr__() to reveal the real class name and socket
   244|         0|            0|            0|  0.00%|        address(es).
   245|         0|            0|            0|  0.00%|        """
   246|         0|            0|            0|  0.00%|        closed = getattr(self, '_closed', False)
   247|         0|            0|            0|  0.00%|        s = "<%s.%s%s fd=%i, family=%s, type=%s, proto=%i" \
   248|         0|            0|            0|  0.00%|            % (self.__class__.__module__,
   249|         0|            0|            0|  0.00%|               self.__class__.__qualname__,
   250|         0|            0|            0|  0.00%|               " [closed]" if closed else "",
   251|         0|            0|            0|  0.00%|               self.fileno(),
   252|         0|            0|            0|  0.00%|               self.family,
   253|         0|            0|            0|  0.00%|               self.type,
   254|         0|            0|            0|  0.00%|               self.proto)
   255|         0|            0|            0|  0.00%|        if not closed:
   256|         0|            0|            0|  0.00%|            try:
   257|         0|            0|            0|  0.00%|                laddr = self.getsockname()
   258|         0|            0|            0|  0.00%|                if laddr:
   259|         0|            0|            0|  0.00%|                    s += ", laddr=%s" % str(laddr)
   260|         0|            0|            0|  0.00%|            except error:
   261|         0|            0|            0|  0.00%|                pass
   262|         0|            0|            0|  0.00%|            try:
   263|         0|            0|            0|  0.00%|                raddr = self.getpeername()
   264|         0|            0|            0|  0.00%|                if raddr:
   265|         0|            0|            0|  0.00%|                    s += ", raddr=%s" % str(raddr)
   266|         0|            0|            0|  0.00%|            except error:
   267|         0|            0|            0|  0.00%|                pass
   268|         0|            0|            0|  0.00%|        s += '>'
   269|         0|            0|            0|  0.00%|        return s
   270|         0|            0|            0|  0.00%|
   271|         0|            0|            0|  0.00%|    def __getstate__(self):
   272|         0|            0|            0|  0.00%|        raise TypeError(f"cannot pickle {self.__class__.__name__!r} object")
   273|         0|            0|            0|  0.00%|
   274|         0|            0|            0|  0.00%|    def dup(self):
   275|         0|            0|            0|  0.00%|        """dup() -> socket object
   276|         0|            0|            0|  0.00%|
   277|         0|            0|            0|  0.00%|        Duplicate the socket. Return a new socket object connected to the same
   278|         0|            0|            0|  0.00%|        system resource. The new socket is non-inheritable.
   279|         0|            0|            0|  0.00%|        """
   280|         0|            0|            0|  0.00%|        fd = dup(self.fileno())
   281|         0|            0|            0|  0.00%|        sock = self.__class__(self.family, self.type, self.proto, fileno=fd)
   282|         0|            0|            0|  0.00%|        sock.settimeout(self.gettimeout())
   283|         0|            0|            0|  0.00%|        return sock
   284|         0|            0|            0|  0.00%|
   285|         0|            0|            0|  0.00%|    def accept(self):
   286|         0|            0|            0|  0.00%|        """accept() -> (socket object, address info)
   287|         0|            0|            0|  0.00%|
   288|         0|            0|            0|  0.00%|        Wait for an incoming connection.  Return a new socket
   289|         0|            0|            0|  0.00%|        representing the connection, and the address of the client.
   290|         0|            0|            0|  0.00%|        For IP sockets, the address info is a pair (hostaddr, port).
   291|         0|            0|            0|  0.00%|        """
   292|         0|            0|            0|  0.00%|        fd, addr = self._accept()
   293|         0|            0|            0|  0.00%|        sock = socket(self.family, self.type, self.proto, fileno=fd)
   294|         0|            0|            0|  0.00%|        # Issue #7995: if no default timeout is set and the listening
   295|         0|            0|            0|  0.00%|        # socket had a (non-zero) timeout, force the new socket in blocking
   296|         0|            0|            0|  0.00%|        # mode to override platform-specific socket flags inheritance.
   297|         0|            0|            0|  0.00%|        if getdefaulttimeout() is None and self.gettimeout():
   298|         0|            0|            0|  0.00%|            sock.setblocking(True)
   299|         0|            0|            0|  0.00%|        return sock, addr
   300|         0|            0|            0|  0.00%|
   301|         0|            0|            0|  0.00%|    def makefile(self, mode="r", buffering=None, *,
   302|         0|            0|            0|  0.00%|                 encoding=None, errors=None, newline=None):
   303|         0|            0|            0|  0.00%|        """makefile(...) -> an I/O stream connected to the socket
   304|         0|            0|            0|  0.00%|
   305|         0|            0|            0|  0.00%|        The arguments are as for io.open() after the filename, except the only
   306|         0|            0|            0|  0.00%|        supported mode values are 'r' (default), 'w' and 'b'.
   307|         0|            0|            0|  0.00%|        """
   308|         0|            0|            0|  0.00%|        # XXX refactor to share code?
   309|         0|            0|            0|  0.00%|        if not set(mode) <= {"r", "w", "b"}:
   310|         0|            0|            0|  0.00%|            raise ValueError("invalid mode %r (only r, w, b allowed)" % (mode,))
   311|         0|            0|            0|  0.00%|        writing = "w" in mode
   312|         0|            0|            0|  0.00%|        reading = "r" in mode or not writing
   313|         0|            0|            0|  0.00%|        assert reading or writing
   314|         0|            0|            0|  0.00%|        binary = "b" in mode
   315|         0|            0|            0|  0.00%|        rawmode = ""
   316|         0|            0|            0|  0.00%|        if reading:
   317|         0|            0|            0|  0.00%|            rawmode += "r"
   318|         0|            0|            0|  0.00%|        if writing:
   319|         0|            0|            0|  0.00%|            rawmode += "w"
   320|         0|            0|            0|  0.00%|        raw = SocketIO(self, rawmode)
   321|         0|            0|            0|  0.00%|        self._io_refs += 1
   322|         0|            0|            0|  0.00%|        if buffering is None:
   323|         0|            0|            0|  0.00%|            buffering = -1
   324|         0|            0|            0|  0.00%|        if buffering < 0:
   325|         0|            0|            0|  0.00%|            buffering = io.DEFAULT_BUFFER_SIZE
   326|         0|            0|            0|  0.00%|        if buffering == 0:
   327|         0|            0|            0|  0.00%|            if not binary:
   328|         0|            0|            0|  0.00%|                raise ValueError("unbuffered streams must be binary")
   329|         0|            0|            0|  0.00%|            return raw
   330|         0|            0|            0|  0.00%|        if reading and writing:
   331|         0|            0|            0|  0.00%|            buffer = io.BufferedRWPair(raw, raw, buffering)
   332|         0|            0|            0|  0.00%|        elif reading:
   333|         0|            0|            0|  0.00%|            buffer = io.BufferedReader(raw, buffering)
   334|         0|            0|            0|  0.00%|        else:
   335|         0|            0|            0|  0.00%|            assert writing
   336|         0|            0|            0|  0.00%|            buffer = io.BufferedWriter(raw, buffering)
   337|         0|            0|            0|  0.00%|        if binary:
   338|         0|            0|            0|  0.00%|            return buffer
   339|         0|            0|            0|  0.00%|        text = io.TextIOWrapper(buffer, encoding, errors, newline)
   340|         0|            0|            0|  0.00%|        text.mode = mode
   341|         0|            0|            0|  0.00%|        return text
   342|         0|            0|            0|  0.00%|
   343|         0|            0|            0|  0.00%|    if hasattr(os, 'sendfile'):
   344|         0|            0|            0|  0.00%|
   345|         0|            0|            0|  0.00%|        def _sendfile_use_sendfile(self, file, offset=0, count=None):
   346|         0|            0|            0|  0.00%|            self._check_sendfile_params(file, offset, count)
   347|         0|            0|            0|  0.00%|            sockno = self.fileno()
   348|         0|            0|            0|  0.00%|            try:
   349|         0|            0|            0|  0.00%|                fileno = file.fileno()
   350|         0|            0|            0|  0.00%|            except (AttributeError, io.UnsupportedOperation) as err:
   351|         0|            0|            0|  0.00%|                raise _GiveupOnSendfile(err)  # not a regular file
   352|         0|            0|            0|  0.00%|            try:
   353|         0|            0|            0|  0.00%|                fsize = os.fstat(fileno).st_size
   354|         0|            0|            0|  0.00%|            except OSError as err:
   355|         0|            0|            0|  0.00%|                raise _GiveupOnSendfile(err)  # not a regular file
   356|         0|            0|            0|  0.00%|            if not fsize:
   357|         0|            0|            0|  0.00%|                return 0  # empty file
   358|         0|            0|            0|  0.00%|            # Truncate to 1GiB to avoid OverflowError, see bpo-38319.
   359|         0|            0|            0|  0.00%|            blocksize = min(count or fsize, 2 ** 30)
   360|         0|            0|            0|  0.00%|            timeout = self.gettimeout()
   361|         0|            0|            0|  0.00%|            if timeout == 0:
   362|         0|            0|            0|  0.00%|                raise ValueError("non-blocking sockets are not supported")
   363|         0|            0|            0|  0.00%|            # poll/select have the advantage of not requiring any
   364|         0|            0|            0|  0.00%|            # extra file descriptor, contrarily to epoll/kqueue
   365|         0|            0|            0|  0.00%|            # (also, they require a single syscall).
   366|         0|            0|            0|  0.00%|            if hasattr(selectors, 'PollSelector'):
   367|         0|            0|            0|  0.00%|                selector = selectors.PollSelector()
   368|         0|            0|            0|  0.00%|            else:
   369|         0|            0|            0|  0.00%|                selector = selectors.SelectSelector()
   370|         0|            0|            0|  0.00%|            selector.register(sockno, selectors.EVENT_WRITE)
   371|         0|            0|            0|  0.00%|
   372|         0|            0|            0|  0.00%|            total_sent = 0
   373|         0|            0|            0|  0.00%|            # localize variable access to minimize overhead
   374|         0|            0|            0|  0.00%|            selector_select = selector.select
   375|         0|            0|            0|  0.00%|            os_sendfile = os.sendfile
   376|         0|            0|            0|  0.00%|            try:
   377|         0|            0|            0|  0.00%|                while True:
   378|         0|            0|            0|  0.00%|                    if timeout and not selector_select(timeout):
   379|         0|            0|            0|  0.00%|                        raise _socket.timeout('timed out')
   380|         0|            0|            0|  0.00%|                    if count:
   381|         0|            0|            0|  0.00%|                        blocksize = count - total_sent
   382|         0|            0|            0|  0.00%|                        if blocksize <= 0:
   383|         0|            0|            0|  0.00%|                            break
   384|         0|            0|            0|  0.00%|                    try:
   385|         0|            0|            0|  0.00%|                        sent = os_sendfile(sockno, fileno, offset, blocksize)
   386|         0|            0|            0|  0.00%|                    except BlockingIOError:
   387|         0|            0|            0|  0.00%|                        if not timeout:
   388|         0|            0|            0|  0.00%|                            # Block until the socket is ready to send some
   389|         0|            0|            0|  0.00%|                            # data; avoids hogging CPU resources.
   390|         0|            0|            0|  0.00%|                            selector_select()
   391|         0|            0|            0|  0.00%|                        continue
   392|         0|            0|            0|  0.00%|                    except OSError as err:
   393|         0|            0|            0|  0.00%|                        if total_sent == 0:
   394|         0|            0|            0|  0.00%|                            # We can get here for different reasons, the main
   395|         0|            0|            0|  0.00%|                            # one being 'file' is not a regular mmap(2)-like
   396|         0|            0|            0|  0.00%|                            # file, in which case we'll fall back on using
   397|         0|            0|            0|  0.00%|                            # plain send().
   398|         0|            0|            0|  0.00%|                            raise _GiveupOnSendfile(err)
   399|         0|            0|            0|  0.00%|                        raise err from None
   400|         0|            0|            0|  0.00%|                    else:
   401|         0|            0|            0|  0.00%|                        if sent == 0:
   402|         0|            0|            0|  0.00%|                            break  # EOF
   403|         0|            0|            0|  0.00%|                        offset += sent
   404|         0|            0|            0|  0.00%|                        total_sent += sent
   405|         0|            0|            0|  0.00%|                return total_sent
   406|         0|            0|            0|  0.00%|            finally:
   407|         0|            0|            0|  0.00%|                if total_sent > 0 and hasattr(file, 'seek'):
   408|         0|            0|            0|  0.00%|                    file.seek(offset)
   409|         0|            0|            0|  0.00%|    else:
   410|         0|            0|            0|  0.00%|        def _sendfile_use_sendfile(self, file, offset=0, count=None):
   411|         0|            0|            0|  0.00%|            raise _GiveupOnSendfile(
   412|         0|            0|            0|  0.00%|                "os.sendfile() not available on this platform")
   413|         0|            0|            0|  0.00%|
   414|         0|            0|            0|  0.00%|    def _sendfile_use_send(self, file, offset=0, count=None):
   415|         0|            0|            0|  0.00%|        self._check_sendfile_params(file, offset, count)
   416|         0|            0|            0|  0.00%|        if self.gettimeout() == 0:
   417|         0|            0|            0|  0.00%|            raise ValueError("non-blocking sockets are not supported")
   418|         0|            0|            0|  0.00%|        if offset:
   419|         0|            0|            0|  0.00%|            file.seek(offset)
   420|         0|            0|            0|  0.00%|        blocksize = min(count, 8192) if count else 8192
   421|         0|            0|            0|  0.00%|        total_sent = 0
   422|         0|            0|            0|  0.00%|        # localize variable access to minimize overhead
   423|         0|            0|            0|  0.00%|        file_read = file.read
   424|         0|            0|            0|  0.00%|        sock_send = self.send
   425|         0|            0|            0|  0.00%|        try:
   426|         0|            0|            0|  0.00%|            while True:
   427|         0|            0|            0|  0.00%|                if count:
   428|         0|            0|            0|  0.00%|                    blocksize = min(count - total_sent, blocksize)
   429|         0|            0|            0|  0.00%|                    if blocksize <= 0:
   430|         0|            0|            0|  0.00%|                        break
   431|         0|            0|            0|  0.00%|                data = memoryview(file_read(blocksize))
   432|         0|            0|            0|  0.00%|                if not data:
   433|         0|            0|            0|  0.00%|                    break  # EOF
   434|         0|            0|            0|  0.00%|                while True:
   435|         0|            0|            0|  0.00%|                    try:
   436|         0|            0|            0|  0.00%|                        sent = sock_send(data)
   437|         0|            0|            0|  0.00%|                    except BlockingIOError:
   438|         0|            0|            0|  0.00%|                        continue
   439|         0|            0|            0|  0.00%|                    else:
   440|         0|            0|            0|  0.00%|                        total_sent += sent
   441|         0|            0|            0|  0.00%|                        if sent < len(data):
   442|         0|            0|            0|  0.00%|                            data = data[sent:]
   443|         0|            0|            0|  0.00%|                        else:
   444|         0|            0|            0|  0.00%|                            break
   445|         0|            0|            0|  0.00%|            return total_sent
   446|         0|            0|            0|  0.00%|        finally:
   447|         0|            0|            0|  0.00%|            if total_sent > 0 and hasattr(file, 'seek'):
   448|         0|            0|            0|  0.00%|                file.seek(offset + total_sent)
   449|         0|            0|            0|  0.00%|
   450|         0|            0|            0|  0.00%|    def _check_sendfile_params(self, file, offset, count):
   451|         0|            0|            0|  0.00%|        if 'b' not in getattr(file, 'mode', 'b'):
   452|         0|            0|            0|  0.00%|            raise ValueError("file should be opened in binary mode")
   453|         0|            0|            0|  0.00%|        if not self.type & SOCK_STREAM:
   454|         0|            0|            0|  0.00%|            raise ValueError("only SOCK_STREAM type sockets are supported")
   455|         0|            0|            0|  0.00%|        if count is not None:
   456|         0|            0|            0|  0.00%|            if not isinstance(count, int):
   457|         0|            0|            0|  0.00%|                raise TypeError(
   458|         0|            0|            0|  0.00%|                    "count must be a positive integer (got {!r})".format(count))
   459|         0|            0|            0|  0.00%|            if count <= 0:
   460|         0|            0|            0|  0.00%|                raise ValueError(
   461|         0|            0|            0|  0.00%|                    "count must be a positive integer (got {!r})".format(count))
   462|         0|            0|            0|  0.00%|
   463|         0|            0|            0|  0.00%|    def sendfile(self, file, offset=0, count=None):
   464|         0|            0|            0|  0.00%|        """sendfile(file[, offset[, count]]) -> sent
   465|         0|            0|            0|  0.00%|
   466|         0|            0|            0|  0.00%|        Send a file until EOF is reached by using high-performance
   467|         0|            0|            0|  0.00%|        os.sendfile() and return the total number of bytes which
   468|         0|            0|            0|  0.00%|        were sent.
   469|         0|            0|            0|  0.00%|        *file* must be a regular file object opened in binary mode.
   470|         0|            0|            0|  0.00%|        If os.sendfile() is not available (e.g. Windows) or file is
   471|         0|            0|            0|  0.00%|        not a regular file socket.send() will be used instead.
   472|         0|            0|            0|  0.00%|        *offset* tells from where to start reading the file.
   473|         0|            0|            0|  0.00%|        If specified, *count* is the total number of bytes to transmit
   474|         0|            0|            0|  0.00%|        as opposed to sending the file until EOF is reached.
   475|         0|            0|            0|  0.00%|        File position is updated on return or also in case of error in
   476|         0|            0|            0|  0.00%|        which case file.tell() can be used to figure out the number of
   477|         0|            0|            0|  0.00%|        bytes which were sent.
   478|         0|            0|            0|  0.00%|        The socket must be of SOCK_STREAM type.
   479|         0|            0|            0|  0.00%|        Non-blocking sockets are not supported.
   480|         0|            0|            0|  0.00%|        """
   481|         0|            0|            0|  0.00%|        try:
   482|         0|            0|            0|  0.00%|            return self._sendfile_use_sendfile(file, offset, count)
   483|         0|            0|            0|  0.00%|        except _GiveupOnSendfile:
   484|         0|            0|            0|  0.00%|            return self._sendfile_use_send(file, offset, count)
   485|         0|            0|            0|  0.00%|
   486|         0|            0|            0|  0.00%|    def _decref_socketios(self):
   487|         0|            0|            0|  0.00%|        if self._io_refs > 0:
   488|         0|            0|            0|  0.00%|            self._io_refs -= 1
   489|         0|            0|            0|  0.00%|        if self._closed:
   490|         0|            0|            0|  0.00%|            self.close()
   491|         0|            0|            0|  0.00%|
   492|       200|  0.000850677|  4.25339e-06|  0.00%|    def _real_close(self, _ss=_socket.socket):
   493|         0|            0|            0|  0.00%|        # This function should not reference any globals. See issue #808164.
   494|       200|   0.00164509|  8.22544e-06|  0.00%|        _ss.close(self)
   495|         0|            0|            0|  0.00%|
   496|       200|   0.00087595|  4.37975e-06|  0.00%|    def close(self):
   497|         0|            0|            0|  0.00%|        # This function should not reference any globals. See issue #808164.
   498|       200|  0.000835657|  4.17829e-06|  0.00%|        self._closed = True
   499|       200|  0.000812531|  4.06265e-06|  0.00%|        if self._io_refs <= 0:
   500|       200|   0.00261092|  1.30546e-05|  0.00%|            self._real_close()
(call)|       200|   0.00249577|  1.24788e-05|  0.00%|# /opt/conda/lib/python3.8/socket.py:492 _real_close
   501|         0|            0|            0|  0.00%|
   502|       200|   0.00114465|  5.72324e-06|  0.00%|    def detach(self):
   503|         0|            0|            0|  0.00%|        """detach() -> file descriptor
   504|         0|            0|            0|  0.00%|
   505|         0|            0|            0|  0.00%|        Close the socket object without closing the underlying file descriptor.
   506|         0|            0|            0|  0.00%|        The object cannot be used after this call, but the file descriptor
   507|         0|            0|            0|  0.00%|        can be reused for other purposes.  The file descriptor is returned.
   508|         0|            0|            0|  0.00%|        """
   509|       200|  0.000937462|  4.68731e-06|  0.00%|        self._closed = True
   510|       200|   0.00167751|  8.38757e-06|  0.00%|        return super().detach()
   511|         0|            0|            0|  0.00%|
   512|         0|            0|            0|  0.00%|    @property
   513|         0|            0|            0|  0.00%|    def family(self):
   514|         0|            0|            0|  0.00%|        """Read-only access to the address family for this socket.
   515|         0|            0|            0|  0.00%|        """
   516|         0|            0|            0|  0.00%|        return _intenum_converter(super().family, AddressFamily)
   517|         0|            0|            0|  0.00%|
   518|         0|            0|            0|  0.00%|    @property
   519|         0|            0|            0|  0.00%|    def type(self):
   520|         0|            0|            0|  0.00%|        """Read-only access to the socket type.
   521|         0|            0|            0|  0.00%|        """
   522|         0|            0|            0|  0.00%|        return _intenum_converter(super().type, SocketKind)
   523|         0|            0|            0|  0.00%|
   524|         0|            0|            0|  0.00%|    if os.name == 'nt':
   525|         0|            0|            0|  0.00%|        def get_inheritable(self):
   526|         0|            0|            0|  0.00%|            return os.get_handle_inheritable(self.fileno())
   527|         0|            0|            0|  0.00%|        def set_inheritable(self, inheritable):
   528|         0|            0|            0|  0.00%|            os.set_handle_inheritable(self.fileno(), inheritable)
   529|         0|            0|            0|  0.00%|    else:
   530|         0|            0|            0|  0.00%|        def get_inheritable(self):
   531|         0|            0|            0|  0.00%|            return os.get_inheritable(self.fileno())
   532|         0|            0|            0|  0.00%|        def set_inheritable(self, inheritable):
   533|         0|            0|            0|  0.00%|            os.set_inheritable(self.fileno(), inheritable)
   534|         0|            0|            0|  0.00%|    get_inheritable.__doc__ = "Get the inheritable flag of the socket"
   535|         0|            0|            0|  0.00%|    set_inheritable.__doc__ = "Set the inheritable flag of the socket"
   536|         0|            0|            0|  0.00%|
   537|       200|   0.00101566|  5.07832e-06|  0.00%|def fromfd(fd, family, type, proto=0):
   538|         0|            0|            0|  0.00%|    """ fromfd(fd, family, type[, proto]) -> socket object
   539|         0|            0|            0|  0.00%|
   540|         0|            0|            0|  0.00%|    Create a socket object from a duplicate of the given file
   541|         0|            0|            0|  0.00%|    descriptor.  The remaining arguments are the same as for socket().
   542|         0|            0|            0|  0.00%|    """
   543|       200|   0.00188684|  9.43422e-06|  0.00%|    nfd = dup(fd)
   544|       200|   0.00288725|  1.44362e-05|  0.01%|    return socket(family, type, proto, nfd)
(call)|       200|   0.00504994|  2.52497e-05|  0.01%|# /opt/conda/lib/python3.8/socket.py:219 __init__
   545|         0|            0|            0|  0.00%|
   546|         0|            0|            0|  0.00%|if hasattr(_socket.socket, "share"):
   547|         0|            0|            0|  0.00%|    def fromshare(info):
   548|         0|            0|            0|  0.00%|        """ fromshare(info) -> socket object
   549|         0|            0|            0|  0.00%|
   550|         0|            0|            0|  0.00%|        Create a socket object from the bytes object returned by
   551|         0|            0|            0|  0.00%|        socket.share(pid).
   552|         0|            0|            0|  0.00%|        """
   553|         0|            0|            0|  0.00%|        return socket(0, 0, 0, info)
   554|         0|            0|            0|  0.00%|    __all__.append("fromshare")
   555|         0|            0|            0|  0.00%|
   556|         0|            0|            0|  0.00%|if hasattr(_socket, "socketpair"):
   557|         0|            0|            0|  0.00%|
   558|         0|            0|            0|  0.00%|    def socketpair(family=None, type=SOCK_STREAM, proto=0):
   559|         0|            0|            0|  0.00%|        """socketpair([family[, type[, proto]]]) -> (socket object, socket object)
   560|         0|            0|            0|  0.00%|
   561|         0|            0|            0|  0.00%|        Create a pair of socket objects from the sockets returned by the platform
   562|         0|            0|            0|  0.00%|        socketpair() function.
   563|         0|            0|            0|  0.00%|        The arguments are the same as for socket() except the default family is
   564|         0|            0|            0|  0.00%|        AF_UNIX if defined on the platform; otherwise, the default is AF_INET.
   565|         0|            0|            0|  0.00%|        """
   566|         0|            0|            0|  0.00%|        if family is None:
   567|         0|            0|            0|  0.00%|            try:
   568|         0|            0|            0|  0.00%|                family = AF_UNIX
   569|         0|            0|            0|  0.00%|            except NameError:
   570|         0|            0|            0|  0.00%|                family = AF_INET
   571|         0|            0|            0|  0.00%|        a, b = _socket.socketpair(family, type, proto)
   572|         0|            0|            0|  0.00%|        a = socket(family, type, proto, a.detach())
   573|         0|            0|            0|  0.00%|        b = socket(family, type, proto, b.detach())
   574|         0|            0|            0|  0.00%|        return a, b
   575|         0|            0|            0|  0.00%|
   576|         0|            0|            0|  0.00%|else:
   577|         0|            0|            0|  0.00%|
   578|         0|            0|            0|  0.00%|    # Origin: https://gist.github.com/4325783, by Geert Jansen.  Public domain.
   579|         0|            0|            0|  0.00%|    def socketpair(family=AF_INET, type=SOCK_STREAM, proto=0):
   580|         0|            0|            0|  0.00%|        if family == AF_INET:
   581|         0|            0|            0|  0.00%|            host = _LOCALHOST
   582|         0|            0|            0|  0.00%|        elif family == AF_INET6:
   583|         0|            0|            0|  0.00%|            host = _LOCALHOST_V6
   584|         0|            0|            0|  0.00%|        else:
   585|         0|            0|            0|  0.00%|            raise ValueError("Only AF_INET and AF_INET6 socket address families "
   586|         0|            0|            0|  0.00%|                             "are supported")
   587|         0|            0|            0|  0.00%|        if type != SOCK_STREAM:
   588|         0|            0|            0|  0.00%|            raise ValueError("Only SOCK_STREAM socket type is supported")
   589|         0|            0|            0|  0.00%|        if proto != 0:
   590|         0|            0|            0|  0.00%|            raise ValueError("Only protocol zero is supported")
   591|         0|            0|            0|  0.00%|
   592|         0|            0|            0|  0.00%|        # We create a connected TCP socket. Note the trick with
   593|         0|            0|            0|  0.00%|        # setblocking(False) that prevents us from having to create a thread.
   594|         0|            0|            0|  0.00%|        lsock = socket(family, type, proto)
   595|         0|            0|            0|  0.00%|        try:
   596|         0|            0|            0|  0.00%|            lsock.bind((host, 0))
   597|         0|            0|            0|  0.00%|            lsock.listen()
   598|         0|            0|            0|  0.00%|            # On IPv6, ignore flow_info and scope_id
   599|         0|            0|            0|  0.00%|            addr, port = lsock.getsockname()[:2]
   600|         0|            0|            0|  0.00%|            csock = socket(family, type, proto)
   601|         0|            0|            0|  0.00%|            try:
   602|         0|            0|            0|  0.00%|                csock.setblocking(False)
   603|         0|            0|            0|  0.00%|                try:
   604|         0|            0|            0|  0.00%|                    csock.connect((addr, port))
   605|         0|            0|            0|  0.00%|                except (BlockingIOError, InterruptedError):
   606|         0|            0|            0|  0.00%|                    pass
   607|         0|            0|            0|  0.00%|                csock.setblocking(True)
   608|         0|            0|            0|  0.00%|                ssock, _ = lsock.accept()
   609|         0|            0|            0|  0.00%|            except:
   610|         0|            0|            0|  0.00%|                csock.close()
   611|         0|            0|            0|  0.00%|                raise
   612|         0|            0|            0|  0.00%|        finally:
   613|         0|            0|            0|  0.00%|            lsock.close()
   614|         0|            0|            0|  0.00%|        return (ssock, csock)
   615|         0|            0|            0|  0.00%|    __all__.append("socketpair")
   616|         0|            0|            0|  0.00%|
   617|         0|            0|            0|  0.00%|socketpair.__doc__ = """socketpair([family[, type[, proto]]]) -> (socket object, socket object)
   618|         0|            0|            0|  0.00%|Create a pair of socket objects from the sockets returned by the platform
   619|         0|            0|            0|  0.00%|socketpair() function.
   620|         0|            0|            0|  0.00%|The arguments are the same as for socket() except the default family is AF_UNIX
   621|         0|            0|            0|  0.00%|if defined on the platform; otherwise, the default is AF_INET.
   622|         0|            0|            0|  0.00%|"""
   623|         0|            0|            0|  0.00%|
   624|         0|            0|            0|  0.00%|_blocking_errnos = { EAGAIN, EWOULDBLOCK }
   625|         0|            0|            0|  0.00%|
   626|         0|            0|            0|  0.00%|class SocketIO(io.RawIOBase):
   627|         0|            0|            0|  0.00%|
   628|         0|            0|            0|  0.00%|    """Raw I/O implementation for stream sockets.
   629|         0|            0|            0|  0.00%|
   630|         0|            0|            0|  0.00%|    This class supports the makefile() method on sockets.  It provides
   631|         0|            0|            0|  0.00%|    the raw I/O interface on top of a socket object.
   632|         0|            0|            0|  0.00%|    """
   633|         0|            0|            0|  0.00%|
   634|         0|            0|            0|  0.00%|    # One might wonder why not let FileIO do the job instead.  There are two
   635|         0|            0|            0|  0.00%|    # main reasons why FileIO is not adapted:
   636|         0|            0|            0|  0.00%|    # - it wouldn't work under Windows (where you can't used read() and
   637|         0|            0|            0|  0.00%|    #   write() on a socket handle)
   638|         0|            0|            0|  0.00%|    # - it wouldn't work with socket timeouts (FileIO would ignore the
   639|         0|            0|            0|  0.00%|    #   timeout and consider the socket non-blocking)
   640|         0|            0|            0|  0.00%|
   641|         0|            0|            0|  0.00%|    # XXX More docs
   642|         0|            0|            0|  0.00%|
   643|         0|            0|            0|  0.00%|    def __init__(self, sock, mode):
   644|         0|            0|            0|  0.00%|        if mode not in ("r", "w", "rw", "rb", "wb", "rwb"):
   645|         0|            0|            0|  0.00%|            raise ValueError("invalid mode: %r" % mode)
   646|         0|            0|            0|  0.00%|        io.RawIOBase.__init__(self)
   647|         0|            0|            0|  0.00%|        self._sock = sock
   648|         0|            0|            0|  0.00%|        if "b" not in mode:
   649|         0|            0|            0|  0.00%|            mode += "b"
   650|         0|            0|            0|  0.00%|        self._mode = mode
   651|         0|            0|            0|  0.00%|        self._reading = "r" in mode
   652|         0|            0|            0|  0.00%|        self._writing = "w" in mode
   653|         0|            0|            0|  0.00%|        self._timeout_occurred = False
   654|         0|            0|            0|  0.00%|
   655|         0|            0|            0|  0.00%|    def readinto(self, b):
   656|         0|            0|            0|  0.00%|        """Read up to len(b) bytes into the writable buffer *b* and return
   657|         0|            0|            0|  0.00%|        the number of bytes read.  If the socket is non-blocking and no bytes
   658|         0|            0|            0|  0.00%|        are available, None is returned.
   659|         0|            0|            0|  0.00%|
   660|         0|            0|            0|  0.00%|        If *b* is non-empty, a 0 return value indicates that the connection
   661|         0|            0|            0|  0.00%|        was shutdown at the other end.
   662|         0|            0|            0|  0.00%|        """
   663|         0|            0|            0|  0.00%|        self._checkClosed()
   664|         0|            0|            0|  0.00%|        self._checkReadable()
   665|         0|            0|            0|  0.00%|        if self._timeout_occurred:
   666|         0|            0|            0|  0.00%|            raise OSError("cannot read from timed out object")
   667|         0|            0|            0|  0.00%|        while True:
   668|         0|            0|            0|  0.00%|            try:
   669|         0|            0|            0|  0.00%|                return self._sock.recv_into(b)
   670|         0|            0|            0|  0.00%|            except timeout:
   671|         0|            0|            0|  0.00%|                self._timeout_occurred = True
   672|         0|            0|            0|  0.00%|                raise
   673|         0|            0|            0|  0.00%|            except error as e:
   674|         0|            0|            0|  0.00%|                if e.args[0] in _blocking_errnos:
   675|         0|            0|            0|  0.00%|                    return None
   676|         0|            0|            0|  0.00%|                raise
   677|         0|            0|            0|  0.00%|
   678|         0|            0|            0|  0.00%|    def write(self, b):
   679|         0|            0|            0|  0.00%|        """Write the given bytes or bytearray object *b* to the socket
   680|         0|            0|            0|  0.00%|        and return the number of bytes written.  This can be less than
   681|         0|            0|            0|  0.00%|        len(b) if not all data could be written.  If the socket is
   682|         0|            0|            0|  0.00%|        non-blocking and no bytes could be written None is returned.
   683|         0|            0|            0|  0.00%|        """
   684|         0|            0|            0|  0.00%|        self._checkClosed()
   685|         0|            0|            0|  0.00%|        self._checkWritable()
   686|         0|            0|            0|  0.00%|        try:
   687|         0|            0|            0|  0.00%|            return self._sock.send(b)
   688|         0|            0|            0|  0.00%|        except error as e:
   689|         0|            0|            0|  0.00%|            # XXX what about EINTR?
   690|         0|            0|            0|  0.00%|            if e.args[0] in _blocking_errnos:
   691|         0|            0|            0|  0.00%|                return None
   692|         0|            0|            0|  0.00%|            raise
   693|         0|            0|            0|  0.00%|
   694|         0|            0|            0|  0.00%|    def readable(self):
   695|         0|            0|            0|  0.00%|        """True if the SocketIO is open for reading.
   696|         0|            0|            0|  0.00%|        """
   697|         0|            0|            0|  0.00%|        if self.closed:
   698|         0|            0|            0|  0.00%|            raise ValueError("I/O operation on closed socket.")
   699|         0|            0|            0|  0.00%|        return self._reading
   700|         0|            0|            0|  0.00%|
   701|         0|            0|            0|  0.00%|    def writable(self):
   702|         0|            0|            0|  0.00%|        """True if the SocketIO is open for writing.
   703|         0|            0|            0|  0.00%|        """
   704|         0|            0|            0|  0.00%|        if self.closed:
   705|         0|            0|            0|  0.00%|            raise ValueError("I/O operation on closed socket.")
   706|         0|            0|            0|  0.00%|        return self._writing
   707|         0|            0|            0|  0.00%|
   708|         0|            0|            0|  0.00%|    def seekable(self):
   709|         0|            0|            0|  0.00%|        """True if the SocketIO is open for seeking.
   710|         0|            0|            0|  0.00%|        """
   711|         0|            0|            0|  0.00%|        if self.closed:
   712|         0|            0|            0|  0.00%|            raise ValueError("I/O operation on closed socket.")
   713|         0|            0|            0|  0.00%|        return super().seekable()
   714|         0|            0|            0|  0.00%|
   715|         0|            0|            0|  0.00%|    def fileno(self):
   716|         0|            0|            0|  0.00%|        """Return the file descriptor of the underlying socket.
   717|         0|            0|            0|  0.00%|        """
   718|         0|            0|            0|  0.00%|        self._checkClosed()
   719|         0|            0|            0|  0.00%|        return self._sock.fileno()
   720|         0|            0|            0|  0.00%|
   721|         0|            0|            0|  0.00%|    @property
   722|         0|            0|            0|  0.00%|    def name(self):
   723|         0|            0|            0|  0.00%|        if not self.closed:
   724|         0|            0|            0|  0.00%|            return self.fileno()
   725|         0|            0|            0|  0.00%|        else:
   726|         0|            0|            0|  0.00%|            return -1
   727|         0|            0|            0|  0.00%|
   728|         0|            0|            0|  0.00%|    @property
   729|         0|            0|            0|  0.00%|    def mode(self):
   730|         0|            0|            0|  0.00%|        return self._mode
   731|         0|            0|            0|  0.00%|
   732|         0|            0|            0|  0.00%|    def close(self):
   733|         0|            0|            0|  0.00%|        """Close the SocketIO object.  This doesn't close the underlying
   734|         0|            0|            0|  0.00%|        socket, except if all references to it have disappeared.
   735|         0|            0|            0|  0.00%|        """
   736|         0|            0|            0|  0.00%|        if self.closed:
   737|         0|            0|            0|  0.00%|            return
   738|         0|            0|            0|  0.00%|        io.RawIOBase.close(self)
   739|         0|            0|            0|  0.00%|        self._sock._decref_socketios()
   740|         0|            0|            0|  0.00%|        self._sock = None
   741|         0|            0|            0|  0.00%|
   742|         0|            0|            0|  0.00%|
   743|         0|            0|            0|  0.00%|def getfqdn(name=''):
   744|         0|            0|            0|  0.00%|    """Get fully qualified domain name from name.
   745|         0|            0|            0|  0.00%|
   746|         0|            0|            0|  0.00%|    An empty argument is interpreted as meaning the local host.
   747|         0|            0|            0|  0.00%|
   748|         0|            0|            0|  0.00%|    First the hostname returned by gethostbyaddr() is checked, then
   749|         0|            0|            0|  0.00%|    possibly existing aliases. In case no FQDN is available, hostname
   750|         0|            0|            0|  0.00%|    from gethostname() is returned.
   751|         0|            0|            0|  0.00%|    """
   752|         0|            0|            0|  0.00%|    name = name.strip()
   753|         0|            0|            0|  0.00%|    if not name or name == '0.0.0.0':
   754|         0|            0|            0|  0.00%|        name = gethostname()
   755|         0|            0|            0|  0.00%|    try:
   756|         0|            0|            0|  0.00%|        hostname, aliases, ipaddrs = gethostbyaddr(name)
   757|         0|            0|            0|  0.00%|    except error:
   758|         0|            0|            0|  0.00%|        pass
   759|         0|            0|            0|  0.00%|    else:
   760|         0|            0|            0|  0.00%|        aliases.insert(0, hostname)
   761|         0|            0|            0|  0.00%|        for name in aliases:
   762|         0|            0|            0|  0.00%|            if '.' in name:
   763|         0|            0|            0|  0.00%|                break
   764|         0|            0|            0|  0.00%|        else:
   765|         0|            0|            0|  0.00%|            name = hostname
   766|         0|            0|            0|  0.00%|    return name
   767|         0|            0|            0|  0.00%|
   768|         0|            0|            0|  0.00%|
   769|         0|            0|            0|  0.00%|_GLOBAL_DEFAULT_TIMEOUT = object()
   770|         0|            0|            0|  0.00%|
   771|         0|            0|            0|  0.00%|def create_connection(address, timeout=_GLOBAL_DEFAULT_TIMEOUT,
   772|         0|            0|            0|  0.00%|                      source_address=None):
   773|         0|            0|            0|  0.00%|    """Connect to *address* and return the socket object.
   774|         0|            0|            0|  0.00%|
   775|         0|            0|            0|  0.00%|    Convenience function.  Connect to *address* (a 2-tuple ``(host,
   776|         0|            0|            0|  0.00%|    port)``) and return the socket object.  Passing the optional
   777|         0|            0|            0|  0.00%|    *timeout* parameter will set the timeout on the socket instance
   778|         0|            0|            0|  0.00%|    before attempting to connect.  If no *timeout* is supplied, the
   779|         0|            0|            0|  0.00%|    global default timeout setting returned by :func:`getdefaulttimeout`
   780|         0|            0|            0|  0.00%|    is used.  If *source_address* is set it must be a tuple of (host, port)
   781|         0|            0|            0|  0.00%|    for the socket to bind as a source address before making the connection.
   782|         0|            0|            0|  0.00%|    A host of '' or port 0 tells the OS to use the default.
   783|         0|            0|            0|  0.00%|    """
   784|         0|            0|            0|  0.00%|
   785|         0|            0|            0|  0.00%|    host, port = address
   786|         0|            0|            0|  0.00%|    err = None
   787|         0|            0|            0|  0.00%|    for res in getaddrinfo(host, port, 0, SOCK_STREAM):
   788|         0|            0|            0|  0.00%|        af, socktype, proto, canonname, sa = res
   789|         0|            0|            0|  0.00%|        sock = None
   790|         0|            0|            0|  0.00%|        try:
   791|         0|            0|            0|  0.00%|            sock = socket(af, socktype, proto)
   792|         0|            0|            0|  0.00%|            if timeout is not _GLOBAL_DEFAULT_TIMEOUT:
   793|         0|            0|            0|  0.00%|                sock.settimeout(timeout)
   794|         0|            0|            0|  0.00%|            if source_address:
   795|         0|            0|            0|  0.00%|                sock.bind(source_address)
   796|         0|            0|            0|  0.00%|            sock.connect(sa)
   797|         0|            0|            0|  0.00%|            # Break explicitly a reference cycle
   798|         0|            0|            0|  0.00%|            err = None
   799|         0|            0|            0|  0.00%|            return sock
   800|         0|            0|            0|  0.00%|
   801|         0|            0|            0|  0.00%|        except error as _:
   802|         0|            0|            0|  0.00%|            err = _
   803|         0|            0|            0|  0.00%|            if sock is not None:
   804|         0|            0|            0|  0.00%|                sock.close()
   805|         0|            0|            0|  0.00%|
   806|         0|            0|            0|  0.00%|    if err is not None:
   807|         0|            0|            0|  0.00%|        try:
   808|         0|            0|            0|  0.00%|            raise err
   809|         0|            0|            0|  0.00%|        finally:
   810|         0|            0|            0|  0.00%|            # Break explicitly a reference cycle
   811|         0|            0|            0|  0.00%|            err = None
   812|         0|            0|            0|  0.00%|    else:
   813|         0|            0|            0|  0.00%|        raise error("getaddrinfo returns an empty list")
   814|         0|            0|            0|  0.00%|
   815|         0|            0|            0|  0.00%|
   816|         0|            0|            0|  0.00%|def has_dualstack_ipv6():
   817|         0|            0|            0|  0.00%|    """Return True if the platform supports creating a SOCK_STREAM socket
   818|         0|            0|            0|  0.00%|    which can handle both AF_INET and AF_INET6 (IPv4 / IPv6) connections.
   819|         0|            0|            0|  0.00%|    """
   820|         0|            0|            0|  0.00%|    if not has_ipv6 \
   821|         0|            0|            0|  0.00%|            or not hasattr(_socket, 'IPPROTO_IPV6') \
   822|         0|            0|            0|  0.00%|            or not hasattr(_socket, 'IPV6_V6ONLY'):
   823|         0|            0|            0|  0.00%|        return False
   824|         0|            0|            0|  0.00%|    try:
   825|         0|            0|            0|  0.00%|        with socket(AF_INET6, SOCK_STREAM) as sock:
   826|         0|            0|            0|  0.00%|            sock.setsockopt(IPPROTO_IPV6, IPV6_V6ONLY, 0)
   827|         0|            0|            0|  0.00%|            return True
   828|         0|            0|            0|  0.00%|    except error:
   829|         0|            0|            0|  0.00%|        return False
   830|         0|            0|            0|  0.00%|
   831|         0|            0|            0|  0.00%|
   832|         0|            0|            0|  0.00%|def create_server(address, *, family=AF_INET, backlog=None, reuse_port=False,
   833|         0|            0|            0|  0.00%|                  dualstack_ipv6=False):
   834|         0|            0|            0|  0.00%|    """Convenience function which creates a SOCK_STREAM type socket
   835|         0|            0|            0|  0.00%|    bound to *address* (a 2-tuple (host, port)) and return the socket
   836|         0|            0|            0|  0.00%|    object.
   837|         0|            0|            0|  0.00%|
   838|         0|            0|            0|  0.00%|    *family* should be either AF_INET or AF_INET6.
   839|         0|            0|            0|  0.00%|    *backlog* is the queue size passed to socket.listen().
   840|         0|            0|            0|  0.00%|    *reuse_port* dictates whether to use the SO_REUSEPORT socket option.
   841|         0|            0|            0|  0.00%|    *dualstack_ipv6*: if true and the platform supports it, it will
   842|         0|            0|            0|  0.00%|    create an AF_INET6 socket able to accept both IPv4 or IPv6
   843|         0|            0|            0|  0.00%|    connections. When false it will explicitly disable this option on
   844|         0|            0|            0|  0.00%|    platforms that enable it by default (e.g. Linux).
   845|         0|            0|            0|  0.00%|
   846|         0|            0|            0|  0.00%|    >>> with create_server(('', 8000)) as server:
   847|         0|            0|            0|  0.00%|    ...     while True:
   848|         0|            0|            0|  0.00%|    ...         conn, addr = server.accept()
   849|         0|            0|            0|  0.00%|    ...         # handle new connection
   850|         0|            0|            0|  0.00%|    """
   851|         0|            0|            0|  0.00%|    if reuse_port and not hasattr(_socket, "SO_REUSEPORT"):
   852|         0|            0|            0|  0.00%|        raise ValueError("SO_REUSEPORT not supported on this platform")
   853|         0|            0|            0|  0.00%|    if dualstack_ipv6:
   854|         0|            0|            0|  0.00%|        if not has_dualstack_ipv6():
   855|         0|            0|            0|  0.00%|            raise ValueError("dualstack_ipv6 not supported on this platform")
   856|         0|            0|            0|  0.00%|        if family != AF_INET6:
   857|         0|            0|            0|  0.00%|            raise ValueError("dualstack_ipv6 requires AF_INET6 family")
   858|         0|            0|            0|  0.00%|    sock = socket(family, SOCK_STREAM)
   859|         0|            0|            0|  0.00%|    try:
   860|         0|            0|            0|  0.00%|        # Note about Windows. We don't set SO_REUSEADDR because:
   861|         0|            0|            0|  0.00%|        # 1) It's unnecessary: bind() will succeed even in case of a
   862|         0|            0|            0|  0.00%|        # previous closed socket on the same address and still in
   863|         0|            0|            0|  0.00%|        # TIME_WAIT state.
   864|         0|            0|            0|  0.00%|        # 2) If set, another socket is free to bind() on the same
   865|         0|            0|            0|  0.00%|        # address, effectively preventing this one from accepting
   866|         0|            0|            0|  0.00%|        # connections. Also, it may set the process in a state where
   867|         0|            0|            0|  0.00%|        # it'll no longer respond to any signals or graceful kills.
   868|         0|            0|            0|  0.00%|        # See: msdn2.microsoft.com/en-us/library/ms740621(VS.85).aspx
   869|         0|            0|            0|  0.00%|        if os.name not in ('nt', 'cygwin') and \
   870|         0|            0|            0|  0.00%|                hasattr(_socket, 'SO_REUSEADDR'):
   871|         0|            0|            0|  0.00%|            try:
   872|         0|            0|            0|  0.00%|                sock.setsockopt(SOL_SOCKET, SO_REUSEADDR, 1)
   873|         0|            0|            0|  0.00%|            except error:
   874|         0|            0|            0|  0.00%|                # Fail later on bind(), for platforms which may not
   875|         0|            0|            0|  0.00%|                # support this option.
   876|         0|            0|            0|  0.00%|                pass
   877|         0|            0|            0|  0.00%|        if reuse_port:
   878|         0|            0|            0|  0.00%|            sock.setsockopt(SOL_SOCKET, SO_REUSEPORT, 1)
   879|         0|            0|            0|  0.00%|        if has_ipv6 and family == AF_INET6:
   880|         0|            0|            0|  0.00%|            if dualstack_ipv6:
   881|         0|            0|            0|  0.00%|                sock.setsockopt(IPPROTO_IPV6, IPV6_V6ONLY, 0)
   882|         0|            0|            0|  0.00%|            elif hasattr(_socket, "IPV6_V6ONLY") and \
   883|         0|            0|            0|  0.00%|                    hasattr(_socket, "IPPROTO_IPV6"):
   884|         0|            0|            0|  0.00%|                sock.setsockopt(IPPROTO_IPV6, IPV6_V6ONLY, 1)
   885|         0|            0|            0|  0.00%|        try:
   886|         0|            0|            0|  0.00%|            sock.bind(address)
   887|         0|            0|            0|  0.00%|        except error as err:
   888|         0|            0|            0|  0.00%|            msg = '%s (while attempting to bind on address %r)' % \
   889|         0|            0|            0|  0.00%|                (err.strerror, address)
   890|         0|            0|            0|  0.00%|            raise error(err.errno, msg) from None
   891|         0|            0|            0|  0.00%|        if backlog is None:
   892|         0|            0|            0|  0.00%|            sock.listen()
   893|         0|            0|            0|  0.00%|        else:
   894|         0|            0|            0|  0.00%|            sock.listen(backlog)
   895|         0|            0|            0|  0.00%|        return sock
   896|         0|            0|            0|  0.00%|    except error:
   897|         0|            0|            0|  0.00%|        sock.close()
   898|         0|            0|            0|  0.00%|        raise
   899|         0|            0|            0|  0.00%|
   900|         0|            0|            0|  0.00%|
   901|         0|            0|            0|  0.00%|def getaddrinfo(host, port, family=0, type=0, proto=0, flags=0):
   902|         0|            0|            0|  0.00%|    """Resolve host and port into list of address info entries.
   903|         0|            0|            0|  0.00%|
   904|         0|            0|            0|  0.00%|    Translate the host/port argument into a sequence of 5-tuples that contain
   905|         0|            0|            0|  0.00%|    all the necessary arguments for creating a socket connected to that service.
   906|         0|            0|            0|  0.00%|    host is a domain name, a string representation of an IPv4/v6 address or
   907|         0|            0|            0|  0.00%|    None. port is a string service name such as 'http', a numeric port number or
   908|         0|            0|            0|  0.00%|    None. By passing None as the value of host and port, you can pass NULL to
   909|         0|            0|            0|  0.00%|    the underlying C API.
   910|         0|            0|            0|  0.00%|
   911|         0|            0|            0|  0.00%|    The family, type and proto arguments can be optionally specified in order to
   912|         0|            0|            0|  0.00%|    narrow the list of addresses returned. Passing zero as a value for each of
   913|         0|            0|            0|  0.00%|    these arguments selects the full range of results.
   914|         0|            0|            0|  0.00%|    """
   915|         0|            0|            0|  0.00%|    # We override this function since we want to translate the numeric family
   916|         0|            0|            0|  0.00%|    # and socket type values to enum constants.
   917|         0|            0|            0|  0.00%|    addrlist = []
   918|         0|            0|            0|  0.00%|    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):
   919|         0|            0|            0|  0.00%|        af, socktype, proto, canonname, sa = res
   920|         0|            0|            0|  0.00%|        addrlist.append((_intenum_converter(af, AddressFamily),
   921|         0|            0|            0|  0.00%|                         _intenum_converter(socktype, SocketKind),
   922|         0|            0|            0|  0.00%|                         proto, canonname, sa))
   923|         0|            0|            0|  0.00%|    return addrlist
File: /opt/conda/lib/python3.8/multiprocessing/queues.py
File duration: 0.0408783s (0.08%)
Line #|      Hits|         Time| Time per hit|      %|Source code
------+----------+-------------+-------------+-------+-----------
     1|         0|            0|            0|  0.00%|#
     2|         0|            0|            0|  0.00%|# Module implementing queues
     3|         0|            0|            0|  0.00%|#
     4|         0|            0|            0|  0.00%|# multiprocessing/queues.py
     5|         0|            0|            0|  0.00%|#
     6|         0|            0|            0|  0.00%|# Copyright (c) 2006-2008, R Oudkerk
     7|         0|            0|            0|  0.00%|# Licensed to PSF under a Contributor Agreement.
     8|         0|            0|            0|  0.00%|#
     9|         0|            0|            0|  0.00%|
    10|         0|            0|            0|  0.00%|__all__ = ['Queue', 'SimpleQueue', 'JoinableQueue']
    11|         0|            0|            0|  0.00%|
    12|         0|            0|            0|  0.00%|import sys
    13|         0|            0|            0|  0.00%|import os
    14|         0|            0|            0|  0.00%|import threading
    15|         0|            0|            0|  0.00%|import collections
    16|         0|            0|            0|  0.00%|import time
    17|         0|            0|            0|  0.00%|import weakref
    18|         0|            0|            0|  0.00%|import errno
    19|         0|            0|            0|  0.00%|
    20|         0|            0|            0|  0.00%|from queue import Empty, Full
    21|         0|            0|            0|  0.00%|
    22|         0|            0|            0|  0.00%|import _multiprocessing
    23|         0|            0|            0|  0.00%|
    24|         0|            0|            0|  0.00%|from . import connection
    25|         0|            0|            0|  0.00%|from . import context
    26|         0|            0|            0|  0.00%|_ForkingPickler = context.reduction.ForkingPickler
    27|         0|            0|            0|  0.00%|
    28|         0|            0|            0|  0.00%|from .util import debug, info, Finalize, register_after_fork, is_exiting
    29|         0|            0|            0|  0.00%|
    30|         0|            0|            0|  0.00%|#
    31|         0|            0|            0|  0.00%|# Queue type using a pipe, buffer and thread
    32|         0|            0|            0|  0.00%|#
    33|         0|            0|            0|  0.00%|
    34|         0|            0|            0|  0.00%|class Queue(object):
    35|         0|            0|            0|  0.00%|
    36|        10|  7.67708e-05|  7.67708e-06|  0.00%|    def __init__(self, maxsize=0, *, ctx):
    37|        10|  5.79357e-05|  5.79357e-06|  0.00%|        if maxsize <= 0:
    38|         0|            0|            0|  0.00%|            # Can raise ImportError (see issues #3770 and #23400)
    39|        10|  0.000658274|  6.58274e-05|  0.00%|            from .synchronize import SEM_VALUE_MAX as maxsize
(call)|        10|   0.00025177|   2.5177e-05|  0.00%|# <frozen importlib._bootstrap>:389 parent
    40|        10|  0.000139475|  1.39475e-05|  0.00%|        self._maxsize = maxsize
    41|        10|  0.000950098|  9.50098e-05|  0.00%|        self._reader, self._writer = connection.Pipe(duplex=False)
(call)|        10|   0.00213575|  0.000213575|  0.00%|# /opt/conda/lib/python3.8/multiprocessing/connection.py:516 Pipe
    42|        10|  0.000312328|  3.12328e-05|  0.00%|        self._rlock = ctx.Lock()
(call)|        10|    0.0177441|   0.00177441|  0.03%|# /opt/conda/lib/python3.8/multiprocessing/context.py:65 Lock
    43|        10|  8.84533e-05|  8.84533e-06|  0.00%|        self._opid = os.getpid()
    44|        10|  4.95911e-05|  4.95911e-06|  0.00%|        if sys.platform == 'win32':
    45|         0|            0|            0|  0.00%|            self._wlock = None
    46|         0|            0|            0|  0.00%|        else:
    47|        10|  0.000175714|  1.75714e-05|  0.00%|            self._wlock = ctx.Lock()
(call)|        10|    0.0116527|   0.00116527|  0.02%|# /opt/conda/lib/python3.8/multiprocessing/context.py:65 Lock
    48|        10|  0.000214338|  2.14338e-05|  0.00%|        self._sem = ctx.BoundedSemaphore(maxsize)
(call)|        10|    0.0119531|   0.00119531|  0.02%|# /opt/conda/lib/python3.8/multiprocessing/context.py:85 BoundedSemaphore
    49|         0|            0|            0|  0.00%|        # For use by concurrent.futures
    50|        10|    4.673e-05|    4.673e-06|  0.00%|        self._ignore_epipe = False
    51|         0|            0|            0|  0.00%|
    52|        10|  0.000253677|  2.53677e-05|  0.00%|        self._after_fork()
(call)|        10|   0.00300074|  0.000300074|  0.01%|# /opt/conda/lib/python3.8/multiprocessing/queues.py:67 _after_fork
    53|         0|            0|            0|  0.00%|
    54|        10|  5.57899e-05|  5.57899e-06|  0.00%|        if sys.platform != 'win32':
    55|        10|  0.000164747|  1.64747e-05|  0.00%|            register_after_fork(self, Queue._after_fork)
(call)|        10|  0.000939369|  9.39369e-05|  0.00%|# /opt/conda/lib/python3.8/multiprocessing/util.py:171 register_after_fork
    56|         0|            0|            0|  0.00%|
    57|         0|            0|            0|  0.00%|    def __getstate__(self):
    58|         0|            0|            0|  0.00%|        context.assert_spawning(self)
    59|         0|            0|            0|  0.00%|        return (self._ignore_epipe, self._maxsize, self._reader, self._writer,
    60|         0|            0|            0|  0.00%|                self._rlock, self._wlock, self._sem, self._opid)
    61|         0|            0|            0|  0.00%|
    62|         0|            0|            0|  0.00%|    def __setstate__(self, state):
    63|         0|            0|            0|  0.00%|        (self._ignore_epipe, self._maxsize, self._reader, self._writer,
    64|         0|            0|            0|  0.00%|         self._rlock, self._wlock, self._sem, self._opid) = state
    65|         0|            0|            0|  0.00%|        self._after_fork()
    66|         0|            0|            0|  0.00%|
    67|        10|  5.88894e-05|  5.88894e-06|  0.00%|    def _after_fork(self):
    68|        10|  0.000133276|  1.33276e-05|  0.00%|        debug('Queue._after_fork()')
(call)|        10|  8.89301e-05|  8.89301e-06|  0.00%|# /opt/conda/lib/python3.8/multiprocessing/util.py:48 debug
    69|        10|  0.000476837|  4.76837e-05|  0.00%|        self._notempty = threading.Condition(threading.Lock())
(call)|        10|    0.0012362|   0.00012362|  0.00%|# /opt/conda/lib/python3.8/threading.py:222 __init__
    70|        10|  0.000310659|  3.10659e-05|  0.00%|        self._buffer = collections.deque()
    71|        10|  6.00815e-05|  6.00815e-06|  0.00%|        self._thread = None
    72|        10|  4.57764e-05|  4.57764e-06|  0.00%|        self._jointhread = None
    73|        10|  4.69685e-05|  4.69685e-06|  0.00%|        self._joincancelled = False
    74|        10|  6.03199e-05|  6.03199e-06|  0.00%|        self._closed = False
    75|        10|  0.000104427|  1.04427e-05|  0.00%|        self._close = None
    76|        10|  0.000195503|  1.95503e-05|  0.00%|        self._send_bytes = self._writer.send_bytes
    77|        10|  0.000106096|  1.06096e-05|  0.00%|        self._recv_bytes = self._reader.recv_bytes
    78|        10|  7.67708e-05|  7.67708e-06|  0.00%|        self._poll = self._reader.poll
    79|         0|            0|            0|  0.00%|
    80|       108|  0.000633478|  5.86554e-06|  0.00%|    def put(self, obj, block=True, timeout=None):
    81|       108|  0.000535965|  4.96264e-06|  0.00%|        if self._closed:
    82|         0|            0|            0|  0.00%|            raise ValueError(f"Queue {self!r} is closed")
    83|       108|   0.00116587|  1.07951e-05|  0.00%|        if not self._sem.acquire(block, timeout):
    84|         0|            0|            0|  0.00%|            raise Full
    85|         0|            0|            0|  0.00%|
    86|       108|   0.00179219|  1.65944e-05|  0.00%|        with self._notempty:
(call)|       108|   0.00200272|  1.85437e-05|  0.00%|# /opt/conda/lib/python3.8/threading.py:246 __enter__
    87|       108|  0.000472307|  4.37321e-06|  0.00%|            if self._thread is None:
    88|         8|  0.000140905|  1.76132e-05|  0.00%|                self._start_thread()
(call)|         8|    0.0129883|   0.00162354|  0.02%|# /opt/conda/lib/python3.8/multiprocessing/queues.py:158 _start_thread
    89|       108|  0.000771523|  7.14373e-06|  0.00%|            self._buffer.append(obj)
    90|       108|   0.00280881|  2.60075e-05|  0.01%|            self._notempty.notify()
(call)|       108|    0.0103645|  9.59679e-05|  0.02%|# /opt/conda/lib/python3.8/threading.py:341 notify
(call)|       108|   0.00141287|  1.30821e-05|  0.00%|# /opt/conda/lib/python3.8/threading.py:249 __exit__
    91|         0|            0|            0|  0.00%|
    92|       100|  0.000705719|  7.05719e-06|  0.00%|    def get(self, block=True, timeout=None):
    93|       100|  0.000608444|  6.08444e-06|  0.00%|        if self._closed:
    94|         0|            0|            0|  0.00%|            raise ValueError(f"Queue {self!r} is closed")
    95|       100|  0.000464201|  4.64201e-06|  0.00%|        if block and timeout is None:
    96|         0|            0|            0|  0.00%|            with self._rlock:
    97|         0|            0|            0|  0.00%|                res = self._recv_bytes()
    98|         0|            0|            0|  0.00%|            self._sem.release()
    99|         0|            0|            0|  0.00%|        else:
   100|       100|  0.000455379|  4.55379e-06|  0.00%|            if block:
   101|       100|   0.00100732|  1.00732e-05|  0.00%|                deadline = time.monotonic() + timeout
   102|       100|    0.0014267|   1.4267e-05|  0.00%|            if not self._rlock.acquire(block, timeout):
   103|         0|            0|            0|  0.00%|                raise Empty
   104|       100|  0.000501394|  5.01394e-06|  0.00%|            try:
   105|       100|  0.000445843|  4.45843e-06|  0.00%|                if block:
   106|       100|   0.00074482|   7.4482e-06|  0.00%|                    timeout = deadline - time.monotonic()
   107|       100|   0.00165725|  1.65725e-05|  0.00%|                    if not self._poll(timeout):
(call)|       100|     0.368889|   0.00368889|  0.69%|# /opt/conda/lib/python3.8/multiprocessing/connection.py:253 poll
   108|         0|            0|            0|  0.00%|                        raise Empty
   109|         0|            0|            0|  0.00%|                elif not self._poll():
   110|         0|            0|            0|  0.00%|                    raise Empty
   111|       100|   0.00163674|  1.63674e-05|  0.00%|                res = self._recv_bytes()
(call)|       100|      0.02806|    0.0002806|  0.05%|# /opt/conda/lib/python3.8/multiprocessing/connection.py:208 recv_bytes
   112|       100|   0.00083971|   8.3971e-06|  0.00%|                self._sem.release()
   113|         0|            0|            0|  0.00%|            finally:
   114|       100|  0.000684023|  6.84023e-06|  0.00%|                self._rlock.release()
   115|         0|            0|            0|  0.00%|        # unserialize the data after having released the lock
   116|       100|     0.012758|   0.00012758|  0.02%|        return _ForkingPickler.loads(res)
(call)|       200|     0.798867|   0.00399433|  1.49%|# /opt/conda/lib/python3.8/site-packages/torch/multiprocessing/reductions.py:288 rebuild_storage_fd
(call)|       200|    0.0229375|  0.000114688|  0.04%|# /opt/conda/lib/python3.8/site-packages/torch/multiprocessing/reductions.py:88 rebuild_tensor
   117|         0|            0|            0|  0.00%|
   118|         0|            0|            0|  0.00%|    def qsize(self):
   119|         0|            0|            0|  0.00%|        # Raises NotImplementedError on Mac OSX because of broken sem_getvalue()
   120|         0|            0|            0|  0.00%|        return self._maxsize - self._sem._semlock._get_value()
   121|         0|            0|            0|  0.00%|
   122|         0|            0|            0|  0.00%|    def empty(self):
   123|         0|            0|            0|  0.00%|        return not self._poll()
   124|         0|            0|            0|  0.00%|
   125|         0|            0|            0|  0.00%|    def full(self):
   126|         0|            0|            0|  0.00%|        return self._sem._semlock._is_zero()
   127|         0|            0|            0|  0.00%|
   128|         0|            0|            0|  0.00%|    def get_nowait(self):
   129|         0|            0|            0|  0.00%|        return self.get(False)
   130|         0|            0|            0|  0.00%|
   131|         0|            0|            0|  0.00%|    def put_nowait(self, obj):
   132|         0|            0|            0|  0.00%|        return self.put(obj, False)
   133|         0|            0|            0|  0.00%|
   134|         8|  4.22001e-05|  5.27501e-06|  0.00%|    def close(self):
   135|         8|  3.95775e-05|  4.94719e-06|  0.00%|        self._closed = True
   136|         8|  3.29018e-05|  4.11272e-06|  0.00%|        try:
   137|         8|  0.000124216|   1.5527e-05|  0.00%|            self._reader.close()
(call)|         8|  0.000428915|  5.36144e-05|  0.00%|# /opt/conda/lib/python3.8/multiprocessing/connection.py:173 close
   138|         0|            0|            0|  0.00%|        finally:
   139|         8|  3.67165e-05|  4.58956e-06|  0.00%|            close = self._close
   140|         8|  3.19481e-05|  3.99351e-06|  0.00%|            if close:
   141|         8|  3.14713e-05|  3.93391e-06|  0.00%|                self._close = None
   142|         8|  0.000117779|  1.47223e-05|  0.00%|                close()
(call)|         8|   0.00224733|  0.000280917|  0.00%|# /opt/conda/lib/python3.8/multiprocessing/util.py:205 __call__
   143|         0|            0|            0|  0.00%|
   144|         0|            0|            0|  0.00%|    def join_thread(self):
   145|         0|            0|            0|  0.00%|        debug('Queue.join_thread()')
   146|         0|            0|            0|  0.00%|        assert self._closed, "Queue {0!r} not closed".format(self)
   147|         0|            0|            0|  0.00%|        if self._jointhread:
   148|         0|            0|            0|  0.00%|            self._jointhread()
   149|         0|            0|            0|  0.00%|
   150|        16|   0.00018239|  1.13994e-05|  0.00%|    def cancel_join_thread(self):
   151|        16|  0.000238895|   1.4931e-05|  0.00%|        debug('Queue.cancel_join_thread()')
(call)|        16|  0.000154495|  9.65595e-06|  0.00%|# /opt/conda/lib/python3.8/multiprocessing/util.py:48 debug
   152|        16|  7.20024e-05|  4.50015e-06|  0.00%|        self._joincancelled = True
   153|        16|   6.4373e-05|  4.02331e-06|  0.00%|        try:
   154|        16|  0.000202656|   1.2666e-05|  0.00%|            self._jointhread.cancel()
   155|        16|  9.53674e-05|  5.96046e-06|  0.00%|        except AttributeError:
   156|        16|  6.60419e-05|  4.12762e-06|  0.00%|            pass
   157|         0|            0|            0|  0.00%|
   158|         8|  8.10623e-05|  1.01328e-05|  0.00%|    def _start_thread(self):
   159|         8|  0.000163794|  2.04742e-05|  0.00%|        debug('Queue._start_thread()')
(call)|         8|  0.000105858|  1.32322e-05|  0.00%|# /opt/conda/lib/python3.8/multiprocessing/util.py:48 debug
   160|         0|            0|            0|  0.00%|
   161|         0|            0|            0|  0.00%|        # Start thread which transfers data from buffer to pipe
   162|         8|  9.17912e-05|  1.14739e-05|  0.00%|        self._buffer.clear()
   163|        16|  0.000286341|  1.78963e-05|  0.00%|        self._thread = threading.Thread(
(call)|         8|   0.00366664|   0.00045833|  0.01%|# /opt/conda/lib/python3.8/threading.py:761 __init__
   164|         8|  6.98566e-05|  8.73208e-06|  0.00%|            target=Queue._feed,
   165|        16|  0.000171185|   1.0699e-05|  0.00%|            args=(self._buffer, self._notempty, self._send_bytes,
   166|         8|  7.22408e-05|   9.0301e-06|  0.00%|                  self._wlock, self._writer.close, self._ignore_epipe,
   167|         8|  3.74317e-05|  4.67896e-06|  0.00%|                  self._on_queue_feeder_error, self._sem),
   168|         8|  3.50475e-05|  4.38094e-06|  0.00%|            name='QueueFeederThread'
   169|         0|            0|            0|  0.00%|        )
   170|         8|   0.00011301|  1.41263e-05|  0.00%|        self._thread.daemon = True
(call)|         8|  0.000328779|  4.10974e-05|  0.00%|# /opt/conda/lib/python3.8/threading.py:1110 daemon
   171|         0|            0|            0|  0.00%|
   172|         8|  0.000103712|   1.2964e-05|  0.00%|        debug('doing self._thread.start()')
(call)|         8|  7.08103e-05|  8.85129e-06|  0.00%|# /opt/conda/lib/python3.8/multiprocessing/util.py:48 debug
   173|         8|  0.000118732|  1.48416e-05|  0.00%|        self._thread.start()
(call)|         8|   0.00649714|  0.000812143|  0.01%|# /opt/conda/lib/python3.8/threading.py:834 start
   174|         8|  0.000109434|  1.36793e-05|  0.00%|        debug('... done self._thread.start()')
(call)|         8|  7.15256e-05|   8.9407e-06|  0.00%|# /opt/conda/lib/python3.8/multiprocessing/util.py:48 debug
   175|         0|            0|            0|  0.00%|
   176|         8|  3.52859e-05|  4.41074e-06|  0.00%|        if not self._joincancelled:
   177|         0|            0|            0|  0.00%|            self._jointhread = Finalize(
   178|         0|            0|            0|  0.00%|                self._thread, Queue._finalize_join,
   179|         0|            0|            0|  0.00%|                [weakref.ref(self._thread)],
   180|         0|            0|            0|  0.00%|                exitpriority=-5
   181|         0|            0|            0|  0.00%|                )
   182|         0|            0|            0|  0.00%|
   183|         0|            0|            0|  0.00%|        # Send sentinel to the thread queue object when garbage collected
   184|        16|  0.000208855|  1.30534e-05|  0.00%|        self._close = Finalize(
(call)|         8|  0.000436306|  5.45382e-05|  0.00%|# /opt/conda/lib/python3.8/multiprocessing/util.py:186 __init__
   185|         8|  3.52859e-05|  4.41074e-06|  0.00%|            self, Queue._finalize_close,
   186|         8|  4.43459e-05|  5.54323e-06|  0.00%|            [self._buffer, self._notempty],
   187|         8|  3.38554e-05|  4.23193e-06|  0.00%|            exitpriority=10
   188|         0|            0|            0|  0.00%|            )
   189|         0|            0|            0|  0.00%|
   190|         0|            0|            0|  0.00%|    @staticmethod
   191|         0|            0|            0|  0.00%|    def _finalize_join(twr):
   192|         0|            0|            0|  0.00%|        debug('joining queue thread')
   193|         0|            0|            0|  0.00%|        thread = twr()
   194|         0|            0|            0|  0.00%|        if thread is not None:
   195|         0|            0|            0|  0.00%|            thread.join()
   196|         0|            0|            0|  0.00%|            debug('... queue thread joined')
   197|         0|            0|            0|  0.00%|        else:
   198|         0|            0|            0|  0.00%|            debug('... queue thread already dead')
   199|         0|            0|            0|  0.00%|
   200|         8|  4.64916e-05|  5.81145e-06|  0.00%|    @staticmethod
   201|         0|            0|            0|  0.00%|    def _finalize_close(buffer, notempty):
   202|         8|  9.98974e-05|  1.24872e-05|  0.00%|        debug('telling queue thread to quit')
(call)|         8|  6.67572e-05|  8.34465e-06|  0.00%|# /opt/conda/lib/python3.8/multiprocessing/util.py:48 debug
   203|         8|  0.000120878|  1.51098e-05|  0.00%|        with notempty:
(call)|         8|  0.000102282|  1.27852e-05|  0.00%|# /opt/conda/lib/python3.8/threading.py:246 __enter__
   204|         8|  5.22137e-05|  6.52671e-06|  0.00%|            buffer.append(_sentinel)
   205|         8|  0.000192881|  2.41101e-05|  0.00%|            notempty.notify()
(call)|         8|  0.000703812|  8.79765e-05|  0.00%|# /opt/conda/lib/python3.8/threading.py:341 notify
(call)|         8|   8.9407e-05|  1.11759e-05|  0.00%|# /opt/conda/lib/python3.8/threading.py:249 __exit__
   206|         0|            0|            0|  0.00%|
   207|         0|            0|            0|  0.00%|    @staticmethod
   208|         0|            0|            0|  0.00%|    def _feed(buffer, notempty, send_bytes, writelock, close, ignore_epipe,
   209|         0|            0|            0|  0.00%|              onerror, queue_sem):
   210|         0|            0|            0|  0.00%|        debug('starting thread to feed data to pipe')
   211|         0|            0|            0|  0.00%|        nacquire = notempty.acquire
   212|         0|            0|            0|  0.00%|        nrelease = notempty.release
   213|         0|            0|            0|  0.00%|        nwait = notempty.wait
   214|         0|            0|            0|  0.00%|        bpopleft = buffer.popleft
   215|         0|            0|            0|  0.00%|        sentinel = _sentinel
   216|         0|            0|            0|  0.00%|        if sys.platform != 'win32':
   217|         0|            0|            0|  0.00%|            wacquire = writelock.acquire
   218|         0|            0|            0|  0.00%|            wrelease = writelock.release
   219|         0|            0|            0|  0.00%|        else:
   220|         0|            0|            0|  0.00%|            wacquire = None
   221|         0|            0|            0|  0.00%|
   222|         0|            0|            0|  0.00%|        while 1:
   223|         0|            0|            0|  0.00%|            try:
   224|         0|            0|            0|  0.00%|                nacquire()
   225|         0|            0|            0|  0.00%|                try:
   226|         0|            0|            0|  0.00%|                    if not buffer:
   227|         0|            0|            0|  0.00%|                        nwait()
   228|         0|            0|            0|  0.00%|                finally:
   229|         0|            0|            0|  0.00%|                    nrelease()
   230|         0|            0|            0|  0.00%|                try:
   231|         0|            0|            0|  0.00%|                    while 1:
   232|         0|            0|            0|  0.00%|                        obj = bpopleft()
   233|         0|            0|            0|  0.00%|                        if obj is sentinel:
   234|         0|            0|            0|  0.00%|                            debug('feeder thread got sentinel -- exiting')
   235|         0|            0|            0|  0.00%|                            close()
   236|         0|            0|            0|  0.00%|                            return
   237|         0|            0|            0|  0.00%|
   238|         0|            0|            0|  0.00%|                        # serialize the data before acquiring the lock
   239|         0|            0|            0|  0.00%|                        obj = _ForkingPickler.dumps(obj)
   240|         0|            0|            0|  0.00%|                        if wacquire is None:
   241|         0|            0|            0|  0.00%|                            send_bytes(obj)
   242|         0|            0|            0|  0.00%|                        else:
   243|         0|            0|            0|  0.00%|                            wacquire()
   244|         0|            0|            0|  0.00%|                            try:
   245|         0|            0|            0|  0.00%|                                send_bytes(obj)
   246|         0|            0|            0|  0.00%|                            finally:
   247|         0|            0|            0|  0.00%|                                wrelease()
   248|         0|            0|            0|  0.00%|                except IndexError:
   249|         0|            0|            0|  0.00%|                    pass
   250|         0|            0|            0|  0.00%|            except Exception as e:
   251|         0|            0|            0|  0.00%|                if ignore_epipe and getattr(e, 'errno', 0) == errno.EPIPE:
   252|         0|            0|            0|  0.00%|                    return
   253|         0|            0|            0|  0.00%|                # Since this runs in a daemon thread the resources it uses
   254|         0|            0|            0|  0.00%|                # may be become unusable while the process is cleaning up.
   255|         0|            0|            0|  0.00%|                # We ignore errors which happen after the process has
   256|         0|            0|            0|  0.00%|                # started to cleanup.
   257|         0|            0|            0|  0.00%|                if is_exiting():
   258|         0|            0|            0|  0.00%|                    info('error in queue thread: %s', e)
   259|         0|            0|            0|  0.00%|                    return
   260|         0|            0|            0|  0.00%|                else:
   261|         0|            0|            0|  0.00%|                    # Since the object has not been sent in the queue, we need
   262|         0|            0|            0|  0.00%|                    # to decrease the size of the queue. The error acts as
   263|         0|            0|            0|  0.00%|                    # if the object had been silently removed from the queue
   264|         0|            0|            0|  0.00%|                    # and this step is necessary to have a properly working
   265|         0|            0|            0|  0.00%|                    # queue.
   266|         0|            0|            0|  0.00%|                    queue_sem.release()
   267|         0|            0|            0|  0.00%|                    onerror(e, obj)
   268|         0|            0|            0|  0.00%|
   269|         0|            0|            0|  0.00%|    @staticmethod
   270|         0|            0|            0|  0.00%|    def _on_queue_feeder_error(e, obj):
   271|         0|            0|            0|  0.00%|        """
   272|         0|            0|            0|  0.00%|        Private API hook called when feeding data in the background thread
   273|         0|            0|            0|  0.00%|        raises an exception.  For overriding by concurrent.futures.
   274|         0|            0|            0|  0.00%|        """
   275|         0|            0|            0|  0.00%|        import traceback
   276|         0|            0|            0|  0.00%|        traceback.print_exc()
   277|         0|            0|            0|  0.00%|
   278|         0|            0|            0|  0.00%|
   279|         0|            0|            0|  0.00%|_sentinel = object()
   280|         0|            0|            0|  0.00%|
   281|         0|            0|            0|  0.00%|#
   282|         0|            0|            0|  0.00%|# A queue type which also supports join() and task_done() methods
   283|         0|            0|            0|  0.00%|#
   284|         0|            0|            0|  0.00%|# Note that if you do not call task_done() for each finished task then
   285|         0|            0|            0|  0.00%|# eventually the counter's semaphore may overflow causing Bad Things
   286|         0|            0|            0|  0.00%|# to happen.
   287|         0|            0|            0|  0.00%|#
   288|         0|            0|            0|  0.00%|
   289|         0|            0|            0|  0.00%|class JoinableQueue(Queue):
   290|         0|            0|            0|  0.00%|
   291|         0|            0|            0|  0.00%|    def __init__(self, maxsize=0, *, ctx):
   292|         0|            0|            0|  0.00%|        Queue.__init__(self, maxsize, ctx=ctx)
   293|         0|            0|            0|  0.00%|        self._unfinished_tasks = ctx.Semaphore(0)
   294|         0|            0|            0|  0.00%|        self._cond = ctx.Condition()
   295|         0|            0|            0|  0.00%|
   296|         0|            0|            0|  0.00%|    def __getstate__(self):
   297|         0|            0|            0|  0.00%|        return Queue.__getstate__(self) + (self._cond, self._unfinished_tasks)
   298|         0|            0|            0|  0.00%|
   299|         0|            0|            0|  0.00%|    def __setstate__(self, state):
   300|         0|            0|            0|  0.00%|        Queue.__setstate__(self, state[:-2])
   301|         0|            0|            0|  0.00%|        self._cond, self._unfinished_tasks = state[-2:]
   302|         0|            0|            0|  0.00%|
   303|         0|            0|            0|  0.00%|    def put(self, obj, block=True, timeout=None):
   304|         0|            0|            0|  0.00%|        if self._closed:
   305|         0|            0|            0|  0.00%|            raise ValueError(f"Queue {self!r} is closed")
   306|         0|            0|            0|  0.00%|        if not self._sem.acquire(block, timeout):
   307|         0|            0|            0|  0.00%|            raise Full
   308|         0|            0|            0|  0.00%|
   309|         0|            0|            0|  0.00%|        with self._notempty, self._cond:
   310|         0|            0|            0|  0.00%|            if self._thread is None:
   311|         0|            0|            0|  0.00%|                self._start_thread()
   312|         0|            0|            0|  0.00%|            self._buffer.append(obj)
   313|         0|            0|            0|  0.00%|            self._unfinished_tasks.release()
   314|         0|            0|            0|  0.00%|            self._notempty.notify()
   315|         0|            0|            0|  0.00%|
   316|         0|            0|            0|  0.00%|    def task_done(self):
   317|         0|            0|            0|  0.00%|        with self._cond:
   318|         0|            0|            0|  0.00%|            if not self._unfinished_tasks.acquire(False):
   319|         0|            0|            0|  0.00%|                raise ValueError('task_done() called too many times')
   320|         0|            0|            0|  0.00%|            if self._unfinished_tasks._semlock._is_zero():
   321|         0|            0|            0|  0.00%|                self._cond.notify_all()
   322|         0|            0|            0|  0.00%|
   323|         0|            0|            0|  0.00%|    def join(self):
   324|         0|            0|            0|  0.00%|        with self._cond:
   325|         0|            0|            0|  0.00%|            if not self._unfinished_tasks._semlock._is_zero():
   326|         0|            0|            0|  0.00%|                self._cond.wait()
   327|         0|            0|            0|  0.00%|
   328|         0|            0|            0|  0.00%|#
   329|         0|            0|            0|  0.00%|# Simplified Queue type -- really just a locked pipe
   330|         0|            0|            0|  0.00%|#
   331|         0|            0|            0|  0.00%|
   332|         0|            0|            0|  0.00%|class SimpleQueue(object):
   333|         0|            0|            0|  0.00%|
   334|         0|            0|            0|  0.00%|    def __init__(self, *, ctx):
   335|         0|            0|            0|  0.00%|        self._reader, self._writer = connection.Pipe(duplex=False)
   336|         0|            0|            0|  0.00%|        self._rlock = ctx.Lock()
   337|         0|            0|            0|  0.00%|        self._poll = self._reader.poll
   338|         0|            0|            0|  0.00%|        if sys.platform == 'win32':
   339|         0|            0|            0|  0.00%|            self._wlock = None
   340|         0|            0|            0|  0.00%|        else:
   341|         0|            0|            0|  0.00%|            self._wlock = ctx.Lock()
   342|         0|            0|            0|  0.00%|
   343|         0|            0|            0|  0.00%|    def empty(self):
   344|         0|            0|            0|  0.00%|        return not self._poll()
   345|         0|            0|            0|  0.00%|
   346|         0|            0|            0|  0.00%|    def __getstate__(self):
   347|         0|            0|            0|  0.00%|        context.assert_spawning(self)
   348|         0|            0|            0|  0.00%|        return (self._reader, self._writer, self._rlock, self._wlock)
   349|         0|            0|            0|  0.00%|
   350|         0|            0|            0|  0.00%|    def __setstate__(self, state):
   351|         0|            0|            0|  0.00%|        (self._reader, self._writer, self._rlock, self._wlock) = state
   352|         0|            0|            0|  0.00%|        self._poll = self._reader.poll
   353|         0|            0|            0|  0.00%|
   354|         0|            0|            0|  0.00%|    def get(self):
   355|         0|            0|            0|  0.00%|        with self._rlock:
   356|         0|            0|            0|  0.00%|            res = self._reader.recv_bytes()
   357|         0|            0|            0|  0.00%|        # unserialize the data after having released the lock
   358|         0|            0|            0|  0.00%|        return _ForkingPickler.loads(res)
   359|         0|            0|            0|  0.00%|
   360|         0|            0|            0|  0.00%|    def put(self, obj):
   361|         0|            0|            0|  0.00%|        # serialize the data before acquiring the lock
   362|         0|            0|            0|  0.00%|        obj = _ForkingPickler.dumps(obj)
   363|         0|            0|            0|  0.00%|        if self._wlock is None:
   364|         0|            0|            0|  0.00%|            # writes to a message oriented win32 pipe are atomic
   365|         0|            0|            0|  0.00%|            self._writer.send_bytes(obj)
   366|         0|            0|            0|  0.00%|        else:
   367|         0|            0|            0|  0.00%|            with self._wlock:
   368|         0|            0|            0|  0.00%|                self._writer.send_bytes(obj)
File: /opt/conda/lib/python3.8/site-packages/torch/nn/modules/activation.py
File duration: 0.0368781s (0.07%)
Line #|      Hits|         Time| Time per hit|      %|Source code
------+----------+-------------+-------------+-------+-----------
     1|         0|            0|            0|  0.00%|import warnings
     2|         0|            0|            0|  0.00%|from typing import Optional, Tuple
     3|         0|            0|            0|  0.00%|
     4|         0|            0|            0|  0.00%|import torch
     5|         0|            0|            0|  0.00%|from torch import Tensor
     6|         0|            0|            0|  0.00%|from .linear import NonDynamicallyQuantizableLinear
     7|         0|            0|            0|  0.00%|from torch.nn.init import xavier_uniform_
     8|         0|            0|            0|  0.00%|from torch.nn.init import constant_
     9|         0|            0|            0|  0.00%|from torch.nn.init import xavier_normal_
    10|         0|            0|            0|  0.00%|from torch.nn.parameter import Parameter
    11|         0|            0|            0|  0.00%|from .module import Module
    12|         0|            0|            0|  0.00%|from .. import functional as F
    13|         0|            0|            0|  0.00%|
    14|         0|            0|            0|  0.00%|
    15|         0|            0|            0|  0.00%|class Threshold(Module):
    16|         0|            0|            0|  0.00%|    r"""Thresholds each element of the input Tensor.
    17|         0|            0|            0|  0.00%|
    18|         0|            0|            0|  0.00%|    Threshold is defined as:
    19|         0|            0|            0|  0.00%|
    20|         0|            0|            0|  0.00%|    .. math::
    21|         0|            0|            0|  0.00%|        y =
    22|         0|            0|            0|  0.00%|        \begin{cases}
    23|         0|            0|            0|  0.00%|        x, &\text{ if } x > \text{threshold} \\
    24|         0|            0|            0|  0.00%|        \text{value}, &\text{ otherwise }
    25|         0|            0|            0|  0.00%|        \end{cases}
    26|         0|            0|            0|  0.00%|
    27|         0|            0|            0|  0.00%|    Args:
    28|         0|            0|            0|  0.00%|        threshold: The value to threshold at
    29|         0|            0|            0|  0.00%|        value: The value to replace with
    30|         0|            0|            0|  0.00%|        inplace: can optionally do the operation in-place. Default: ``False``
    31|         0|            0|            0|  0.00%|
    32|         0|            0|            0|  0.00%|    Shape:
    33|         0|            0|            0|  0.00%|        - Input: :math:`(N, *)` where `*` means, any number of additional
    34|         0|            0|            0|  0.00%|          dimensions
    35|         0|            0|            0|  0.00%|        - Output: :math:`(N, *)`, same shape as the input
    36|         0|            0|            0|  0.00%|
    37|         0|            0|            0|  0.00%|    Examples::
    38|         0|            0|            0|  0.00%|
    39|         0|            0|            0|  0.00%|        >>> m = nn.Threshold(0.1, 20)
    40|         0|            0|            0|  0.00%|        >>> input = torch.randn(2)
    41|         0|            0|            0|  0.00%|        >>> output = m(input)
    42|         0|            0|            0|  0.00%|    """
    43|         0|            0|            0|  0.00%|    __constants__ = ['threshold', 'value', 'inplace']
    44|         0|            0|            0|  0.00%|
    45|         0|            0|            0|  0.00%|    threshold: float
    46|         0|            0|            0|  0.00%|    value: float
    47|         0|            0|            0|  0.00%|    inplace: bool
    48|         0|            0|            0|  0.00%|
    49|         0|            0|            0|  0.00%|    def __init__(self, threshold: float, value: float, inplace: bool = False) -> None:
    50|         0|            0|            0|  0.00%|        super(Threshold, self).__init__()
    51|         0|            0|            0|  0.00%|        self.threshold = threshold
    52|         0|            0|            0|  0.00%|        self.value = value
    53|         0|            0|            0|  0.00%|        self.inplace = inplace
    54|         0|            0|            0|  0.00%|        # TODO: check in THNN (if inplace == True, then assert value <= threshold)
    55|         0|            0|            0|  0.00%|
    56|         0|            0|            0|  0.00%|    def forward(self, input: Tensor) -> Tensor:
    57|         0|            0|            0|  0.00%|        return F.threshold(input, self.threshold, self.value, self.inplace)
    58|         0|            0|            0|  0.00%|
    59|         0|            0|            0|  0.00%|    def extra_repr(self):
    60|         0|            0|            0|  0.00%|        inplace_str = ', inplace=True' if self.inplace else ''
    61|         0|            0|            0|  0.00%|        return 'threshold={}, value={}{}'.format(
    62|         0|            0|            0|  0.00%|            self.threshold, self.value, inplace_str
    63|         0|            0|            0|  0.00%|        )
    64|         0|            0|            0|  0.00%|
    65|         0|            0|            0|  0.00%|
    66|         0|            0|            0|  0.00%|class ReLU(Module):
    67|         0|            0|            0|  0.00%|    r"""Applies the rectified linear unit function element-wise:
    68|         0|            0|            0|  0.00%|
    69|         0|            0|            0|  0.00%|    :math:`\text{ReLU}(x) = (x)^+ = \max(0, x)`
    70|         0|            0|            0|  0.00%|
    71|         0|            0|            0|  0.00%|    Args:
    72|         0|            0|            0|  0.00%|        inplace: can optionally do the operation in-place. Default: ``False``
    73|         0|            0|            0|  0.00%|
    74|         0|            0|            0|  0.00%|    Shape:
    75|         0|            0|            0|  0.00%|        - Input: :math:`(N, *)` where `*` means, any number of additional
    76|         0|            0|            0|  0.00%|          dimensions
    77|         0|            0|            0|  0.00%|        - Output: :math:`(N, *)`, same shape as the input
    78|         0|            0|            0|  0.00%|
    79|         0|            0|            0|  0.00%|    .. image:: ../scripts/activation_images/ReLU.png
    80|         0|            0|            0|  0.00%|
    81|         0|            0|            0|  0.00%|    Examples::
    82|         0|            0|            0|  0.00%|
    83|         0|            0|            0|  0.00%|        >>> m = nn.ReLU()
    84|         0|            0|            0|  0.00%|        >>> input = torch.randn(2)
    85|         0|            0|            0|  0.00%|        >>> output = m(input)
    86|         0|            0|            0|  0.00%|
    87|         0|            0|            0|  0.00%|
    88|         0|            0|            0|  0.00%|      An implementation of CReLU - https://arxiv.org/abs/1603.05201
    89|         0|            0|            0|  0.00%|
    90|         0|            0|            0|  0.00%|        >>> m = nn.ReLU()
    91|         0|            0|            0|  0.00%|        >>> input = torch.randn(2).unsqueeze(0)
    92|         0|            0|            0|  0.00%|        >>> output = torch.cat((m(input),m(-input)))
    93|         0|            0|            0|  0.00%|    """
    94|         0|            0|            0|  0.00%|    __constants__ = ['inplace']
    95|         0|            0|            0|  0.00%|    inplace: bool
    96|         0|            0|            0|  0.00%|
    97|         0|            0|            0|  0.00%|    def __init__(self, inplace: bool = False):
    98|         0|            0|            0|  0.00%|        super(ReLU, self).__init__()
    99|         0|            0|            0|  0.00%|        self.inplace = inplace
   100|         0|            0|            0|  0.00%|
   101|      1700|   0.00887728|  5.22193e-06|  0.02%|    def forward(self, input: Tensor) -> Tensor:
   102|      1700|    0.0280008|  1.64711e-05|  0.05%|        return F.relu(input, inplace=self.inplace)
(call)|      1700|     0.186405|   0.00010965|  0.35%|# /opt/conda/lib/python3.8/site-packages/torch/nn/functional.py:1287 relu
   103|         0|            0|            0|  0.00%|
   104|         0|            0|            0|  0.00%|    def extra_repr(self) -> str:
   105|         0|            0|            0|  0.00%|        inplace_str = 'inplace=True' if self.inplace else ''
   106|         0|            0|            0|  0.00%|        return inplace_str
   107|         0|            0|            0|  0.00%|
   108|         0|            0|            0|  0.00%|
   109|         0|            0|            0|  0.00%|class RReLU(Module):
   110|         0|            0|            0|  0.00%|    r"""Applies the randomized leaky rectified liner unit function, element-wise,
   111|         0|            0|            0|  0.00%|    as described in the paper:
   112|         0|            0|            0|  0.00%|
   113|         0|            0|            0|  0.00%|    `Empirical Evaluation of Rectified Activations in Convolutional Network`_.
   114|         0|            0|            0|  0.00%|
   115|         0|            0|            0|  0.00%|    The function is defined as:
   116|         0|            0|            0|  0.00%|
   117|         0|            0|            0|  0.00%|    .. math::
   118|         0|            0|            0|  0.00%|        \text{RReLU}(x) =
   119|         0|            0|            0|  0.00%|        \begin{cases}
   120|         0|            0|            0|  0.00%|            x & \text{if } x \geq 0 \\
   121|         0|            0|            0|  0.00%|            ax & \text{ otherwise }
   122|         0|            0|            0|  0.00%|        \end{cases}
   123|         0|            0|            0|  0.00%|
   124|         0|            0|            0|  0.00%|    where :math:`a` is randomly sampled from uniform distribution
   125|         0|            0|            0|  0.00%|    :math:`\mathcal{U}(\text{lower}, \text{upper})`.
   126|         0|            0|            0|  0.00%|
   127|         0|            0|            0|  0.00%|     See: https://arxiv.org/pdf/1505.00853.pdf
   128|         0|            0|            0|  0.00%|
   129|         0|            0|            0|  0.00%|    Args:
   130|         0|            0|            0|  0.00%|        lower: lower bound of the uniform distribution. Default: :math:`\frac{1}{8}`
   131|         0|            0|            0|  0.00%|        upper: upper bound of the uniform distribution. Default: :math:`\frac{1}{3}`
   132|         0|            0|            0|  0.00%|        inplace: can optionally do the operation in-place. Default: ``False``
   133|         0|            0|            0|  0.00%|
   134|         0|            0|            0|  0.00%|    Shape:
   135|         0|            0|            0|  0.00%|        - Input: :math:`(N, *)` where `*` means, any number of additional
   136|         0|            0|            0|  0.00%|          dimensions
   137|         0|            0|            0|  0.00%|        - Output: :math:`(N, *)`, same shape as the input
   138|         0|            0|            0|  0.00%|
   139|         0|            0|            0|  0.00%|    Examples::
   140|         0|            0|            0|  0.00%|
   141|         0|            0|            0|  0.00%|        >>> m = nn.RReLU(0.1, 0.3)
   142|         0|            0|            0|  0.00%|        >>> input = torch.randn(2)
   143|         0|            0|            0|  0.00%|        >>> output = m(input)
   144|         0|            0|            0|  0.00%|
   145|         0|            0|            0|  0.00%|    .. _`Empirical Evaluation of Rectified Activations in Convolutional Network`:
   146|         0|            0|            0|  0.00%|        https://arxiv.org/abs/1505.00853
   147|         0|            0|            0|  0.00%|    """
   148|         0|            0|            0|  0.00%|    __constants__ = ['lower', 'upper', 'inplace']
   149|         0|            0|            0|  0.00%|
   150|         0|            0|            0|  0.00%|    lower: float
   151|         0|            0|            0|  0.00%|    upper: float
   152|         0|            0|            0|  0.00%|    inplace: bool
   153|         0|            0|            0|  0.00%|
   154|         0|            0|            0|  0.00%|    def __init__(
   155|         0|            0|            0|  0.00%|        self,
   156|         0|            0|            0|  0.00%|        lower: float = 1. / 8,
   157|         0|            0|            0|  0.00%|        upper: float = 1. / 3,
   158|         0|            0|            0|  0.00%|        inplace: bool = False
   159|         0|            0|            0|  0.00%|    ):
   160|         0|            0|            0|  0.00%|        super(RReLU, self).__init__()
   161|         0|            0|            0|  0.00%|        self.lower = lower
   162|         0|            0|            0|  0.00%|        self.upper = upper
   163|         0|            0|            0|  0.00%|        self.inplace = inplace
   164|         0|            0|            0|  0.00%|
   165|         0|            0|            0|  0.00%|    def forward(self, input: Tensor) -> Tensor:
   166|         0|            0|            0|  0.00%|        return F.rrelu(input, self.lower, self.upper, self.training, self.inplace)
   167|         0|            0|            0|  0.00%|
   168|         0|            0|            0|  0.00%|    def extra_repr(self):
   169|         0|            0|            0|  0.00%|        inplace_str = ', inplace=True' if self.inplace else ''
   170|         0|            0|            0|  0.00%|        return 'lower={}, upper={}{}'.format(self.lower, self.upper, inplace_str)
   171|         0|            0|            0|  0.00%|
   172|         0|            0|            0|  0.00%|
   173|         0|            0|            0|  0.00%|class Hardtanh(Module):
   174|         0|            0|            0|  0.00%|    r"""Applies the HardTanh function element-wise
   175|         0|            0|            0|  0.00%|
   176|         0|            0|            0|  0.00%|    HardTanh is defined as:
   177|         0|            0|            0|  0.00%|
   178|         0|            0|            0|  0.00%|    .. math::
   179|         0|            0|            0|  0.00%|        \text{HardTanh}(x) = \begin{cases}
   180|         0|            0|            0|  0.00%|            1 & \text{ if } x > 1 \\
   181|         0|            0|            0|  0.00%|            -1 & \text{ if } x < -1 \\
   182|         0|            0|            0|  0.00%|            x & \text{ otherwise } \\
   183|         0|            0|            0|  0.00%|        \end{cases}
   184|         0|            0|            0|  0.00%|
   185|         0|            0|            0|  0.00%|    The range of the linear region :math:`[-1, 1]` can be adjusted using
   186|         0|            0|            0|  0.00%|    :attr:`min_val` and :attr:`max_val`.
   187|         0|            0|            0|  0.00%|
   188|         0|            0|            0|  0.00%|    Args:
   189|         0|            0|            0|  0.00%|        min_val: minimum value of the linear region range. Default: -1
   190|         0|            0|            0|  0.00%|        max_val: maximum value of the linear region range. Default: 1
   191|         0|            0|            0|  0.00%|        inplace: can optionally do the operation in-place. Default: ``False``
   192|         0|            0|            0|  0.00%|
   193|         0|            0|            0|  0.00%|    Keyword arguments :attr:`min_value` and :attr:`max_value`
   194|         0|            0|            0|  0.00%|    have been deprecated in favor of :attr:`min_val` and :attr:`max_val`.
   195|         0|            0|            0|  0.00%|
   196|         0|            0|            0|  0.00%|    Shape:
   197|         0|            0|            0|  0.00%|        - Input: :math:`(N, *)` where `*` means, any number of additional
   198|         0|            0|            0|  0.00%|          dimensions
   199|         0|            0|            0|  0.00%|        - Output: :math:`(N, *)`, same shape as the input
   200|         0|            0|            0|  0.00%|
   201|         0|            0|            0|  0.00%|    .. image:: ../scripts/activation_images/Hardtanh.png
   202|         0|            0|            0|  0.00%|
   203|         0|            0|            0|  0.00%|    Examples::
   204|         0|            0|            0|  0.00%|
   205|         0|            0|            0|  0.00%|        >>> m = nn.Hardtanh(-2, 2)
   206|         0|            0|            0|  0.00%|        >>> input = torch.randn(2)
   207|         0|            0|            0|  0.00%|        >>> output = m(input)
   208|         0|            0|            0|  0.00%|    """
   209|         0|            0|            0|  0.00%|    __constants__ = ['min_val', 'max_val', 'inplace']
   210|         0|            0|            0|  0.00%|
   211|         0|            0|            0|  0.00%|    min_val: float
   212|         0|            0|            0|  0.00%|    max_val: float
   213|         0|            0|            0|  0.00%|    inplace: bool
   214|         0|            0|            0|  0.00%|
   215|         0|            0|            0|  0.00%|    def __init__(
   216|         0|            0|            0|  0.00%|        self,
   217|         0|            0|            0|  0.00%|        min_val: float = -1.,
   218|         0|            0|            0|  0.00%|        max_val: float = 1.,
   219|         0|            0|            0|  0.00%|        inplace: bool = False,
   220|         0|            0|            0|  0.00%|        min_value: Optional[float] = None,
   221|         0|            0|            0|  0.00%|        max_value: Optional[float] = None
   222|         0|            0|            0|  0.00%|    ) -> None:
   223|         0|            0|            0|  0.00%|        super(Hardtanh, self).__init__()
   224|         0|            0|            0|  0.00%|        if min_value is not None:
   225|         0|            0|            0|  0.00%|            warnings.warn("keyword argument min_value is deprecated and rename to min_val")
   226|         0|            0|            0|  0.00%|            min_val = min_value
   227|         0|            0|            0|  0.00%|        if max_value is not None:
   228|         0|            0|            0|  0.00%|            warnings.warn("keyword argument max_value is deprecated and rename to max_val")
   229|         0|            0|            0|  0.00%|            max_val = max_value
   230|         0|            0|            0|  0.00%|
   231|         0|            0|            0|  0.00%|        self.min_val = min_val
   232|         0|            0|            0|  0.00%|        self.max_val = max_val
   233|         0|            0|            0|  0.00%|        self.inplace = inplace
   234|         0|            0|            0|  0.00%|        assert self.max_val > self.min_val
   235|         0|            0|            0|  0.00%|
   236|         0|            0|            0|  0.00%|    def forward(self, input: Tensor) -> Tensor:
   237|         0|            0|            0|  0.00%|        return F.hardtanh(input, self.min_val, self.max_val, self.inplace)
   238|         0|            0|            0|  0.00%|
   239|         0|            0|            0|  0.00%|    def extra_repr(self) -> str:
   240|         0|            0|            0|  0.00%|        inplace_str = ', inplace=True' if self.inplace else ''
   241|         0|            0|            0|  0.00%|        return 'min_val={}, max_val={}{}'.format(
   242|         0|            0|            0|  0.00%|            self.min_val, self.max_val, inplace_str
   243|         0|            0|            0|  0.00%|        )
   244|         0|            0|            0|  0.00%|
   245|         0|            0|            0|  0.00%|
   246|         0|            0|            0|  0.00%|class ReLU6(Hardtanh):
   247|         0|            0|            0|  0.00%|    r"""Applies the element-wise function:
   248|         0|            0|            0|  0.00%|
   249|         0|            0|            0|  0.00%|    .. math::
   250|         0|            0|            0|  0.00%|        \text{ReLU6}(x) = \min(\max(0,x), 6)
   251|         0|            0|            0|  0.00%|
   252|         0|            0|            0|  0.00%|    Args:
   253|         0|            0|            0|  0.00%|        inplace: can optionally do the operation in-place. Default: ``False``
   254|         0|            0|            0|  0.00%|
   255|         0|            0|            0|  0.00%|    Shape:
   256|         0|            0|            0|  0.00%|        - Input: :math:`(N, *)` where `*` means, any number of additional
   257|         0|            0|            0|  0.00%|          dimensions
   258|         0|            0|            0|  0.00%|        - Output: :math:`(N, *)`, same shape as the input
   259|         0|            0|            0|  0.00%|
   260|         0|            0|            0|  0.00%|    .. image:: ../scripts/activation_images/ReLU6.png
   261|         0|            0|            0|  0.00%|
   262|         0|            0|            0|  0.00%|    Examples::
   263|         0|            0|            0|  0.00%|
   264|         0|            0|            0|  0.00%|        >>> m = nn.ReLU6()
   265|         0|            0|            0|  0.00%|        >>> input = torch.randn(2)
   266|         0|            0|            0|  0.00%|        >>> output = m(input)
   267|         0|            0|            0|  0.00%|    """
   268|         0|            0|            0|  0.00%|
   269|         0|            0|            0|  0.00%|    def __init__(self, inplace: bool = False):
   270|         0|            0|            0|  0.00%|        super(ReLU6, self).__init__(0., 6., inplace)
   271|         0|            0|            0|  0.00%|
   272|         0|            0|            0|  0.00%|    def extra_repr(self) -> str:
   273|         0|            0|            0|  0.00%|        inplace_str = 'inplace=True' if self.inplace else ''
   274|         0|            0|            0|  0.00%|        return inplace_str
   275|         0|            0|            0|  0.00%|
   276|         0|            0|            0|  0.00%|
   277|         0|            0|            0|  0.00%|class Sigmoid(Module):
   278|         0|            0|            0|  0.00%|    r"""Applies the element-wise function:
   279|         0|            0|            0|  0.00%|
   280|         0|            0|            0|  0.00%|    .. math::
   281|         0|            0|            0|  0.00%|        \text{Sigmoid}(x) = \sigma(x) = \frac{1}{1 + \exp(-x)}
   282|         0|            0|            0|  0.00%|
   283|         0|            0|            0|  0.00%|
   284|         0|            0|            0|  0.00%|    Shape:
   285|         0|            0|            0|  0.00%|        - Input: :math:`(N, *)` where `*` means, any number of additional
   286|         0|            0|            0|  0.00%|          dimensions
   287|         0|            0|            0|  0.00%|        - Output: :math:`(N, *)`, same shape as the input
   288|         0|            0|            0|  0.00%|
   289|         0|            0|            0|  0.00%|    .. image:: ../scripts/activation_images/Sigmoid.png
   290|         0|            0|            0|  0.00%|
   291|         0|            0|            0|  0.00%|    Examples::
   292|         0|            0|            0|  0.00%|
   293|         0|            0|            0|  0.00%|        >>> m = nn.Sigmoid()
   294|         0|            0|            0|  0.00%|        >>> input = torch.randn(2)
   295|         0|            0|            0|  0.00%|        >>> output = m(input)
   296|         0|            0|            0|  0.00%|    """
   297|         0|            0|            0|  0.00%|
   298|         0|            0|            0|  0.00%|    def forward(self, input: Tensor) -> Tensor:
   299|         0|            0|            0|  0.00%|        return torch.sigmoid(input)
   300|         0|            0|            0|  0.00%|
   301|         0|            0|            0|  0.00%|
   302|         0|            0|            0|  0.00%|class Hardsigmoid(Module):
   303|         0|            0|            0|  0.00%|    r"""Applies the element-wise function:
   304|         0|            0|            0|  0.00%|
   305|         0|            0|            0|  0.00%|    .. math::
   306|         0|            0|            0|  0.00%|        \text{Hardsigmoid}(x) = \begin{cases}
   307|         0|            0|            0|  0.00%|            0 & \text{if~} x \le -3, \\
   308|         0|            0|            0|  0.00%|            1 & \text{if~} x \ge +3, \\
   309|         0|            0|            0|  0.00%|            x / 6 + 1 / 2 & \text{otherwise}
   310|         0|            0|            0|  0.00%|        \end{cases}
   311|         0|            0|            0|  0.00%|
   312|         0|            0|            0|  0.00%|    Args:
   313|         0|            0|            0|  0.00%|        inplace: can optionally do the operation in-place. Default: ``False``
   314|         0|            0|            0|  0.00%|
   315|         0|            0|            0|  0.00%|    Shape:
   316|         0|            0|            0|  0.00%|        - Input: :math:`(N, *)` where `*` means, any number of additional
   317|         0|            0|            0|  0.00%|          dimensions
   318|         0|            0|            0|  0.00%|        - Output: :math:`(N, *)`, same shape as the input
   319|         0|            0|            0|  0.00%|
   320|         0|            0|            0|  0.00%|    Examples::
   321|         0|            0|            0|  0.00%|
   322|         0|            0|            0|  0.00%|        >>> m = nn.Hardsigmoid()
   323|         0|            0|            0|  0.00%|        >>> input = torch.randn(2)
   324|         0|            0|            0|  0.00%|        >>> output = m(input)
   325|         0|            0|            0|  0.00%|    """
   326|         0|            0|            0|  0.00%|    __constants__ = ['inplace']
   327|         0|            0|            0|  0.00%|
   328|         0|            0|            0|  0.00%|    inplace: bool
   329|         0|            0|            0|  0.00%|
   330|         0|            0|            0|  0.00%|    def __init__(self, inplace : bool = False) -> None:
   331|         0|            0|            0|  0.00%|        super(Hardsigmoid, self).__init__()
   332|         0|            0|            0|  0.00%|        self.inplace = inplace
   333|         0|            0|            0|  0.00%|
   334|         0|            0|            0|  0.00%|    def forward(self, input: Tensor) -> Tensor:
   335|         0|            0|            0|  0.00%|        return F.hardsigmoid(input, self.inplace)
   336|         0|            0|            0|  0.00%|
   337|         0|            0|            0|  0.00%|
   338|         0|            0|            0|  0.00%|class Tanh(Module):
   339|         0|            0|            0|  0.00%|    r"""Applies the element-wise function:
   340|         0|            0|            0|  0.00%|
   341|         0|            0|            0|  0.00%|    .. math::
   342|         0|            0|            0|  0.00%|        \text{Tanh}(x) = \tanh(x) = \frac{\exp(x) - \exp(-x)} {\exp(x) + \exp(-x)}
   343|         0|            0|            0|  0.00%|
   344|         0|            0|            0|  0.00%|    Shape:
   345|         0|            0|            0|  0.00%|        - Input: :math:`(N, *)` where `*` means, any number of additional
   346|         0|            0|            0|  0.00%|          dimensions
   347|         0|            0|            0|  0.00%|        - Output: :math:`(N, *)`, same shape as the input
   348|         0|            0|            0|  0.00%|
   349|         0|            0|            0|  0.00%|    .. image:: ../scripts/activation_images/Tanh.png
   350|         0|            0|            0|  0.00%|
   351|         0|            0|            0|  0.00%|    Examples::
   352|         0|            0|            0|  0.00%|
   353|         0|            0|            0|  0.00%|        >>> m = nn.Tanh()
   354|         0|            0|            0|  0.00%|        >>> input = torch.randn(2)
   355|         0|            0|            0|  0.00%|        >>> output = m(input)
   356|         0|            0|            0|  0.00%|    """
   357|         0|            0|            0|  0.00%|
   358|         0|            0|            0|  0.00%|    def forward(self, input: Tensor) -> Tensor:
   359|         0|            0|            0|  0.00%|        return torch.tanh(input)
   360|         0|            0|            0|  0.00%|
   361|         0|            0|            0|  0.00%|class SiLU(Module):
   362|         0|            0|            0|  0.00%|    r"""Applies the Sigmoid Linear Unit (SiLU) function, element-wise.
   363|         0|            0|            0|  0.00%|    The SiLU function is also known as the swish function.
   364|         0|            0|            0|  0.00%|
   365|         0|            0|            0|  0.00%|    .. math::
   366|         0|            0|            0|  0.00%|        \text{silu}(x) = x * \sigma(x), \text{where } \sigma(x) \text{ is the logistic sigmoid.}
   367|         0|            0|            0|  0.00%|
   368|         0|            0|            0|  0.00%|    .. note::
   369|         0|            0|            0|  0.00%|        See `Gaussian Error Linear Units (GELUs) <https://arxiv.org/abs/1606.08415>`_
   370|         0|            0|            0|  0.00%|        where the SiLU (Sigmoid Linear Unit) was originally coined, and see
   371|         0|            0|            0|  0.00%|        `Sigmoid-Weighted Linear Units for Neural Network Function Approximation
   372|         0|            0|            0|  0.00%|        in Reinforcement Learning <https://arxiv.org/abs/1702.03118>`_ and `Swish:
   373|         0|            0|            0|  0.00%|        a Self-Gated Activation Function <https://arxiv.org/abs/1710.05941v1>`_
   374|         0|            0|            0|  0.00%|        where the SiLU was experimented with later.
   375|         0|            0|            0|  0.00%|
   376|         0|            0|            0|  0.00%|    Shape:
   377|         0|            0|            0|  0.00%|        - Input: :math:`(N, *)` where `*` means, any number of additional
   378|         0|            0|            0|  0.00%|          dimensions
   379|         0|            0|            0|  0.00%|        - Output: :math:`(N, *)`, same shape as the input
   380|         0|            0|            0|  0.00%|
   381|         0|            0|            0|  0.00%|    Examples::
   382|         0|            0|            0|  0.00%|
   383|         0|            0|            0|  0.00%|        >>> m = nn.SiLU()
   384|         0|            0|            0|  0.00%|        >>> input = torch.randn(2)
   385|         0|            0|            0|  0.00%|        >>> output = m(input)
   386|         0|            0|            0|  0.00%|    """
   387|         0|            0|            0|  0.00%|    __constants__ = ['inplace']
   388|         0|            0|            0|  0.00%|    inplace: bool
   389|         0|            0|            0|  0.00%|
   390|         0|            0|            0|  0.00%|    def __init__(self, inplace: bool = False):
   391|         0|            0|            0|  0.00%|        super(SiLU, self).__init__()
   392|         0|            0|            0|  0.00%|        self.inplace = inplace
   393|         0|            0|            0|  0.00%|
   394|         0|            0|            0|  0.00%|    def forward(self, input: Tensor) -> Tensor:
   395|         0|            0|            0|  0.00%|        return F.silu(input, inplace=self.inplace)
   396|         0|            0|            0|  0.00%|
   397|         0|            0|            0|  0.00%|    def extra_repr(self) -> str:
   398|         0|            0|            0|  0.00%|        inplace_str = 'inplace=True' if self.inplace else ''
   399|         0|            0|            0|  0.00%|        return inplace_str
   400|         0|            0|            0|  0.00%|
   401|         0|            0|            0|  0.00%|class Mish(Module):
   402|         0|            0|            0|  0.00%|    r"""Applies the Mish function, element-wise.
   403|         0|            0|            0|  0.00%|    Mish: A Self Regularized Non-Monotonic Neural Activation Function.
   404|         0|            0|            0|  0.00%|
   405|         0|            0|            0|  0.00%|    .. math::
   406|         0|            0|            0|  0.00%|        \text{Mish}(x) = x * \text{Tanh}(\text{Softplus}(x))
   407|         0|            0|            0|  0.00%|
   408|         0|            0|            0|  0.00%|    .. note::
   409|         0|            0|            0|  0.00%|        See `Mish: A Self Regularized Non-Monotonic Neural Activation Function <https://arxiv.org/abs/1908.08681>`_
   410|         0|            0|            0|  0.00%|
   411|         0|            0|            0|  0.00%|    Shape:
   412|         0|            0|            0|  0.00%|        - Input: :math:`(N, *)` where `*` means, any number of additional
   413|         0|            0|            0|  0.00%|          dimensions
   414|         0|            0|            0|  0.00%|        - Output: :math:`(N, *)`, same shape as the input
   415|         0|            0|            0|  0.00%|
   416|         0|            0|            0|  0.00%|    Examples::
   417|         0|            0|            0|  0.00%|
   418|         0|            0|            0|  0.00%|        >>> m = nn.Mish()
   419|         0|            0|            0|  0.00%|        >>> input = torch.randn(2)
   420|         0|            0|            0|  0.00%|        >>> output = m(input)
   421|         0|            0|            0|  0.00%|    """
   422|         0|            0|            0|  0.00%|    __constants__ = ['inplace']
   423|         0|            0|            0|  0.00%|    inplace: bool
   424|         0|            0|            0|  0.00%|
   425|         0|            0|            0|  0.00%|    def __init__(self, inplace: bool = False):
   426|         0|            0|            0|  0.00%|        super(Mish, self).__init__()
   427|         0|            0|            0|  0.00%|        self.inplace = inplace
   428|         0|            0|            0|  0.00%|
   429|         0|            0|            0|  0.00%|    def forward(self, input: Tensor) -> Tensor:
   430|         0|            0|            0|  0.00%|        return F.mish(input, inplace=self.inplace)
   431|         0|            0|            0|  0.00%|
   432|         0|            0|            0|  0.00%|    def extra_repr(self) -> str:
   433|         0|            0|            0|  0.00%|        inplace_str = 'inplace=True' if self.inplace else ''
   434|         0|            0|            0|  0.00%|        return inplace_str
   435|         0|            0|            0|  0.00%|
   436|         0|            0|            0|  0.00%|class Hardswish(Module):
   437|         0|            0|            0|  0.00%|    r"""Applies the hardswish function, element-wise, as described in the paper:
   438|         0|            0|            0|  0.00%|
   439|         0|            0|            0|  0.00%|    `Searching for MobileNetV3`_.
   440|         0|            0|            0|  0.00%|
   441|         0|            0|            0|  0.00%|    .. math::
   442|         0|            0|            0|  0.00%|        \text{Hardswish}(x) = \begin{cases}
   443|         0|            0|            0|  0.00%|            0 & \text{if~} x \le -3, \\
   444|         0|            0|            0|  0.00%|            x & \text{if~} x \ge +3, \\
   445|         0|            0|            0|  0.00%|            x \cdot (x + 3) /6 & \text{otherwise}
   446|         0|            0|            0|  0.00%|        \end{cases}
   447|         0|            0|            0|  0.00%|
   448|         0|            0|            0|  0.00%|    Args:
   449|         0|            0|            0|  0.00%|        inplace: can optionally do the operation in-place. Default: ``False``
   450|         0|            0|            0|  0.00%|
   451|         0|            0|            0|  0.00%|    Shape:
   452|         0|            0|            0|  0.00%|        - Input: :math:`(N, *)` where `*` means, any number of additional
   453|         0|            0|            0|  0.00%|          dimensions
   454|         0|            0|            0|  0.00%|        - Output: :math:`(N, *)`, same shape as the input
   455|         0|            0|            0|  0.00%|
   456|         0|            0|            0|  0.00%|    Examples::
   457|         0|            0|            0|  0.00%|
   458|         0|            0|            0|  0.00%|        >>> m = nn.Hardswish()
   459|         0|            0|            0|  0.00%|        >>> input = torch.randn(2)
   460|         0|            0|            0|  0.00%|        >>> output = m(input)
   461|         0|            0|            0|  0.00%|
   462|         0|            0|            0|  0.00%|    .. _`Searching for MobileNetV3`:
   463|         0|            0|            0|  0.00%|        https://arxiv.org/abs/1905.02244
   464|         0|            0|            0|  0.00%|    """
   465|         0|            0|            0|  0.00%|    __constants__ = ['inplace']
   466|         0|            0|            0|  0.00%|
   467|         0|            0|            0|  0.00%|    inplace: bool
   468|         0|            0|            0|  0.00%|
   469|         0|            0|            0|  0.00%|    def __init__(self, inplace : bool = False) -> None:
   470|         0|            0|            0|  0.00%|        super(Hardswish, self).__init__()
   471|         0|            0|            0|  0.00%|        self.inplace = inplace
   472|         0|            0|            0|  0.00%|
   473|         0|            0|            0|  0.00%|    def forward(self, input: Tensor) -> Tensor:
   474|         0|            0|            0|  0.00%|        return F.hardswish(input, self.inplace)
   475|         0|            0|            0|  0.00%|
   476|         0|            0|            0|  0.00%|
   477|         0|            0|            0|  0.00%|class ELU(Module):
   478|         0|            0|            0|  0.00%|    r"""Applies the element-wise function:
   479|         0|            0|            0|  0.00%|
   480|         0|            0|            0|  0.00%|    .. math::
   481|         0|            0|            0|  0.00%|        \text{ELU}(x) = \begin{cases}
   482|         0|            0|            0|  0.00%|        x, & \text{ if } x > 0\\
   483|         0|            0|            0|  0.00%|        \alpha * (\exp(x) - 1), & \text{ if } x \leq 0
   484|         0|            0|            0|  0.00%|        \end{cases}
   485|         0|            0|            0|  0.00%|
   486|         0|            0|            0|  0.00%|    Args:
   487|         0|            0|            0|  0.00%|        alpha: the :math:`\alpha` value for the ELU formulation. Default: 1.0
   488|         0|            0|            0|  0.00%|        inplace: can optionally do the operation in-place. Default: ``False``
   489|         0|            0|            0|  0.00%|
   490|         0|            0|            0|  0.00%|    Shape:
   491|         0|            0|            0|  0.00%|        - Input: :math:`(N, *)` where `*` means, any number of additional
   492|         0|            0|            0|  0.00%|          dimensions
   493|         0|            0|            0|  0.00%|        - Output: :math:`(N, *)`, same shape as the input
   494|         0|            0|            0|  0.00%|
   495|         0|            0|            0|  0.00%|    .. image:: ../scripts/activation_images/ELU.png
   496|         0|            0|            0|  0.00%|
   497|         0|            0|            0|  0.00%|    Examples::
   498|         0|            0|            0|  0.00%|
   499|         0|            0|            0|  0.00%|        >>> m = nn.ELU()
   500|         0|            0|            0|  0.00%|        >>> input = torch.randn(2)
   501|         0|            0|            0|  0.00%|        >>> output = m(input)
   502|         0|            0|            0|  0.00%|    """
   503|         0|            0|            0|  0.00%|    __constants__ = ['alpha', 'inplace']
   504|         0|            0|            0|  0.00%|    alpha: float
   505|         0|            0|            0|  0.00%|    inplace: bool
   506|         0|            0|            0|  0.00%|
   507|         0|            0|            0|  0.00%|    def __init__(self, alpha: float = 1., inplace: bool = False) -> None:
   508|         0|            0|            0|  0.00%|        super(ELU, self).__init__()
   509|         0|            0|            0|  0.00%|        self.alpha = alpha
   510|         0|            0|            0|  0.00%|        self.inplace = inplace
   511|         0|            0|            0|  0.00%|
   512|         0|            0|            0|  0.00%|    def forward(self, input: Tensor) -> Tensor:
   513|         0|            0|            0|  0.00%|        return F.elu(input, self.alpha, self.inplace)
   514|         0|            0|            0|  0.00%|
   515|         0|            0|            0|  0.00%|    def extra_repr(self) -> str:
   516|         0|            0|            0|  0.00%|        inplace_str = ', inplace=True' if self.inplace else ''
   517|         0|            0|            0|  0.00%|        return 'alpha={}{}'.format(self.alpha, inplace_str)
   518|         0|            0|            0|  0.00%|
   519|         0|            0|            0|  0.00%|
   520|         0|            0|            0|  0.00%|class CELU(Module):
   521|         0|            0|            0|  0.00%|    r"""Applies the element-wise function:
   522|         0|            0|            0|  0.00%|
   523|         0|            0|            0|  0.00%|    .. math::
   524|         0|            0|            0|  0.00%|        \text{CELU}(x) = \max(0,x) + \min(0, \alpha * (\exp(x/\alpha) - 1))
   525|         0|            0|            0|  0.00%|
   526|         0|            0|            0|  0.00%|    More details can be found in the paper `Continuously Differentiable Exponential Linear Units`_ .
   527|         0|            0|            0|  0.00%|
   528|         0|            0|            0|  0.00%|    Args:
   529|         0|            0|            0|  0.00%|        alpha: the :math:`\alpha` value for the CELU formulation. Default: 1.0
   530|         0|            0|            0|  0.00%|        inplace: can optionally do the operation in-place. Default: ``False``
   531|         0|            0|            0|  0.00%|
   532|         0|            0|            0|  0.00%|    Shape:
   533|         0|            0|            0|  0.00%|        - Input: :math:`(N, *)` where `*` means, any number of additional
   534|         0|            0|            0|  0.00%|          dimensions
   535|         0|            0|            0|  0.00%|        - Output: :math:`(N, *)`, same shape as the input
   536|         0|            0|            0|  0.00%|
   537|         0|            0|            0|  0.00%|    .. image:: ../scripts/activation_images/CELU.png
   538|         0|            0|            0|  0.00%|
   539|         0|            0|            0|  0.00%|    Examples::
   540|         0|            0|            0|  0.00%|
   541|         0|            0|            0|  0.00%|        >>> m = nn.CELU()
   542|         0|            0|            0|  0.00%|        >>> input = torch.randn(2)
   543|         0|            0|            0|  0.00%|        >>> output = m(input)
   544|         0|            0|            0|  0.00%|
   545|         0|            0|            0|  0.00%|    .. _`Continuously Differentiable Exponential Linear Units`:
   546|         0|            0|            0|  0.00%|        https://arxiv.org/abs/1704.07483
   547|         0|            0|            0|  0.00%|    """
   548|         0|            0|            0|  0.00%|    __constants__ = ['alpha', 'inplace']
   549|         0|            0|            0|  0.00%|    alpha: float
   550|         0|            0|            0|  0.00%|    inplace: bool
   551|         0|            0|            0|  0.00%|
   552|         0|            0|            0|  0.00%|    def __init__(self, alpha: float = 1., inplace: bool = False) -> None:
   553|         0|            0|            0|  0.00%|        super(CELU, self).__init__()
   554|         0|            0|            0|  0.00%|        self.alpha = alpha
   555|         0|            0|            0|  0.00%|        self.inplace = inplace
   556|         0|            0|            0|  0.00%|
   557|         0|            0|            0|  0.00%|    def forward(self, input: Tensor) -> Tensor:
   558|         0|            0|            0|  0.00%|        return F.celu(input, self.alpha, self.inplace)
   559|         0|            0|            0|  0.00%|
   560|         0|            0|            0|  0.00%|    def extra_repr(self) -> str:
   561|         0|            0|            0|  0.00%|        inplace_str = ', inplace=True' if self.inplace else ''
   562|         0|            0|            0|  0.00%|        return 'alpha={}{}'.format(self.alpha, inplace_str)
   563|         0|            0|            0|  0.00%|
   564|         0|            0|            0|  0.00%|
   565|         0|            0|            0|  0.00%|class SELU(Module):
   566|         0|            0|            0|  0.00%|    r"""Applied element-wise, as:
   567|         0|            0|            0|  0.00%|
   568|         0|            0|            0|  0.00%|    .. math::
   569|         0|            0|            0|  0.00%|        \text{SELU}(x) = \text{scale} * (\max(0,x) + \min(0, \alpha * (\exp(x) - 1)))
   570|         0|            0|            0|  0.00%|
   571|         0|            0|            0|  0.00%|    with :math:`\alpha = 1.6732632423543772848170429916717` and
   572|         0|            0|            0|  0.00%|    :math:`\text{scale} = 1.0507009873554804934193349852946`.
   573|         0|            0|            0|  0.00%|
   574|         0|            0|            0|  0.00%|    .. warning::
   575|         0|            0|            0|  0.00%|        When using ``kaiming_normal`` or ``kaiming_normal_`` for initialisation,
   576|         0|            0|            0|  0.00%|        ``nonlinearity='linear'`` should be used instead of ``nonlinearity='selu'``
   577|         0|            0|            0|  0.00%|        in order to get `Self-Normalizing Neural Networks`_.
   578|         0|            0|            0|  0.00%|        See :func:`torch.nn.init.calculate_gain` for more information.
   579|         0|            0|            0|  0.00%|
   580|         0|            0|            0|  0.00%|    More details can be found in the paper `Self-Normalizing Neural Networks`_ .
   581|         0|            0|            0|  0.00%|
   582|         0|            0|            0|  0.00%|    Args:
   583|         0|            0|            0|  0.00%|        inplace (bool, optional): can optionally do the operation in-place. Default: ``False``
   584|         0|            0|            0|  0.00%|
   585|         0|            0|            0|  0.00%|    Shape:
   586|         0|            0|            0|  0.00%|        - Input: :math:`(N, *)` where `*` means, any number of additional
   587|         0|            0|            0|  0.00%|          dimensions
   588|         0|            0|            0|  0.00%|        - Output: :math:`(N, *)`, same shape as the input
   589|         0|            0|            0|  0.00%|
   590|         0|            0|            0|  0.00%|    .. image:: ../scripts/activation_images/SELU.png
   591|         0|            0|            0|  0.00%|
   592|         0|            0|            0|  0.00%|    Examples::
   593|         0|            0|            0|  0.00%|
   594|         0|            0|            0|  0.00%|        >>> m = nn.SELU()
   595|         0|            0|            0|  0.00%|        >>> input = torch.randn(2)
   596|         0|            0|            0|  0.00%|        >>> output = m(input)
   597|         0|            0|            0|  0.00%|
   598|         0|            0|            0|  0.00%|    .. _Self-Normalizing Neural Networks: https://arxiv.org/abs/1706.02515
   599|         0|            0|            0|  0.00%|    """
   600|         0|            0|            0|  0.00%|    __constants__ = ['inplace']
   601|         0|            0|            0|  0.00%|    inplace: bool
   602|         0|            0|            0|  0.00%|
   603|         0|            0|            0|  0.00%|    def __init__(self, inplace: bool = False) -> None:
   604|         0|            0|            0|  0.00%|        super(SELU, self).__init__()
   605|         0|            0|            0|  0.00%|        self.inplace = inplace
   606|         0|            0|            0|  0.00%|
   607|         0|            0|            0|  0.00%|    def forward(self, input: Tensor) -> Tensor:
   608|         0|            0|            0|  0.00%|        return F.selu(input, self.inplace)
   609|         0|            0|            0|  0.00%|
   610|         0|            0|            0|  0.00%|    def extra_repr(self) -> str:
   611|         0|            0|            0|  0.00%|        inplace_str = 'inplace=True' if self.inplace else ''
   612|         0|            0|            0|  0.00%|        return inplace_str
   613|         0|            0|            0|  0.00%|
   614|         0|            0|            0|  0.00%|
   615|         0|            0|            0|  0.00%|class GLU(Module):
   616|         0|            0|            0|  0.00%|    r"""Applies the gated linear unit function
   617|         0|            0|            0|  0.00%|    :math:`{GLU}(a, b)= a \otimes \sigma(b)` where :math:`a` is the first half
   618|         0|            0|            0|  0.00%|    of the input matrices and :math:`b` is the second half.
   619|         0|            0|            0|  0.00%|
   620|         0|            0|            0|  0.00%|    Args:
   621|         0|            0|            0|  0.00%|        dim (int): the dimension on which to split the input. Default: -1
   622|         0|            0|            0|  0.00%|
   623|         0|            0|            0|  0.00%|    Shape:
   624|         0|            0|            0|  0.00%|        - Input: :math:`(\ast_1, N, \ast_2)` where `*` means, any number of additional
   625|         0|            0|            0|  0.00%|          dimensions
   626|         0|            0|            0|  0.00%|        - Output: :math:`(\ast_1, M, \ast_2)` where :math:`M=N/2`
   627|         0|            0|            0|  0.00%|
   628|         0|            0|            0|  0.00%|    Examples::
   629|         0|            0|            0|  0.00%|
   630|         0|            0|            0|  0.00%|        >>> m = nn.GLU()
   631|         0|            0|            0|  0.00%|        >>> input = torch.randn(4, 2)
   632|         0|            0|            0|  0.00%|        >>> output = m(input)
   633|         0|            0|            0|  0.00%|    """
   634|         0|            0|            0|  0.00%|    __constants__ = ['dim']
   635|         0|            0|            0|  0.00%|    dim: int
   636|         0|            0|            0|  0.00%|
   637|         0|            0|            0|  0.00%|    def __init__(self, dim: int = -1) -> None:
   638|         0|            0|            0|  0.00%|        super(GLU, self).__init__()
   639|         0|            0|            0|  0.00%|        self.dim = dim
   640|         0|            0|            0|  0.00%|
   641|         0|            0|            0|  0.00%|    def forward(self, input: Tensor) -> Tensor:
   642|         0|            0|            0|  0.00%|        return F.glu(input, self.dim)
   643|         0|            0|            0|  0.00%|
   644|         0|            0|            0|  0.00%|    def extra_repr(self) -> str:
   645|         0|            0|            0|  0.00%|        return 'dim={}'.format(self.dim)
   646|         0|            0|            0|  0.00%|
   647|         0|            0|            0|  0.00%|
   648|         0|            0|            0|  0.00%|class GELU(Module):
   649|         0|            0|            0|  0.00%|    r"""Applies the Gaussian Error Linear Units function:
   650|         0|            0|            0|  0.00%|
   651|         0|            0|            0|  0.00%|    .. math:: \text{GELU}(x) = x * \Phi(x)
   652|         0|            0|            0|  0.00%|
   653|         0|            0|            0|  0.00%|    where :math:`\Phi(x)` is the Cumulative Distribution Function for Gaussian Distribution.
   654|         0|            0|            0|  0.00%|
   655|         0|            0|            0|  0.00%|    Shape:
   656|         0|            0|            0|  0.00%|        - Input: :math:`(N, *)` where `*` means, any number of additional
   657|         0|            0|            0|  0.00%|          dimensions
   658|         0|            0|            0|  0.00%|        - Output: :math:`(N, *)`, same shape as the input
   659|         0|            0|            0|  0.00%|
   660|         0|            0|            0|  0.00%|    .. image:: ../scripts/activation_images/GELU.png
   661|         0|            0|            0|  0.00%|
   662|         0|            0|            0|  0.00%|    Examples::
   663|         0|            0|            0|  0.00%|
   664|         0|            0|            0|  0.00%|        >>> m = nn.GELU()
   665|         0|            0|            0|  0.00%|        >>> input = torch.randn(2)
   666|         0|            0|            0|  0.00%|        >>> output = m(input)
   667|         0|            0|            0|  0.00%|    """
   668|         0|            0|            0|  0.00%|    def forward(self, input: Tensor) -> Tensor:
   669|         0|            0|            0|  0.00%|        return F.gelu(input)
   670|         0|            0|            0|  0.00%|
   671|         0|            0|            0|  0.00%|
   672|         0|            0|            0|  0.00%|class Hardshrink(Module):
   673|         0|            0|            0|  0.00%|    r"""Applies the hard shrinkage function element-wise:
   674|         0|            0|            0|  0.00%|
   675|         0|            0|            0|  0.00%|    .. math::
   676|         0|            0|            0|  0.00%|        \text{HardShrink}(x) =
   677|         0|            0|            0|  0.00%|        \begin{cases}
   678|         0|            0|            0|  0.00%|        x, & \text{ if } x > \lambda \\
   679|         0|            0|            0|  0.00%|        x, & \text{ if } x < -\lambda \\
   680|         0|            0|            0|  0.00%|        0, & \text{ otherwise }
   681|         0|            0|            0|  0.00%|        \end{cases}
   682|         0|            0|            0|  0.00%|
   683|         0|            0|            0|  0.00%|    Args:
   684|         0|            0|            0|  0.00%|        lambd: the :math:`\lambda` value for the Hardshrink formulation. Default: 0.5
   685|         0|            0|            0|  0.00%|
   686|         0|            0|            0|  0.00%|    Shape:
   687|         0|            0|            0|  0.00%|        - Input: :math:`(N, *)` where `*` means, any number of additional
   688|         0|            0|            0|  0.00%|          dimensions
   689|         0|            0|            0|  0.00%|        - Output: :math:`(N, *)`, same shape as the input
   690|         0|            0|            0|  0.00%|
   691|         0|            0|            0|  0.00%|    .. image:: ../scripts/activation_images/Hardshrink.png
   692|         0|            0|            0|  0.00%|
   693|         0|            0|            0|  0.00%|    Examples::
   694|         0|            0|            0|  0.00%|
   695|         0|            0|            0|  0.00%|        >>> m = nn.Hardshrink()
   696|         0|            0|            0|  0.00%|        >>> input = torch.randn(2)
   697|         0|            0|            0|  0.00%|        >>> output = m(input)
   698|         0|            0|            0|  0.00%|    """
   699|         0|            0|            0|  0.00%|    __constants__ = ['lambd']
   700|         0|            0|            0|  0.00%|    lambd: float
   701|         0|            0|            0|  0.00%|
   702|         0|            0|            0|  0.00%|    def __init__(self, lambd: float = 0.5) -> None:
   703|         0|            0|            0|  0.00%|        super(Hardshrink, self).__init__()
   704|         0|            0|            0|  0.00%|        self.lambd = lambd
   705|         0|            0|            0|  0.00%|
   706|         0|            0|            0|  0.00%|    def forward(self, input: Tensor) -> Tensor:
   707|         0|            0|            0|  0.00%|        return F.hardshrink(input, self.lambd)
   708|         0|            0|            0|  0.00%|
   709|         0|            0|            0|  0.00%|    def extra_repr(self) -> str:
   710|         0|            0|            0|  0.00%|        return '{}'.format(self.lambd)
   711|         0|            0|            0|  0.00%|
   712|         0|            0|            0|  0.00%|
   713|         0|            0|            0|  0.00%|class LeakyReLU(Module):
   714|         0|            0|            0|  0.00%|    r"""Applies the element-wise function:
   715|         0|            0|            0|  0.00%|
   716|         0|            0|            0|  0.00%|    .. math::
   717|         0|            0|            0|  0.00%|        \text{LeakyReLU}(x) = \max(0, x) + \text{negative\_slope} * \min(0, x)
   718|         0|            0|            0|  0.00%|
   719|         0|            0|            0|  0.00%|
   720|         0|            0|            0|  0.00%|    or
   721|         0|            0|            0|  0.00%|
   722|         0|            0|            0|  0.00%|    .. math::
   723|         0|            0|            0|  0.00%|        \text{LeakyRELU}(x) =
   724|         0|            0|            0|  0.00%|        \begin{cases}
   725|         0|            0|            0|  0.00%|        x, & \text{ if } x \geq 0 \\
   726|         0|            0|            0|  0.00%|        \text{negative\_slope} \times x, & \text{ otherwise }
   727|         0|            0|            0|  0.00%|        \end{cases}
   728|         0|            0|            0|  0.00%|
   729|         0|            0|            0|  0.00%|    Args:
   730|         0|            0|            0|  0.00%|        negative_slope: Controls the angle of the negative slope. Default: 1e-2
   731|         0|            0|            0|  0.00%|        inplace: can optionally do the operation in-place. Default: ``False``
   732|         0|            0|            0|  0.00%|
   733|         0|            0|            0|  0.00%|    Shape:
   734|         0|            0|            0|  0.00%|        - Input: :math:`(N, *)` where `*` means, any number of additional
   735|         0|            0|            0|  0.00%|          dimensions
   736|         0|            0|            0|  0.00%|        - Output: :math:`(N, *)`, same shape as the input
   737|         0|            0|            0|  0.00%|
   738|         0|            0|            0|  0.00%|    .. image:: ../scripts/activation_images/LeakyReLU.png
   739|         0|            0|            0|  0.00%|
   740|         0|            0|            0|  0.00%|    Examples::
   741|         0|            0|            0|  0.00%|
   742|         0|            0|            0|  0.00%|        >>> m = nn.LeakyReLU(0.1)
   743|         0|            0|            0|  0.00%|        >>> input = torch.randn(2)
   744|         0|            0|            0|  0.00%|        >>> output = m(input)
   745|         0|            0|            0|  0.00%|    """
   746|         0|            0|            0|  0.00%|    __constants__ = ['inplace', 'negative_slope']
   747|         0|            0|            0|  0.00%|    inplace: bool
   748|         0|            0|            0|  0.00%|    negative_slope: float
   749|         0|            0|            0|  0.00%|
   750|         0|            0|            0|  0.00%|    def __init__(self, negative_slope: float = 1e-2, inplace: bool = False) -> None:
   751|         0|            0|            0|  0.00%|        super(LeakyReLU, self).__init__()
   752|         0|            0|            0|  0.00%|        self.negative_slope = negative_slope
   753|         0|            0|            0|  0.00%|        self.inplace = inplace
   754|         0|            0|            0|  0.00%|
   755|         0|            0|            0|  0.00%|    def forward(self, input: Tensor) -> Tensor:
   756|         0|            0|            0|  0.00%|        return F.leaky_relu(input, self.negative_slope, self.inplace)
   757|         0|            0|            0|  0.00%|
   758|         0|            0|            0|  0.00%|    def extra_repr(self) -> str:
   759|         0|            0|            0|  0.00%|        inplace_str = ', inplace=True' if self.inplace else ''
   760|         0|            0|            0|  0.00%|        return 'negative_slope={}{}'.format(self.negative_slope, inplace_str)
   761|         0|            0|            0|  0.00%|
   762|         0|            0|            0|  0.00%|
   763|         0|            0|            0|  0.00%|class LogSigmoid(Module):
   764|         0|            0|            0|  0.00%|    r"""Applies the element-wise function:
   765|         0|            0|            0|  0.00%|
   766|         0|            0|            0|  0.00%|    .. math::
   767|         0|            0|            0|  0.00%|        \text{LogSigmoid}(x) = \log\left(\frac{ 1 }{ 1 + \exp(-x)}\right)
   768|         0|            0|            0|  0.00%|
   769|         0|            0|            0|  0.00%|    Shape:
   770|         0|            0|            0|  0.00%|        - Input: :math:`(N, *)` where `*` means, any number of additional
   771|         0|            0|            0|  0.00%|          dimensions
   772|         0|            0|            0|  0.00%|        - Output: :math:`(N, *)`, same shape as the input
   773|         0|            0|            0|  0.00%|
   774|         0|            0|            0|  0.00%|    .. image:: ../scripts/activation_images/LogSigmoid.png
   775|         0|            0|            0|  0.00%|
   776|         0|            0|            0|  0.00%|    Examples::
   777|         0|            0|            0|  0.00%|
   778|         0|            0|            0|  0.00%|        >>> m = nn.LogSigmoid()
   779|         0|            0|            0|  0.00%|        >>> input = torch.randn(2)
   780|         0|            0|            0|  0.00%|        >>> output = m(input)
   781|         0|            0|            0|  0.00%|    """
   782|         0|            0|            0|  0.00%|
   783|         0|            0|            0|  0.00%|    def forward(self, input: Tensor) -> Tensor:
   784|         0|            0|            0|  0.00%|        return F.logsigmoid(input)
   785|         0|            0|            0|  0.00%|
   786|         0|            0|            0|  0.00%|
   787|         0|            0|            0|  0.00%|class Softplus(Module):
   788|         0|            0|            0|  0.00%|    r"""Applies the element-wise function:
   789|         0|            0|            0|  0.00%|
   790|         0|            0|            0|  0.00%|    .. math::
   791|         0|            0|            0|  0.00%|        \text{Softplus}(x) = \frac{1}{\beta} * \log(1 + \exp(\beta * x))
   792|         0|            0|            0|  0.00%|
   793|         0|            0|            0|  0.00%|    SoftPlus is a smooth approximation to the ReLU function and can be used
   794|         0|            0|            0|  0.00%|    to constrain the output of a machine to always be positive.
   795|         0|            0|            0|  0.00%|
   796|         0|            0|            0|  0.00%|    For numerical stability the implementation reverts to the linear function
   797|         0|            0|            0|  0.00%|    when :math:`input \times \beta > threshold`.
   798|         0|            0|            0|  0.00%|
   799|         0|            0|            0|  0.00%|    Args:
   800|         0|            0|            0|  0.00%|        beta: the :math:`\beta` value for the Softplus formulation. Default: 1
   801|         0|            0|            0|  0.00%|        threshold: values above this revert to a linear function. Default: 20
   802|         0|            0|            0|  0.00%|
   803|         0|            0|            0|  0.00%|    Shape:
   804|         0|            0|            0|  0.00%|        - Input: :math:`(N, *)` where `*` means, any number of additional
   805|         0|            0|            0|  0.00%|          dimensions
   806|         0|            0|            0|  0.00%|        - Output: :math:`(N, *)`, same shape as the input
   807|         0|            0|            0|  0.00%|
   808|         0|            0|            0|  0.00%|    .. image:: ../scripts/activation_images/Softplus.png
   809|         0|            0|            0|  0.00%|
   810|         0|            0|            0|  0.00%|    Examples::
   811|         0|            0|            0|  0.00%|
   812|         0|            0|            0|  0.00%|        >>> m = nn.Softplus()
   813|         0|            0|            0|  0.00%|        >>> input = torch.randn(2)
   814|         0|            0|            0|  0.00%|        >>> output = m(input)
   815|         0|            0|            0|  0.00%|    """
   816|         0|            0|            0|  0.00%|    __constants__ = ['beta', 'threshold']
   817|         0|            0|            0|  0.00%|    beta: int
   818|         0|            0|            0|  0.00%|    threshold: int
   819|         0|            0|            0|  0.00%|
   820|         0|            0|            0|  0.00%|    def __init__(self, beta: int = 1, threshold: int = 20) -> None:
   821|         0|            0|            0|  0.00%|        super(Softplus, self).__init__()
   822|         0|            0|            0|  0.00%|        self.beta = beta
   823|         0|            0|            0|  0.00%|        self.threshold = threshold
   824|         0|            0|            0|  0.00%|
   825|         0|            0|            0|  0.00%|    def forward(self, input: Tensor) -> Tensor:
   826|         0|            0|            0|  0.00%|        return F.softplus(input, self.beta, self.threshold)
   827|         0|            0|            0|  0.00%|
   828|         0|            0|            0|  0.00%|    def extra_repr(self) -> str:
   829|         0|            0|            0|  0.00%|        return 'beta={}, threshold={}'.format(self.beta, self.threshold)
   830|         0|            0|            0|  0.00%|
   831|         0|            0|            0|  0.00%|
   832|         0|            0|            0|  0.00%|class Softshrink(Module):
   833|         0|            0|            0|  0.00%|    r"""Applies the soft shrinkage function elementwise:
   834|         0|            0|            0|  0.00%|
   835|         0|            0|            0|  0.00%|    .. math::
   836|         0|            0|            0|  0.00%|        \text{SoftShrinkage}(x) =
   837|         0|            0|            0|  0.00%|        \begin{cases}
   838|         0|            0|            0|  0.00%|        x - \lambda, & \text{ if } x > \lambda \\
   839|         0|            0|            0|  0.00%|        x + \lambda, & \text{ if } x < -\lambda \\
   840|         0|            0|            0|  0.00%|        0, & \text{ otherwise }
   841|         0|            0|            0|  0.00%|        \end{cases}
   842|         0|            0|            0|  0.00%|
   843|         0|            0|            0|  0.00%|    Args:
   844|         0|            0|            0|  0.00%|        lambd: the :math:`\lambda` (must be no less than zero) value for the Softshrink formulation. Default: 0.5
   845|         0|            0|            0|  0.00%|
   846|         0|            0|            0|  0.00%|    Shape:
   847|         0|            0|            0|  0.00%|        - Input: :math:`(N, *)` where `*` means, any number of additional
   848|         0|            0|            0|  0.00%|          dimensions
   849|         0|            0|            0|  0.00%|        - Output: :math:`(N, *)`, same shape as the input
   850|         0|            0|            0|  0.00%|
   851|         0|            0|            0|  0.00%|    .. image:: ../scripts/activation_images/Softshrink.png
   852|         0|            0|            0|  0.00%|
   853|         0|            0|            0|  0.00%|    Examples::
   854|         0|            0|            0|  0.00%|
   855|         0|            0|            0|  0.00%|        >>> m = nn.Softshrink()
   856|         0|            0|            0|  0.00%|        >>> input = torch.randn(2)
   857|         0|            0|            0|  0.00%|        >>> output = m(input)
   858|         0|            0|            0|  0.00%|    """
   859|         0|            0|            0|  0.00%|    __constants__ = ['lambd']
   860|         0|            0|            0|  0.00%|    lambd: float
   861|         0|            0|            0|  0.00%|
   862|         0|            0|            0|  0.00%|    def __init__(self, lambd: float = 0.5) -> None:
   863|         0|            0|            0|  0.00%|        super(Softshrink, self).__init__()
   864|         0|            0|            0|  0.00%|        self.lambd = lambd
   865|         0|            0|            0|  0.00%|
   866|         0|            0|            0|  0.00%|    def forward(self, input: Tensor) -> Tensor:
   867|         0|            0|            0|  0.00%|        return F.softshrink(input, self.lambd)
   868|         0|            0|            0|  0.00%|
   869|         0|            0|            0|  0.00%|    def extra_repr(self) -> str:
   870|         0|            0|            0|  0.00%|        return str(self.lambd)
   871|         0|            0|            0|  0.00%|
   872|         0|            0|            0|  0.00%|
   873|         0|            0|            0|  0.00%|class MultiheadAttention(Module):
   874|         0|            0|            0|  0.00%|    r"""Allows the model to jointly attend to information
   875|         0|            0|            0|  0.00%|    from different representation subspaces.
   876|         0|            0|            0|  0.00%|    See `Attention Is All You Need <https://arxiv.org/abs/1706.03762>`_
   877|         0|            0|            0|  0.00%|
   878|         0|            0|            0|  0.00%|    .. math::
   879|         0|            0|            0|  0.00%|        \text{MultiHead}(Q, K, V) = \text{Concat}(head_1,\dots,head_h)W^O
   880|         0|            0|            0|  0.00%|
   881|         0|            0|            0|  0.00%|    where :math:`head_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)`.
   882|         0|            0|            0|  0.00%|
   883|         0|            0|            0|  0.00%|    Args:
   884|         0|            0|            0|  0.00%|        embed_dim: total dimension of the model.
   885|         0|            0|            0|  0.00%|        num_heads: parallel attention heads.
   886|         0|            0|            0|  0.00%|        dropout: a Dropout layer on attn_output_weights. Default: 0.0.
   887|         0|            0|            0|  0.00%|        bias: add bias as module parameter. Default: True.
   888|         0|            0|            0|  0.00%|        add_bias_kv: add bias to the key and value sequences at dim=0.
   889|         0|            0|            0|  0.00%|        add_zero_attn: add a new batch of zeros to the key and
   890|         0|            0|            0|  0.00%|                       value sequences at dim=1.
   891|         0|            0|            0|  0.00%|        kdim: total number of features in key. Default: None.
   892|         0|            0|            0|  0.00%|        vdim: total number of features in value. Default: None.
   893|         0|            0|            0|  0.00%|        batch_first: If ``True``, then the input and output tensors are provided
   894|         0|            0|            0|  0.00%|            as (batch, seq, feature). Default: ``False`` (seq, batch, feature).
   895|         0|            0|            0|  0.00%|
   896|         0|            0|            0|  0.00%|    Note that if :attr:`kdim` and :attr:`vdim` are None, they will be set
   897|         0|            0|            0|  0.00%|    to :attr:`embed_dim` such that query, key, and value have the same
   898|         0|            0|            0|  0.00%|    number of features.
   899|         0|            0|            0|  0.00%|
   900|         0|            0|            0|  0.00%|    Examples::
   901|         0|            0|            0|  0.00%|
   902|         0|            0|            0|  0.00%|        >>> multihead_attn = nn.MultiheadAttention(embed_dim, num_heads)
   903|         0|            0|            0|  0.00%|        >>> attn_output, attn_output_weights = multihead_attn(query, key, value)
   904|         0|            0|            0|  0.00%|    """
   905|         0|            0|            0|  0.00%|    __constants__ = ['batch_first']
   906|         0|            0|            0|  0.00%|    bias_k: Optional[torch.Tensor]
   907|         0|            0|            0|  0.00%|    bias_v: Optional[torch.Tensor]
   908|         0|            0|            0|  0.00%|
   909|         0|            0|            0|  0.00%|    def __init__(self, embed_dim, num_heads, dropout=0., bias=True, add_bias_kv=False, add_zero_attn=False,
   910|         0|            0|            0|  0.00%|                 kdim=None, vdim=None, batch_first=False, device=None, dtype=None) -> None:
   911|         0|            0|            0|  0.00%|        factory_kwargs = {'device': device, 'dtype': dtype}
   912|         0|            0|            0|  0.00%|        super(MultiheadAttention, self).__init__()
   913|         0|            0|            0|  0.00%|        self.embed_dim = embed_dim
   914|         0|            0|            0|  0.00%|        self.kdim = kdim if kdim is not None else embed_dim
   915|         0|            0|            0|  0.00%|        self.vdim = vdim if vdim is not None else embed_dim
   916|         0|            0|            0|  0.00%|        self._qkv_same_embed_dim = self.kdim == embed_dim and self.vdim == embed_dim
   917|         0|            0|            0|  0.00%|
   918|         0|            0|            0|  0.00%|        self.num_heads = num_heads
   919|         0|            0|            0|  0.00%|        self.dropout = dropout
   920|         0|            0|            0|  0.00%|        self.batch_first = batch_first
   921|         0|            0|            0|  0.00%|        self.head_dim = embed_dim // num_heads
   922|         0|            0|            0|  0.00%|        assert self.head_dim * num_heads == self.embed_dim, "embed_dim must be divisible by num_heads"
   923|         0|            0|            0|  0.00%|
   924|         0|            0|            0|  0.00%|        if self._qkv_same_embed_dim is False:
   925|         0|            0|            0|  0.00%|            self.q_proj_weight = Parameter(torch.empty((embed_dim, embed_dim), **factory_kwargs))
   926|         0|            0|            0|  0.00%|            self.k_proj_weight = Parameter(torch.empty((embed_dim, self.kdim), **factory_kwargs))
   927|         0|            0|            0|  0.00%|            self.v_proj_weight = Parameter(torch.empty((embed_dim, self.vdim), **factory_kwargs))
   928|         0|            0|            0|  0.00%|            self.register_parameter('in_proj_weight', None)
   929|         0|            0|            0|  0.00%|        else:
   930|         0|            0|            0|  0.00%|            self.in_proj_weight = Parameter(torch.empty((3 * embed_dim, embed_dim), **factory_kwargs))
   931|         0|            0|            0|  0.00%|            self.register_parameter('q_proj_weight', None)
   932|         0|            0|            0|  0.00%|            self.register_parameter('k_proj_weight', None)
   933|         0|            0|            0|  0.00%|            self.register_parameter('v_proj_weight', None)
   934|         0|            0|            0|  0.00%|
   935|         0|            0|            0|  0.00%|        if bias:
   936|         0|            0|            0|  0.00%|            self.in_proj_bias = Parameter(torch.empty(3 * embed_dim, **factory_kwargs))
   937|         0|            0|            0|  0.00%|        else:
   938|         0|            0|            0|  0.00%|            self.register_parameter('in_proj_bias', None)
   939|         0|            0|            0|  0.00%|        self.out_proj = NonDynamicallyQuantizableLinear(embed_dim, embed_dim, bias=bias, **factory_kwargs)
   940|         0|            0|            0|  0.00%|
   941|         0|            0|            0|  0.00%|        if add_bias_kv:
   942|         0|            0|            0|  0.00%|            self.bias_k = Parameter(torch.empty((1, 1, embed_dim), **factory_kwargs))
   943|         0|            0|            0|  0.00%|            self.bias_v = Parameter(torch.empty((1, 1, embed_dim), **factory_kwargs))
   944|         0|            0|            0|  0.00%|        else:
   945|         0|            0|            0|  0.00%|            self.bias_k = self.bias_v = None
   946|         0|            0|            0|  0.00%|
   947|         0|            0|            0|  0.00%|        self.add_zero_attn = add_zero_attn
   948|         0|            0|            0|  0.00%|
   949|         0|            0|            0|  0.00%|        self._reset_parameters()
   950|         0|            0|            0|  0.00%|
   951|         0|            0|            0|  0.00%|    def _reset_parameters(self):
   952|         0|            0|            0|  0.00%|        if self._qkv_same_embed_dim:
   953|         0|            0|            0|  0.00%|            xavier_uniform_(self.in_proj_weight)
   954|         0|            0|            0|  0.00%|        else:
   955|         0|            0|            0|  0.00%|            xavier_uniform_(self.q_proj_weight)
   956|         0|            0|            0|  0.00%|            xavier_uniform_(self.k_proj_weight)
   957|         0|            0|            0|  0.00%|            xavier_uniform_(self.v_proj_weight)
   958|         0|            0|            0|  0.00%|
   959|         0|            0|            0|  0.00%|        if self.in_proj_bias is not None:
   960|         0|            0|            0|  0.00%|            constant_(self.in_proj_bias, 0.)
   961|         0|            0|            0|  0.00%|            constant_(self.out_proj.bias, 0.)
   962|         0|            0|            0|  0.00%|        if self.bias_k is not None:
   963|         0|            0|            0|  0.00%|            xavier_normal_(self.bias_k)
   964|         0|            0|            0|  0.00%|        if self.bias_v is not None:
   965|         0|            0|            0|  0.00%|            xavier_normal_(self.bias_v)
   966|         0|            0|            0|  0.00%|
   967|         0|            0|            0|  0.00%|    def __setstate__(self, state):
   968|         0|            0|            0|  0.00%|        # Support loading old MultiheadAttention checkpoints generated by v1.1.0
   969|         0|            0|            0|  0.00%|        if '_qkv_same_embed_dim' not in state:
   970|         0|            0|            0|  0.00%|            state['_qkv_same_embed_dim'] = True
   971|         0|            0|            0|  0.00%|
   972|         0|            0|            0|  0.00%|        super(MultiheadAttention, self).__setstate__(state)
   973|         0|            0|            0|  0.00%|
   974|         0|            0|            0|  0.00%|    def forward(self, query: Tensor, key: Tensor, value: Tensor, key_padding_mask: Optional[Tensor] = None,
   975|         0|            0|            0|  0.00%|                need_weights: bool = True, attn_mask: Optional[Tensor] = None) -> Tuple[Tensor, Optional[Tensor]]:
   976|         0|            0|            0|  0.00%|        r"""
   977|         0|            0|            0|  0.00%|    Args:
   978|         0|            0|            0|  0.00%|        query, key, value: map a query and a set of key-value pairs to an output.
   979|         0|            0|            0|  0.00%|            See "Attention Is All You Need" for more details.
   980|         0|            0|            0|  0.00%|        key_padding_mask: if provided, specified padding elements in the key will
   981|         0|            0|            0|  0.00%|            be ignored by the attention. When given a binary mask and a value is True,
   982|         0|            0|            0|  0.00%|            the corresponding value on the attention layer will be ignored. When given
   983|         0|            0|            0|  0.00%|            a byte mask and a value is non-zero, the corresponding value on the attention
   984|         0|            0|            0|  0.00%|            layer will be ignored
   985|         0|            0|            0|  0.00%|        need_weights: output attn_output_weights.
   986|         0|            0|            0|  0.00%|        attn_mask: 2D or 3D mask that prevents attention to certain positions. A 2D mask will be broadcasted for all
   987|         0|            0|            0|  0.00%|            the batches while a 3D mask allows to specify a different mask for the entries of each batch.
   988|         0|            0|            0|  0.00%|
   989|         0|            0|            0|  0.00%|    Shapes for inputs:
   990|         0|            0|            0|  0.00%|        - query: :math:`(L, N, E)` where L is the target sequence length, N is the batch size, E is
   991|         0|            0|            0|  0.00%|          the embedding dimension. :math:`(N, L, E)` if ``batch_first`` is ``True``.
   992|         0|            0|            0|  0.00%|        - key: :math:`(S, N, E)`, where S is the source sequence length, N is the batch size, E is
   993|         0|            0|            0|  0.00%|          the embedding dimension. :math:`(N, S, E)` if ``batch_first`` is ``True``.
   994|         0|            0|            0|  0.00%|        - value: :math:`(S, N, E)` where S is the source sequence length, N is the batch size, E is
   995|         0|            0|            0|  0.00%|          the embedding dimension. :math:`(N, S, E)` if ``batch_first`` is ``True``.
   996|         0|            0|            0|  0.00%|        - key_padding_mask: :math:`(N, S)` where N is the batch size, S is the source sequence length.
   997|         0|            0|            0|  0.00%|          If a ByteTensor is provided, the non-zero positions will be ignored while the position
   998|         0|            0|            0|  0.00%|          with the zero positions will be unchanged. If a BoolTensor is provided, the positions with the
   999|         0|            0|            0|  0.00%|          value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged.
  1000|         0|            0|            0|  0.00%|        - attn_mask: if a 2D mask: :math:`(L, S)` where L is the target sequence length, S is the
  1001|         0|            0|            0|  0.00%|          source sequence length.
  1002|         0|            0|            0|  0.00%|
  1003|         0|            0|            0|  0.00%|          If a 3D mask: :math:`(N\cdot\text{num\_heads}, L, S)` where N is the batch size, L is the target sequence
  1004|         0|            0|            0|  0.00%|          length, S is the source sequence length. ``attn_mask`` ensure that position i is allowed to attend
  1005|         0|            0|            0|  0.00%|          the unmasked positions. If a ByteTensor is provided, the non-zero positions are not allowed to attend
  1006|         0|            0|            0|  0.00%|          while the zero positions will be unchanged. If a BoolTensor is provided, positions with ``True``
  1007|         0|            0|            0|  0.00%|          is not allowed to attend while ``False`` values will be unchanged. If a FloatTensor
  1008|         0|            0|            0|  0.00%|          is provided, it will be added to the attention weight.
  1009|         0|            0|            0|  0.00%|
  1010|         0|            0|            0|  0.00%|    Shapes for outputs:
  1011|         0|            0|            0|  0.00%|        - attn_output: :math:`(L, N, E)` where L is the target sequence length, N is the batch size,
  1012|         0|            0|            0|  0.00%|          E is the embedding dimension. :math:`(N, L, E)` if ``batch_first`` is ``True``.
  1013|         0|            0|            0|  0.00%|        - attn_output_weights: :math:`(N, L, S)` where N is the batch size,
  1014|         0|            0|            0|  0.00%|          L is the target sequence length, S is the source sequence length.
  1015|         0|            0|            0|  0.00%|        """
  1016|         0|            0|            0|  0.00%|        if self.batch_first:
  1017|         0|            0|            0|  0.00%|            query, key, value = [x.transpose(1, 0) for x in (query, key, value)]
  1018|         0|            0|            0|  0.00%|
  1019|         0|            0|            0|  0.00%|        if not self._qkv_same_embed_dim:
  1020|         0|            0|            0|  0.00%|            attn_output, attn_output_weights = F.multi_head_attention_forward(
  1021|         0|            0|            0|  0.00%|                query, key, value, self.embed_dim, self.num_heads,
  1022|         0|            0|            0|  0.00%|                self.in_proj_weight, self.in_proj_bias,
  1023|         0|            0|            0|  0.00%|                self.bias_k, self.bias_v, self.add_zero_attn,
  1024|         0|            0|            0|  0.00%|                self.dropout, self.out_proj.weight, self.out_proj.bias,
  1025|         0|            0|            0|  0.00%|                training=self.training,
  1026|         0|            0|            0|  0.00%|                key_padding_mask=key_padding_mask, need_weights=need_weights,
  1027|         0|            0|            0|  0.00%|                attn_mask=attn_mask, use_separate_proj_weight=True,
  1028|         0|            0|            0|  0.00%|                q_proj_weight=self.q_proj_weight, k_proj_weight=self.k_proj_weight,
  1029|         0|            0|            0|  0.00%|                v_proj_weight=self.v_proj_weight)
  1030|         0|            0|            0|  0.00%|        else:
  1031|         0|            0|            0|  0.00%|            attn_output, attn_output_weights = F.multi_head_attention_forward(
  1032|         0|            0|            0|  0.00%|                query, key, value, self.embed_dim, self.num_heads,
  1033|         0|            0|            0|  0.00%|                self.in_proj_weight, self.in_proj_bias,
  1034|         0|            0|            0|  0.00%|                self.bias_k, self.bias_v, self.add_zero_attn,
  1035|         0|            0|            0|  0.00%|                self.dropout, self.out_proj.weight, self.out_proj.bias,
  1036|         0|            0|            0|  0.00%|                training=self.training,
  1037|         0|            0|            0|  0.00%|                key_padding_mask=key_padding_mask, need_weights=need_weights,
  1038|         0|            0|            0|  0.00%|                attn_mask=attn_mask)
  1039|         0|            0|            0|  0.00%|        if self.batch_first:
  1040|         0|            0|            0|  0.00%|            return attn_output.transpose(1, 0), attn_output_weights
  1041|         0|            0|            0|  0.00%|        else:
  1042|         0|            0|            0|  0.00%|            return attn_output, attn_output_weights
  1043|         0|            0|            0|  0.00%|
  1044|         0|            0|            0|  0.00%|class PReLU(Module):
  1045|         0|            0|            0|  0.00%|    r"""Applies the element-wise function:
  1046|         0|            0|            0|  0.00%|
  1047|         0|            0|            0|  0.00%|    .. math::
  1048|         0|            0|            0|  0.00%|        \text{PReLU}(x) = \max(0,x) + a * \min(0,x)
  1049|         0|            0|            0|  0.00%|
  1050|         0|            0|            0|  0.00%|    or
  1051|         0|            0|            0|  0.00%|
  1052|         0|            0|            0|  0.00%|    .. math::
  1053|         0|            0|            0|  0.00%|        \text{PReLU}(x) =
  1054|         0|            0|            0|  0.00%|        \begin{cases}
  1055|         0|            0|            0|  0.00%|        x, & \text{ if } x \geq 0 \\
  1056|         0|            0|            0|  0.00%|        ax, & \text{ otherwise }
  1057|         0|            0|            0|  0.00%|        \end{cases}
  1058|         0|            0|            0|  0.00%|
  1059|         0|            0|            0|  0.00%|    Here :math:`a` is a learnable parameter. When called without arguments, `nn.PReLU()` uses a single
  1060|         0|            0|            0|  0.00%|    parameter :math:`a` across all input channels. If called with `nn.PReLU(nChannels)`,
  1061|         0|            0|            0|  0.00%|    a separate :math:`a` is used for each input channel.
  1062|         0|            0|            0|  0.00%|
  1063|         0|            0|            0|  0.00%|
  1064|         0|            0|            0|  0.00%|    .. note::
  1065|         0|            0|            0|  0.00%|        weight decay should not be used when learning :math:`a` for good performance.
  1066|         0|            0|            0|  0.00%|
  1067|         0|            0|            0|  0.00%|    .. note::
  1068|         0|            0|            0|  0.00%|        Channel dim is the 2nd dim of input. When input has dims < 2, then there is
  1069|         0|            0|            0|  0.00%|        no channel dim and the number of channels = 1.
  1070|         0|            0|            0|  0.00%|
  1071|         0|            0|            0|  0.00%|    Args:
  1072|         0|            0|            0|  0.00%|        num_parameters (int): number of :math:`a` to learn.
  1073|         0|            0|            0|  0.00%|            Although it takes an int as input, there is only two values are legitimate:
  1074|         0|            0|            0|  0.00%|            1, or the number of channels at input. Default: 1
  1075|         0|            0|            0|  0.00%|        init (float): the initial value of :math:`a`. Default: 0.25
  1076|         0|            0|            0|  0.00%|
  1077|         0|            0|            0|  0.00%|    Shape:
  1078|         0|            0|            0|  0.00%|        - Input: :math:`(N, *)` where `*` means, any number of additional
  1079|         0|            0|            0|  0.00%|          dimensions
  1080|         0|            0|            0|  0.00%|        - Output: :math:`(N, *)`, same shape as the input
  1081|         0|            0|            0|  0.00%|
  1082|         0|            0|            0|  0.00%|    Attributes:
  1083|         0|            0|            0|  0.00%|        weight (Tensor): the learnable weights of shape (:attr:`num_parameters`).
  1084|         0|            0|            0|  0.00%|
  1085|         0|            0|            0|  0.00%|    .. image:: ../scripts/activation_images/PReLU.png
  1086|         0|            0|            0|  0.00%|
  1087|         0|            0|            0|  0.00%|    Examples::
  1088|         0|            0|            0|  0.00%|
  1089|         0|            0|            0|  0.00%|        >>> m = nn.PReLU()
  1090|         0|            0|            0|  0.00%|        >>> input = torch.randn(2)
  1091|         0|            0|            0|  0.00%|        >>> output = m(input)
  1092|         0|            0|            0|  0.00%|    """
  1093|         0|            0|            0|  0.00%|    __constants__ = ['num_parameters']
  1094|         0|            0|            0|  0.00%|    num_parameters: int
  1095|         0|            0|            0|  0.00%|
  1096|         0|            0|            0|  0.00%|    def __init__(self, num_parameters: int = 1, init: float = 0.25,
  1097|         0|            0|            0|  0.00%|                 device=None, dtype=None) -> None:
  1098|         0|            0|            0|  0.00%|        factory_kwargs = {'device': device, 'dtype': dtype}
  1099|         0|            0|            0|  0.00%|        self.num_parameters = num_parameters
  1100|         0|            0|            0|  0.00%|        super(PReLU, self).__init__()
  1101|         0|            0|            0|  0.00%|        self.weight = Parameter(torch.empty(num_parameters, **factory_kwargs).fill_(init))
  1102|         0|            0|            0|  0.00%|
  1103|         0|            0|            0|  0.00%|    def forward(self, input: Tensor) -> Tensor:
  1104|         0|            0|            0|  0.00%|        return F.prelu(input, self.weight)
  1105|         0|            0|            0|  0.00%|
  1106|         0|            0|            0|  0.00%|    def extra_repr(self) -> str:
  1107|         0|            0|            0|  0.00%|        return 'num_parameters={}'.format(self.num_parameters)
  1108|         0|            0|            0|  0.00%|
  1109|         0|            0|            0|  0.00%|
  1110|         0|            0|            0|  0.00%|class Softsign(Module):
  1111|         0|            0|            0|  0.00%|    r"""Applies the element-wise function:
  1112|         0|            0|            0|  0.00%|
  1113|         0|            0|            0|  0.00%|    .. math::
  1114|         0|            0|            0|  0.00%|        \text{SoftSign}(x) = \frac{x}{ 1 + |x|}
  1115|         0|            0|            0|  0.00%|
  1116|         0|            0|            0|  0.00%|    Shape:
  1117|         0|            0|            0|  0.00%|        - Input: :math:`(N, *)` where `*` means, any number of additional
  1118|         0|            0|            0|  0.00%|          dimensions
  1119|         0|            0|            0|  0.00%|        - Output: :math:`(N, *)`, same shape as the input
  1120|         0|            0|            0|  0.00%|
  1121|         0|            0|            0|  0.00%|    .. image:: ../scripts/activation_images/Softsign.png
  1122|         0|            0|            0|  0.00%|
  1123|         0|            0|            0|  0.00%|    Examples::
  1124|         0|            0|            0|  0.00%|
  1125|         0|            0|            0|  0.00%|        >>> m = nn.Softsign()
  1126|         0|            0|            0|  0.00%|        >>> input = torch.randn(2)
  1127|         0|            0|            0|  0.00%|        >>> output = m(input)
  1128|         0|            0|            0|  0.00%|    """
  1129|         0|            0|            0|  0.00%|
  1130|         0|            0|            0|  0.00%|    def forward(self, input: Tensor) -> Tensor:
  1131|         0|            0|            0|  0.00%|        return F.softsign(input)
  1132|         0|            0|            0|  0.00%|
  1133|         0|            0|            0|  0.00%|
  1134|         0|            0|            0|  0.00%|class Tanhshrink(Module):
  1135|         0|            0|            0|  0.00%|    r"""Applies the element-wise function:
  1136|         0|            0|            0|  0.00%|
  1137|         0|            0|            0|  0.00%|    .. math::
  1138|         0|            0|            0|  0.00%|        \text{Tanhshrink}(x) = x - \tanh(x)
  1139|         0|            0|            0|  0.00%|
  1140|         0|            0|            0|  0.00%|    Shape:
  1141|         0|            0|            0|  0.00%|        - Input: :math:`(N, *)` where `*` means, any number of additional
  1142|         0|            0|            0|  0.00%|          dimensions
  1143|         0|            0|            0|  0.00%|        - Output: :math:`(N, *)`, same shape as the input
  1144|         0|            0|            0|  0.00%|
  1145|         0|            0|            0|  0.00%|    .. image:: ../scripts/activation_images/Tanhshrink.png
  1146|         0|            0|            0|  0.00%|
  1147|         0|            0|            0|  0.00%|    Examples::
  1148|         0|            0|            0|  0.00%|
  1149|         0|            0|            0|  0.00%|        >>> m = nn.Tanhshrink()
  1150|         0|            0|            0|  0.00%|        >>> input = torch.randn(2)
  1151|         0|            0|            0|  0.00%|        >>> output = m(input)
  1152|         0|            0|            0|  0.00%|    """
  1153|         0|            0|            0|  0.00%|
  1154|         0|            0|            0|  0.00%|    def forward(self, input: Tensor) -> Tensor:
  1155|         0|            0|            0|  0.00%|        return F.tanhshrink(input)
  1156|         0|            0|            0|  0.00%|
  1157|         0|            0|            0|  0.00%|
  1158|         0|            0|            0|  0.00%|class Softmin(Module):
  1159|         0|            0|            0|  0.00%|    r"""Applies the Softmin function to an n-dimensional input Tensor
  1160|         0|            0|            0|  0.00%|    rescaling them so that the elements of the n-dimensional output Tensor
  1161|         0|            0|            0|  0.00%|    lie in the range `[0, 1]` and sum to 1.
  1162|         0|            0|            0|  0.00%|
  1163|         0|            0|            0|  0.00%|    Softmin is defined as:
  1164|         0|            0|            0|  0.00%|
  1165|         0|            0|            0|  0.00%|    .. math::
  1166|         0|            0|            0|  0.00%|        \text{Softmin}(x_{i}) = \frac{\exp(-x_i)}{\sum_j \exp(-x_j)}
  1167|         0|            0|            0|  0.00%|
  1168|         0|            0|            0|  0.00%|    Shape:
  1169|         0|            0|            0|  0.00%|        - Input: :math:`(*)` where `*` means, any number of additional
  1170|         0|            0|            0|  0.00%|          dimensions
  1171|         0|            0|            0|  0.00%|        - Output: :math:`(*)`, same shape as the input
  1172|         0|            0|            0|  0.00%|
  1173|         0|            0|            0|  0.00%|    Args:
  1174|         0|            0|            0|  0.00%|        dim (int): A dimension along which Softmin will be computed (so every slice
  1175|         0|            0|            0|  0.00%|            along dim will sum to 1).
  1176|         0|            0|            0|  0.00%|
  1177|         0|            0|            0|  0.00%|    Returns:
  1178|         0|            0|            0|  0.00%|        a Tensor of the same dimension and shape as the input, with
  1179|         0|            0|            0|  0.00%|        values in the range [0, 1]
  1180|         0|            0|            0|  0.00%|
  1181|         0|            0|            0|  0.00%|    Examples::
  1182|         0|            0|            0|  0.00%|
  1183|         0|            0|            0|  0.00%|        >>> m = nn.Softmin()
  1184|         0|            0|            0|  0.00%|        >>> input = torch.randn(2, 3)
  1185|         0|            0|            0|  0.00%|        >>> output = m(input)
  1186|         0|            0|            0|  0.00%|    """
  1187|         0|            0|            0|  0.00%|    __constants__ = ['dim']
  1188|         0|            0|            0|  0.00%|    dim: Optional[int]
  1189|         0|            0|            0|  0.00%|
  1190|         0|            0|            0|  0.00%|    def __init__(self, dim: Optional[int] = None) -> None:
  1191|         0|            0|            0|  0.00%|        super(Softmin, self).__init__()
  1192|         0|            0|            0|  0.00%|        self.dim = dim
  1193|         0|            0|            0|  0.00%|
  1194|         0|            0|            0|  0.00%|    def __setstate__(self, state):
  1195|         0|            0|            0|  0.00%|        self.__dict__.update(state)
  1196|         0|            0|            0|  0.00%|        if not hasattr(self, 'dim'):
  1197|         0|            0|            0|  0.00%|            self.dim = None
  1198|         0|            0|            0|  0.00%|
  1199|         0|            0|            0|  0.00%|    def forward(self, input: Tensor) -> Tensor:
  1200|         0|            0|            0|  0.00%|        return F.softmin(input, self.dim, _stacklevel=5)
  1201|         0|            0|            0|  0.00%|
  1202|         0|            0|            0|  0.00%|    def extra_repr(self):
  1203|         0|            0|            0|  0.00%|        return 'dim={dim}'.format(dim=self.dim)
  1204|         0|            0|            0|  0.00%|
  1205|         0|            0|            0|  0.00%|class Softmax(Module):
  1206|         0|            0|            0|  0.00%|    r"""Applies the Softmax function to an n-dimensional input Tensor
  1207|         0|            0|            0|  0.00%|    rescaling them so that the elements of the n-dimensional output Tensor
  1208|         0|            0|            0|  0.00%|    lie in the range [0,1] and sum to 1.
  1209|         0|            0|            0|  0.00%|
  1210|         0|            0|            0|  0.00%|    Softmax is defined as:
  1211|         0|            0|            0|  0.00%|
  1212|         0|            0|            0|  0.00%|    .. math::
  1213|         0|            0|            0|  0.00%|        \text{Softmax}(x_{i}) = \frac{\exp(x_i)}{\sum_j \exp(x_j)}
  1214|         0|            0|            0|  0.00%|
  1215|         0|            0|            0|  0.00%|    When the input Tensor is a sparse tensor then the unspecifed
  1216|         0|            0|            0|  0.00%|    values are treated as ``-inf``.
  1217|         0|            0|            0|  0.00%|
  1218|         0|            0|            0|  0.00%|    Shape:
  1219|         0|            0|            0|  0.00%|        - Input: :math:`(*)` where `*` means, any number of additional
  1220|         0|            0|            0|  0.00%|          dimensions
  1221|         0|            0|            0|  0.00%|        - Output: :math:`(*)`, same shape as the input
  1222|         0|            0|            0|  0.00%|
  1223|         0|            0|            0|  0.00%|    Returns:
  1224|         0|            0|            0|  0.00%|        a Tensor of the same dimension and shape as the input with
  1225|         0|            0|            0|  0.00%|        values in the range [0, 1]
  1226|         0|            0|            0|  0.00%|
  1227|         0|            0|            0|  0.00%|    Args:
  1228|         0|            0|            0|  0.00%|        dim (int): A dimension along which Softmax will be computed (so every slice
  1229|         0|            0|            0|  0.00%|            along dim will sum to 1).
  1230|         0|            0|            0|  0.00%|
  1231|         0|            0|            0|  0.00%|    .. note::
  1232|         0|            0|            0|  0.00%|        This module doesn't work directly with NLLLoss,
  1233|         0|            0|            0|  0.00%|        which expects the Log to be computed between the Softmax and itself.
  1234|         0|            0|            0|  0.00%|        Use `LogSoftmax` instead (it's faster and has better numerical properties).
  1235|         0|            0|            0|  0.00%|
  1236|         0|            0|            0|  0.00%|    Examples::
  1237|         0|            0|            0|  0.00%|
  1238|         0|            0|            0|  0.00%|        >>> m = nn.Softmax(dim=1)
  1239|         0|            0|            0|  0.00%|        >>> input = torch.randn(2, 3)
  1240|         0|            0|            0|  0.00%|        >>> output = m(input)
  1241|         0|            0|            0|  0.00%|
  1242|         0|            0|            0|  0.00%|    """
  1243|         0|            0|            0|  0.00%|    __constants__ = ['dim']
  1244|         0|            0|            0|  0.00%|    dim: Optional[int]
  1245|         0|            0|            0|  0.00%|
  1246|         0|            0|            0|  0.00%|    def __init__(self, dim: Optional[int] = None) -> None:
  1247|         0|            0|            0|  0.00%|        super(Softmax, self).__init__()
  1248|         0|            0|            0|  0.00%|        self.dim = dim
  1249|         0|            0|            0|  0.00%|
  1250|         0|            0|            0|  0.00%|    def __setstate__(self, state):
  1251|         0|            0|            0|  0.00%|        self.__dict__.update(state)
  1252|         0|            0|            0|  0.00%|        if not hasattr(self, 'dim'):
  1253|         0|            0|            0|  0.00%|            self.dim = None
  1254|         0|            0|            0|  0.00%|
  1255|         0|            0|            0|  0.00%|    def forward(self, input: Tensor) -> Tensor:
  1256|         0|            0|            0|  0.00%|        return F.softmax(input, self.dim, _stacklevel=5)
  1257|         0|            0|            0|  0.00%|
  1258|         0|            0|            0|  0.00%|    def extra_repr(self) -> str:
  1259|         0|            0|            0|  0.00%|        return 'dim={dim}'.format(dim=self.dim)
  1260|         0|            0|            0|  0.00%|
  1261|         0|            0|            0|  0.00%|
  1262|         0|            0|            0|  0.00%|class Softmax2d(Module):
  1263|         0|            0|            0|  0.00%|    r"""Applies SoftMax over features to each spatial location.
  1264|         0|            0|            0|  0.00%|
  1265|         0|            0|            0|  0.00%|    When given an image of ``Channels x Height x Width``, it will
  1266|         0|            0|            0|  0.00%|    apply `Softmax` to each location :math:`(Channels, h_i, w_j)`
  1267|         0|            0|            0|  0.00%|
  1268|         0|            0|            0|  0.00%|    Shape:
  1269|         0|            0|            0|  0.00%|        - Input: :math:`(N, C, H, W)`
  1270|         0|            0|            0|  0.00%|        - Output: :math:`(N, C, H, W)` (same shape as input)
  1271|         0|            0|            0|  0.00%|
  1272|         0|            0|            0|  0.00%|    Returns:
  1273|         0|            0|            0|  0.00%|        a Tensor of the same dimension and shape as the input with
  1274|         0|            0|            0|  0.00%|        values in the range [0, 1]
  1275|         0|            0|            0|  0.00%|
  1276|         0|            0|            0|  0.00%|    Examples::
  1277|         0|            0|            0|  0.00%|
  1278|         0|            0|            0|  0.00%|        >>> m = nn.Softmax2d()
  1279|         0|            0|            0|  0.00%|        >>> # you softmax over the 2nd dimension
  1280|         0|            0|            0|  0.00%|        >>> input = torch.randn(2, 3, 12, 13)
  1281|         0|            0|            0|  0.00%|        >>> output = m(input)
  1282|         0|            0|            0|  0.00%|    """
  1283|         0|            0|            0|  0.00%|
  1284|         0|            0|            0|  0.00%|    def forward(self, input: Tensor) -> Tensor:
  1285|         0|            0|            0|  0.00%|        assert input.dim() == 4, 'Softmax2d requires a 4D tensor as input'
  1286|         0|            0|            0|  0.00%|        return F.softmax(input, 1, _stacklevel=5)
  1287|         0|            0|            0|  0.00%|
  1288|         0|            0|            0|  0.00%|
  1289|         0|            0|            0|  0.00%|class LogSoftmax(Module):
  1290|         0|            0|            0|  0.00%|    r"""Applies the :math:`\log(\text{Softmax}(x))` function to an n-dimensional
  1291|         0|            0|            0|  0.00%|    input Tensor. The LogSoftmax formulation can be simplified as:
  1292|         0|            0|            0|  0.00%|
  1293|         0|            0|            0|  0.00%|    .. math::
  1294|         0|            0|            0|  0.00%|        \text{LogSoftmax}(x_{i}) = \log\left(\frac{\exp(x_i) }{ \sum_j \exp(x_j)} \right)
  1295|         0|            0|            0|  0.00%|
  1296|         0|            0|            0|  0.00%|    Shape:
  1297|         0|            0|            0|  0.00%|        - Input: :math:`(*)` where `*` means, any number of additional
  1298|         0|            0|            0|  0.00%|          dimensions
  1299|         0|            0|            0|  0.00%|        - Output: :math:`(*)`, same shape as the input
  1300|         0|            0|            0|  0.00%|
  1301|         0|            0|            0|  0.00%|    Args:
  1302|         0|            0|            0|  0.00%|        dim (int): A dimension along which LogSoftmax will be computed.
  1303|         0|            0|            0|  0.00%|
  1304|         0|            0|            0|  0.00%|    Returns:
  1305|         0|            0|            0|  0.00%|        a Tensor of the same dimension and shape as the input with
  1306|         0|            0|            0|  0.00%|        values in the range [-inf, 0)
  1307|         0|            0|            0|  0.00%|
  1308|         0|            0|            0|  0.00%|    Examples::
  1309|         0|            0|            0|  0.00%|
  1310|         0|            0|            0|  0.00%|        >>> m = nn.LogSoftmax()
  1311|         0|            0|            0|  0.00%|        >>> input = torch.randn(2, 3)
  1312|         0|            0|            0|  0.00%|        >>> output = m(input)
  1313|         0|            0|            0|  0.00%|    """
  1314|         0|            0|            0|  0.00%|    __constants__ = ['dim']
  1315|         0|            0|            0|  0.00%|    dim: Optional[int]
  1316|         0|            0|            0|  0.00%|
  1317|         0|            0|            0|  0.00%|    def __init__(self, dim: Optional[int] = None) -> None:
  1318|         0|            0|            0|  0.00%|        super(LogSoftmax, self).__init__()
  1319|         0|            0|            0|  0.00%|        self.dim = dim
  1320|         0|            0|            0|  0.00%|
  1321|         0|            0|            0|  0.00%|    def __setstate__(self, state):
  1322|         0|            0|            0|  0.00%|        self.__dict__.update(state)
  1323|         0|            0|            0|  0.00%|        if not hasattr(self, 'dim'):
  1324|         0|            0|            0|  0.00%|            self.dim = None
  1325|         0|            0|            0|  0.00%|
  1326|         0|            0|            0|  0.00%|    def forward(self, input: Tensor) -> Tensor:
  1327|         0|            0|            0|  0.00%|        return F.log_softmax(input, self.dim, _stacklevel=5)
  1328|         0|            0|            0|  0.00%|
  1329|         0|            0|            0|  0.00%|    def extra_repr(self):
  1330|         0|            0|            0|  0.00%|        return 'dim={dim}'.format(dim=self.dim)
File: /opt/conda/lib/python3.8/multiprocessing/resource_sharer.py
File duration: 0.028569s (0.05%)
Line #|      Hits|         Time| Time per hit|      %|Source code
------+----------+-------------+-------------+-------+-----------
     1|         0|            0|            0|  0.00%|#
     2|         0|            0|            0|  0.00%|# We use a background thread for sharing fds on Unix, and for sharing sockets on
     3|         0|            0|            0|  0.00%|# Windows.
     4|         0|            0|            0|  0.00%|#
     5|         0|            0|            0|  0.00%|# A client which wants to pickle a resource registers it with the resource
     6|         0|            0|            0|  0.00%|# sharer and gets an identifier in return.  The unpickling process will connect
     7|         0|            0|            0|  0.00%|# to the resource sharer, sends the identifier and its pid, and then receives
     8|         0|            0|            0|  0.00%|# the resource.
     9|         0|            0|            0|  0.00%|#
    10|         0|            0|            0|  0.00%|
    11|         0|            0|            0|  0.00%|import os
    12|         0|            0|            0|  0.00%|import signal
    13|         0|            0|            0|  0.00%|import socket
    14|         0|            0|            0|  0.00%|import sys
    15|         0|            0|            0|  0.00%|import threading
    16|         0|            0|            0|  0.00%|
    17|         0|            0|            0|  0.00%|from . import process
    18|         0|            0|            0|  0.00%|from .context import reduction
    19|         0|            0|            0|  0.00%|from . import util
    20|         0|            0|            0|  0.00%|
    21|         0|            0|            0|  0.00%|__all__ = ['stop']
    22|         0|            0|            0|  0.00%|
    23|         0|            0|            0|  0.00%|
    24|         0|            0|            0|  0.00%|if sys.platform == 'win32':
    25|         0|            0|            0|  0.00%|    __all__ += ['DupSocket']
    26|         0|            0|            0|  0.00%|
    27|         0|            0|            0|  0.00%|    class DupSocket(object):
    28|         0|            0|            0|  0.00%|        '''Picklable wrapper for a socket.'''
    29|         0|            0|            0|  0.00%|        def __init__(self, sock):
    30|         0|            0|            0|  0.00%|            new_sock = sock.dup()
    31|         0|            0|            0|  0.00%|            def send(conn, pid):
    32|         0|            0|            0|  0.00%|                share = new_sock.share(pid)
    33|         0|            0|            0|  0.00%|                conn.send_bytes(share)
    34|         0|            0|            0|  0.00%|            self._id = _resource_sharer.register(send, new_sock.close)
    35|         0|            0|            0|  0.00%|
    36|         0|            0|            0|  0.00%|        def detach(self):
    37|         0|            0|            0|  0.00%|            '''Get the socket.  This should only be called once.'''
    38|         0|            0|            0|  0.00%|            with _resource_sharer.get_connection(self._id) as conn:
    39|         0|            0|            0|  0.00%|                share = conn.recv_bytes()
    40|         0|            0|            0|  0.00%|                return socket.fromshare(share)
    41|         0|            0|            0|  0.00%|
    42|         0|            0|            0|  0.00%|else:
    43|         0|            0|            0|  0.00%|    __all__ += ['DupFd']
    44|         0|            0|            0|  0.00%|
    45|         0|            0|            0|  0.00%|    class DupFd(object):
    46|         0|            0|            0|  0.00%|        '''Wrapper for fd which can be used at any time.'''
    47|         0|            0|            0|  0.00%|        def __init__(self, fd):
    48|         0|            0|            0|  0.00%|            new_fd = os.dup(fd)
    49|         0|            0|            0|  0.00%|            def send(conn, pid):
    50|         0|            0|            0|  0.00%|                reduction.send_handle(conn, new_fd, pid)
    51|         0|            0|            0|  0.00%|            def close():
    52|         0|            0|            0|  0.00%|                os.close(new_fd)
    53|         0|            0|            0|  0.00%|            self._id = _resource_sharer.register(send, close)
    54|         0|            0|            0|  0.00%|
    55|       200|   0.00103545|  5.17726e-06|  0.00%|        def detach(self):
    56|         0|            0|            0|  0.00%|            '''Get the fd.  This should only be called once.'''
    57|       200|   0.00476122|  2.38061e-05|  0.01%|            with _resource_sharer.get_connection(self._id) as conn:
(call)|       200|     0.598039|    0.0029902|  1.12%|# /opt/conda/lib/python3.8/multiprocessing/resource_sharer.py:82 get_connection
(call)|       200|   0.00176644|  8.83222e-06|  0.00%|# /opt/conda/lib/python3.8/multiprocessing/connection.py:259 __enter__
    58|       200|   0.00484061|  2.42031e-05|  0.01%|                return reduction.recv_handle(conn)
(call)|       200|    0.0662117|  0.000331059|  0.12%|# /opt/conda/lib/python3.8/multiprocessing/reduction.py:186 recv_handle
(call)|       200|    0.0144026|  7.20131e-05|  0.03%|# /opt/conda/lib/python3.8/multiprocessing/connection.py:262 __exit__
    59|         0|            0|            0|  0.00%|
    60|         0|            0|            0|  0.00%|
    61|         0|            0|            0|  0.00%|class _ResourceSharer(object):
    62|         0|            0|            0|  0.00%|    '''Manager for resources using background thread.'''
    63|         0|            0|            0|  0.00%|    def __init__(self):
    64|         0|            0|            0|  0.00%|        self._key = 0
    65|         0|            0|            0|  0.00%|        self._cache = {}
    66|         0|            0|            0|  0.00%|        self._old_locks = []
    67|         0|            0|            0|  0.00%|        self._lock = threading.Lock()
    68|         0|            0|            0|  0.00%|        self._listener = None
    69|         0|            0|            0|  0.00%|        self._address = None
    70|         0|            0|            0|  0.00%|        self._thread = None
    71|         0|            0|            0|  0.00%|        util.register_after_fork(self, _ResourceSharer._afterfork)
    72|         0|            0|            0|  0.00%|
    73|         0|            0|            0|  0.00%|    def register(self, send, close):
    74|         0|            0|            0|  0.00%|        '''Register resource, returning an identifier.'''
    75|         0|            0|            0|  0.00%|        with self._lock:
    76|         0|            0|            0|  0.00%|            if self._address is None:
    77|         0|            0|            0|  0.00%|                self._start()
    78|         0|            0|            0|  0.00%|            self._key += 1
    79|         0|            0|            0|  0.00%|            self._cache[self._key] = (send, close)
    80|         0|            0|            0|  0.00%|            return (self._address, self._key)
    81|         0|            0|            0|  0.00%|
    82|       200|   0.00108528|  5.42641e-06|  0.00%|    @staticmethod
    83|         0|            0|            0|  0.00%|    def get_connection(ident):
    84|         0|            0|            0|  0.00%|        '''Return connection from which to receive identified resource.'''
    85|       200|   0.00454593|  2.27296e-05|  0.01%|        from .connection import Client
(call)|       200|   0.00356889|  1.78444e-05|  0.01%|# <frozen importlib._bootstrap>:389 parent
    86|       200|   0.00104022|   5.2011e-06|  0.00%|        address, key = ident
    87|       200|   0.00669289|  3.34644e-05|  0.01%|        c = Client(address, authkey=process.current_process().authkey)
(call)|       200|   0.00193691|  9.68456e-06|  0.00%|# /opt/conda/lib/python3.8/multiprocessing/process.py:37 current_process
(call)|       200|   0.00188136|  9.40681e-06|  0.00%|# /opt/conda/lib/python3.8/multiprocessing/process.py:213 authkey
(call)|       200|      0.52464|    0.0026232|  0.98%|# /opt/conda/lib/python3.8/multiprocessing/connection.py:493 Client
    88|       200|   0.00370336|  1.85168e-05|  0.01%|        c.send((key, os.getpid()))
(call)|       200|    0.0480802|  0.000240401|  0.09%|# /opt/conda/lib/python3.8/multiprocessing/connection.py:202 send
    89|       200|  0.000864029|  4.32014e-06|  0.00%|        return c
    90|         0|            0|            0|  0.00%|
    91|         0|            0|            0|  0.00%|    def stop(self, timeout=None):
    92|         0|            0|            0|  0.00%|        '''Stop the background thread and clear registered resources.'''
    93|         0|            0|            0|  0.00%|        from .connection import Client
    94|         0|            0|            0|  0.00%|        with self._lock:
    95|         0|            0|            0|  0.00%|            if self._address is not None:
    96|         0|            0|            0|  0.00%|                c = Client(self._address,
    97|         0|            0|            0|  0.00%|                           authkey=process.current_process().authkey)
    98|         0|            0|            0|  0.00%|                c.send(None)
    99|         0|            0|            0|  0.00%|                c.close()
   100|         0|            0|            0|  0.00%|                self._thread.join(timeout)
   101|         0|            0|            0|  0.00%|                if self._thread.is_alive():
   102|         0|            0|            0|  0.00%|                    util.sub_warning('_ResourceSharer thread did '
   103|         0|            0|            0|  0.00%|                                     'not stop when asked')
   104|         0|            0|            0|  0.00%|                self._listener.close()
   105|         0|            0|            0|  0.00%|                self._thread = None
   106|         0|            0|            0|  0.00%|                self._address = None
   107|         0|            0|            0|  0.00%|                self._listener = None
   108|         0|            0|            0|  0.00%|                for key, (send, close) in self._cache.items():
   109|         0|            0|            0|  0.00%|                    close()
   110|         0|            0|            0|  0.00%|                self._cache.clear()
   111|         0|            0|            0|  0.00%|
   112|         0|            0|            0|  0.00%|    def _afterfork(self):
   113|         0|            0|            0|  0.00%|        for key, (send, close) in self._cache.items():
   114|         0|            0|            0|  0.00%|            close()
   115|         0|            0|            0|  0.00%|        self._cache.clear()
   116|         0|            0|            0|  0.00%|        # If self._lock was locked at the time of the fork, it may be broken
   117|         0|            0|            0|  0.00%|        # -- see issue 6721.  Replace it without letting it be gc'ed.
   118|         0|            0|            0|  0.00%|        self._old_locks.append(self._lock)
   119|         0|            0|            0|  0.00%|        self._lock = threading.Lock()
   120|         0|            0|            0|  0.00%|        if self._listener is not None:
   121|         0|            0|            0|  0.00%|            self._listener.close()
   122|         0|            0|            0|  0.00%|        self._listener = None
   123|         0|            0|            0|  0.00%|        self._address = None
   124|         0|            0|            0|  0.00%|        self._thread = None
   125|         0|            0|            0|  0.00%|
   126|         0|            0|            0|  0.00%|    def _start(self):
   127|         0|            0|            0|  0.00%|        from .connection import Listener
   128|         0|            0|            0|  0.00%|        assert self._listener is None, "Already have Listener"
   129|         0|            0|            0|  0.00%|        util.debug('starting listener and thread for sending handles')
   130|         0|            0|            0|  0.00%|        self._listener = Listener(authkey=process.current_process().authkey)
   131|         0|            0|            0|  0.00%|        self._address = self._listener.address
   132|         0|            0|            0|  0.00%|        t = threading.Thread(target=self._serve)
   133|         0|            0|            0|  0.00%|        t.daemon = True
   134|         0|            0|            0|  0.00%|        t.start()
   135|         0|            0|            0|  0.00%|        self._thread = t
   136|         0|            0|            0|  0.00%|
   137|         0|            0|            0|  0.00%|    def _serve(self):
   138|         0|            0|            0|  0.00%|        if hasattr(signal, 'pthread_sigmask'):
   139|         0|            0|            0|  0.00%|            signal.pthread_sigmask(signal.SIG_BLOCK, signal.valid_signals())
   140|         0|            0|            0|  0.00%|        while 1:
   141|         0|            0|            0|  0.00%|            try:
   142|         0|            0|            0|  0.00%|                with self._listener.accept() as conn:
   143|         0|            0|            0|  0.00%|                    msg = conn.recv()
   144|         0|            0|            0|  0.00%|                    if msg is None:
   145|         0|            0|            0|  0.00%|                        break
   146|         0|            0|            0|  0.00%|                    key, destination_pid = msg
   147|         0|            0|            0|  0.00%|                    send, close = self._cache.pop(key)
   148|         0|            0|            0|  0.00%|                    try:
   149|         0|            0|            0|  0.00%|                        send(conn, destination_pid)
   150|         0|            0|            0|  0.00%|                    finally:
   151|         0|            0|            0|  0.00%|                        close()
   152|         0|            0|            0|  0.00%|            except:
   153|         0|            0|            0|  0.00%|                if not util.is_exiting():
   154|         0|            0|            0|  0.00%|                    sys.excepthook(*sys.exc_info())
   155|         0|            0|            0|  0.00%|
   156|         0|            0|            0|  0.00%|
   157|         0|            0|            0|  0.00%|_resource_sharer = _ResourceSharer()
   158|         0|            0|            0|  0.00%|stop = _resource_sharer.stop
File: /opt/conda/lib/python3.8/threading.py
File duration: 0.0263228s (0.05%)
Line #|      Hits|         Time| Time per hit|      %|Source code
------+----------+-------------+-------------+-------+-----------
     1|         0|            0|            0|  0.00%|"""Thread module emulating a subset of Java's threading model."""
     2|         0|            0|            0|  0.00%|
     3|         0|            0|            0|  0.00%|import os as _os
     4|         0|            0|            0|  0.00%|import sys as _sys
     5|         0|            0|            0|  0.00%|import _thread
     6|         0|            0|            0|  0.00%|
     7|         0|            0|            0|  0.00%|from time import monotonic as _time
     8|         0|            0|            0|  0.00%|from _weakrefset import WeakSet
     9|         0|            0|            0|  0.00%|from itertools import islice as _islice, count as _count
    10|         0|            0|            0|  0.00%|try:
    11|         0|            0|            0|  0.00%|    from _collections import deque as _deque
    12|         0|            0|            0|  0.00%|except ImportError:
    13|         0|            0|            0|  0.00%|    from collections import deque as _deque
    14|         0|            0|            0|  0.00%|
    15|         0|            0|            0|  0.00%|# Note regarding PEP 8 compliant names
    16|         0|            0|            0|  0.00%|#  This threading model was originally inspired by Java, and inherited
    17|         0|            0|            0|  0.00%|# the convention of camelCase function and method names from that
    18|         0|            0|            0|  0.00%|# language. Those original names are not in any imminent danger of
    19|         0|            0|            0|  0.00%|# being deprecated (even for Py3k),so this module provides them as an
    20|         0|            0|            0|  0.00%|# alias for the PEP 8 compliant names
    21|         0|            0|            0|  0.00%|# Note that using the new PEP 8 compliant names facilitates substitution
    22|         0|            0|            0|  0.00%|# with the multiprocessing module, which doesn't provide the old
    23|         0|            0|            0|  0.00%|# Java inspired names.
    24|         0|            0|            0|  0.00%|
    25|         0|            0|            0|  0.00%|__all__ = ['get_ident', 'active_count', 'Condition', 'current_thread',
    26|         0|            0|            0|  0.00%|           'enumerate', 'main_thread', 'TIMEOUT_MAX',
    27|         0|            0|            0|  0.00%|           'Event', 'Lock', 'RLock', 'Semaphore', 'BoundedSemaphore', 'Thread',
    28|         0|            0|            0|  0.00%|           'Barrier', 'BrokenBarrierError', 'Timer', 'ThreadError',
    29|         0|            0|            0|  0.00%|           'setprofile', 'settrace', 'local', 'stack_size',
    30|         0|            0|            0|  0.00%|           'excepthook', 'ExceptHookArgs']
    31|         0|            0|            0|  0.00%|
    32|         0|            0|            0|  0.00%|# Rename some stuff so "from threading import *" is safe
    33|         0|            0|            0|  0.00%|_start_new_thread = _thread.start_new_thread
    34|         0|            0|            0|  0.00%|_allocate_lock = _thread.allocate_lock
    35|         0|            0|            0|  0.00%|_set_sentinel = _thread._set_sentinel
    36|         0|            0|            0|  0.00%|get_ident = _thread.get_ident
    37|         0|            0|            0|  0.00%|try:
    38|         0|            0|            0|  0.00%|    get_native_id = _thread.get_native_id
    39|         0|            0|            0|  0.00%|    _HAVE_THREAD_NATIVE_ID = True
    40|         0|            0|            0|  0.00%|    __all__.append('get_native_id')
    41|         0|            0|            0|  0.00%|except AttributeError:
    42|         0|            0|            0|  0.00%|    _HAVE_THREAD_NATIVE_ID = False
    43|         0|            0|            0|  0.00%|ThreadError = _thread.error
    44|         0|            0|            0|  0.00%|try:
    45|         0|            0|            0|  0.00%|    _CRLock = _thread.RLock
    46|         0|            0|            0|  0.00%|except AttributeError:
    47|         0|            0|            0|  0.00%|    _CRLock = None
    48|         0|            0|            0|  0.00%|TIMEOUT_MAX = _thread.TIMEOUT_MAX
    49|         0|            0|            0|  0.00%|del _thread
    50|         0|            0|            0|  0.00%|
    51|         0|            0|            0|  0.00%|
    52|         0|            0|            0|  0.00%|# Support for profile and trace hooks
    53|         0|            0|            0|  0.00%|
    54|         0|            0|            0|  0.00%|_profile_hook = None
    55|         0|            0|            0|  0.00%|_trace_hook = None
    56|         0|            0|            0|  0.00%|
    57|         0|            0|            0|  0.00%|def setprofile(func):
    58|         0|            0|            0|  0.00%|    """Set a profile function for all threads started from the threading module.
    59|         0|            0|            0|  0.00%|
    60|         0|            0|            0|  0.00%|    The func will be passed to sys.setprofile() for each thread, before its
    61|         0|            0|            0|  0.00%|    run() method is called.
    62|         0|            0|            0|  0.00%|
    63|         0|            0|            0|  0.00%|    """
    64|         0|            0|            0|  0.00%|    global _profile_hook
    65|         0|            0|            0|  0.00%|    _profile_hook = func
    66|         0|            0|            0|  0.00%|
    67|         0|            0|            0|  0.00%|def settrace(func):
    68|         0|            0|            0|  0.00%|    """Set a trace function for all threads started from the threading module.
    69|         0|            0|            0|  0.00%|
    70|         0|            0|            0|  0.00%|    The func will be passed to sys.settrace() for each thread, before its run()
    71|         0|            0|            0|  0.00%|    method is called.
    72|         0|            0|            0|  0.00%|
    73|         0|            0|            0|  0.00%|    """
    74|         0|            0|            0|  0.00%|    global _trace_hook
    75|         0|            0|            0|  0.00%|    _trace_hook = func
    76|         0|            0|            0|  0.00%|
    77|         0|            0|            0|  0.00%|# Synchronization classes
    78|         0|            0|            0|  0.00%|
    79|         0|            0|            0|  0.00%|Lock = _allocate_lock
    80|         0|            0|            0|  0.00%|
    81|         0|            0|            0|  0.00%|def RLock(*args, **kwargs):
    82|         0|            0|            0|  0.00%|    """Factory function that returns a new reentrant lock.
    83|         0|            0|            0|  0.00%|
    84|         0|            0|            0|  0.00%|    A reentrant lock must be released by the thread that acquired it. Once a
    85|         0|            0|            0|  0.00%|    thread has acquired a reentrant lock, the same thread may acquire it again
    86|         0|            0|            0|  0.00%|    without blocking; the thread must release it once for each time it has
    87|         0|            0|            0|  0.00%|    acquired it.
    88|         0|            0|            0|  0.00%|
    89|         0|            0|            0|  0.00%|    """
    90|         0|            0|            0|  0.00%|    if _CRLock is None:
    91|         0|            0|            0|  0.00%|        return _PyRLock(*args, **kwargs)
    92|         0|            0|            0|  0.00%|    return _CRLock(*args, **kwargs)
    93|         0|            0|            0|  0.00%|
    94|         0|            0|            0|  0.00%|class _RLock:
    95|         0|            0|            0|  0.00%|    """This class implements reentrant lock objects.
    96|         0|            0|            0|  0.00%|
    97|         0|            0|            0|  0.00%|    A reentrant lock must be released by the thread that acquired it. Once a
    98|         0|            0|            0|  0.00%|    thread has acquired a reentrant lock, the same thread may acquire it
    99|         0|            0|            0|  0.00%|    again without blocking; the thread must release it once for each time it
   100|         0|            0|            0|  0.00%|    has acquired it.
   101|         0|            0|            0|  0.00%|
   102|         0|            0|            0|  0.00%|    """
   103|         0|            0|            0|  0.00%|
   104|         0|            0|            0|  0.00%|    def __init__(self):
   105|         0|            0|            0|  0.00%|        self._block = _allocate_lock()
   106|         0|            0|            0|  0.00%|        self._owner = None
   107|         0|            0|            0|  0.00%|        self._count = 0
   108|         0|            0|            0|  0.00%|
   109|         0|            0|            0|  0.00%|    def __repr__(self):
   110|         0|            0|            0|  0.00%|        owner = self._owner
   111|         0|            0|            0|  0.00%|        try:
   112|         0|            0|            0|  0.00%|            owner = _active[owner].name
   113|         0|            0|            0|  0.00%|        except KeyError:
   114|         0|            0|            0|  0.00%|            pass
   115|         0|            0|            0|  0.00%|        return "<%s %s.%s object owner=%r count=%d at %s>" % (
   116|         0|            0|            0|  0.00%|            "locked" if self._block.locked() else "unlocked",
   117|         0|            0|            0|  0.00%|            self.__class__.__module__,
   118|         0|            0|            0|  0.00%|            self.__class__.__qualname__,
   119|         0|            0|            0|  0.00%|            owner,
   120|         0|            0|            0|  0.00%|            self._count,
   121|         0|            0|            0|  0.00%|            hex(id(self))
   122|         0|            0|            0|  0.00%|        )
   123|         0|            0|            0|  0.00%|
   124|         0|            0|            0|  0.00%|    def acquire(self, blocking=True, timeout=-1):
   125|         0|            0|            0|  0.00%|        """Acquire a lock, blocking or non-blocking.
   126|         0|            0|            0|  0.00%|
   127|         0|            0|            0|  0.00%|        When invoked without arguments: if this thread already owns the lock,
   128|         0|            0|            0|  0.00%|        increment the recursion level by one, and return immediately. Otherwise,
   129|         0|            0|            0|  0.00%|        if another thread owns the lock, block until the lock is unlocked. Once
   130|         0|            0|            0|  0.00%|        the lock is unlocked (not owned by any thread), then grab ownership, set
   131|         0|            0|            0|  0.00%|        the recursion level to one, and return. If more than one thread is
   132|         0|            0|            0|  0.00%|        blocked waiting until the lock is unlocked, only one at a time will be
   133|         0|            0|            0|  0.00%|        able to grab ownership of the lock. There is no return value in this
   134|         0|            0|            0|  0.00%|        case.
   135|         0|            0|            0|  0.00%|
   136|         0|            0|            0|  0.00%|        When invoked with the blocking argument set to true, do the same thing
   137|         0|            0|            0|  0.00%|        as when called without arguments, and return true.
   138|         0|            0|            0|  0.00%|
   139|         0|            0|            0|  0.00%|        When invoked with the blocking argument set to false, do not block. If a
   140|         0|            0|            0|  0.00%|        call without an argument would block, return false immediately;
   141|         0|            0|            0|  0.00%|        otherwise, do the same thing as when called without arguments, and
   142|         0|            0|            0|  0.00%|        return true.
   143|         0|            0|            0|  0.00%|
   144|         0|            0|            0|  0.00%|        When invoked with the floating-point timeout argument set to a positive
   145|         0|            0|            0|  0.00%|        value, block for at most the number of seconds specified by timeout
   146|         0|            0|            0|  0.00%|        and as long as the lock cannot be acquired.  Return true if the lock has
   147|         0|            0|            0|  0.00%|        been acquired, false if the timeout has elapsed.
   148|         0|            0|            0|  0.00%|
   149|         0|            0|            0|  0.00%|        """
   150|         0|            0|            0|  0.00%|        me = get_ident()
   151|         0|            0|            0|  0.00%|        if self._owner == me:
   152|         0|            0|            0|  0.00%|            self._count += 1
   153|         0|            0|            0|  0.00%|            return 1
   154|         0|            0|            0|  0.00%|        rc = self._block.acquire(blocking, timeout)
   155|         0|            0|            0|  0.00%|        if rc:
   156|         0|            0|            0|  0.00%|            self._owner = me
   157|         0|            0|            0|  0.00%|            self._count = 1
   158|         0|            0|            0|  0.00%|        return rc
   159|         0|            0|            0|  0.00%|
   160|         0|            0|            0|  0.00%|    __enter__ = acquire
   161|         0|            0|            0|  0.00%|
   162|         0|            0|            0|  0.00%|    def release(self):
   163|         0|            0|            0|  0.00%|        """Release a lock, decrementing the recursion level.
   164|         0|            0|            0|  0.00%|
   165|         0|            0|            0|  0.00%|        If after the decrement it is zero, reset the lock to unlocked (not owned
   166|         0|            0|            0|  0.00%|        by any thread), and if any other threads are blocked waiting for the
   167|         0|            0|            0|  0.00%|        lock to become unlocked, allow exactly one of them to proceed. If after
   168|         0|            0|            0|  0.00%|        the decrement the recursion level is still nonzero, the lock remains
   169|         0|            0|            0|  0.00%|        locked and owned by the calling thread.
   170|         0|            0|            0|  0.00%|
   171|         0|            0|            0|  0.00%|        Only call this method when the calling thread owns the lock. A
   172|         0|            0|            0|  0.00%|        RuntimeError is raised if this method is called when the lock is
   173|         0|            0|            0|  0.00%|        unlocked.
   174|         0|            0|            0|  0.00%|
   175|         0|            0|            0|  0.00%|        There is no return value.
   176|         0|            0|            0|  0.00%|
   177|         0|            0|            0|  0.00%|        """
   178|         0|            0|            0|  0.00%|        if self._owner != get_ident():
   179|         0|            0|            0|  0.00%|            raise RuntimeError("cannot release un-acquired lock")
   180|         0|            0|            0|  0.00%|        self._count = count = self._count - 1
   181|         0|            0|            0|  0.00%|        if not count:
   182|         0|            0|            0|  0.00%|            self._owner = None
   183|         0|            0|            0|  0.00%|            self._block.release()
   184|         0|            0|            0|  0.00%|
   185|         0|            0|            0|  0.00%|    def __exit__(self, t, v, tb):
   186|         0|            0|            0|  0.00%|        self.release()
   187|         0|            0|            0|  0.00%|
   188|         0|            0|            0|  0.00%|    # Internal methods used by condition variables
   189|         0|            0|            0|  0.00%|
   190|         0|            0|            0|  0.00%|    def _acquire_restore(self, state):
   191|         0|            0|            0|  0.00%|        self._block.acquire()
   192|         0|            0|            0|  0.00%|        self._count, self._owner = state
   193|         0|            0|            0|  0.00%|
   194|         0|            0|            0|  0.00%|    def _release_save(self):
   195|         0|            0|            0|  0.00%|        if self._count == 0:
   196|         0|            0|            0|  0.00%|            raise RuntimeError("cannot release un-acquired lock")
   197|         0|            0|            0|  0.00%|        count = self._count
   198|         0|            0|            0|  0.00%|        self._count = 0
   199|         0|            0|            0|  0.00%|        owner = self._owner
   200|         0|            0|            0|  0.00%|        self._owner = None
   201|         0|            0|            0|  0.00%|        self._block.release()
   202|         0|            0|            0|  0.00%|        return (count, owner)
   203|         0|            0|            0|  0.00%|
   204|         0|            0|            0|  0.00%|    def _is_owned(self):
   205|         0|            0|            0|  0.00%|        return self._owner == get_ident()
   206|         0|            0|            0|  0.00%|
   207|         0|            0|            0|  0.00%|_PyRLock = _RLock
   208|         0|            0|            0|  0.00%|
   209|         0|            0|            0|  0.00%|
   210|         0|            0|            0|  0.00%|class Condition:
   211|         0|            0|            0|  0.00%|    """Class that implements a condition variable.
   212|         0|            0|            0|  0.00%|
   213|         0|            0|            0|  0.00%|    A condition variable allows one or more threads to wait until they are
   214|         0|            0|            0|  0.00%|    notified by another thread.
   215|         0|            0|            0|  0.00%|
   216|         0|            0|            0|  0.00%|    If the lock argument is given and not None, it must be a Lock or RLock
   217|         0|            0|            0|  0.00%|    object, and it is used as the underlying lock. Otherwise, a new RLock object
   218|         0|            0|            0|  0.00%|    is created and used as the underlying lock.
   219|         0|            0|            0|  0.00%|
   220|         0|            0|            0|  0.00%|    """
   221|         0|            0|            0|  0.00%|
   222|        18|  0.000125885|  6.99361e-06|  0.00%|    def __init__(self, lock=None):
   223|        18|  0.000109673|  6.09292e-06|  0.00%|        if lock is None:
   224|         0|            0|            0|  0.00%|            lock = RLock()
   225|        18|  7.93934e-05|  4.41074e-06|  0.00%|        self._lock = lock
   226|         0|            0|            0|  0.00%|        # Export the lock's acquire() and release() methods
   227|        18|  7.96318e-05|  4.42399e-06|  0.00%|        self.acquire = lock.acquire
   228|        18|  8.17776e-05|   4.5432e-06|  0.00%|        self.release = lock.release
   229|         0|            0|            0|  0.00%|        # If the lock defines _release_save() and/or _acquire_restore(),
   230|         0|            0|            0|  0.00%|        # these override the default implementations (which just call
   231|         0|            0|            0|  0.00%|        # release() and acquire() on the lock).  Ditto for _is_owned().
   232|        18|  6.79493e-05|  3.77496e-06|  0.00%|        try:
   233|        18|   0.00028944|    1.608e-05|  0.00%|            self._release_save = lock._release_save
   234|        18|  9.53674e-05|  5.29819e-06|  0.00%|        except AttributeError:
   235|        18|  0.000105143|  5.84126e-06|  0.00%|            pass
   236|        18|  6.96182e-05|  3.86768e-06|  0.00%|        try:
   237|        18|  0.000125647|  6.98037e-06|  0.00%|            self._acquire_restore = lock._acquire_restore
   238|        18|  7.89165e-05|  4.38425e-06|  0.00%|        except AttributeError:
   239|        18|  9.46522e-05|  5.25845e-06|  0.00%|            pass
   240|        18|  6.98566e-05|  3.88092e-06|  0.00%|        try:
   241|        18|  0.000111341|  6.18564e-06|  0.00%|            self._is_owned = lock._is_owned
   242|        18|  8.03471e-05|  4.46373e-06|  0.00%|        except AttributeError:
   243|        18|  9.34601e-05|  5.19223e-06|  0.00%|            pass
   244|        18|  0.000199556|  1.10865e-05|  0.00%|        self._waiters = _deque()
   245|         0|            0|            0|  0.00%|
   246|       124|   0.00067544|   5.4471e-06|  0.00%|    def __enter__(self):
   247|       124|    0.0015533|  1.25266e-05|  0.00%|        return self._lock.__enter__()
   248|         0|            0|            0|  0.00%|
   249|       124|  0.000625134|   5.0414e-06|  0.00%|    def __exit__(self, *args):
   250|       124|  0.000980616|  7.90819e-06|  0.00%|        return self._lock.__exit__(*args)
   251|         0|            0|            0|  0.00%|
   252|         0|            0|            0|  0.00%|    def __repr__(self):
   253|         0|            0|            0|  0.00%|        return "<Condition(%s, %d)>" % (self._lock, len(self._waiters))
   254|         0|            0|            0|  0.00%|
   255|         8|  4.62532e-05|  5.78165e-06|  0.00%|    def _release_save(self):
   256|         8|  6.10352e-05|  7.62939e-06|  0.00%|        self._lock.release()           # No state to save
   257|         0|            0|            0|  0.00%|
   258|         8|  6.22272e-05|  7.77841e-06|  0.00%|    def _acquire_restore(self, x):
   259|         8|  7.08103e-05|  8.85129e-06|  0.00%|        self._lock.acquire()           # Ignore saved state
   260|         0|            0|            0|  0.00%|
   261|       124|  0.000621796|  5.01448e-06|  0.00%|    def _is_owned(self):
   262|         0|            0|            0|  0.00%|        # Return True if lock is owned by current_thread.
   263|         0|            0|            0|  0.00%|        # This method is called only if _lock doesn't have _is_owned().
   264|       124|  0.000927448|  7.47942e-06|  0.00%|        if self._lock.acquire(0):
   265|         0|            0|            0|  0.00%|            self._lock.release()
   266|         0|            0|            0|  0.00%|            return False
   267|         0|            0|            0|  0.00%|        else:
   268|       124|  0.000535488|  4.31845e-06|  0.00%|            return True
   269|         0|            0|            0|  0.00%|
   270|         8|  5.76973e-05|  7.21216e-06|  0.00%|    def wait(self, timeout=None):
   271|         0|            0|            0|  0.00%|        """Wait until notified or until a timeout occurs.
   272|         0|            0|            0|  0.00%|
   273|         0|            0|            0|  0.00%|        If the calling thread has not acquired the lock when this method is
   274|         0|            0|            0|  0.00%|        called, a RuntimeError is raised.
   275|         0|            0|            0|  0.00%|
   276|         0|            0|            0|  0.00%|        This method releases the underlying lock, and then blocks until it is
   277|         0|            0|            0|  0.00%|        awakened by a notify() or notify_all() call for the same condition
   278|         0|            0|            0|  0.00%|        variable in another thread, or until the optional timeout occurs. Once
   279|         0|            0|            0|  0.00%|        awakened or timed out, it re-acquires the lock and returns.
   280|         0|            0|            0|  0.00%|
   281|         0|            0|            0|  0.00%|        When the timeout argument is present and not None, it should be a
   282|         0|            0|            0|  0.00%|        floating point number specifying a timeout for the operation in seconds
   283|         0|            0|            0|  0.00%|        (or fractions thereof).
   284|         0|            0|            0|  0.00%|
   285|         0|            0|            0|  0.00%|        When the underlying lock is an RLock, it is not released using its
   286|         0|            0|            0|  0.00%|        release() method, since this may not actually unlock the lock when it
   287|         0|            0|            0|  0.00%|        was acquired multiple times recursively. Instead, an internal interface
   288|         0|            0|            0|  0.00%|        of the RLock class is used, which really unlocks it even when it has
   289|         0|            0|            0|  0.00%|        been recursively acquired several times. Another internal interface is
   290|         0|            0|            0|  0.00%|        then used to restore the recursion level when the lock is reacquired.
   291|         0|            0|            0|  0.00%|
   292|         0|            0|            0|  0.00%|        """
   293|         8|  0.000145912|   1.8239e-05|  0.00%|        if not self._is_owned():
(call)|         8|  0.000142336|   1.7792e-05|  0.00%|# /opt/conda/lib/python3.8/threading.py:261 _is_owned
   294|         0|            0|            0|  0.00%|            raise RuntimeError("cannot wait on un-acquired lock")
   295|         8|  5.91278e-05|  7.39098e-06|  0.00%|        waiter = _allocate_lock()
   296|         8|  5.65052e-05|  7.06315e-06|  0.00%|        waiter.acquire()
   297|         8|  5.65052e-05|  7.06315e-06|  0.00%|        self._waiters.append(waiter)
   298|         8|  0.000152111|  1.90139e-05|  0.00%|        saved_state = self._release_save()
(call)|         8|  0.000107288|   1.3411e-05|  0.00%|# /opt/conda/lib/python3.8/threading.py:255 _release_save
   299|         8|  3.69549e-05|  4.61936e-06|  0.00%|        gotit = False
   300|         8|  4.02927e-05|  5.03659e-06|  0.00%|        try:    # restore state no matter what (e.g., KeyboardInterrupt)
   301|         8|  4.31538e-05|  5.39422e-06|  0.00%|            if timeout is None:
   302|         8|   0.00312281|  0.000390351|  0.01%|                waiter.acquire()
   303|         8|  7.62939e-05|  9.53674e-06|  0.00%|                gotit = True
   304|         0|            0|            0|  0.00%|            else:
   305|         0|            0|            0|  0.00%|                if timeout > 0:
   306|         0|            0|            0|  0.00%|                    gotit = waiter.acquire(True, timeout)
   307|         0|            0|            0|  0.00%|                else:
   308|         0|            0|            0|  0.00%|                    gotit = waiter.acquire(False)
   309|        16|  6.74725e-05|  4.21703e-06|  0.00%|            return gotit
   310|         0|            0|            0|  0.00%|        finally:
   311|         8|  0.000138283|  1.72853e-05|  0.00%|            self._acquire_restore(saved_state)
(call)|         8|  0.000133038|  1.66297e-05|  0.00%|# /opt/conda/lib/python3.8/threading.py:258 _acquire_restore
   312|         8|  3.67165e-05|  4.58956e-06|  0.00%|            if not gotit:
   313|         0|            0|            0|  0.00%|                try:
   314|         0|            0|            0|  0.00%|                    self._waiters.remove(waiter)
   315|         0|            0|            0|  0.00%|                except ValueError:
   316|         0|            0|            0|  0.00%|                    pass
   317|         0|            0|            0|  0.00%|
   318|         0|            0|            0|  0.00%|    def wait_for(self, predicate, timeout=None):
   319|         0|            0|            0|  0.00%|        """Wait until a condition evaluates to True.
   320|         0|            0|            0|  0.00%|
   321|         0|            0|            0|  0.00%|        predicate should be a callable which result will be interpreted as a
   322|         0|            0|            0|  0.00%|        boolean value.  A timeout may be provided giving the maximum time to
   323|         0|            0|            0|  0.00%|        wait.
   324|         0|            0|            0|  0.00%|
   325|         0|            0|            0|  0.00%|        """
   326|         0|            0|            0|  0.00%|        endtime = None
   327|         0|            0|            0|  0.00%|        waittime = timeout
   328|         0|            0|            0|  0.00%|        result = predicate()
   329|         0|            0|            0|  0.00%|        while not result:
   330|         0|            0|            0|  0.00%|            if waittime is not None:
   331|         0|            0|            0|  0.00%|                if endtime is None:
   332|         0|            0|            0|  0.00%|                    endtime = _time() + waittime
   333|         0|            0|            0|  0.00%|                else:
   334|         0|            0|            0|  0.00%|                    waittime = endtime - _time()
   335|         0|            0|            0|  0.00%|                    if waittime <= 0:
   336|         0|            0|            0|  0.00%|                        break
   337|         0|            0|            0|  0.00%|            self.wait(waittime)
   338|         0|            0|            0|  0.00%|            result = predicate()
   339|         0|            0|            0|  0.00%|        return result
   340|         0|            0|            0|  0.00%|
   341|       116|  0.000700474|  6.03857e-06|  0.00%|    def notify(self, n=1):
   342|         0|            0|            0|  0.00%|        """Wake up one or more threads waiting on this condition, if any.
   343|         0|            0|            0|  0.00%|
   344|         0|            0|            0|  0.00%|        If the calling thread has not acquired the lock when this method is
   345|         0|            0|            0|  0.00%|        called, a RuntimeError is raised.
   346|         0|            0|            0|  0.00%|
   347|         0|            0|            0|  0.00%|        This method wakes up at most n of the threads waiting for the condition
   348|         0|            0|            0|  0.00%|        variable; it is a no-op if no threads are waiting.
   349|         0|            0|            0|  0.00%|
   350|         0|            0|            0|  0.00%|        """
   351|       116|   0.00177217|  1.52773e-05|  0.00%|        if not self._is_owned():
(call)|       116|    0.0019424|  1.67448e-05|  0.00%|# /opt/conda/lib/python3.8/threading.py:261 _is_owned
   352|         0|            0|            0|  0.00%|            raise RuntimeError("cannot notify on un-acquired lock")
   353|       116|  0.000589609|  5.08284e-06|  0.00%|        all_waiters = self._waiters
   354|       116|   0.00101066|  8.71255e-06|  0.00%|        waiters_to_notify = _deque(_islice(all_waiters, n))
   355|       116|  0.000540733|  4.66149e-06|  0.00%|        if not waiters_to_notify:
   356|        10|  5.00679e-05|  5.00679e-06|  0.00%|            return
   357|       212|  0.000951052|  4.48609e-06|  0.00%|        for waiter in waiters_to_notify:
   358|       106|   0.00209284|  1.97438e-05|  0.00%|            waiter.release()
   359|       106|  0.000612259|  5.77603e-06|  0.00%|            try:
   360|       106|  0.000806093|  7.60465e-06|  0.00%|                all_waiters.remove(waiter)
   361|         0|            0|            0|  0.00%|            except ValueError:
   362|         0|            0|            0|  0.00%|                pass
   363|         0|            0|            0|  0.00%|
   364|         0|            0|            0|  0.00%|    def notify_all(self):
   365|         0|            0|            0|  0.00%|        """Wake up all threads waiting on this condition.
   366|         0|            0|            0|  0.00%|
   367|         0|            0|            0|  0.00%|        If the calling thread has not acquired the lock when this method
   368|         0|            0|            0|  0.00%|        is called, a RuntimeError is raised.
   369|         0|            0|            0|  0.00%|
   370|         0|            0|            0|  0.00%|        """
   371|         0|            0|            0|  0.00%|        self.notify(len(self._waiters))
   372|         0|            0|            0|  0.00%|
   373|         0|            0|            0|  0.00%|    notifyAll = notify_all
   374|         0|            0|            0|  0.00%|
   375|         0|            0|            0|  0.00%|
   376|         0|            0|            0|  0.00%|class Semaphore:
   377|         0|            0|            0|  0.00%|    """This class implements semaphore objects.
   378|         0|            0|            0|  0.00%|
   379|         0|            0|            0|  0.00%|    Semaphores manage a counter representing the number of release() calls minus
   380|         0|            0|            0|  0.00%|    the number of acquire() calls, plus an initial value. The acquire() method
   381|         0|            0|            0|  0.00%|    blocks if necessary until it can return without making the counter
   382|         0|            0|            0|  0.00%|    negative. If not given, value defaults to 1.
   383|         0|            0|            0|  0.00%|
   384|         0|            0|            0|  0.00%|    """
   385|         0|            0|            0|  0.00%|
   386|         0|            0|            0|  0.00%|    # After Tim Peters' semaphore class, but not quite the same (no maximum)
   387|         0|            0|            0|  0.00%|
   388|         0|            0|            0|  0.00%|    def __init__(self, value=1):
   389|         0|            0|            0|  0.00%|        if value < 0:
   390|         0|            0|            0|  0.00%|            raise ValueError("semaphore initial value must be >= 0")
   391|         0|            0|            0|  0.00%|        self._cond = Condition(Lock())
   392|         0|            0|            0|  0.00%|        self._value = value
   393|         0|            0|            0|  0.00%|
   394|         0|            0|            0|  0.00%|    def acquire(self, blocking=True, timeout=None):
   395|         0|            0|            0|  0.00%|        """Acquire a semaphore, decrementing the internal counter by one.
   396|         0|            0|            0|  0.00%|
   397|         0|            0|            0|  0.00%|        When invoked without arguments: if the internal counter is larger than
   398|         0|            0|            0|  0.00%|        zero on entry, decrement it by one and return immediately. If it is zero
   399|         0|            0|            0|  0.00%|        on entry, block, waiting until some other thread has called release() to
   400|         0|            0|            0|  0.00%|        make it larger than zero. This is done with proper interlocking so that
   401|         0|            0|            0|  0.00%|        if multiple acquire() calls are blocked, release() will wake exactly one
   402|         0|            0|            0|  0.00%|        of them up. The implementation may pick one at random, so the order in
   403|         0|            0|            0|  0.00%|        which blocked threads are awakened should not be relied on. There is no
   404|         0|            0|            0|  0.00%|        return value in this case.
   405|         0|            0|            0|  0.00%|
   406|         0|            0|            0|  0.00%|        When invoked with blocking set to true, do the same thing as when called
   407|         0|            0|            0|  0.00%|        without arguments, and return true.
   408|         0|            0|            0|  0.00%|
   409|         0|            0|            0|  0.00%|        When invoked with blocking set to false, do not block. If a call without
   410|         0|            0|            0|  0.00%|        an argument would block, return false immediately; otherwise, do the
   411|         0|            0|            0|  0.00%|        same thing as when called without arguments, and return true.
   412|         0|            0|            0|  0.00%|
   413|         0|            0|            0|  0.00%|        When invoked with a timeout other than None, it will block for at
   414|         0|            0|            0|  0.00%|        most timeout seconds.  If acquire does not complete successfully in
   415|         0|            0|            0|  0.00%|        that interval, return false.  Return true otherwise.
   416|         0|            0|            0|  0.00%|
   417|         0|            0|            0|  0.00%|        """
   418|         0|            0|            0|  0.00%|        if not blocking and timeout is not None:
   419|         0|            0|            0|  0.00%|            raise ValueError("can't specify timeout for non-blocking acquire")
   420|         0|            0|            0|  0.00%|        rc = False
   421|         0|            0|            0|  0.00%|        endtime = None
   422|         0|            0|            0|  0.00%|        with self._cond:
   423|         0|            0|            0|  0.00%|            while self._value == 0:
   424|         0|            0|            0|  0.00%|                if not blocking:
   425|         0|            0|            0|  0.00%|                    break
   426|         0|            0|            0|  0.00%|                if timeout is not None:
   427|         0|            0|            0|  0.00%|                    if endtime is None:
   428|         0|            0|            0|  0.00%|                        endtime = _time() + timeout
   429|         0|            0|            0|  0.00%|                    else:
   430|         0|            0|            0|  0.00%|                        timeout = endtime - _time()
   431|         0|            0|            0|  0.00%|                        if timeout <= 0:
   432|         0|            0|            0|  0.00%|                            break
   433|         0|            0|            0|  0.00%|                self._cond.wait(timeout)
   434|         0|            0|            0|  0.00%|            else:
   435|         0|            0|            0|  0.00%|                self._value -= 1
   436|         0|            0|            0|  0.00%|                rc = True
   437|         0|            0|            0|  0.00%|        return rc
   438|         0|            0|            0|  0.00%|
   439|         0|            0|            0|  0.00%|    __enter__ = acquire
   440|         0|            0|            0|  0.00%|
   441|         0|            0|            0|  0.00%|    def release(self):
   442|         0|            0|            0|  0.00%|        """Release a semaphore, incrementing the internal counter by one.
   443|         0|            0|            0|  0.00%|
   444|         0|            0|            0|  0.00%|        When the counter is zero on entry and another thread is waiting for it
   445|         0|            0|            0|  0.00%|        to become larger than zero again, wake up that thread.
   446|         0|            0|            0|  0.00%|
   447|         0|            0|            0|  0.00%|        """
   448|         0|            0|            0|  0.00%|        with self._cond:
   449|         0|            0|            0|  0.00%|            self._value += 1
   450|         0|            0|            0|  0.00%|            self._cond.notify()
   451|         0|            0|            0|  0.00%|
   452|         0|            0|            0|  0.00%|    def __exit__(self, t, v, tb):
   453|         0|            0|            0|  0.00%|        self.release()
   454|         0|            0|            0|  0.00%|
   455|         0|            0|            0|  0.00%|
   456|         0|            0|            0|  0.00%|class BoundedSemaphore(Semaphore):
   457|         0|            0|            0|  0.00%|    """Implements a bounded semaphore.
   458|         0|            0|            0|  0.00%|
   459|         0|            0|            0|  0.00%|    A bounded semaphore checks to make sure its current value doesn't exceed its
   460|         0|            0|            0|  0.00%|    initial value. If it does, ValueError is raised. In most situations
   461|         0|            0|            0|  0.00%|    semaphores are used to guard resources with limited capacity.
   462|         0|            0|            0|  0.00%|
   463|         0|            0|            0|  0.00%|    If the semaphore is released too many times it's a sign of a bug. If not
   464|         0|            0|            0|  0.00%|    given, value defaults to 1.
   465|         0|            0|            0|  0.00%|
   466|         0|            0|            0|  0.00%|    Like regular semaphores, bounded semaphores manage a counter representing
   467|         0|            0|            0|  0.00%|    the number of release() calls minus the number of acquire() calls, plus an
   468|         0|            0|            0|  0.00%|    initial value. The acquire() method blocks if necessary until it can return
   469|         0|            0|            0|  0.00%|    without making the counter negative. If not given, value defaults to 1.
   470|         0|            0|            0|  0.00%|
   471|         0|            0|            0|  0.00%|    """
   472|         0|            0|            0|  0.00%|
   473|         0|            0|            0|  0.00%|    def __init__(self, value=1):
   474|         0|            0|            0|  0.00%|        Semaphore.__init__(self, value)
   475|         0|            0|            0|  0.00%|        self._initial_value = value
   476|         0|            0|            0|  0.00%|
   477|         0|            0|            0|  0.00%|    def release(self):
   478|         0|            0|            0|  0.00%|        """Release a semaphore, incrementing the internal counter by one.
   479|         0|            0|            0|  0.00%|
   480|         0|            0|            0|  0.00%|        When the counter is zero on entry and another thread is waiting for it
   481|         0|            0|            0|  0.00%|        to become larger than zero again, wake up that thread.
   482|         0|            0|            0|  0.00%|
   483|         0|            0|            0|  0.00%|        If the number of releases exceeds the number of acquires,
   484|         0|            0|            0|  0.00%|        raise a ValueError.
   485|         0|            0|            0|  0.00%|
   486|         0|            0|            0|  0.00%|        """
   487|         0|            0|            0|  0.00%|        with self._cond:
   488|         0|            0|            0|  0.00%|            if self._value >= self._initial_value:
   489|         0|            0|            0|  0.00%|                raise ValueError("Semaphore released too many times")
   490|         0|            0|            0|  0.00%|            self._value += 1
   491|         0|            0|            0|  0.00%|            self._cond.notify()
   492|         0|            0|            0|  0.00%|
   493|         0|            0|            0|  0.00%|
   494|         0|            0|            0|  0.00%|class Event:
   495|         0|            0|            0|  0.00%|    """Class implementing event objects.
   496|         0|            0|            0|  0.00%|
   497|         0|            0|            0|  0.00%|    Events manage a flag that can be set to true with the set() method and reset
   498|         0|            0|            0|  0.00%|    to false with the clear() method. The wait() method blocks until the flag is
   499|         0|            0|            0|  0.00%|    true.  The flag is initially false.
   500|         0|            0|            0|  0.00%|
   501|         0|            0|            0|  0.00%|    """
   502|         0|            0|            0|  0.00%|
   503|         0|            0|            0|  0.00%|    # After Tim Peters' event class (without is_posted())
   504|         0|            0|            0|  0.00%|
   505|         8|  4.26769e-05|  5.33462e-06|  0.00%|    def __init__(self):
   506|         8|  0.000184298|  2.30372e-05|  0.00%|        self._cond = Condition(Lock())
(call)|         8|  0.000721455|  9.01818e-05|  0.00%|# /opt/conda/lib/python3.8/threading.py:222 __init__
   507|         8|  3.71933e-05|  4.64916e-06|  0.00%|        self._flag = False
   508|         0|            0|            0|  0.00%|
   509|         0|            0|            0|  0.00%|    def _reset_internal_locks(self):
   510|         0|            0|            0|  0.00%|        # private!  called by Thread._reset_internal_locks by _after_fork()
   511|         0|            0|            0|  0.00%|        self._cond.__init__(Lock())
   512|         0|            0|            0|  0.00%|
   513|        16|  8.10623e-05|  5.06639e-06|  0.00%|    def is_set(self):
   514|         0|            0|            0|  0.00%|        """Return true if and only if the internal flag is true."""
   515|        16|  6.93798e-05|  4.33624e-06|  0.00%|        return self._flag
   516|         0|            0|            0|  0.00%|
   517|         0|            0|            0|  0.00%|    isSet = is_set
   518|         0|            0|            0|  0.00%|
   519|         0|            0|            0|  0.00%|    def set(self):
   520|         0|            0|            0|  0.00%|        """Set the internal flag to true.
   521|         0|            0|            0|  0.00%|
   522|         0|            0|            0|  0.00%|        All threads waiting for it to become true are awakened. Threads
   523|         0|            0|            0|  0.00%|        that call wait() once the flag is true will not block at all.
   524|         0|            0|            0|  0.00%|
   525|         0|            0|            0|  0.00%|        """
   526|         0|            0|            0|  0.00%|        with self._cond:
   527|         0|            0|            0|  0.00%|            self._flag = True
   528|         0|            0|            0|  0.00%|            self._cond.notify_all()
   529|         0|            0|            0|  0.00%|
   530|         0|            0|            0|  0.00%|    def clear(self):
   531|         0|            0|            0|  0.00%|        """Reset the internal flag to false.
   532|         0|            0|            0|  0.00%|
   533|         0|            0|            0|  0.00%|        Subsequently, threads calling wait() will block until set() is called to
   534|         0|            0|            0|  0.00%|        set the internal flag to true again.
   535|         0|            0|            0|  0.00%|
   536|         0|            0|            0|  0.00%|        """
   537|         0|            0|            0|  0.00%|        with self._cond:
   538|         0|            0|            0|  0.00%|            self._flag = False
   539|         0|            0|            0|  0.00%|
   540|         8|  8.32081e-05|   1.0401e-05|  0.00%|    def wait(self, timeout=None):
   541|         0|            0|            0|  0.00%|        """Block until the internal flag is true.
   542|         0|            0|            0|  0.00%|
   543|         0|            0|            0|  0.00%|        If the internal flag is true on entry, return immediately. Otherwise,
   544|         0|            0|            0|  0.00%|        block until another thread calls set() to set the flag to true, or until
   545|         0|            0|            0|  0.00%|        the optional timeout occurs.
   546|         0|            0|            0|  0.00%|
   547|         0|            0|            0|  0.00%|        When the timeout argument is present and not None, it should be a
   548|         0|            0|            0|  0.00%|        floating point number specifying a timeout for the operation in seconds
   549|         0|            0|            0|  0.00%|        (or fractions thereof).
   550|         0|            0|            0|  0.00%|
   551|         0|            0|            0|  0.00%|        This method returns the internal flag on exit, so it will always return
   552|         0|            0|            0|  0.00%|        True except if a timeout is given and the operation times out.
   553|         0|            0|            0|  0.00%|
   554|         0|            0|            0|  0.00%|        """
   555|         8|  0.000170946|  2.13683e-05|  0.00%|        with self._cond:
(call)|         8|  0.000123739|  1.54674e-05|  0.00%|# /opt/conda/lib/python3.8/threading.py:246 __enter__
   556|         8|  3.60012e-05|  4.50015e-06|  0.00%|            signaled = self._flag
   557|         8|  3.19481e-05|  3.99351e-06|  0.00%|            if not signaled:
   558|         8|  0.000147581|  1.84476e-05|  0.00%|                signaled = self._cond.wait(timeout)
(call)|         8|   0.00447249|  0.000559062|  0.01%|# /opt/conda/lib/python3.8/threading.py:270 wait
   559|         8|  0.000110865|  1.38581e-05|  0.00%|            return signaled
(call)|         8|  0.000103474|  1.29342e-05|  0.00%|# /opt/conda/lib/python3.8/threading.py:249 __exit__
   560|         0|            0|            0|  0.00%|
   561|         0|            0|            0|  0.00%|
   562|         0|            0|            0|  0.00%|# A barrier class.  Inspired in part by the pthread_barrier_* api and
   563|         0|            0|            0|  0.00%|# the CyclicBarrier class from Java.  See
   564|         0|            0|            0|  0.00%|# http://sourceware.org/pthreads-win32/manual/pthread_barrier_init.html and
   565|         0|            0|            0|  0.00%|# http://java.sun.com/j2se/1.5.0/docs/api/java/util/concurrent/
   566|         0|            0|            0|  0.00%|#        CyclicBarrier.html
   567|         0|            0|            0|  0.00%|# for information.
   568|         0|            0|            0|  0.00%|# We maintain two main states, 'filling' and 'draining' enabling the barrier
   569|         0|            0|            0|  0.00%|# to be cyclic.  Threads are not allowed into it until it has fully drained
   570|         0|            0|            0|  0.00%|# since the previous cycle.  In addition, a 'resetting' state exists which is
   571|         0|            0|            0|  0.00%|# similar to 'draining' except that threads leave with a BrokenBarrierError,
   572|         0|            0|            0|  0.00%|# and a 'broken' state in which all threads get the exception.
   573|         0|            0|            0|  0.00%|class Barrier:
   574|         0|            0|            0|  0.00%|    """Implements a Barrier.
   575|         0|            0|            0|  0.00%|
   576|         0|            0|            0|  0.00%|    Useful for synchronizing a fixed number of threads at known synchronization
   577|         0|            0|            0|  0.00%|    points.  Threads block on 'wait()' and are simultaneously awoken once they
   578|         0|            0|            0|  0.00%|    have all made that call.
   579|         0|            0|            0|  0.00%|
   580|         0|            0|            0|  0.00%|    """
   581|         0|            0|            0|  0.00%|
   582|         0|            0|            0|  0.00%|    def __init__(self, parties, action=None, timeout=None):
   583|         0|            0|            0|  0.00%|        """Create a barrier, initialised to 'parties' threads.
   584|         0|            0|            0|  0.00%|
   585|         0|            0|            0|  0.00%|        'action' is a callable which, when supplied, will be called by one of
   586|         0|            0|            0|  0.00%|        the threads after they have all entered the barrier and just prior to
   587|         0|            0|            0|  0.00%|        releasing them all. If a 'timeout' is provided, it is used as the
   588|         0|            0|            0|  0.00%|        default for all subsequent 'wait()' calls.
   589|         0|            0|            0|  0.00%|
   590|         0|            0|            0|  0.00%|        """
   591|         0|            0|            0|  0.00%|        self._cond = Condition(Lock())
   592|         0|            0|            0|  0.00%|        self._action = action
   593|         0|            0|            0|  0.00%|        self._timeout = timeout
   594|         0|            0|            0|  0.00%|        self._parties = parties
   595|         0|            0|            0|  0.00%|        self._state = 0 #0 filling, 1, draining, -1 resetting, -2 broken
   596|         0|            0|            0|  0.00%|        self._count = 0
   597|         0|            0|            0|  0.00%|
   598|         0|            0|            0|  0.00%|    def wait(self, timeout=None):
   599|         0|            0|            0|  0.00%|        """Wait for the barrier.
   600|         0|            0|            0|  0.00%|
   601|         0|            0|            0|  0.00%|        When the specified number of threads have started waiting, they are all
   602|         0|            0|            0|  0.00%|        simultaneously awoken. If an 'action' was provided for the barrier, one
   603|         0|            0|            0|  0.00%|        of the threads will have executed that callback prior to returning.
   604|         0|            0|            0|  0.00%|        Returns an individual index number from 0 to 'parties-1'.
   605|         0|            0|            0|  0.00%|
   606|         0|            0|            0|  0.00%|        """
   607|         0|            0|            0|  0.00%|        if timeout is None:
   608|         0|            0|            0|  0.00%|            timeout = self._timeout
   609|         0|            0|            0|  0.00%|        with self._cond:
   610|         0|            0|            0|  0.00%|            self._enter() # Block while the barrier drains.
   611|         0|            0|            0|  0.00%|            index = self._count
   612|         0|            0|            0|  0.00%|            self._count += 1
   613|         0|            0|            0|  0.00%|            try:
   614|         0|            0|            0|  0.00%|                if index + 1 == self._parties:
   615|         0|            0|            0|  0.00%|                    # We release the barrier
   616|         0|            0|            0|  0.00%|                    self._release()
   617|         0|            0|            0|  0.00%|                else:
   618|         0|            0|            0|  0.00%|                    # We wait until someone releases us
   619|         0|            0|            0|  0.00%|                    self._wait(timeout)
   620|         0|            0|            0|  0.00%|                return index
   621|         0|            0|            0|  0.00%|            finally:
   622|         0|            0|            0|  0.00%|                self._count -= 1
   623|         0|            0|            0|  0.00%|                # Wake up any threads waiting for barrier to drain.
   624|         0|            0|            0|  0.00%|                self._exit()
   625|         0|            0|            0|  0.00%|
   626|         0|            0|            0|  0.00%|    # Block until the barrier is ready for us, or raise an exception
   627|         0|            0|            0|  0.00%|    # if it is broken.
   628|         0|            0|            0|  0.00%|    def _enter(self):
   629|         0|            0|            0|  0.00%|        while self._state in (-1, 1):
   630|         0|            0|            0|  0.00%|            # It is draining or resetting, wait until done
   631|         0|            0|            0|  0.00%|            self._cond.wait()
   632|         0|            0|            0|  0.00%|        #see if the barrier is in a broken state
   633|         0|            0|            0|  0.00%|        if self._state < 0:
   634|         0|            0|            0|  0.00%|            raise BrokenBarrierError
   635|         0|            0|            0|  0.00%|        assert self._state == 0
   636|         0|            0|            0|  0.00%|
   637|         0|            0|            0|  0.00%|    # Optionally run the 'action' and release the threads waiting
   638|         0|            0|            0|  0.00%|    # in the barrier.
   639|         0|            0|            0|  0.00%|    def _release(self):
   640|         0|            0|            0|  0.00%|        try:
   641|         0|            0|            0|  0.00%|            if self._action:
   642|         0|            0|            0|  0.00%|                self._action()
   643|         0|            0|            0|  0.00%|            # enter draining state
   644|         0|            0|            0|  0.00%|            self._state = 1
   645|         0|            0|            0|  0.00%|            self._cond.notify_all()
   646|         0|            0|            0|  0.00%|        except:
   647|         0|            0|            0|  0.00%|            #an exception during the _action handler.  Break and reraise
   648|         0|            0|            0|  0.00%|            self._break()
   649|         0|            0|            0|  0.00%|            raise
   650|         0|            0|            0|  0.00%|
   651|         0|            0|            0|  0.00%|    # Wait in the barrier until we are released.  Raise an exception
   652|         0|            0|            0|  0.00%|    # if the barrier is reset or broken.
   653|         0|            0|            0|  0.00%|    def _wait(self, timeout):
   654|         0|            0|            0|  0.00%|        if not self._cond.wait_for(lambda : self._state != 0, timeout):
   655|         0|            0|            0|  0.00%|            #timed out.  Break the barrier
   656|         0|            0|            0|  0.00%|            self._break()
   657|         0|            0|            0|  0.00%|            raise BrokenBarrierError
   658|         0|            0|            0|  0.00%|        if self._state < 0:
   659|         0|            0|            0|  0.00%|            raise BrokenBarrierError
   660|         0|            0|            0|  0.00%|        assert self._state == 1
   661|         0|            0|            0|  0.00%|
   662|         0|            0|            0|  0.00%|    # If we are the last thread to exit the barrier, signal any threads
   663|         0|            0|            0|  0.00%|    # waiting for the barrier to drain.
   664|         0|            0|            0|  0.00%|    def _exit(self):
   665|         0|            0|            0|  0.00%|        if self._count == 0:
   666|         0|            0|            0|  0.00%|            if self._state in (-1, 1):
   667|         0|            0|            0|  0.00%|                #resetting or draining
   668|         0|            0|            0|  0.00%|                self._state = 0
   669|         0|            0|            0|  0.00%|                self._cond.notify_all()
   670|         0|            0|            0|  0.00%|
   671|         0|            0|            0|  0.00%|    def reset(self):
   672|         0|            0|            0|  0.00%|        """Reset the barrier to the initial state.
   673|         0|            0|            0|  0.00%|
   674|         0|            0|            0|  0.00%|        Any threads currently waiting will get the BrokenBarrier exception
   675|         0|            0|            0|  0.00%|        raised.
   676|         0|            0|            0|  0.00%|
   677|         0|            0|            0|  0.00%|        """
   678|         0|            0|            0|  0.00%|        with self._cond:
   679|         0|            0|            0|  0.00%|            if self._count > 0:
   680|         0|            0|            0|  0.00%|                if self._state == 0:
   681|         0|            0|            0|  0.00%|                    #reset the barrier, waking up threads
   682|         0|            0|            0|  0.00%|                    self._state = -1
   683|         0|            0|            0|  0.00%|                elif self._state == -2:
   684|         0|            0|            0|  0.00%|                    #was broken, set it to reset state
   685|         0|            0|            0|  0.00%|                    #which clears when the last thread exits
   686|         0|            0|            0|  0.00%|                    self._state = -1
   687|         0|            0|            0|  0.00%|            else:
   688|         0|            0|            0|  0.00%|                self._state = 0
   689|         0|            0|            0|  0.00%|            self._cond.notify_all()
   690|         0|            0|            0|  0.00%|
   691|         0|            0|            0|  0.00%|    def abort(self):
   692|         0|            0|            0|  0.00%|        """Place the barrier into a 'broken' state.
   693|         0|            0|            0|  0.00%|
   694|         0|            0|            0|  0.00%|        Useful in case of error.  Any currently waiting threads and threads
   695|         0|            0|            0|  0.00%|        attempting to 'wait()' will have BrokenBarrierError raised.
   696|         0|            0|            0|  0.00%|
   697|         0|            0|            0|  0.00%|        """
   698|         0|            0|            0|  0.00%|        with self._cond:
   699|         0|            0|            0|  0.00%|            self._break()
   700|         0|            0|            0|  0.00%|
   701|         0|            0|            0|  0.00%|    def _break(self):
   702|         0|            0|            0|  0.00%|        # An internal error was detected.  The barrier is set to
   703|         0|            0|            0|  0.00%|        # a broken state all parties awakened.
   704|         0|            0|            0|  0.00%|        self._state = -2
   705|         0|            0|            0|  0.00%|        self._cond.notify_all()
   706|         0|            0|            0|  0.00%|
   707|         0|            0|            0|  0.00%|    @property
   708|         0|            0|            0|  0.00%|    def parties(self):
   709|         0|            0|            0|  0.00%|        """Return the number of threads required to trip the barrier."""
   710|         0|            0|            0|  0.00%|        return self._parties
   711|         0|            0|            0|  0.00%|
   712|         0|            0|            0|  0.00%|    @property
   713|         0|            0|            0|  0.00%|    def n_waiting(self):
   714|         0|            0|            0|  0.00%|        """Return the number of threads currently waiting at the barrier."""
   715|         0|            0|            0|  0.00%|        # We don't need synchronization here since this is an ephemeral result
   716|         0|            0|            0|  0.00%|        # anyway.  It returns the correct value in the steady state.
   717|         0|            0|            0|  0.00%|        if self._state == 0:
   718|         0|            0|            0|  0.00%|            return self._count
   719|         0|            0|            0|  0.00%|        return 0
   720|         0|            0|            0|  0.00%|
   721|         0|            0|            0|  0.00%|    @property
   722|         0|            0|            0|  0.00%|    def broken(self):
   723|         0|            0|            0|  0.00%|        """Return True if the barrier is in a broken state."""
   724|         0|            0|            0|  0.00%|        return self._state == -2
   725|         0|            0|            0|  0.00%|
   726|         0|            0|            0|  0.00%|# exception raised by the Barrier class
   727|         0|            0|            0|  0.00%|class BrokenBarrierError(RuntimeError):
   728|         0|            0|            0|  0.00%|    pass
   729|         0|            0|            0|  0.00%|
   730|         0|            0|            0|  0.00%|
   731|         0|            0|            0|  0.00%|# Helper to generate new thread names
   732|         0|            0|            0|  0.00%|_counter = _count().__next__
   733|         0|            0|            0|  0.00%|_counter() # Consume 0 so first non-main thread has id 1.
   734|         0|            0|            0|  0.00%|def _newname(template="Thread-%d"):
   735|         0|            0|            0|  0.00%|    return template % _counter()
   736|         0|            0|            0|  0.00%|
   737|         0|            0|            0|  0.00%|# Active thread administration
   738|         0|            0|            0|  0.00%|_active_limbo_lock = _allocate_lock()
   739|         0|            0|            0|  0.00%|_active = {}    # maps thread id to Thread object
   740|         0|            0|            0|  0.00%|_limbo = {}
   741|         0|            0|            0|  0.00%|_dangling = WeakSet()
   742|         0|            0|            0|  0.00%|# Set of Thread._tstate_lock locks of non-daemon threads used by _shutdown()
   743|         0|            0|            0|  0.00%|# to wait until all Python thread states get deleted:
   744|         0|            0|            0|  0.00%|# see Thread._set_tstate_lock().
   745|         0|            0|            0|  0.00%|_shutdown_locks_lock = _allocate_lock()
   746|         0|            0|            0|  0.00%|_shutdown_locks = set()
   747|         0|            0|            0|  0.00%|
   748|         0|            0|            0|  0.00%|# Main class for threads
   749|         0|            0|            0|  0.00%|
   750|         0|            0|            0|  0.00%|class Thread:
   751|         0|            0|            0|  0.00%|    """A class that represents a thread of control.
   752|         0|            0|            0|  0.00%|
   753|         0|            0|            0|  0.00%|    This class can be safely subclassed in a limited fashion. There are two ways
   754|         0|            0|            0|  0.00%|    to specify the activity: by passing a callable object to the constructor, or
   755|         0|            0|            0|  0.00%|    by overriding the run() method in a subclass.
   756|         0|            0|            0|  0.00%|
   757|         0|            0|            0|  0.00%|    """
   758|         0|            0|            0|  0.00%|
   759|         0|            0|            0|  0.00%|    _initialized = False
   760|         0|            0|            0|  0.00%|
   761|         8|  0.000100136|   1.2517e-05|  0.00%|    def __init__(self, group=None, target=None, name=None,
   762|         0|            0|            0|  0.00%|                 args=(), kwargs=None, *, daemon=None):
   763|         0|            0|            0|  0.00%|        """This constructor should always be called with keyword arguments. Arguments are:
   764|         0|            0|            0|  0.00%|
   765|         0|            0|            0|  0.00%|        *group* should be None; reserved for future extension when a ThreadGroup
   766|         0|            0|            0|  0.00%|        class is implemented.
   767|         0|            0|            0|  0.00%|
   768|         0|            0|            0|  0.00%|        *target* is the callable object to be invoked by the run()
   769|         0|            0|            0|  0.00%|        method. Defaults to None, meaning nothing is called.
   770|         0|            0|            0|  0.00%|
   771|         0|            0|            0|  0.00%|        *name* is the thread name. By default, a unique name is constructed of
   772|         0|            0|            0|  0.00%|        the form "Thread-N" where N is a small decimal number.
   773|         0|            0|            0|  0.00%|
   774|         0|            0|            0|  0.00%|        *args* is the argument tuple for the target invocation. Defaults to ().
   775|         0|            0|            0|  0.00%|
   776|         0|            0|            0|  0.00%|        *kwargs* is a dictionary of keyword arguments for the target
   777|         0|            0|            0|  0.00%|        invocation. Defaults to {}.
   778|         0|            0|            0|  0.00%|
   779|         0|            0|            0|  0.00%|        If a subclass overrides the constructor, it must make sure to invoke
   780|         0|            0|            0|  0.00%|        the base class constructor (Thread.__init__()) before doing anything
   781|         0|            0|            0|  0.00%|        else to the thread.
   782|         0|            0|            0|  0.00%|
   783|         0|            0|            0|  0.00%|        """
   784|         8|  4.22001e-05|  5.27501e-06|  0.00%|        assert group is None, "group argument must be None for now"
   785|         8|   3.3617e-05|  4.20213e-06|  0.00%|        if kwargs is None:
   786|         8|   0.00014925|  1.86563e-05|  0.00%|            kwargs = {}
   787|         8|   5.4121e-05|  6.76513e-06|  0.00%|        self._target = target
   788|         8|  5.05447e-05|  6.31809e-06|  0.00%|        self._name = str(name or _newname())
   789|         8|  3.45707e-05|  4.32134e-06|  0.00%|        self._args = args
   790|         8|  3.45707e-05|  4.32134e-06|  0.00%|        self._kwargs = kwargs
   791|         8|  3.33786e-05|  4.17233e-06|  0.00%|        if daemon is not None:
   792|         0|            0|            0|  0.00%|            self._daemonic = daemon
   793|         0|            0|            0|  0.00%|        else:
   794|         8|  0.000225306|  2.81632e-05|  0.00%|            self._daemonic = current_thread().daemon
(call)|         8|  0.000127554|  1.59442e-05|  0.00%|# /opt/conda/lib/python3.8/threading.py:1306 current_thread
(call)|         8|  0.000122309|  1.52886e-05|  0.00%|# /opt/conda/lib/python3.8/threading.py:1095 daemon
   795|         8|  4.45843e-05|  5.57303e-06|  0.00%|        self._ident = None
   796|         8|  3.48091e-05|  4.35114e-06|  0.00%|        if _HAVE_THREAD_NATIVE_ID:
   797|         8|  4.14848e-05|   5.1856e-06|  0.00%|            self._native_id = None
   798|         8|  5.53131e-05|  6.91414e-06|  0.00%|        self._tstate_lock = None
   799|         8|  0.000176191|  2.20239e-05|  0.00%|        self._started = Event()
(call)|         8|  0.000985622|  0.000123203|  0.00%|# /opt/conda/lib/python3.8/threading.py:505 __init__
   800|         8|  3.93391e-05|  4.91738e-06|  0.00%|        self._is_stopped = False
   801|         8|  0.000102758|  1.28448e-05|  0.00%|        self._initialized = True
   802|         0|            0|            0|  0.00%|        # Copy of sys.stderr used by self._invoke_excepthook()
   803|         8|   8.9407e-05|  1.11759e-05|  0.00%|        self._stderr = _sys.stderr
   804|         8|  0.000194788|  2.43485e-05|  0.00%|        self._invoke_excepthook = _make_invoke_excepthook()
(call)|         8|  0.000544548|  6.80685e-05|  0.00%|# /opt/conda/lib/python3.8/threading.py:1177 _make_invoke_excepthook
   805|         0|            0|            0|  0.00%|        # For debugging and _after_fork()
   806|         8|  0.000156879|  1.96099e-05|  0.00%|        _dangling.add(self)
(call)|         8|  0.000193357|  2.41697e-05|  0.00%|# /opt/conda/lib/python3.8/_weakrefset.py:81 add
   807|         0|            0|            0|  0.00%|
   808|         0|            0|            0|  0.00%|    def _reset_internal_locks(self, is_alive):
   809|         0|            0|            0|  0.00%|        # private!  Called by _after_fork() to reset our internal locks as
   810|         0|            0|            0|  0.00%|        # they may be in an invalid state leading to a deadlock or crash.
   811|         0|            0|            0|  0.00%|        self._started._reset_internal_locks()
   812|         0|            0|            0|  0.00%|        if is_alive:
   813|         0|            0|            0|  0.00%|            self._set_tstate_lock()
   814|         0|            0|            0|  0.00%|        else:
   815|         0|            0|            0|  0.00%|            # The thread isn't alive after fork: it doesn't have a tstate
   816|         0|            0|            0|  0.00%|            # anymore.
   817|         0|            0|            0|  0.00%|            self._is_stopped = True
   818|         0|            0|            0|  0.00%|            self._tstate_lock = None
   819|         0|            0|            0|  0.00%|
   820|         0|            0|            0|  0.00%|    def __repr__(self):
   821|         0|            0|            0|  0.00%|        assert self._initialized, "Thread.__init__() was not called"
   822|         0|            0|            0|  0.00%|        status = "initial"
   823|         0|            0|            0|  0.00%|        if self._started.is_set():
   824|         0|            0|            0|  0.00%|            status = "started"
   825|         0|            0|            0|  0.00%|        self.is_alive() # easy way to get ._is_stopped set when appropriate
   826|         0|            0|            0|  0.00%|        if self._is_stopped:
   827|         0|            0|            0|  0.00%|            status = "stopped"
   828|         0|            0|            0|  0.00%|        if self._daemonic:
   829|         0|            0|            0|  0.00%|            status += " daemon"
   830|         0|            0|            0|  0.00%|        if self._ident is not None:
   831|         0|            0|            0|  0.00%|            status += " %s" % self._ident
   832|         0|            0|            0|  0.00%|        return "<%s(%s, %s)>" % (self.__class__.__name__, self._name, status)
   833|         0|            0|            0|  0.00%|
   834|         8|  5.26905e-05|  6.58631e-06|  0.00%|    def start(self):
   835|         0|            0|            0|  0.00%|        """Start the thread's activity.
   836|         0|            0|            0|  0.00%|
   837|         0|            0|            0|  0.00%|        It must be called at most once per thread object. It arranges for the
   838|         0|            0|            0|  0.00%|        object's run() method to be invoked in a separate thread of control.
   839|         0|            0|            0|  0.00%|
   840|         0|            0|            0|  0.00%|        This method will raise a RuntimeError if called more than once on the
   841|         0|            0|            0|  0.00%|        same thread object.
   842|         0|            0|            0|  0.00%|
   843|         0|            0|            0|  0.00%|        """
   844|         8|  3.62396e-05|  4.52995e-06|  0.00%|        if not self._initialized:
   845|         0|            0|            0|  0.00%|            raise RuntimeError("thread.__init__() not called")
   846|         0|            0|            0|  0.00%|
   847|         8|  9.32217e-05|  1.16527e-05|  0.00%|        if self._started.is_set():
(call)|         8|  6.36578e-05|  7.95722e-06|  0.00%|# /opt/conda/lib/python3.8/threading.py:513 is_set
   848|         0|            0|            0|  0.00%|            raise RuntimeError("threads can only be started once")
   849|         8|   7.1764e-05|   8.9705e-06|  0.00%|        with _active_limbo_lock:
   850|         8|  5.36442e-05|  6.70552e-06|  0.00%|            _limbo[self] = self
   851|         8|   3.3617e-05|  4.20213e-06|  0.00%|        try:
   852|         8|   0.00066185|  8.27312e-05|  0.00%|            _start_new_thread(self._bootstrap, ())
   853|         0|            0|            0|  0.00%|        except Exception:
   854|         0|            0|            0|  0.00%|            with _active_limbo_lock:
   855|         0|            0|            0|  0.00%|                del _limbo[self]
   856|         0|            0|            0|  0.00%|            raise
   857|         8|  0.000150204|  1.87755e-05|  0.00%|        self._started.wait()
(call)|         8|   0.00528026|  0.000660032|  0.01%|# /opt/conda/lib/python3.8/threading.py:540 wait
   858|         0|            0|            0|  0.00%|
   859|         0|            0|            0|  0.00%|    def run(self):
   860|         0|            0|            0|  0.00%|        """Method representing the thread's activity.
   861|         0|            0|            0|  0.00%|
   862|         0|            0|            0|  0.00%|        You may override this method in a subclass. The standard run() method
   863|         0|            0|            0|  0.00%|        invokes the callable object passed to the object's constructor as the
   864|         0|            0|            0|  0.00%|        target argument, if any, with sequential and keyword arguments taken
   865|         0|            0|            0|  0.00%|        from the args and kwargs arguments, respectively.
   866|         0|            0|            0|  0.00%|
   867|         0|            0|            0|  0.00%|        """
   868|         0|            0|            0|  0.00%|        try:
   869|         0|            0|            0|  0.00%|            if self._target:
   870|         0|            0|            0|  0.00%|                self._target(*self._args, **self._kwargs)
   871|         0|            0|            0|  0.00%|        finally:
   872|         0|            0|            0|  0.00%|            # Avoid a refcycle if the thread is running a function with
   873|         0|            0|            0|  0.00%|            # an argument that has a member that points to the thread.
   874|         0|            0|            0|  0.00%|            del self._target, self._args, self._kwargs
   875|         0|            0|            0|  0.00%|
   876|         0|            0|            0|  0.00%|    def _bootstrap(self):
   877|         0|            0|            0|  0.00%|        # Wrapper around the real bootstrap code that ignores
   878|         0|            0|            0|  0.00%|        # exceptions during interpreter cleanup.  Those typically
   879|         0|            0|            0|  0.00%|        # happen when a daemon thread wakes up at an unfortunate
   880|         0|            0|            0|  0.00%|        # moment, finds the world around it destroyed, and raises some
   881|         0|            0|            0|  0.00%|        # random exception *** while trying to report the exception in
   882|         0|            0|            0|  0.00%|        # _bootstrap_inner() below ***.  Those random exceptions
   883|         0|            0|            0|  0.00%|        # don't help anybody, and they confuse users, so we suppress
   884|         0|            0|            0|  0.00%|        # them.  We suppress them only when it appears that the world
   885|         0|            0|            0|  0.00%|        # indeed has already been destroyed, so that exceptions in
   886|         0|            0|            0|  0.00%|        # _bootstrap_inner() during normal business hours are properly
   887|         0|            0|            0|  0.00%|        # reported.  Also, we only suppress them for daemonic threads;
   888|         0|            0|            0|  0.00%|        # if a non-daemonic encounters this, something else is wrong.
   889|         0|            0|            0|  0.00%|        try:
   890|         0|            0|            0|  0.00%|            self._bootstrap_inner()
   891|         0|            0|            0|  0.00%|        except:
   892|         0|            0|            0|  0.00%|            if self._daemonic and _sys is None:
   893|         0|            0|            0|  0.00%|                return
   894|         0|            0|            0|  0.00%|            raise
   895|         0|            0|            0|  0.00%|
   896|         0|            0|            0|  0.00%|    def _set_ident(self):
   897|         0|            0|            0|  0.00%|        self._ident = get_ident()
   898|         0|            0|            0|  0.00%|
   899|         0|            0|            0|  0.00%|    if _HAVE_THREAD_NATIVE_ID:
   900|         0|            0|            0|  0.00%|        def _set_native_id(self):
   901|         0|            0|            0|  0.00%|            self._native_id = get_native_id()
   902|         0|            0|            0|  0.00%|
   903|         0|            0|            0|  0.00%|    def _set_tstate_lock(self):
   904|         0|            0|            0|  0.00%|        """
   905|         0|            0|            0|  0.00%|        Set a lock object which will be released by the interpreter when
   906|         0|            0|            0|  0.00%|        the underlying thread state (see pystate.h) gets deleted.
   907|         0|            0|            0|  0.00%|        """
   908|         0|            0|            0|  0.00%|        self._tstate_lock = _set_sentinel()
   909|         0|            0|            0|  0.00%|        self._tstate_lock.acquire()
   910|         0|            0|            0|  0.00%|
   911|         0|            0|            0|  0.00%|        if not self.daemon:
   912|         0|            0|            0|  0.00%|            with _shutdown_locks_lock:
   913|         0|            0|            0|  0.00%|                _shutdown_locks.add(self._tstate_lock)
   914|         0|            0|            0|  0.00%|
   915|         0|            0|            0|  0.00%|    def _bootstrap_inner(self):
   916|         0|            0|            0|  0.00%|        try:
   917|         0|            0|            0|  0.00%|            self._set_ident()
   918|         0|            0|            0|  0.00%|            self._set_tstate_lock()
   919|         0|            0|            0|  0.00%|            if _HAVE_THREAD_NATIVE_ID:
   920|         0|            0|            0|  0.00%|                self._set_native_id()
   921|         0|            0|            0|  0.00%|            self._started.set()
   922|         0|            0|            0|  0.00%|            with _active_limbo_lock:
   923|         0|            0|            0|  0.00%|                _active[self._ident] = self
   924|         0|            0|            0|  0.00%|                del _limbo[self]
   925|         0|            0|            0|  0.00%|
   926|         0|            0|            0|  0.00%|            if _trace_hook:
   927|         0|            0|            0|  0.00%|                _sys.settrace(_trace_hook)
   928|         0|            0|            0|  0.00%|            if _profile_hook:
   929|         0|            0|            0|  0.00%|                _sys.setprofile(_profile_hook)
   930|         0|            0|            0|  0.00%|
   931|         0|            0|            0|  0.00%|            try:
   932|         0|            0|            0|  0.00%|                self.run()
   933|         0|            0|            0|  0.00%|            except:
   934|         0|            0|            0|  0.00%|                self._invoke_excepthook(self)
   935|         0|            0|            0|  0.00%|        finally:
   936|         0|            0|            0|  0.00%|            with _active_limbo_lock:
   937|         0|            0|            0|  0.00%|                try:
   938|         0|            0|            0|  0.00%|                    # We don't call self._delete() because it also
   939|         0|            0|            0|  0.00%|                    # grabs _active_limbo_lock.
   940|         0|            0|            0|  0.00%|                    del _active[get_ident()]
   941|         0|            0|            0|  0.00%|                except:
   942|         0|            0|            0|  0.00%|                    pass
   943|         0|            0|            0|  0.00%|
   944|         0|            0|            0|  0.00%|    def _stop(self):
   945|         0|            0|            0|  0.00%|        # After calling ._stop(), .is_alive() returns False and .join() returns
   946|         0|            0|            0|  0.00%|        # immediately.  ._tstate_lock must be released before calling ._stop().
   947|         0|            0|            0|  0.00%|        #
   948|         0|            0|            0|  0.00%|        # Normal case:  C code at the end of the thread's life
   949|         0|            0|            0|  0.00%|        # (release_sentinel in _threadmodule.c) releases ._tstate_lock, and
   950|         0|            0|            0|  0.00%|        # that's detected by our ._wait_for_tstate_lock(), called by .join()
   951|         0|            0|            0|  0.00%|        # and .is_alive().  Any number of threads _may_ call ._stop()
   952|         0|            0|            0|  0.00%|        # simultaneously (for example, if multiple threads are blocked in
   953|         0|            0|            0|  0.00%|        # .join() calls), and they're not serialized.  That's harmless -
   954|         0|            0|            0|  0.00%|        # they'll just make redundant rebindings of ._is_stopped and
   955|         0|            0|            0|  0.00%|        # ._tstate_lock.  Obscure:  we rebind ._tstate_lock last so that the
   956|         0|            0|            0|  0.00%|        # "assert self._is_stopped" in ._wait_for_tstate_lock() always works
   957|         0|            0|            0|  0.00%|        # (the assert is executed only if ._tstate_lock is None).
   958|         0|            0|            0|  0.00%|        #
   959|         0|            0|            0|  0.00%|        # Special case:  _main_thread releases ._tstate_lock via this
   960|         0|            0|            0|  0.00%|        # module's _shutdown() function.
   961|         0|            0|            0|  0.00%|        lock = self._tstate_lock
   962|         0|            0|            0|  0.00%|        if lock is not None:
   963|         0|            0|            0|  0.00%|            assert not lock.locked()
   964|         0|            0|            0|  0.00%|        self._is_stopped = True
   965|         0|            0|            0|  0.00%|        self._tstate_lock = None
   966|         0|            0|            0|  0.00%|        if not self.daemon:
   967|         0|            0|            0|  0.00%|            with _shutdown_locks_lock:
   968|         0|            0|            0|  0.00%|                _shutdown_locks.discard(lock)
   969|         0|            0|            0|  0.00%|
   970|         0|            0|            0|  0.00%|    def _delete(self):
   971|         0|            0|            0|  0.00%|        "Remove current thread from the dict of currently running threads."
   972|         0|            0|            0|  0.00%|        with _active_limbo_lock:
   973|         0|            0|            0|  0.00%|            del _active[get_ident()]
   974|         0|            0|            0|  0.00%|            # There must not be any python code between the previous line
   975|         0|            0|            0|  0.00%|            # and after the lock is released.  Otherwise a tracing function
   976|         0|            0|            0|  0.00%|            # could try to acquire the lock again in the same thread, (in
   977|         0|            0|            0|  0.00%|            # current_thread()), and would block.
   978|         0|            0|            0|  0.00%|
   979|         0|            0|            0|  0.00%|    def join(self, timeout=None):
   980|         0|            0|            0|  0.00%|        """Wait until the thread terminates.
   981|         0|            0|            0|  0.00%|
   982|         0|            0|            0|  0.00%|        This blocks the calling thread until the thread whose join() method is
   983|         0|            0|            0|  0.00%|        called terminates -- either normally or through an unhandled exception
   984|         0|            0|            0|  0.00%|        or until the optional timeout occurs.
   985|         0|            0|            0|  0.00%|
   986|         0|            0|            0|  0.00%|        When the timeout argument is present and not None, it should be a
   987|         0|            0|            0|  0.00%|        floating point number specifying a timeout for the operation in seconds
   988|         0|            0|            0|  0.00%|        (or fractions thereof). As join() always returns None, you must call
   989|         0|            0|            0|  0.00%|        is_alive() after join() to decide whether a timeout happened -- if the
   990|         0|            0|            0|  0.00%|        thread is still alive, the join() call timed out.
   991|         0|            0|            0|  0.00%|
   992|         0|            0|            0|  0.00%|        When the timeout argument is not present or None, the operation will
   993|         0|            0|            0|  0.00%|        block until the thread terminates.
   994|         0|            0|            0|  0.00%|
   995|         0|            0|            0|  0.00%|        A thread can be join()ed many times.
   996|         0|            0|            0|  0.00%|
   997|         0|            0|            0|  0.00%|        join() raises a RuntimeError if an attempt is made to join the current
   998|         0|            0|            0|  0.00%|        thread as that would cause a deadlock. It is also an error to join() a
   999|         0|            0|            0|  0.00%|        thread before it has been started and attempts to do so raises the same
  1000|         0|            0|            0|  0.00%|        exception.
  1001|         0|            0|            0|  0.00%|
  1002|         0|            0|            0|  0.00%|        """
  1003|         0|            0|            0|  0.00%|        if not self._initialized:
  1004|         0|            0|            0|  0.00%|            raise RuntimeError("Thread.__init__() not called")
  1005|         0|            0|            0|  0.00%|        if not self._started.is_set():
  1006|         0|            0|            0|  0.00%|            raise RuntimeError("cannot join thread before it is started")
  1007|         0|            0|            0|  0.00%|        if self is current_thread():
  1008|         0|            0|            0|  0.00%|            raise RuntimeError("cannot join current thread")
  1009|         0|            0|            0|  0.00%|
  1010|         0|            0|            0|  0.00%|        if timeout is None:
  1011|         0|            0|            0|  0.00%|            self._wait_for_tstate_lock()
  1012|         0|            0|            0|  0.00%|        else:
  1013|         0|            0|            0|  0.00%|            # the behavior of a negative timeout isn't documented, but
  1014|         0|            0|            0|  0.00%|            # historically .join(timeout=x) for x<0 has acted as if timeout=0
  1015|         0|            0|            0|  0.00%|            self._wait_for_tstate_lock(timeout=max(timeout, 0))
  1016|         0|            0|            0|  0.00%|
  1017|         0|            0|            0|  0.00%|    def _wait_for_tstate_lock(self, block=True, timeout=-1):
  1018|         0|            0|            0|  0.00%|        # Issue #18808: wait for the thread state to be gone.
  1019|         0|            0|            0|  0.00%|        # At the end of the thread's life, after all knowledge of the thread
  1020|         0|            0|            0|  0.00%|        # is removed from C data structures, C code releases our _tstate_lock.
  1021|         0|            0|            0|  0.00%|        # This method passes its arguments to _tstate_lock.acquire().
  1022|         0|            0|            0|  0.00%|        # If the lock is acquired, the C code is done, and self._stop() is
  1023|         0|            0|            0|  0.00%|        # called.  That sets ._is_stopped to True, and ._tstate_lock to None.
  1024|         0|            0|            0|  0.00%|        lock = self._tstate_lock
  1025|         0|            0|            0|  0.00%|        if lock is None:  # already determined that the C code is done
  1026|         0|            0|            0|  0.00%|            assert self._is_stopped
  1027|         0|            0|            0|  0.00%|        elif lock.acquire(block, timeout):
  1028|         0|            0|            0|  0.00%|            lock.release()
  1029|         0|            0|            0|  0.00%|            self._stop()
  1030|         0|            0|            0|  0.00%|
  1031|         0|            0|            0|  0.00%|    @property
  1032|         0|            0|            0|  0.00%|    def name(self):
  1033|         0|            0|            0|  0.00%|        """A string used for identification purposes only.
  1034|         0|            0|            0|  0.00%|
  1035|         0|            0|            0|  0.00%|        It has no semantics. Multiple threads may be given the same name. The
  1036|         0|            0|            0|  0.00%|        initial name is set by the constructor.
  1037|         0|            0|            0|  0.00%|
  1038|         0|            0|            0|  0.00%|        """
  1039|         0|            0|            0|  0.00%|        assert self._initialized, "Thread.__init__() not called"
  1040|         0|            0|            0|  0.00%|        return self._name
  1041|         0|            0|            0|  0.00%|
  1042|         0|            0|            0|  0.00%|    @name.setter
  1043|         0|            0|            0|  0.00%|    def name(self, name):
  1044|         0|            0|            0|  0.00%|        assert self._initialized, "Thread.__init__() not called"
  1045|         0|            0|            0|  0.00%|        self._name = str(name)
  1046|         0|            0|            0|  0.00%|
  1047|         0|            0|            0|  0.00%|    @property
  1048|         0|            0|            0|  0.00%|    def ident(self):
  1049|         0|            0|            0|  0.00%|        """Thread identifier of this thread or None if it has not been started.
  1050|         0|            0|            0|  0.00%|
  1051|         0|            0|            0|  0.00%|        This is a nonzero integer. See the get_ident() function. Thread
  1052|         0|            0|            0|  0.00%|        identifiers may be recycled when a thread exits and another thread is
  1053|         0|            0|            0|  0.00%|        created. The identifier is available even after the thread has exited.
  1054|         0|            0|            0|  0.00%|
  1055|         0|            0|            0|  0.00%|        """
  1056|         0|            0|            0|  0.00%|        assert self._initialized, "Thread.__init__() not called"
  1057|         0|            0|            0|  0.00%|        return self._ident
  1058|         0|            0|            0|  0.00%|
  1059|         0|            0|            0|  0.00%|    if _HAVE_THREAD_NATIVE_ID:
  1060|         0|            0|            0|  0.00%|        @property
  1061|         0|            0|            0|  0.00%|        def native_id(self):
  1062|         0|            0|            0|  0.00%|            """Native integral thread ID of this thread, or None if it has not been started.
  1063|         0|            0|            0|  0.00%|
  1064|         0|            0|            0|  0.00%|            This is a non-negative integer. See the get_native_id() function.
  1065|         0|            0|            0|  0.00%|            This represents the Thread ID as reported by the kernel.
  1066|         0|            0|            0|  0.00%|
  1067|         0|            0|            0|  0.00%|            """
  1068|         0|            0|            0|  0.00%|            assert self._initialized, "Thread.__init__() not called"
  1069|         0|            0|            0|  0.00%|            return self._native_id
  1070|         0|            0|            0|  0.00%|
  1071|         0|            0|            0|  0.00%|    def is_alive(self):
  1072|         0|            0|            0|  0.00%|        """Return whether the thread is alive.
  1073|         0|            0|            0|  0.00%|
  1074|         0|            0|            0|  0.00%|        This method returns True just before the run() method starts until just
  1075|         0|            0|            0|  0.00%|        after the run() method terminates. The module function enumerate()
  1076|         0|            0|            0|  0.00%|        returns a list of all alive threads.
  1077|         0|            0|            0|  0.00%|
  1078|         0|            0|            0|  0.00%|        """
  1079|         0|            0|            0|  0.00%|        assert self._initialized, "Thread.__init__() not called"
  1080|         0|            0|            0|  0.00%|        if self._is_stopped or not self._started.is_set():
  1081|         0|            0|            0|  0.00%|            return False
  1082|         0|            0|            0|  0.00%|        self._wait_for_tstate_lock(False)
  1083|         0|            0|            0|  0.00%|        return not self._is_stopped
  1084|         0|            0|            0|  0.00%|
  1085|         0|            0|            0|  0.00%|    def isAlive(self):
  1086|         0|            0|            0|  0.00%|        """Return whether the thread is alive.
  1087|         0|            0|            0|  0.00%|
  1088|         0|            0|            0|  0.00%|        This method is deprecated, use is_alive() instead.
  1089|         0|            0|            0|  0.00%|        """
  1090|         0|            0|            0|  0.00%|        import warnings
  1091|         0|            0|            0|  0.00%|        warnings.warn('isAlive() is deprecated, use is_alive() instead',
  1092|         0|            0|            0|  0.00%|                      DeprecationWarning, stacklevel=2)
  1093|         0|            0|            0|  0.00%|        return self.is_alive()
  1094|         0|            0|            0|  0.00%|
  1095|         8|  5.36442e-05|  6.70552e-06|  0.00%|    @property
  1096|         0|            0|            0|  0.00%|    def daemon(self):
  1097|         0|            0|            0|  0.00%|        """A boolean value indicating whether this thread is a daemon thread.
  1098|         0|            0|            0|  0.00%|
  1099|         0|            0|            0|  0.00%|        This must be set before start() is called, otherwise RuntimeError is
  1100|         0|            0|            0|  0.00%|        raised. Its initial value is inherited from the creating thread; the
  1101|         0|            0|            0|  0.00%|        main thread is not a daemon thread and therefore all threads created in
  1102|         0|            0|            0|  0.00%|        the main thread default to daemon = False.
  1103|         0|            0|            0|  0.00%|
  1104|         0|            0|            0|  0.00%|        The entire Python program exits when only daemon threads are left.
  1105|         0|            0|            0|  0.00%|
  1106|         0|            0|            0|  0.00%|        """
  1107|         8|  3.69549e-05|  4.61936e-06|  0.00%|        assert self._initialized, "Thread.__init__() not called"
  1108|         8|  3.17097e-05|  3.96371e-06|  0.00%|        return self._daemonic
  1109|         0|            0|            0|  0.00%|
  1110|         8|  5.34058e-05|  6.67572e-06|  0.00%|    @daemon.setter
  1111|         0|            0|            0|  0.00%|    def daemon(self, daemonic):
  1112|         8|  3.62396e-05|  4.52995e-06|  0.00%|        if not self._initialized:
  1113|         0|            0|            0|  0.00%|            raise RuntimeError("Thread.__init__() not called")
  1114|         8|  0.000117779|  1.47223e-05|  0.00%|        if self._started.is_set():
(call)|         8|  8.67844e-05|   1.0848e-05|  0.00%|# /opt/conda/lib/python3.8/threading.py:513 is_set
  1115|         0|            0|            0|  0.00%|            raise RuntimeError("cannot set daemon status of active thread")
  1116|         8|  3.45707e-05|  4.32134e-06|  0.00%|        self._daemonic = daemonic
  1117|         0|            0|            0|  0.00%|
  1118|         0|            0|            0|  0.00%|    def isDaemon(self):
  1119|         0|            0|            0|  0.00%|        return self.daemon
  1120|         0|            0|            0|  0.00%|
  1121|         0|            0|            0|  0.00%|    def setDaemon(self, daemonic):
  1122|         0|            0|            0|  0.00%|        self.daemon = daemonic
  1123|         0|            0|            0|  0.00%|
  1124|         0|            0|            0|  0.00%|    def getName(self):
  1125|         0|            0|            0|  0.00%|        return self.name
  1126|         0|            0|            0|  0.00%|
  1127|         0|            0|            0|  0.00%|    def setName(self, name):
  1128|         0|            0|            0|  0.00%|        self.name = name
  1129|         0|            0|            0|  0.00%|
  1130|         0|            0|            0|  0.00%|
  1131|         0|            0|            0|  0.00%|try:
  1132|         0|            0|            0|  0.00%|    from _thread import (_excepthook as excepthook,
  1133|         0|            0|            0|  0.00%|                         _ExceptHookArgs as ExceptHookArgs)
  1134|         0|            0|            0|  0.00%|except ImportError:
  1135|         0|            0|            0|  0.00%|    # Simple Python implementation if _thread._excepthook() is not available
  1136|         0|            0|            0|  0.00%|    from traceback import print_exception as _print_exception
  1137|         0|            0|            0|  0.00%|    from collections import namedtuple
  1138|         0|            0|            0|  0.00%|
  1139|         0|            0|            0|  0.00%|    _ExceptHookArgs = namedtuple(
  1140|         0|            0|            0|  0.00%|        'ExceptHookArgs',
  1141|         0|            0|            0|  0.00%|        'exc_type exc_value exc_traceback thread')
  1142|         0|            0|            0|  0.00%|
  1143|         0|            0|            0|  0.00%|    def ExceptHookArgs(args):
  1144|         0|            0|            0|  0.00%|        return _ExceptHookArgs(*args)
  1145|         0|            0|            0|  0.00%|
  1146|         0|            0|            0|  0.00%|    def excepthook(args, /):
  1147|         0|            0|            0|  0.00%|        """
  1148|         0|            0|            0|  0.00%|        Handle uncaught Thread.run() exception.
  1149|         0|            0|            0|  0.00%|        """
  1150|         0|            0|            0|  0.00%|        if args.exc_type == SystemExit:
  1151|         0|            0|            0|  0.00%|            # silently ignore SystemExit
  1152|         0|            0|            0|  0.00%|            return
  1153|         0|            0|            0|  0.00%|
  1154|         0|            0|            0|  0.00%|        if _sys is not None and _sys.stderr is not None:
  1155|         0|            0|            0|  0.00%|            stderr = _sys.stderr
  1156|         0|            0|            0|  0.00%|        elif args.thread is not None:
  1157|         0|            0|            0|  0.00%|            stderr = args.thread._stderr
  1158|         0|            0|            0|  0.00%|            if stderr is None:
  1159|         0|            0|            0|  0.00%|                # do nothing if sys.stderr is None and sys.stderr was None
  1160|         0|            0|            0|  0.00%|                # when the thread was created
  1161|         0|            0|            0|  0.00%|                return
  1162|         0|            0|            0|  0.00%|        else:
  1163|         0|            0|            0|  0.00%|            # do nothing if sys.stderr is None and args.thread is None
  1164|         0|            0|            0|  0.00%|            return
  1165|         0|            0|            0|  0.00%|
  1166|         0|            0|            0|  0.00%|        if args.thread is not None:
  1167|         0|            0|            0|  0.00%|            name = args.thread.name
  1168|         0|            0|            0|  0.00%|        else:
  1169|         0|            0|            0|  0.00%|            name = get_ident()
  1170|         0|            0|            0|  0.00%|        print(f"Exception in thread {name}:",
  1171|         0|            0|            0|  0.00%|              file=stderr, flush=True)
  1172|         0|            0|            0|  0.00%|        _print_exception(args.exc_type, args.exc_value, args.exc_traceback,
  1173|         0|            0|            0|  0.00%|                         file=stderr)
  1174|         0|            0|            0|  0.00%|        stderr.flush()
  1175|         0|            0|            0|  0.00%|
  1176|         0|            0|            0|  0.00%|
  1177|         8|  8.67844e-05|   1.0848e-05|  0.00%|def _make_invoke_excepthook():
  1178|         0|            0|            0|  0.00%|    # Create a local namespace to ensure that variables remain alive
  1179|         0|            0|            0|  0.00%|    # when _invoke_excepthook() is called, even if it is called late during
  1180|         0|            0|            0|  0.00%|    # Python shutdown. It is mostly needed for daemon threads.
  1181|         0|            0|            0|  0.00%|
  1182|         8|  6.10352e-05|  7.62939e-06|  0.00%|    old_excepthook = excepthook
  1183|         8|  7.89165e-05|  9.86457e-06|  0.00%|    old_sys_excepthook = _sys.excepthook
  1184|         8|  4.26769e-05|  5.33462e-06|  0.00%|    if old_excepthook is None:
  1185|         0|            0|            0|  0.00%|        raise RuntimeError("threading.excepthook is None")
  1186|         8|  3.95775e-05|  4.94719e-06|  0.00%|    if old_sys_excepthook is None:
  1187|         0|            0|            0|  0.00%|        raise RuntimeError("sys.excepthook is None")
  1188|         0|            0|            0|  0.00%|
  1189|         8|  4.00543e-05|  5.00679e-06|  0.00%|    sys_exc_info = _sys.exc_info
  1190|         8|   4.1008e-05|    5.126e-06|  0.00%|    local_print = print
  1191|         8|  3.91006e-05|  4.88758e-06|  0.00%|    local_sys = _sys
  1192|         0|            0|            0|  0.00%|
  1193|         8|  7.48634e-05|  9.35793e-06|  0.00%|    def invoke_excepthook(thread):
  1194|         0|            0|            0|  0.00%|        global excepthook
  1195|         0|            0|            0|  0.00%|        try:
  1196|         0|            0|            0|  0.00%|            hook = excepthook
  1197|         0|            0|            0|  0.00%|            if hook is None:
  1198|         0|            0|            0|  0.00%|                hook = old_excepthook
  1199|         0|            0|            0|  0.00%|
  1200|         0|            0|            0|  0.00%|            args = ExceptHookArgs([*sys_exc_info(), thread])
  1201|         0|            0|            0|  0.00%|
  1202|         0|            0|            0|  0.00%|            hook(args)
  1203|         0|            0|            0|  0.00%|        except Exception as exc:
  1204|         0|            0|            0|  0.00%|            exc.__suppress_context__ = True
  1205|         0|            0|            0|  0.00%|            del exc
  1206|         0|            0|            0|  0.00%|
  1207|         0|            0|            0|  0.00%|            if local_sys is not None and local_sys.stderr is not None:
  1208|         0|            0|            0|  0.00%|                stderr = local_sys.stderr
  1209|         0|            0|            0|  0.00%|            else:
  1210|         0|            0|            0|  0.00%|                stderr = thread._stderr
  1211|         0|            0|            0|  0.00%|
  1212|         0|            0|            0|  0.00%|            local_print("Exception in threading.excepthook:",
  1213|         0|            0|            0|  0.00%|                        file=stderr, flush=True)
  1214|         0|            0|            0|  0.00%|
  1215|         0|            0|            0|  0.00%|            if local_sys is not None and local_sys.excepthook is not None:
  1216|         0|            0|            0|  0.00%|                sys_excepthook = local_sys.excepthook
  1217|         0|            0|            0|  0.00%|            else:
  1218|         0|            0|            0|  0.00%|                sys_excepthook = old_sys_excepthook
  1219|         0|            0|            0|  0.00%|
  1220|         0|            0|            0|  0.00%|            sys_excepthook(*sys_exc_info())
  1221|         0|            0|            0|  0.00%|        finally:
  1222|         0|            0|            0|  0.00%|            # Break reference cycle (exception stored in a variable)
  1223|         0|            0|            0|  0.00%|            args = None
  1224|         0|            0|            0|  0.00%|
  1225|         8|  4.05312e-05|  5.06639e-06|  0.00%|    return invoke_excepthook
  1226|         0|            0|            0|  0.00%|
  1227|         0|            0|            0|  0.00%|
  1228|         0|            0|            0|  0.00%|# The timer class was contributed by Itamar Shtull-Trauring
  1229|         0|            0|            0|  0.00%|
  1230|         0|            0|            0|  0.00%|class Timer(Thread):
  1231|         0|            0|            0|  0.00%|    """Call a function after a specified number of seconds:
  1232|         0|            0|            0|  0.00%|
  1233|         0|            0|            0|  0.00%|            t = Timer(30.0, f, args=None, kwargs=None)
  1234|         0|            0|            0|  0.00%|            t.start()
  1235|         0|            0|            0|  0.00%|            t.cancel()     # stop the timer's action if it's still waiting
  1236|         0|            0|            0|  0.00%|
  1237|         0|            0|            0|  0.00%|    """
  1238|         0|            0|            0|  0.00%|
  1239|         0|            0|            0|  0.00%|    def __init__(self, interval, function, args=None, kwargs=None):
  1240|         0|            0|            0|  0.00%|        Thread.__init__(self)
  1241|         0|            0|            0|  0.00%|        self.interval = interval
  1242|         0|            0|            0|  0.00%|        self.function = function
  1243|         0|            0|            0|  0.00%|        self.args = args if args is not None else []
  1244|         0|            0|            0|  0.00%|        self.kwargs = kwargs if kwargs is not None else {}
  1245|         0|            0|            0|  0.00%|        self.finished = Event()
  1246|         0|            0|            0|  0.00%|
  1247|         0|            0|            0|  0.00%|    def cancel(self):
  1248|         0|            0|            0|  0.00%|        """Stop the timer if it hasn't finished yet."""
  1249|         0|            0|            0|  0.00%|        self.finished.set()
  1250|         0|            0|            0|  0.00%|
  1251|         0|            0|            0|  0.00%|    def run(self):
  1252|         0|            0|            0|  0.00%|        self.finished.wait(self.interval)
  1253|         0|            0|            0|  0.00%|        if not self.finished.is_set():
  1254|         0|            0|            0|  0.00%|            self.function(*self.args, **self.kwargs)
  1255|         0|            0|            0|  0.00%|        self.finished.set()
  1256|         0|            0|            0|  0.00%|
  1257|         0|            0|            0|  0.00%|
  1258|         0|            0|            0|  0.00%|# Special thread class to represent the main thread
  1259|         0|            0|            0|  0.00%|
  1260|         0|            0|            0|  0.00%|class _MainThread(Thread):
  1261|         0|            0|            0|  0.00%|
  1262|         0|            0|            0|  0.00%|    def __init__(self):
  1263|         0|            0|            0|  0.00%|        Thread.__init__(self, name="MainThread", daemon=False)
  1264|         0|            0|            0|  0.00%|        self._set_tstate_lock()
  1265|         0|            0|            0|  0.00%|        self._started.set()
  1266|         0|            0|            0|  0.00%|        self._set_ident()
  1267|         0|            0|            0|  0.00%|        if _HAVE_THREAD_NATIVE_ID:
  1268|         0|            0|            0|  0.00%|            self._set_native_id()
  1269|         0|            0|            0|  0.00%|        with _active_limbo_lock:
  1270|         0|            0|            0|  0.00%|            _active[self._ident] = self
  1271|         0|            0|            0|  0.00%|
  1272|         0|            0|            0|  0.00%|
  1273|         0|            0|            0|  0.00%|# Dummy thread class to represent threads not started here.
  1274|         0|            0|            0|  0.00%|# These aren't garbage collected when they die, nor can they be waited for.
  1275|         0|            0|            0|  0.00%|# If they invoke anything in threading.py that calls current_thread(), they
  1276|         0|            0|            0|  0.00%|# leave an entry in the _active dict forever after.
  1277|         0|            0|            0|  0.00%|# Their purpose is to return *something* from current_thread().
  1278|         0|            0|            0|  0.00%|# They are marked as daemon threads so we won't wait for them
  1279|         0|            0|            0|  0.00%|# when we exit (conform previous semantics).
  1280|         0|            0|            0|  0.00%|
  1281|         0|            0|            0|  0.00%|class _DummyThread(Thread):
  1282|         0|            0|            0|  0.00%|
  1283|         0|            0|            0|  0.00%|    def __init__(self):
  1284|         0|            0|            0|  0.00%|        Thread.__init__(self, name=_newname("Dummy-%d"), daemon=True)
  1285|         0|            0|            0|  0.00%|
  1286|         0|            0|            0|  0.00%|        self._started.set()
  1287|         0|            0|            0|  0.00%|        self._set_ident()
  1288|         0|            0|            0|  0.00%|        if _HAVE_THREAD_NATIVE_ID:
  1289|         0|            0|            0|  0.00%|            self._set_native_id()
  1290|         0|            0|            0|  0.00%|        with _active_limbo_lock:
  1291|         0|            0|            0|  0.00%|            _active[self._ident] = self
  1292|         0|            0|            0|  0.00%|
  1293|         0|            0|            0|  0.00%|    def _stop(self):
  1294|         0|            0|            0|  0.00%|        pass
  1295|         0|            0|            0|  0.00%|
  1296|         0|            0|            0|  0.00%|    def is_alive(self):
  1297|         0|            0|            0|  0.00%|        assert not self._is_stopped and self._started.is_set()
  1298|         0|            0|            0|  0.00%|        return True
  1299|         0|            0|            0|  0.00%|
  1300|         0|            0|            0|  0.00%|    def join(self, timeout=None):
  1301|         0|            0|            0|  0.00%|        assert False, "cannot join a dummy thread"
  1302|         0|            0|            0|  0.00%|
  1303|         0|            0|            0|  0.00%|
  1304|         0|            0|            0|  0.00%|# Global API functions
  1305|         0|            0|            0|  0.00%|
  1306|        10|  8.46386e-05|  8.46386e-06|  0.00%|def current_thread():
  1307|         0|            0|            0|  0.00%|    """Return the current Thread object, corresponding to the caller's thread of control.
  1308|         0|            0|            0|  0.00%|
  1309|         0|            0|            0|  0.00%|    If the caller's thread of control was not created through the threading
  1310|         0|            0|            0|  0.00%|    module, a dummy thread object with limited functionality is returned.
  1311|         0|            0|            0|  0.00%|
  1312|         0|            0|            0|  0.00%|    """
  1313|        10|  4.91142e-05|  4.91142e-06|  0.00%|    try:
  1314|        10|  0.000105619|  1.05619e-05|  0.00%|        return _active[get_ident()]
  1315|         0|            0|            0|  0.00%|    except KeyError:
  1316|         0|            0|            0|  0.00%|        return _DummyThread()
  1317|         0|            0|            0|  0.00%|
  1318|         0|            0|            0|  0.00%|currentThread = current_thread
  1319|         0|            0|            0|  0.00%|
  1320|         0|            0|            0|  0.00%|def active_count():
  1321|         0|            0|            0|  0.00%|    """Return the number of Thread objects currently alive.
  1322|         0|            0|            0|  0.00%|
  1323|         0|            0|            0|  0.00%|    The returned count is equal to the length of the list returned by
  1324|         0|            0|            0|  0.00%|    enumerate().
  1325|         0|            0|            0|  0.00%|
  1326|         0|            0|            0|  0.00%|    """
  1327|         0|            0|            0|  0.00%|    with _active_limbo_lock:
  1328|         0|            0|            0|  0.00%|        return len(_active) + len(_limbo)
  1329|         0|            0|            0|  0.00%|
  1330|         0|            0|            0|  0.00%|activeCount = active_count
  1331|         0|            0|            0|  0.00%|
  1332|         0|            0|            0|  0.00%|def _enumerate():
  1333|         0|            0|            0|  0.00%|    # Same as enumerate(), but without the lock. Internal use only.
  1334|         0|            0|            0|  0.00%|    return list(_active.values()) + list(_limbo.values())
  1335|         0|            0|            0|  0.00%|
  1336|         0|            0|            0|  0.00%|def enumerate():
  1337|         0|            0|            0|  0.00%|    """Return a list of all Thread objects currently alive.
  1338|         0|            0|            0|  0.00%|
  1339|         0|            0|            0|  0.00%|    The list includes daemonic threads, dummy thread objects created by
  1340|         0|            0|            0|  0.00%|    current_thread(), and the main thread. It excludes terminated threads and
  1341|         0|            0|            0|  0.00%|    threads that have not yet been started.
  1342|         0|            0|            0|  0.00%|
  1343|         0|            0|            0|  0.00%|    """
  1344|         0|            0|            0|  0.00%|    with _active_limbo_lock:
  1345|         0|            0|            0|  0.00%|        return list(_active.values()) + list(_limbo.values())
  1346|         0|            0|            0|  0.00%|
  1347|         0|            0|            0|  0.00%|from _thread import stack_size
  1348|         0|            0|            0|  0.00%|
  1349|         0|            0|            0|  0.00%|# Create the main thread object,
  1350|         0|            0|            0|  0.00%|# and make it available for the interpreter
  1351|         0|            0|            0|  0.00%|# (Py_Main) as threading._shutdown.
  1352|         0|            0|            0|  0.00%|
  1353|         0|            0|            0|  0.00%|_main_thread = _MainThread()
  1354|         0|            0|            0|  0.00%|
  1355|         0|            0|            0|  0.00%|def _shutdown():
  1356|         0|            0|            0|  0.00%|    """
  1357|         0|            0|            0|  0.00%|    Wait until the Python thread state of all non-daemon threads get deleted.
  1358|         0|            0|            0|  0.00%|    """
  1359|         0|            0|            0|  0.00%|    # Obscure:  other threads may be waiting to join _main_thread.  That's
  1360|         0|            0|            0|  0.00%|    # dubious, but some code does it.  We can't wait for C code to release
  1361|         0|            0|            0|  0.00%|    # the main thread's tstate_lock - that won't happen until the interpreter
  1362|         0|            0|            0|  0.00%|    # is nearly dead.  So we release it here.  Note that just calling _stop()
  1363|         0|            0|            0|  0.00%|    # isn't enough:  other threads may already be waiting on _tstate_lock.
  1364|         0|            0|            0|  0.00%|    if _main_thread._is_stopped:
  1365|         0|            0|            0|  0.00%|        # _shutdown() was already called
  1366|         0|            0|            0|  0.00%|        return
  1367|         0|            0|            0|  0.00%|
  1368|         0|            0|            0|  0.00%|    # Main thread
  1369|         0|            0|            0|  0.00%|    tlock = _main_thread._tstate_lock
  1370|         0|            0|            0|  0.00%|    # The main thread isn't finished yet, so its thread state lock can't have
  1371|         0|            0|            0|  0.00%|    # been released.
  1372|         0|            0|            0|  0.00%|    assert tlock is not None
  1373|         0|            0|            0|  0.00%|    assert tlock.locked()
  1374|         0|            0|            0|  0.00%|    tlock.release()
  1375|         0|            0|            0|  0.00%|    _main_thread._stop()
  1376|         0|            0|            0|  0.00%|
  1377|         0|            0|            0|  0.00%|    # Join all non-deamon threads
  1378|         0|            0|            0|  0.00%|    while True:
  1379|         0|            0|            0|  0.00%|        with _shutdown_locks_lock:
  1380|         0|            0|            0|  0.00%|            locks = list(_shutdown_locks)
  1381|         0|            0|            0|  0.00%|            _shutdown_locks.clear()
  1382|         0|            0|            0|  0.00%|
  1383|         0|            0|            0|  0.00%|        if not locks:
  1384|         0|            0|            0|  0.00%|            break
  1385|         0|            0|            0|  0.00%|
  1386|         0|            0|            0|  0.00%|        for lock in locks:
  1387|         0|            0|            0|  0.00%|            # mimick Thread.join()
  1388|         0|            0|            0|  0.00%|            lock.acquire()
  1389|         0|            0|            0|  0.00%|            lock.release()
  1390|         0|            0|            0|  0.00%|
  1391|         0|            0|            0|  0.00%|        # new threads can be spawned while we were waiting for the other
  1392|         0|            0|            0|  0.00%|        # threads to complete
  1393|         0|            0|            0|  0.00%|
  1394|         0|            0|            0|  0.00%|
  1395|         0|            0|            0|  0.00%|def main_thread():
  1396|         0|            0|            0|  0.00%|    """Return the main thread object.
  1397|         0|            0|            0|  0.00%|
  1398|         0|            0|            0|  0.00%|    In normal conditions, the main thread is the thread from which the
  1399|         0|            0|            0|  0.00%|    Python interpreter was started.
  1400|         0|            0|            0|  0.00%|    """
  1401|         0|            0|            0|  0.00%|    return _main_thread
  1402|         0|            0|            0|  0.00%|
  1403|         0|            0|            0|  0.00%|# get thread-local implementation, either from the thread
  1404|         0|            0|            0|  0.00%|# module, or from the python fallback
  1405|         0|            0|            0|  0.00%|
  1406|         0|            0|            0|  0.00%|try:
  1407|         0|            0|            0|  0.00%|    from _thread import _local as local
  1408|         0|            0|            0|  0.00%|except ImportError:
  1409|         0|            0|            0|  0.00%|    from _threading_local import local
  1410|         0|            0|            0|  0.00%|
  1411|         0|            0|            0|  0.00%|
  1412|         0|            0|            0|  0.00%|def _after_fork():
  1413|         0|            0|            0|  0.00%|    """
  1414|         0|            0|            0|  0.00%|    Cleanup threading module state that should not exist after a fork.
  1415|         0|            0|            0|  0.00%|    """
  1416|         0|            0|            0|  0.00%|    # Reset _active_limbo_lock, in case we forked while the lock was held
  1417|         0|            0|            0|  0.00%|    # by another (non-forked) thread.  http://bugs.python.org/issue874900
  1418|         0|            0|            0|  0.00%|    global _active_limbo_lock, _main_thread
  1419|         0|            0|            0|  0.00%|    global _shutdown_locks_lock, _shutdown_locks
  1420|         0|            0|            0|  0.00%|    _active_limbo_lock = _allocate_lock()
  1421|         0|            0|            0|  0.00%|
  1422|         0|            0|            0|  0.00%|    # fork() only copied the current thread; clear references to others.
  1423|         0|            0|            0|  0.00%|    new_active = {}
  1424|         0|            0|            0|  0.00%|
  1425|         0|            0|            0|  0.00%|    try:
  1426|         0|            0|            0|  0.00%|        current = _active[get_ident()]
  1427|         0|            0|            0|  0.00%|    except KeyError:
  1428|         0|            0|            0|  0.00%|        # fork() was called in a thread which was not spawned
  1429|         0|            0|            0|  0.00%|        # by threading.Thread. For example, a thread spawned
  1430|         0|            0|            0|  0.00%|        # by thread.start_new_thread().
  1431|         0|            0|            0|  0.00%|        current = _MainThread()
  1432|         0|            0|            0|  0.00%|
  1433|         0|            0|            0|  0.00%|    _main_thread = current
  1434|         0|            0|            0|  0.00%|
  1435|         0|            0|            0|  0.00%|    # reset _shutdown() locks: threads re-register their _tstate_lock below
  1436|         0|            0|            0|  0.00%|    _shutdown_locks_lock = _allocate_lock()
  1437|         0|            0|            0|  0.00%|    _shutdown_locks = set()
  1438|         0|            0|            0|  0.00%|
  1439|         0|            0|            0|  0.00%|    with _active_limbo_lock:
  1440|         0|            0|            0|  0.00%|        # Dangling thread instances must still have their locks reset,
  1441|         0|            0|            0|  0.00%|        # because someone may join() them.
  1442|         0|            0|            0|  0.00%|        threads = set(_enumerate())
  1443|         0|            0|            0|  0.00%|        threads.update(_dangling)
  1444|         0|            0|            0|  0.00%|        for thread in threads:
  1445|         0|            0|            0|  0.00%|            # Any lock/condition variable may be currently locked or in an
  1446|         0|            0|            0|  0.00%|            # invalid state, so we reinitialize them.
  1447|         0|            0|            0|  0.00%|            if thread is current:
  1448|         0|            0|            0|  0.00%|                # There is only one active thread. We reset the ident to
  1449|         0|            0|            0|  0.00%|                # its new value since it can have changed.
  1450|         0|            0|            0|  0.00%|                thread._reset_internal_locks(True)
  1451|         0|            0|            0|  0.00%|                ident = get_ident()
  1452|         0|            0|            0|  0.00%|                thread._ident = ident
  1453|         0|            0|            0|  0.00%|                new_active[ident] = thread
  1454|         0|            0|            0|  0.00%|            else:
  1455|         0|            0|            0|  0.00%|                # All the others are already stopped.
  1456|         0|            0|            0|  0.00%|                thread._reset_internal_locks(False)
  1457|         0|            0|            0|  0.00%|                thread._stop()
  1458|         0|            0|            0|  0.00%|
  1459|         0|            0|            0|  0.00%|        _limbo.clear()
  1460|         0|            0|            0|  0.00%|        _active.clear()
  1461|         0|            0|            0|  0.00%|        _active.update(new_active)
  1462|         0|            0|            0|  0.00%|        assert len(_active) == 1
  1463|         0|            0|            0|  0.00%|
  1464|         0|            0|            0|  0.00%|
  1465|         0|            0|            0|  0.00%|if hasattr(_os, "register_at_fork"):
  1466|         0|            0|            0|  0.00%|    _os.register_at_fork(after_in_child=_after_fork)
File: /opt/conda/lib/python3.8/site-packages/torch/backends/__init__.py
File duration: 0.0234418s (0.04%)
Line #|      Hits|         Time| Time per hit|      %|Source code
------+----------+-------------+-------------+-------+-----------
     1|         0|            0|            0|  0.00%|from contextlib import contextmanager
     2|         0|            0|            0|  0.00%|import types
     3|         0|            0|            0|  0.00%|# The idea for this parameter is that we forbid bare assignment
     4|         0|            0|            0|  0.00%|# to torch.backends.<cudnn|mkldnn>.enabled and friends when running our
     5|         0|            0|            0|  0.00%|# test suite, where it's very easy to forget to undo the change
     6|         0|            0|            0|  0.00%|# later.
     7|         0|            0|            0|  0.00%|__allow_nonbracketed_mutation_flag = True
     8|         0|            0|            0|  0.00%|
     9|         0|            0|            0|  0.00%|def disable_global_flags():
    10|         0|            0|            0|  0.00%|    global __allow_nonbracketed_mutation_flag
    11|         0|            0|            0|  0.00%|    __allow_nonbracketed_mutation_flag = False
    12|         0|            0|            0|  0.00%|
    13|         0|            0|            0|  0.00%|def flags_frozen():
    14|         0|            0|            0|  0.00%|    return not __allow_nonbracketed_mutation_flag
    15|         0|            0|            0|  0.00%|
    16|         0|            0|            0|  0.00%|@contextmanager
    17|         0|            0|            0|  0.00%|def __allow_nonbracketed_mutation():
    18|         0|            0|            0|  0.00%|    global __allow_nonbracketed_mutation_flag
    19|         0|            0|            0|  0.00%|    old = __allow_nonbracketed_mutation_flag
    20|         0|            0|            0|  0.00%|    __allow_nonbracketed_mutation_flag = True
    21|         0|            0|            0|  0.00%|    try:
    22|         0|            0|            0|  0.00%|        yield
    23|         0|            0|            0|  0.00%|    finally:
    24|         0|            0|            0|  0.00%|        __allow_nonbracketed_mutation_flag = old
    25|         0|            0|            0|  0.00%|
    26|         0|            0|            0|  0.00%|class ContextProp(object):
    27|         0|            0|            0|  0.00%|    def __init__(self, getter, setter):
    28|         0|            0|            0|  0.00%|        self.getter = getter
    29|         0|            0|            0|  0.00%|        self.setter = setter
    30|         0|            0|            0|  0.00%|
    31|      2000|   0.00954366|  4.77183e-06|  0.02%|    def __get__(self, obj, objtype):
    32|      2000|    0.0138981|  6.94907e-06|  0.03%|        return self.getter()
    33|         0|            0|            0|  0.00%|
    34|         0|            0|            0|  0.00%|    def __set__(self, obj, val):
    35|         0|            0|            0|  0.00%|        if not flags_frozen():
    36|         0|            0|            0|  0.00%|            self.setter(val)
    37|         0|            0|            0|  0.00%|        else:
    38|         0|            0|            0|  0.00%|            raise RuntimeError("not allowed to set %s flags "
    39|         0|            0|            0|  0.00%|                               "after disable_global_flags; please use flags() context manager instead" % obj.__name__)
    40|         0|            0|            0|  0.00%|
    41|         0|            0|            0|  0.00%|class PropModule(types.ModuleType):
    42|         0|            0|            0|  0.00%|    def __init__(self, m, name):
    43|         0|            0|            0|  0.00%|        super(PropModule, self).__init__(name)
    44|         0|            0|            0|  0.00%|        self.m = m
    45|         0|            0|            0|  0.00%|
    46|         0|            0|            0|  0.00%|    def __getattr__(self, attr):
    47|         0|            0|            0|  0.00%|        return self.m.__getattribute__(attr)
File: /opt/conda/lib/python3.8/site-packages/torch/_utils.py
File duration: 0.0224037s (0.04%)
Line #|      Hits|         Time| Time per hit|      %|Source code
------+----------+-------------+-------------+-------+-----------
     1|         0|            0|            0|  0.00%|import torch
     2|         0|            0|            0|  0.00%|from typing import Optional, List, DefaultDict, Any
     3|         0|            0|            0|  0.00%|import warnings
     4|         0|            0|            0|  0.00%|from collections import defaultdict
     5|         0|            0|            0|  0.00%|import sys
     6|         0|            0|            0|  0.00%|import traceback
     7|         0|            0|            0|  0.00%|
     8|         0|            0|            0|  0.00%|
     9|         0|            0|            0|  0.00%|
    10|         0|            0|            0|  0.00%|def _type(self, dtype=None, non_blocking=False, **kwargs):
    11|         0|            0|            0|  0.00%|    """Returns the type if `dtype` is not provided, else casts this object to
    12|         0|            0|            0|  0.00%|    the specified type.
    13|         0|            0|            0|  0.00%|
    14|         0|            0|            0|  0.00%|    If this is already of the correct type, no copy is performed and the
    15|         0|            0|            0|  0.00%|    original object is returned.
    16|         0|            0|            0|  0.00%|
    17|         0|            0|            0|  0.00%|    Args:
    18|         0|            0|            0|  0.00%|        dtype (type or string): The desired type
    19|         0|            0|            0|  0.00%|        non_blocking (bool): If ``True``, and the source is in pinned memory
    20|         0|            0|            0|  0.00%|            and destination is on the GPU or vice versa, the copy is performed
    21|         0|            0|            0|  0.00%|            asynchronously with respect to the host. Otherwise, the argument
    22|         0|            0|            0|  0.00%|            has no effect.
    23|         0|            0|            0|  0.00%|        **kwargs: For compatibility, may contain the key ``async`` in place of
    24|         0|            0|            0|  0.00%|            the ``non_blocking`` argument. The ``async`` arg is deprecated.
    25|         0|            0|            0|  0.00%|    """
    26|         0|            0|            0|  0.00%|    non_blocking = _get_async_or_non_blocking('type', non_blocking, kwargs)
    27|         0|            0|            0|  0.00%|    if dtype is None:
    28|         0|            0|            0|  0.00%|        return self.__module__ + '.' + self.__class__.__name__
    29|         0|            0|            0|  0.00%|
    30|         0|            0|            0|  0.00%|    if isinstance(dtype, str):
    31|         0|            0|            0|  0.00%|        dtype = _import_dotted_name(dtype)
    32|         0|            0|            0|  0.00%|    if dtype == type(self):
    33|         0|            0|            0|  0.00%|        return self
    34|         0|            0|            0|  0.00%|    if self.is_sparse:
    35|         0|            0|            0|  0.00%|        if not dtype.is_sparse:
    36|         0|            0|            0|  0.00%|            raise RuntimeError("Cannot cast sparse tensor to dense tensor")
    37|         0|            0|            0|  0.00%|        new_module_name = dtype.__module__.replace('.sparse', '')
    38|         0|            0|            0|  0.00%|        new_values_type_name = new_module_name + '.' + dtype.__name__
    39|         0|            0|            0|  0.00%|        new_values = torch.Tensor._values(self).type(new_values_type_name, non_blocking)
    40|         0|            0|            0|  0.00%|        new_indices_type_name = new_module_name + '.LongTensor'
    41|         0|            0|            0|  0.00%|        new_indices = torch.Tensor._indices(self).type(new_indices_type_name, non_blocking)
    42|         0|            0|            0|  0.00%|        return dtype(new_indices, new_values, self.size())
    43|         0|            0|            0|  0.00%|    if dtype.is_sparse:
    44|         0|            0|            0|  0.00%|        raise RuntimeError("Cannot cast dense tensor to sparse tensor")
    45|         0|            0|            0|  0.00%|    return dtype(self.size()).copy_(self, non_blocking)
    46|         0|            0|            0|  0.00%|
    47|         0|            0|            0|  0.00%|
    48|         0|            0|            0|  0.00%|def _cuda(self, device=None, non_blocking=False, **kwargs):
    49|         0|            0|            0|  0.00%|    """Returns a copy of this object in CUDA memory.
    50|         0|            0|            0|  0.00%|
    51|         0|            0|            0|  0.00%|    If this object is already in CUDA memory and on the correct device, then
    52|         0|            0|            0|  0.00%|    no copy is performed and the original object is returned.
    53|         0|            0|            0|  0.00%|
    54|         0|            0|            0|  0.00%|    Args:
    55|         0|            0|            0|  0.00%|        device (int): The destination GPU id. Defaults to the current device.
    56|         0|            0|            0|  0.00%|        non_blocking (bool): If ``True`` and the source is in pinned memory,
    57|         0|            0|            0|  0.00%|            the copy will be asynchronous with respect to the host. Otherwise,
    58|         0|            0|            0|  0.00%|            the argument has no effect.
    59|         0|            0|            0|  0.00%|        **kwargs: For compatibility, may contain the key ``async`` in place of
    60|         0|            0|            0|  0.00%|            the ``non_blocking`` argument.
    61|         0|            0|            0|  0.00%|    """
    62|         0|            0|            0|  0.00%|    non_blocking = _get_async_or_non_blocking('cuda', non_blocking, kwargs)
    63|         0|            0|            0|  0.00%|    if self.is_cuda:
    64|         0|            0|            0|  0.00%|        if device is None:
    65|         0|            0|            0|  0.00%|            device = torch.cuda.current_device()
    66|         0|            0|            0|  0.00%|        if self.get_device() == device:
    67|         0|            0|            0|  0.00%|            return self
    68|         0|            0|            0|  0.00%|    else:
    69|         0|            0|            0|  0.00%|        if device is None:
    70|         0|            0|            0|  0.00%|            device = -1
    71|         0|            0|            0|  0.00%|    with torch.cuda.device(device):
    72|         0|            0|            0|  0.00%|        if self.is_sparse:
    73|         0|            0|            0|  0.00%|            new_type = getattr(torch.cuda.sparse, self.__class__.__name__)
    74|         0|            0|            0|  0.00%|            indices = torch.Tensor._indices(self).cuda(device, non_blocking)
    75|         0|            0|            0|  0.00%|            values = torch.Tensor._values(self).cuda(device, non_blocking)
    76|         0|            0|            0|  0.00%|            return new_type(indices, values, self.size())
    77|         0|            0|            0|  0.00%|        else:
    78|         0|            0|            0|  0.00%|            new_type = getattr(torch.cuda, self.__class__.__name__)
    79|         0|            0|            0|  0.00%|            return new_type(self.size()).copy_(self, non_blocking)
    80|         0|            0|            0|  0.00%|
    81|         0|            0|            0|  0.00%|
    82|         0|            0|            0|  0.00%|def _get_async_or_non_blocking(function_name, non_blocking, kwargs):
    83|         0|            0|            0|  0.00%|    if not kwargs:
    84|         0|            0|            0|  0.00%|        return non_blocking
    85|         0|            0|            0|  0.00%|    if len(kwargs) != 1 or 'async' not in kwargs:
    86|         0|            0|            0|  0.00%|        message = "{}() got an unexpected keyword argument '{}'"
    87|         0|            0|            0|  0.00%|        argument = list(kwargs.keys()).pop()
    88|         0|            0|            0|  0.00%|        raise TypeError(message.format(function_name, argument))
    89|         0|            0|            0|  0.00%|    warnings.warn("'async' is deprecated; use 'non_blocking'")
    90|         0|            0|            0|  0.00%|    return kwargs['async']
    91|         0|            0|            0|  0.00%|
    92|         0|            0|            0|  0.00%|
    93|         0|            0|            0|  0.00%|# Note [Don't serialize hooks]
    94|         0|            0|            0|  0.00%|# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    95|         0|            0|            0|  0.00%|# Since time immemorial, we have serialized the backward hooks associated with
    96|         0|            0|            0|  0.00%|# variables.  This kind of half-worked--Python can pickle global functions
    97|         0|            0|            0|  0.00%|# (but not closures!)--but there were problems.
    98|         0|            0|            0|  0.00%|#
    99|         0|            0|            0|  0.00%|#   - It's fragile.  If you serialize a backward hook into a saved
   100|         0|            0|            0|  0.00%|#     model, and then you rename the function associated with the hook,
   101|         0|            0|            0|  0.00%|#     now your saved model is broken and you can't load it anymore.
   102|         0|            0|            0|  0.00%|#
   103|         0|            0|            0|  0.00%|#   - It's not actually used.  The standard recommendation is to
   104|         0|            0|            0|  0.00%|#     serialize the *state_dict* of a model, not the model itself
   105|         0|            0|            0|  0.00%|#     (since this is more stable to code changes affecting the model
   106|         0|            0|            0|  0.00%|#     serialization), and the state dict saves "data" only, thus
   107|         0|            0|            0|  0.00%|#     stripping the the backward hooks.  In some cases, hooks are
   108|         0|            0|            0|  0.00%|#     essential to the well-functioning of a model (e.g., DDP),
   109|         0|            0|            0|  0.00%|#     but DDP already manages readding the hooks!
   110|         0|            0|            0|  0.00%|#
   111|         0|            0|            0|  0.00%|#   - We didn't serialize them in many cases.  Prior to #10220, we
   112|         0|            0|            0|  0.00%|#     were dropping backward hooks in ForkingPickler.  We "fixed" this
   113|         0|            0|            0|  0.00%|#     to be convenient with other serialization sites, but lack of
   114|         0|            0|            0|  0.00%|#     serializing backward hooks wasn't actually the root cause of
   115|         0|            0|            0|  0.00%|#     the bug.
   116|         0|            0|            0|  0.00%|#
   117|         0|            0|            0|  0.00%|# With these cases in mind, we have decided that a better strategy
   118|         0|            0|            0|  0.00%|# is to just NOT serialize hooks at all.
   119|         0|            0|            0|  0.00%|#
   120|         0|            0|            0|  0.00%|# Since this is a BC-breaking change, we should warn when we previously
   121|         0|            0|            0|  0.00%|# serialized a hook, but no longer do so. This will be done by adding a special
   122|         0|            0|            0|  0.00%|# sentinel property to hooks will be used to suppress this warning. If a hook
   123|         0|            0|            0|  0.00%|# has the property _torch_serialize_ignore, we will not emit a warning if we
   124|         0|            0|            0|  0.00%|# attempt to serialize a Tensor with this hook attached to it.
   125|         0|            0|            0|  0.00%|#
   126|         0|            0|            0|  0.00%|# By the way, when _backward_hooks is skipped, we must give an EMPTY
   127|         0|            0|            0|  0.00%|# OrderedDict(), if you pass a None you'll run afoul #12219.
   128|         0|            0|            0|  0.00%|
   129|         0|            0|            0|  0.00%|
   130|       200|   0.00112486|  5.62429e-06|  0.00%|def _rebuild_tensor(storage, storage_offset, size, stride):
   131|         0|            0|            0|  0.00%|    # first construct a tensor with the correct dtype/device
   132|       200|   0.00702477|  3.51238e-05|  0.01%|    t = torch.tensor([], dtype=storage.dtype, device=storage.device)
   133|       200|   0.00502205|  2.51102e-05|  0.01%|    return t.set_(storage, storage_offset, size, stride)
   134|         0|            0|            0|  0.00%|
   135|         0|            0|            0|  0.00%|
   136|         0|            0|            0|  0.00%|def _rebuild_tensor_v2(storage, storage_offset, size, stride, requires_grad, backward_hooks):
   137|         0|            0|            0|  0.00%|    tensor = _rebuild_tensor(storage, storage_offset, size, stride)
   138|         0|            0|            0|  0.00%|    tensor.requires_grad = requires_grad
   139|         0|            0|            0|  0.00%|    # NB: This line exists only for backwards compatibility; the
   140|         0|            0|            0|  0.00%|    # general expectation is that backward_hooks is an empty
   141|         0|            0|            0|  0.00%|    # OrderedDict.  See Note [Don't serialize hooks]
   142|         0|            0|            0|  0.00%|    tensor._backward_hooks = backward_hooks
   143|         0|            0|            0|  0.00%|    return tensor
   144|         0|            0|            0|  0.00%|
   145|         0|            0|            0|  0.00%|
   146|         0|            0|            0|  0.00%|_sparse_tensors_to_validate: List["torch.Tensor"] = []
   147|         0|            0|            0|  0.00%|
   148|         0|            0|            0|  0.00%|# In _legacy_load() in serialization.py we unpickle storages after the sparse
   149|         0|            0|            0|  0.00%|# tensors have been already unpickled. Those storages contain data necessary for
   150|         0|            0|            0|  0.00%|# validating sparse tensors: indices and values. That's why sparse tensors are
   151|         0|            0|            0|  0.00%|# first unpickled without any validation, and then this function is called just
   152|         0|            0|            0|  0.00%|# before _legacy_load() returns, so that all the sparse tensors can be validated
   153|         0|            0|            0|  0.00%|# in bulk.
   154|         0|            0|            0|  0.00%|#
   155|         0|            0|            0|  0.00%|# The same procedure must be followed by _load() in serialization.py because due
   156|         0|            0|            0|  0.00%|# to Pickler semantics, we have to use the same (non-validating) function for
   157|         0|            0|            0|  0.00%|# unpickling sparse tensors, regardless of the caller.
   158|         0|            0|            0|  0.00%|def _validate_loaded_sparse_tensors():
   159|         0|            0|            0|  0.00%|    try:
   160|         0|            0|            0|  0.00%|        for t in _sparse_tensors_to_validate:
   161|         0|            0|            0|  0.00%|            torch._validate_sparse_coo_tensor_args(t._indices(), t._values(),
   162|         0|            0|            0|  0.00%|                                                   t.size())
   163|         0|            0|            0|  0.00%|    finally:
   164|         0|            0|            0|  0.00%|        _sparse_tensors_to_validate.clear()
   165|         0|            0|            0|  0.00%|
   166|         0|            0|            0|  0.00%|def _rebuild_sparse_tensor(layout, data):
   167|         0|            0|            0|  0.00%|    if layout == torch.sparse_coo:
   168|         0|            0|            0|  0.00%|        indices, values, size = data
   169|         0|            0|            0|  0.00%|        result = torch._sparse_coo_tensor_unsafe(indices, values, size)
   170|         0|            0|            0|  0.00%|        _sparse_tensors_to_validate.append(result)
   171|         0|            0|            0|  0.00%|        return result
   172|         0|            0|            0|  0.00%|
   173|         0|            0|            0|  0.00%|    raise NotImplementedError("rebuilding sparse tensor for layout %s" % (layout))
   174|         0|            0|            0|  0.00%|
   175|         0|            0|            0|  0.00%|
   176|         0|            0|            0|  0.00%|def _rebuild_xla_tensor(data, dtype, device, requires_grad):
   177|         0|            0|            0|  0.00%|    tensor = torch.from_numpy(data).to(dtype=dtype, device=device)
   178|         0|            0|            0|  0.00%|    tensor.requires_grad = requires_grad
   179|         0|            0|            0|  0.00%|    return tensor
   180|         0|            0|            0|  0.00%|
   181|         0|            0|            0|  0.00%|
   182|         0|            0|            0|  0.00%|def _rebuild_mlc_tensor(data, dtype, device, requires_grad):
   183|         0|            0|            0|  0.00%|    tensor = torch.from_numpy(data).to(dtype=dtype, device=device)
   184|         0|            0|            0|  0.00%|    tensor.requires_grad = requires_grad
   185|         0|            0|            0|  0.00%|    return tensor
   186|         0|            0|            0|  0.00%|
   187|         0|            0|            0|  0.00%|
   188|         0|            0|            0|  0.00%|def _rebuild_qtensor(storage, storage_offset, size, stride, quantizer_params, requires_grad, backward_hooks):
   189|         0|            0|            0|  0.00%|    qscheme = quantizer_params[0]
   190|         0|            0|            0|  0.00%|    if qscheme == torch.per_tensor_affine:
   191|         0|            0|            0|  0.00%|        _, scale, zero_point = quantizer_params
   192|         0|            0|            0|  0.00%|        tensor = torch._empty_affine_quantized(size, scale=scale, zero_point=zero_point, dtype=storage.dtype)
   193|         0|            0|            0|  0.00%|    elif qscheme in (torch.per_channel_affine, torch.per_channel_affine_float_qparams):
   194|         0|            0|            0|  0.00%|        _, scales, zero_points, axis = quantizer_params
   195|         0|            0|            0|  0.00%|        if type(scales) is list and type(zero_points) is list:
   196|         0|            0|            0|  0.00%|            if qscheme == torch.per_channel_affine:
   197|         0|            0|            0|  0.00%|                scales = torch.tensor(scales, dtype=torch.double)
   198|         0|            0|            0|  0.00%|                zero_points = torch.tensor(zero_points, dtype=torch.long)
   199|         0|            0|            0|  0.00%|            else:
   200|         0|            0|            0|  0.00%|                scales = torch.tensor(scales, dtype=torch.float)
   201|         0|            0|            0|  0.00%|                zero_points = torch.tensor(zero_points, dtype=torch.float)
   202|         0|            0|            0|  0.00%|        tensor = torch._empty_per_channel_affine_quantized(
   203|         0|            0|            0|  0.00%|            size, scales=scales, zero_points=zero_points, axis=axis, dtype=storage.dtype)
   204|         0|            0|            0|  0.00%|    else:
   205|         0|            0|            0|  0.00%|        raise RuntimeError("Can't deserialize quantized tensor with qscheme {}".format(qscheme))
   206|         0|            0|            0|  0.00%|    tensor.set_(storage, storage_offset, size, stride)
   207|         0|            0|            0|  0.00%|    tensor.requires_grad = requires_grad
   208|         0|            0|            0|  0.00%|    # NB: This line exists only for backwards compatibility; the
   209|         0|            0|            0|  0.00%|    # general expectation is that backward_hooks is an empty
   210|         0|            0|            0|  0.00%|    # OrderedDict.  See Note [Don't serialize hooks]
   211|         0|            0|            0|  0.00%|    tensor._backward_hooks = backward_hooks
   212|         0|            0|            0|  0.00%|    return tensor
   213|         0|            0|            0|  0.00%|
   214|         0|            0|            0|  0.00%|def _rebuild_parameter(data, requires_grad, backward_hooks):
   215|         0|            0|            0|  0.00%|    param = torch.nn.Parameter(data, requires_grad)
   216|         0|            0|            0|  0.00%|    # NB: This line exists only for backwards compatibility; the
   217|         0|            0|            0|  0.00%|    # general expectation is that backward_hooks is an empty
   218|         0|            0|            0|  0.00%|    # OrderedDict.  See Note [Don't serialize hooks]
   219|         0|            0|            0|  0.00%|    param._backward_hooks = backward_hooks
   220|         0|            0|            0|  0.00%|
   221|         0|            0|            0|  0.00%|    return param
   222|         0|            0|            0|  0.00%|
   223|         0|            0|            0|  0.00%|
   224|         0|            0|            0|  0.00%|def _import_dotted_name(name):
   225|         0|            0|            0|  0.00%|    components = name.split('.')
   226|         0|            0|            0|  0.00%|    obj = __import__(components[0])
   227|         0|            0|            0|  0.00%|    for component in components[1:]:
   228|         0|            0|            0|  0.00%|        obj = getattr(obj, component)
   229|         0|            0|            0|  0.00%|    return obj
   230|         0|            0|            0|  0.00%|
   231|         0|            0|            0|  0.00%|
   232|         0|            0|            0|  0.00%|# Taken from python 3.5 docs
   233|         0|            0|            0|  0.00%|def _accumulate(iterable, fn=lambda x, y: x + y):
   234|         0|            0|            0|  0.00%|    'Return running totals'
   235|         0|            0|            0|  0.00%|    # _accumulate([1,2,3,4,5]) --> 1 3 6 10 15
   236|         0|            0|            0|  0.00%|    # _accumulate([1,2,3,4,5], operator.mul) --> 1 2 6 24 120
   237|         0|            0|            0|  0.00%|    it = iter(iterable)
   238|         0|            0|            0|  0.00%|    try:
   239|         0|            0|            0|  0.00%|        total = next(it)
   240|         0|            0|            0|  0.00%|    except StopIteration:
   241|         0|            0|            0|  0.00%|        return
   242|         0|            0|            0|  0.00%|    yield total
   243|         0|            0|            0|  0.00%|    for element in it:
   244|         0|            0|            0|  0.00%|        total = fn(total, element)
   245|         0|            0|            0|  0.00%|        yield total
   246|         0|            0|            0|  0.00%|
   247|         0|            0|            0|  0.00%|
   248|         0|            0|            0|  0.00%|def _flatten_dense_tensors(tensors):
   249|         0|            0|            0|  0.00%|    """Flatten dense tensors into a contiguous 1D buffer. Assume tensors are of
   250|         0|            0|            0|  0.00%|    same dense type.
   251|         0|            0|            0|  0.00%|
   252|         0|            0|            0|  0.00%|    Since inputs are dense, the resulting tensor will be a concatenated 1D
   253|         0|            0|            0|  0.00%|    buffer. Element-wise operation on this buffer will be equivalent to
   254|         0|            0|            0|  0.00%|    operating individually.
   255|         0|            0|            0|  0.00%|
   256|         0|            0|            0|  0.00%|    Args:
   257|         0|            0|            0|  0.00%|        tensors (Iterable[Tensor]): dense tensors to flatten.
   258|         0|            0|            0|  0.00%|
   259|         0|            0|            0|  0.00%|    Returns:
   260|         0|            0|            0|  0.00%|        A contiguous 1D buffer containing input tensors.
   261|         0|            0|            0|  0.00%|    """
   262|         0|            0|            0|  0.00%|    return torch._C._nn.flatten_dense_tensors(tensors)
   263|         0|            0|            0|  0.00%|
   264|         0|            0|            0|  0.00%|
   265|         0|            0|            0|  0.00%|def _flatten_sparse_tensors(tensors):
   266|         0|            0|            0|  0.00%|    """Flatten sparse tensors into two contiguous 1D buffers, one of indices and
   267|         0|            0|            0|  0.00%|    one of values. Assume tensors are of same sparse type.
   268|         0|            0|            0|  0.00%|
   269|         0|            0|            0|  0.00%|    Args:
   270|         0|            0|            0|  0.00%|        tensors (Iterable[Tensor]): sparse tensors to flatten.
   271|         0|            0|            0|  0.00%|
   272|         0|            0|            0|  0.00%|    Returns:
   273|         0|            0|            0|  0.00%|        A tuple of two contiguous 1D buffers, one containing input tensors'
   274|         0|            0|            0|  0.00%|        indices and the other containing the values.
   275|         0|            0|            0|  0.00%|    """
   276|         0|            0|            0|  0.00%|    flat_indices = torch._C._nn.flatten_dense_tensors([torch.Tensor._indices(t) for t in tensors])
   277|         0|            0|            0|  0.00%|    flat_values = torch._C._nn.flatten_dense_tensors([torch.Tensor._values(t) for t in tensors])
   278|         0|            0|            0|  0.00%|    return flat_indices, flat_values
   279|         0|            0|            0|  0.00%|
   280|         0|            0|            0|  0.00%|
   281|         0|            0|            0|  0.00%|def _unflatten_dense_tensors(flat, tensors):
   282|         0|            0|            0|  0.00%|    """View a flat buffer using the sizes of tensors. Assume that tensors are of
   283|         0|            0|            0|  0.00%|    same dense type, and that flat is given by _flatten_dense_tensors.
   284|         0|            0|            0|  0.00%|
   285|         0|            0|            0|  0.00%|    Args:
   286|         0|            0|            0|  0.00%|        flat (Tensor): flattened dense tensors to unflatten.
   287|         0|            0|            0|  0.00%|        tensors (Iterable[Tensor]): dense tensors whose sizes will be used to
   288|         0|            0|            0|  0.00%|          unflatten flat.
   289|         0|            0|            0|  0.00%|
   290|         0|            0|            0|  0.00%|    Returns:
   291|         0|            0|            0|  0.00%|        Unflattened dense tensors with sizes same as tensors and values from
   292|         0|            0|            0|  0.00%|        flat.
   293|         0|            0|            0|  0.00%|    """
   294|         0|            0|            0|  0.00%|    return torch._C._nn.unflatten_dense_tensors(flat, tensors)
   295|         0|            0|            0|  0.00%|
   296|         0|            0|            0|  0.00%|
   297|         0|            0|            0|  0.00%|def _unflatten_sparse_tensors(flat, tensors):
   298|         0|            0|            0|  0.00%|    """View flat buffer (containing indices and values) using the sizes of
   299|         0|            0|            0|  0.00%|    tensors. Assume that tensors are of same sparse type, and that flat is given
   300|         0|            0|            0|  0.00%|    by _flatten_sparse_tensors.
   301|         0|            0|            0|  0.00%|
   302|         0|            0|            0|  0.00%|    Args:
   303|         0|            0|            0|  0.00%|        flat (tuple(Tensor, Tensor)): flattened indices and values of sparse
   304|         0|            0|            0|  0.00%|          tensors to unflatten.
   305|         0|            0|            0|  0.00%|        tensors (Iterable[Tensor]): sparse tensors whose sizes will be used to
   306|         0|            0|            0|  0.00%|          unflatten flat.
   307|         0|            0|            0|  0.00%|
   308|         0|            0|            0|  0.00%|    Returns:
   309|         0|            0|            0|  0.00%|        Unflattened sparse tensors with sizes same as tensors and values from
   310|         0|            0|            0|  0.00%|        flat.
   311|         0|            0|            0|  0.00%|    """
   312|         0|            0|            0|  0.00%|    flat_indices, flat_values = flat
   313|         0|            0|            0|  0.00%|    indices = torch._C._nn.unflatten_dense_tensors(flat_indices, [torch.Tensor._indices(t) for t in tensors])
   314|         0|            0|            0|  0.00%|    values = torch._C._nn.unflatten_dense_tensors(flat_values, [torch.Tensor._values(t) for t in tensors])
   315|         0|            0|            0|  0.00%|    outputs = []
   316|         0|            0|            0|  0.00%|    for t, i, v in zip(tensors, indices, values):
   317|         0|            0|            0|  0.00%|        outputs.append(t.new(i, v, t.size()))
   318|         0|            0|            0|  0.00%|    return tuple(outputs)
   319|         0|            0|            0|  0.00%|
   320|         0|            0|            0|  0.00%|
   321|         0|            0|            0|  0.00%|def _reorder_tensors_as(tensors, ordered_tensors):
   322|         0|            0|            0|  0.00%|    """Assume that tensors are of same order as ordered_tensors within their
   323|         0|            0|            0|  0.00%|    types, e.g., from _take_tensors. Reorder them to be of same order as
   324|         0|            0|            0|  0.00%|    ordered_tensors.
   325|         0|            0|            0|  0.00%|
   326|         0|            0|            0|  0.00%|    Args:
   327|         0|            0|            0|  0.00%|        tensors (Iterable[Tensor]): tensors to be reordered. They should be of
   328|         0|            0|            0|  0.00%|          the same order as ordered_tensors within their own types.
   329|         0|            0|            0|  0.00%|        ordered_tensors (Iterable[Tensor]): tensors whose order will be the
   330|         0|            0|            0|  0.00%|          reference.
   331|         0|            0|            0|  0.00%|
   332|         0|            0|            0|  0.00%|    Returns:
   333|         0|            0|            0|  0.00%|        Ordered tuple of tensors with contents from tensors and order of
   334|         0|            0|            0|  0.00%|        ordered_tensors.
   335|         0|            0|            0|  0.00%|    """
   336|         0|            0|            0|  0.00%|    type_dict = defaultdict(list)
   337|         0|            0|            0|  0.00%|    for tensor in tensors:
   338|         0|            0|            0|  0.00%|        type_dict[tensor.type()].append(tensor)
   339|         0|            0|            0|  0.00%|    type_dict_ = {t: iter(coll) for t, coll in type_dict.items()}
   340|         0|            0|            0|  0.00%|    return tuple(next(type_dict_[tensor.type()]) for tensor in ordered_tensors)
   341|         0|            0|            0|  0.00%|
   342|         0|            0|            0|  0.00%|
   343|         0|            0|            0|  0.00%|def _take_tensors(tensors, size_limit):
   344|         0|            0|            0|  0.00%|    """Group tensors into chunks. This generator yields a chunk at each time,
   345|         0|            0|            0|  0.00%|    each containing tensors of same type up to certain byte limit in total size.
   346|         0|            0|            0|  0.00%|
   347|         0|            0|            0|  0.00%|    Args:
   348|         0|            0|            0|  0.00%|        tensors (Sequence): A sequence of tensors to be separated into chunks.
   349|         0|            0|            0|  0.00%|        size_limit (int): The limit of each chunk in bytes.
   350|         0|            0|            0|  0.00%|
   351|         0|            0|            0|  0.00%|    Yields:
   352|         0|            0|            0|  0.00%|        Blocks of tensors of same type and within size_limit. The yielded
   353|         0|            0|            0|  0.00%|        tensors are only ordered as the original sequence within its types.
   354|         0|            0|            0|  0.00%|    """
   355|         0|            0|            0|  0.00%|    buf_dict: DefaultDict[str, List] = defaultdict(lambda: [[], 0])
   356|         0|            0|            0|  0.00%|    for tensor in tensors:
   357|         0|            0|            0|  0.00%|        t = tensor.type()
   358|         0|            0|            0|  0.00%|        if tensor.is_sparse:
   359|         0|            0|            0|  0.00%|            indices = torch.Tensor._indices(tensor)
   360|         0|            0|            0|  0.00%|            values = torch.Tensor._values(tensor)
   361|         0|            0|            0|  0.00%|            size = indices.numel() * indices.element_size() + values.numel() * values.element_size()
   362|         0|            0|            0|  0.00%|        else:
   363|         0|            0|            0|  0.00%|            size = tensor.numel() * tensor.element_size()
   364|         0|            0|            0|  0.00%|        buf_and_size = buf_dict[t]
   365|         0|            0|            0|  0.00%|        if buf_and_size[1] + size > size_limit and buf_and_size[1] > 0:
   366|         0|            0|            0|  0.00%|            yield buf_and_size[0]
   367|         0|            0|            0|  0.00%|            buf_and_size = buf_dict[t] = [[], 0]
   368|         0|            0|            0|  0.00%|        buf_and_size[0].append(tensor)
   369|         0|            0|            0|  0.00%|        buf_and_size[1] += size
   370|         0|            0|            0|  0.00%|    for buf, _ in buf_dict.values():
   371|         0|            0|            0|  0.00%|        if len(buf) > 0:
   372|         0|            0|            0|  0.00%|            yield buf
   373|         0|            0|            0|  0.00%|
   374|         0|            0|            0|  0.00%|
   375|         0|            0|            0|  0.00%|# annotation decorator to get annotations in a way that is compatible
   376|         0|            0|            0|  0.00%|# with both Python 2 and 3
   377|         0|            0|            0|  0.00%|def annotate(ret, **kwargs):
   378|         0|            0|            0|  0.00%|    def dec(fun):
   379|         0|            0|            0|  0.00%|        fun.__annotations__ = dict(kwargs)
   380|         0|            0|            0|  0.00%|        fun.__annotations__['return'] = ret
   381|         0|            0|            0|  0.00%|        return fun
   382|         0|            0|            0|  0.00%|    return dec
   383|         0|            0|            0|  0.00%|
   384|         0|            0|            0|  0.00%|
   385|         0|            0|            0|  0.00%|# NOTE [ Python Traceback Reference Cycle Problem ]
   386|         0|            0|            0|  0.00%|#
   387|         0|            0|            0|  0.00%|# When using sys.exc_info(), it is important to **not** store the exc_info[2],
   388|         0|            0|            0|  0.00%|# which is the traceback, because otherwise you will run into the traceback
   389|         0|            0|            0|  0.00%|# reference cycle problem, i.e., the traceback holding reference to the frame,
   390|         0|            0|            0|  0.00%|# and the frame (which holds reference to all the object in its temporary scope)
   391|         0|            0|            0|  0.00%|# holding reference the traceback.
   392|         0|            0|            0|  0.00%|
   393|         0|            0|            0|  0.00%|class KeyErrorMessage(str):
   394|         0|            0|            0|  0.00%|    r"""str subclass that returns itself in repr"""
   395|         0|            0|            0|  0.00%|    def __repr__(self):
   396|         0|            0|            0|  0.00%|        return self
   397|         0|            0|            0|  0.00%|
   398|         0|            0|            0|  0.00%|
   399|         0|            0|            0|  0.00%|class ExceptionWrapper(object):
   400|         0|            0|            0|  0.00%|    r"""Wraps an exception plus traceback to communicate across threads"""
   401|         0|            0|            0|  0.00%|    def __init__(self, exc_info=None, where="in background"):
   402|         0|            0|            0|  0.00%|        # It is important that we don't store exc_info, see
   403|         0|            0|            0|  0.00%|        # NOTE [ Python Traceback Reference Cycle Problem ]
   404|         0|            0|            0|  0.00%|        if exc_info is None:
   405|         0|            0|            0|  0.00%|            exc_info = sys.exc_info()
   406|         0|            0|            0|  0.00%|        self.exc_type = exc_info[0]
   407|         0|            0|            0|  0.00%|        self.exc_msg = "".join(traceback.format_exception(*exc_info))
   408|         0|            0|            0|  0.00%|        self.where = where
   409|         0|            0|            0|  0.00%|
   410|         0|            0|            0|  0.00%|    def reraise(self):
   411|         0|            0|            0|  0.00%|        r"""Reraises the wrapped exception in the current thread"""
   412|         0|            0|            0|  0.00%|        # Format a message such as: "Caught ValueError in DataLoader worker
   413|         0|            0|            0|  0.00%|        # process 2. Original Traceback:", followed by the traceback.
   414|         0|            0|            0|  0.00%|        msg = "Caught {} {}.\nOriginal {}".format(
   415|         0|            0|            0|  0.00%|            self.exc_type.__name__, self.where, self.exc_msg)
   416|         0|            0|            0|  0.00%|        if self.exc_type == KeyError:
   417|         0|            0|            0|  0.00%|            # KeyError calls repr() on its argument (usually a dict key). This
   418|         0|            0|            0|  0.00%|            # makes stack traces unreadable. It will not be changed in Python
   419|         0|            0|            0|  0.00%|            # (https://bugs.python.org/issue2651), so we work around it.
   420|         0|            0|            0|  0.00%|            msg = KeyErrorMessage(msg)
   421|         0|            0|            0|  0.00%|        elif getattr(self.exc_type, "message", None):
   422|         0|            0|            0|  0.00%|            # Some exceptions have first argument as non-str but explicitly
   423|         0|            0|            0|  0.00%|            # have message field
   424|         0|            0|            0|  0.00%|            raise self.exc_type(message=msg)
   425|         0|            0|            0|  0.00%|        raise self.exc_type(msg)
   426|         0|            0|            0|  0.00%|
   427|         0|            0|            0|  0.00%|
   428|         0|            0|            0|  0.00%|def _get_available_device_type():
   429|         0|            0|            0|  0.00%|    if torch.cuda.is_available():
   430|         0|            0|            0|  0.00%|        return "cuda"
   431|         0|            0|            0|  0.00%|    # add more available device types here
   432|         0|            0|            0|  0.00%|    return None
   433|         0|            0|            0|  0.00%|
   434|         0|            0|            0|  0.00%|
   435|         0|            0|            0|  0.00%|def _get_device_attr(get_member):
   436|         0|            0|            0|  0.00%|    device_type = _get_available_device_type()
   437|         0|            0|            0|  0.00%|    if device_type and device_type.lower() == "cuda":
   438|         0|            0|            0|  0.00%|        return get_member(torch.cuda)
   439|         0|            0|            0|  0.00%|    # add more available device types here
   440|         0|            0|            0|  0.00%|    return None
   441|         0|            0|            0|  0.00%|
   442|         0|            0|            0|  0.00%|
   443|         0|            0|            0|  0.00%|def _get_current_device_index():
   444|         0|            0|            0|  0.00%|    # current device index
   445|         0|            0|            0|  0.00%|    return _get_device_attr(lambda m: m.current_device())
   446|         0|            0|            0|  0.00%|
   447|         0|            0|            0|  0.00%|
   448|         0|            0|            0|  0.00%|def _get_all_device_indices():
   449|         0|            0|            0|  0.00%|    # all device index
   450|         0|            0|            0|  0.00%|    return _get_device_attr(lambda m: list(range(m.device_count())))
   451|         0|            0|            0|  0.00%|
   452|         0|            0|            0|  0.00%|
   453|         0|            0|            0|  0.00%|def _get_devices_properties(device_ids):
   454|         0|            0|            0|  0.00%|    # all device properties
   455|         0|            0|            0|  0.00%|    return [_get_device_attr(lambda m: m.get_device_properties(i)) for i in device_ids]
   456|         0|            0|            0|  0.00%|
   457|         0|            0|            0|  0.00%|def get_current_device_index() -> int:
   458|         0|            0|            0|  0.00%|    r"""Checks if there are CUDA devices available and
   459|         0|            0|            0|  0.00%|    returns the device index of the current default CUDA device.
   460|         0|            0|            0|  0.00%|    Returns -1 in case there are no CUDA devices available.
   461|         0|            0|            0|  0.00%|    Arguments: ``None``
   462|         0|            0|            0|  0.00%|    """
   463|         0|            0|            0|  0.00%|    if torch.cuda.device_count() > 0:
   464|         0|            0|            0|  0.00%|        return torch.cuda.current_device()
   465|         0|            0|            0|  0.00%|    return -1
   466|         0|            0|            0|  0.00%|
   467|       244|   0.00118804|  4.86902e-06|  0.00%|def _get_device_index(device: Any, optional: bool = False, allow_cpu: bool = False) -> int:
   468|         0|            0|            0|  0.00%|    r"""Gets the device index from :attr:`device`, which can be a torch.device
   469|         0|            0|            0|  0.00%|    object, a Python integer, or ``None``.
   470|         0|            0|            0|  0.00%|
   471|         0|            0|            0|  0.00%|    If :attr:`device` is a torch.device object, returns the device index if it
   472|         0|            0|            0|  0.00%|    has index. Note that for a device without a specified index,
   473|         0|            0|            0|  0.00%|    i.e., ``torch.device('xxx')``, this will return the current default
   474|         0|            0|            0|  0.00%|    device of that type if :attr:`optional` is ``True``. If :attr:`allow_cpu` is ``True``,
   475|         0|            0|            0|  0.00%|    CPU devices will be accepted and ``-1`` will be returned in this case.
   476|         0|            0|            0|  0.00%|
   477|         0|            0|            0|  0.00%|    If :attr:`device` is a Python integer, it is returned as is.
   478|         0|            0|            0|  0.00%|
   479|         0|            0|            0|  0.00%|    If :attr:`device` is ``None``, this will return the current default
   480|         0|            0|            0|  0.00%|    device of the supported runtime platform if :attr:`optional` is ``True``.
   481|         0|            0|            0|  0.00%|    i.e., the current default CUDA device will be returned if CUDA runtime is supported.
   482|         0|            0|            0|  0.00%|    """
   483|       244|    0.0015583|  6.38649e-06|  0.00%|    if isinstance(device, str):
   484|         0|            0|            0|  0.00%|        device = torch.device(device)
   485|       244|   0.00100207|  4.10686e-06|  0.00%|    device_idx: Optional[int] = None
   486|       244|   0.00140285|  5.74941e-06|  0.00%|    if isinstance(device, torch.device):
   487|         0|            0|            0|  0.00%|        if not allow_cpu and device.type == 'cpu':
   488|         0|            0|            0|  0.00%|            raise ValueError('Expected a non cpu device, but got: {}'.format(device))
   489|         0|            0|            0|  0.00%|        device_idx = -1 if device.type == 'cpu' else device.index
   490|       244|   0.00135398|  5.54909e-06|  0.00%|    if isinstance(device, int):
   491|       244|  0.000928164|  3.80395e-06|  0.00%|        device_idx = device
   492|       244|  0.000904799|  3.70819e-06|  0.00%|    if device_idx is None:
   493|         0|            0|            0|  0.00%|        if optional:
   494|         0|            0|            0|  0.00%|            # The eager API _get_current_device_index uses `lambda` functions which are
   495|         0|            0|            0|  0.00%|            # not supported in JIT and hence not scriptable. The JIT equivalent API to get
   496|         0|            0|            0|  0.00%|            # the current device index is `get_current_device_index()` which can
   497|         0|            0|            0|  0.00%|            # be scripted. We use is_scripting to check the mode we are in and call the
   498|         0|            0|            0|  0.00%|            # appropriate API.
   499|         0|            0|            0|  0.00%|            if torch.jit.is_scripting():
   500|         0|            0|            0|  0.00%|                device_idx = get_current_device_index()
   501|         0|            0|            0|  0.00%|            else:
   502|         0|            0|            0|  0.00%|                device_idx = _get_current_device_index()
   503|         0|            0|            0|  0.00%|        else:
   504|         0|            0|            0|  0.00%|            raise ValueError('Expected a torch.device with a specified index '
   505|         0|            0|            0|  0.00%|                             'or an integer, but got:{}'.format(device))
   506|       244|  0.000893831|  3.66324e-06|  0.00%|    return device_idx
   507|         0|            0|            0|  0.00%|
   508|         0|            0|            0|  0.00%|
   509|         0|            0|            0|  0.00%|def _handle_complex(tensor):
   510|         0|            0|            0|  0.00%|    """
   511|         0|            0|            0|  0.00%|    Returns a real view of a tensor if complex dtype else just the tensor
   512|         0|            0|            0|  0.00%|    need to check if a UninitializedParameter because otherwise checking is_complex is an error for a LazyModule
   513|         0|            0|            0|  0.00%|    """
   514|         0|            0|            0|  0.00%|    return torch.view_as_real(tensor) if not isinstance(tensor,
   515|         0|            0|            0|  0.00%|                                                        torch.nn.UninitializedParameter) and tensor.is_complex() else tensor
File: /opt/conda/lib/python3.8/random.py
File duration: 0.0204699s (0.04%)
Line #|      Hits|         Time| Time per hit|      %|Source code
------+----------+-------------+-------------+-------+-----------
     1|         0|            0|            0|  0.00%|"""Random variable generators.
     2|         0|            0|            0|  0.00%|
     3|         0|            0|            0|  0.00%|    integers
     4|         0|            0|            0|  0.00%|    --------
     5|         0|            0|            0|  0.00%|           uniform within range
     6|         0|            0|            0|  0.00%|
     7|         0|            0|            0|  0.00%|    sequences
     8|         0|            0|            0|  0.00%|    ---------
     9|         0|            0|            0|  0.00%|           pick random element
    10|         0|            0|            0|  0.00%|           pick random sample
    11|         0|            0|            0|  0.00%|           pick weighted random sample
    12|         0|            0|            0|  0.00%|           generate random permutation
    13|         0|            0|            0|  0.00%|
    14|         0|            0|            0|  0.00%|    distributions on the real line:
    15|         0|            0|            0|  0.00%|    ------------------------------
    16|         0|            0|            0|  0.00%|           uniform
    17|         0|            0|            0|  0.00%|           triangular
    18|         0|            0|            0|  0.00%|           normal (Gaussian)
    19|         0|            0|            0|  0.00%|           lognormal
    20|         0|            0|            0|  0.00%|           negative exponential
    21|         0|            0|            0|  0.00%|           gamma
    22|         0|            0|            0|  0.00%|           beta
    23|         0|            0|            0|  0.00%|           pareto
    24|         0|            0|            0|  0.00%|           Weibull
    25|         0|            0|            0|  0.00%|
    26|         0|            0|            0|  0.00%|    distributions on the circle (angles 0 to 2pi)
    27|         0|            0|            0|  0.00%|    ---------------------------------------------
    28|         0|            0|            0|  0.00%|           circular uniform
    29|         0|            0|            0|  0.00%|           von Mises
    30|         0|            0|            0|  0.00%|
    31|         0|            0|            0|  0.00%|General notes on the underlying Mersenne Twister core generator:
    32|         0|            0|            0|  0.00%|
    33|         0|            0|            0|  0.00%|* The period is 2**19937-1.
    34|         0|            0|            0|  0.00%|* It is one of the most extensively tested generators in existence.
    35|         0|            0|            0|  0.00%|* The random() method is implemented in C, executes in a single Python step,
    36|         0|            0|            0|  0.00%|  and is, therefore, threadsafe.
    37|         0|            0|            0|  0.00%|
    38|         0|            0|            0|  0.00%|"""
    39|         0|            0|            0|  0.00%|
    40|         0|            0|            0|  0.00%|from warnings import warn as _warn
    41|         0|            0|            0|  0.00%|from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil
    42|         0|            0|            0|  0.00%|from math import sqrt as _sqrt, acos as _acos, cos as _cos, sin as _sin
    43|         0|            0|            0|  0.00%|from os import urandom as _urandom
    44|         0|            0|            0|  0.00%|from _collections_abc import Set as _Set, Sequence as _Sequence
    45|         0|            0|            0|  0.00%|from itertools import accumulate as _accumulate, repeat as _repeat
    46|         0|            0|            0|  0.00%|from bisect import bisect as _bisect
    47|         0|            0|            0|  0.00%|import os as _os
    48|         0|            0|            0|  0.00%|
    49|         0|            0|            0|  0.00%|try:
    50|         0|            0|            0|  0.00%|    # hashlib is pretty heavy to load, try lean internal module first
    51|         0|            0|            0|  0.00%|    from _sha512 import sha512 as _sha512
    52|         0|            0|            0|  0.00%|except ImportError:
    53|         0|            0|            0|  0.00%|    # fallback to official implementation
    54|         0|            0|            0|  0.00%|    from hashlib import sha512 as _sha512
    55|         0|            0|            0|  0.00%|
    56|         0|            0|            0|  0.00%|
    57|         0|            0|            0|  0.00%|__all__ = ["Random","seed","random","uniform","randint","choice","sample",
    58|         0|            0|            0|  0.00%|           "randrange","shuffle","normalvariate","lognormvariate",
    59|         0|            0|            0|  0.00%|           "expovariate","vonmisesvariate","gammavariate","triangular",
    60|         0|            0|            0|  0.00%|           "gauss","betavariate","paretovariate","weibullvariate",
    61|         0|            0|            0|  0.00%|           "getstate","setstate", "getrandbits", "choices",
    62|         0|            0|            0|  0.00%|           "SystemRandom"]
    63|         0|            0|            0|  0.00%|
    64|         0|            0|            0|  0.00%|NV_MAGICCONST = 4 * _exp(-0.5)/_sqrt(2.0)
    65|         0|            0|            0|  0.00%|TWOPI = 2.0*_pi
    66|         0|            0|            0|  0.00%|LOG4 = _log(4.0)
    67|         0|            0|            0|  0.00%|SG_MAGICCONST = 1.0 + _log(4.5)
    68|         0|            0|            0|  0.00%|BPF = 53        # Number of bits in a float
    69|         0|            0|            0|  0.00%|RECIP_BPF = 2**-BPF
    70|         0|            0|            0|  0.00%|
    71|         0|            0|            0|  0.00%|
    72|         0|            0|            0|  0.00%|# Translated by Guido van Rossum from C source provided by
    73|         0|            0|            0|  0.00%|# Adrian Baddeley.  Adapted by Raymond Hettinger for use with
    74|         0|            0|            0|  0.00%|# the Mersenne Twister  and os.urandom() core generators.
    75|         0|            0|            0|  0.00%|
    76|         0|            0|            0|  0.00%|import _random
    77|         0|            0|            0|  0.00%|
    78|         0|            0|            0|  0.00%|class Random(_random.Random):
    79|         0|            0|            0|  0.00%|    """Random number generator base class used by bound module functions.
    80|         0|            0|            0|  0.00%|
    81|         0|            0|            0|  0.00%|    Used to instantiate instances of Random to get generators that don't
    82|         0|            0|            0|  0.00%|    share state.
    83|         0|            0|            0|  0.00%|
    84|         0|            0|            0|  0.00%|    Class Random can also be subclassed if you want to use a different basic
    85|         0|            0|            0|  0.00%|    generator of your own devising: in that case, override the following
    86|         0|            0|            0|  0.00%|    methods:  random(), seed(), getstate(), and setstate().
    87|         0|            0|            0|  0.00%|    Optionally, implement a getrandbits() method so that randrange()
    88|         0|            0|            0|  0.00%|    can cover arbitrarily large ranges.
    89|         0|            0|            0|  0.00%|
    90|         0|            0|            0|  0.00%|    """
    91|         0|            0|            0|  0.00%|
    92|         0|            0|            0|  0.00%|    VERSION = 3     # used by getstate/setstate
    93|         0|            0|            0|  0.00%|
    94|         0|            0|            0|  0.00%|    def __init__(self, x=None):
    95|         0|            0|            0|  0.00%|        """Initialize an instance.
    96|         0|            0|            0|  0.00%|
    97|         0|            0|            0|  0.00%|        Optional argument x controls seeding, as for Random.seed().
    98|         0|            0|            0|  0.00%|        """
    99|         0|            0|            0|  0.00%|
   100|         0|            0|            0|  0.00%|        self.seed(x)
   101|         0|            0|            0|  0.00%|        self.gauss_next = None
   102|         0|            0|            0|  0.00%|
   103|         0|            0|            0|  0.00%|    def __init_subclass__(cls, /, **kwargs):
   104|         0|            0|            0|  0.00%|        """Control how subclasses generate random integers.
   105|         0|            0|            0|  0.00%|
   106|         0|            0|            0|  0.00%|        The algorithm a subclass can use depends on the random() and/or
   107|         0|            0|            0|  0.00%|        getrandbits() implementation available to it and determines
   108|         0|            0|            0|  0.00%|        whether it can generate random integers from arbitrarily large
   109|         0|            0|            0|  0.00%|        ranges.
   110|         0|            0|            0|  0.00%|        """
   111|         0|            0|            0|  0.00%|
   112|         0|            0|            0|  0.00%|        for c in cls.__mro__:
   113|         0|            0|            0|  0.00%|            if '_randbelow' in c.__dict__:
   114|         0|            0|            0|  0.00%|                # just inherit it
   115|         0|            0|            0|  0.00%|                break
   116|         0|            0|            0|  0.00%|            if 'getrandbits' in c.__dict__:
   117|         0|            0|            0|  0.00%|                cls._randbelow = cls._randbelow_with_getrandbits
   118|         0|            0|            0|  0.00%|                break
   119|         0|            0|            0|  0.00%|            if 'random' in c.__dict__:
   120|         0|            0|            0|  0.00%|                cls._randbelow = cls._randbelow_without_getrandbits
   121|         0|            0|            0|  0.00%|                break
   122|         0|            0|            0|  0.00%|
   123|         0|            0|            0|  0.00%|    def seed(self, a=None, version=2):
   124|         0|            0|            0|  0.00%|        """Initialize internal state from hashable object.
   125|         0|            0|            0|  0.00%|
   126|         0|            0|            0|  0.00%|        None or no argument seeds from current time or from an operating
   127|         0|            0|            0|  0.00%|        system specific randomness source if available.
   128|         0|            0|            0|  0.00%|
   129|         0|            0|            0|  0.00%|        If *a* is an int, all bits are used.
   130|         0|            0|            0|  0.00%|
   131|         0|            0|            0|  0.00%|        For version 2 (the default), all of the bits are used if *a* is a str,
   132|         0|            0|            0|  0.00%|        bytes, or bytearray.  For version 1 (provided for reproducing random
   133|         0|            0|            0|  0.00%|        sequences from older versions of Python), the algorithm for str and
   134|         0|            0|            0|  0.00%|        bytes generates a narrower range of seeds.
   135|         0|            0|            0|  0.00%|
   136|         0|            0|            0|  0.00%|        """
   137|         0|            0|            0|  0.00%|
   138|         0|            0|            0|  0.00%|        if version == 1 and isinstance(a, (str, bytes)):
   139|         0|            0|            0|  0.00%|            a = a.decode('latin-1') if isinstance(a, bytes) else a
   140|         0|            0|            0|  0.00%|            x = ord(a[0]) << 7 if a else 0
   141|         0|            0|            0|  0.00%|            for c in map(ord, a):
   142|         0|            0|            0|  0.00%|                x = ((1000003 * x) ^ c) & 0xFFFFFFFFFFFFFFFF
   143|         0|            0|            0|  0.00%|            x ^= len(a)
   144|         0|            0|            0|  0.00%|            a = -2 if x == -1 else x
   145|         0|            0|            0|  0.00%|
   146|         0|            0|            0|  0.00%|        if version == 2 and isinstance(a, (str, bytes, bytearray)):
   147|         0|            0|            0|  0.00%|            if isinstance(a, str):
   148|         0|            0|            0|  0.00%|                a = a.encode()
   149|         0|            0|            0|  0.00%|            a += _sha512(a).digest()
   150|         0|            0|            0|  0.00%|            a = int.from_bytes(a, 'big')
   151|         0|            0|            0|  0.00%|
   152|         0|            0|            0|  0.00%|        super().seed(a)
   153|         0|            0|            0|  0.00%|        self.gauss_next = None
   154|         0|            0|            0|  0.00%|
   155|         0|            0|            0|  0.00%|    def getstate(self):
   156|         0|            0|            0|  0.00%|        """Return internal state; can be passed to setstate() later."""
   157|         0|            0|            0|  0.00%|        return self.VERSION, super().getstate(), self.gauss_next
   158|         0|            0|            0|  0.00%|
   159|         0|            0|            0|  0.00%|    def setstate(self, state):
   160|         0|            0|            0|  0.00%|        """Restore internal state from object returned by getstate()."""
   161|         0|            0|            0|  0.00%|        version = state[0]
   162|         0|            0|            0|  0.00%|        if version == 3:
   163|         0|            0|            0|  0.00%|            version, internalstate, self.gauss_next = state
   164|         0|            0|            0|  0.00%|            super().setstate(internalstate)
   165|         0|            0|            0|  0.00%|        elif version == 2:
   166|         0|            0|            0|  0.00%|            version, internalstate, self.gauss_next = state
   167|         0|            0|            0|  0.00%|            # In version 2, the state was saved as signed ints, which causes
   168|         0|            0|            0|  0.00%|            #   inconsistencies between 32/64-bit systems. The state is
   169|         0|            0|            0|  0.00%|            #   really unsigned 32-bit ints, so we convert negative ints from
   170|         0|            0|            0|  0.00%|            #   version 2 to positive longs for version 3.
   171|         0|            0|            0|  0.00%|            try:
   172|         0|            0|            0|  0.00%|                internalstate = tuple(x % (2**32) for x in internalstate)
   173|         0|            0|            0|  0.00%|            except ValueError as e:
   174|         0|            0|            0|  0.00%|                raise TypeError from e
   175|         0|            0|            0|  0.00%|            super().setstate(internalstate)
   176|         0|            0|            0|  0.00%|        else:
   177|         0|            0|            0|  0.00%|            raise ValueError("state with version %s passed to "
   178|         0|            0|            0|  0.00%|                             "Random.setstate() of version %s" %
   179|         0|            0|            0|  0.00%|                             (version, self.VERSION))
   180|         0|            0|            0|  0.00%|
   181|         0|            0|            0|  0.00%|## ---- Methods below this point do not need to be overridden when
   182|         0|            0|            0|  0.00%|## ---- subclassing for the purpose of using a different core generator.
   183|         0|            0|            0|  0.00%|
   184|         0|            0|            0|  0.00%|## -------------------- pickle support  -------------------
   185|         0|            0|            0|  0.00%|
   186|         0|            0|            0|  0.00%|    # Issue 17489: Since __reduce__ was defined to fix #759889 this is no
   187|         0|            0|            0|  0.00%|    # longer called; we leave it here because it has been here since random was
   188|         0|            0|            0|  0.00%|    # rewritten back in 2001 and why risk breaking something.
   189|         0|            0|            0|  0.00%|    def __getstate__(self): # for pickle
   190|         0|            0|            0|  0.00%|        return self.getstate()
   191|         0|            0|            0|  0.00%|
   192|         0|            0|            0|  0.00%|    def __setstate__(self, state):  # for pickle
   193|         0|            0|            0|  0.00%|        self.setstate(state)
   194|         0|            0|            0|  0.00%|
   195|         0|            0|            0|  0.00%|    def __reduce__(self):
   196|         0|            0|            0|  0.00%|        return self.__class__, (), self.getstate()
   197|         0|            0|            0|  0.00%|
   198|         0|            0|            0|  0.00%|## -------------------- integer methods  -------------------
   199|         0|            0|            0|  0.00%|
   200|         0|            0|            0|  0.00%|    def randrange(self, start, stop=None, step=1, _int=int):
   201|         0|            0|            0|  0.00%|        """Choose a random item from range(start, stop[, step]).
   202|         0|            0|            0|  0.00%|
   203|         0|            0|            0|  0.00%|        This fixes the problem with randint() which includes the
   204|         0|            0|            0|  0.00%|        endpoint; in Python this is usually not what you want.
   205|         0|            0|            0|  0.00%|
   206|         0|            0|            0|  0.00%|        """
   207|         0|            0|            0|  0.00%|
   208|         0|            0|            0|  0.00%|        # This code is a bit messy to make it fast for the
   209|         0|            0|            0|  0.00%|        # common case while still doing adequate error checking.
   210|         0|            0|            0|  0.00%|        istart = _int(start)
   211|         0|            0|            0|  0.00%|        if istart != start:
   212|         0|            0|            0|  0.00%|            raise ValueError("non-integer arg 1 for randrange()")
   213|         0|            0|            0|  0.00%|        if stop is None:
   214|         0|            0|            0|  0.00%|            if istart > 0:
   215|         0|            0|            0|  0.00%|                return self._randbelow(istart)
   216|         0|            0|            0|  0.00%|            raise ValueError("empty range for randrange()")
   217|         0|            0|            0|  0.00%|
   218|         0|            0|            0|  0.00%|        # stop argument supplied.
   219|         0|            0|            0|  0.00%|        istop = _int(stop)
   220|         0|            0|            0|  0.00%|        if istop != stop:
   221|         0|            0|            0|  0.00%|            raise ValueError("non-integer stop for randrange()")
   222|         0|            0|            0|  0.00%|        width = istop - istart
   223|         0|            0|            0|  0.00%|        if step == 1 and width > 0:
   224|         0|            0|            0|  0.00%|            return istart + self._randbelow(width)
   225|         0|            0|            0|  0.00%|        if step == 1:
   226|         0|            0|            0|  0.00%|            raise ValueError("empty range for randrange() (%d, %d, %d)" % (istart, istop, width))
   227|         0|            0|            0|  0.00%|
   228|         0|            0|            0|  0.00%|        # Non-unit step argument supplied.
   229|         0|            0|            0|  0.00%|        istep = _int(step)
   230|         0|            0|            0|  0.00%|        if istep != step:
   231|         0|            0|            0|  0.00%|            raise ValueError("non-integer step for randrange()")
   232|         0|            0|            0|  0.00%|        if istep > 0:
   233|         0|            0|            0|  0.00%|            n = (width + istep - 1) // istep
   234|         0|            0|            0|  0.00%|        elif istep < 0:
   235|         0|            0|            0|  0.00%|            n = (width + istep + 1) // istep
   236|         0|            0|            0|  0.00%|        else:
   237|         0|            0|            0|  0.00%|            raise ValueError("zero step for randrange()")
   238|         0|            0|            0|  0.00%|
   239|         0|            0|            0|  0.00%|        if n <= 0:
   240|         0|            0|            0|  0.00%|            raise ValueError("empty range for randrange()")
   241|         0|            0|            0|  0.00%|
   242|         0|            0|            0|  0.00%|        return istart + istep*self._randbelow(n)
   243|         0|            0|            0|  0.00%|
   244|         0|            0|            0|  0.00%|    def randint(self, a, b):
   245|         0|            0|            0|  0.00%|        """Return random integer in range [a, b], including both end points.
   246|         0|            0|            0|  0.00%|        """
   247|         0|            0|            0|  0.00%|
   248|         0|            0|            0|  0.00%|        return self.randrange(a, b+1)
   249|         0|            0|            0|  0.00%|
   250|       320|   0.00146103|  4.56572e-06|  0.00%|    def _randbelow_with_getrandbits(self, n):
   251|         0|            0|            0|  0.00%|        "Return a random int in the range [0,n).  Raises ValueError if n==0."
   252|         0|            0|            0|  0.00%|
   253|       320|   0.00149012|  4.65661e-06|  0.00%|        getrandbits = self.getrandbits
   254|       320|   0.00199795|  6.24359e-06|  0.00%|        k = n.bit_length()  # don't use (n-1) here because n can be 1
   255|       320|   0.00187564|  5.86137e-06|  0.00%|        r = getrandbits(k)          # 0 <= r < 2**k
   256|       528|   0.00205994|  3.90139e-06|  0.00%|        while r >= n:
   257|       208|   0.00113201|  5.44236e-06|  0.00%|            r = getrandbits(k)
   258|       320|     0.001194|  3.73125e-06|  0.00%|        return r
   259|         0|            0|            0|  0.00%|
   260|         0|            0|            0|  0.00%|    def _randbelow_without_getrandbits(self, n, int=int, maxsize=1<<BPF):
   261|         0|            0|            0|  0.00%|        """Return a random int in the range [0,n).  Raises ValueError if n==0.
   262|         0|            0|            0|  0.00%|
   263|         0|            0|            0|  0.00%|        The implementation does not use getrandbits, but only random.
   264|         0|            0|            0|  0.00%|        """
   265|         0|            0|            0|  0.00%|
   266|         0|            0|            0|  0.00%|        random = self.random
   267|         0|            0|            0|  0.00%|        if n >= maxsize:
   268|         0|            0|            0|  0.00%|            _warn("Underlying random() generator does not supply \n"
   269|         0|            0|            0|  0.00%|                "enough bits to choose from a population range this large.\n"
   270|         0|            0|            0|  0.00%|                "To remove the range limitation, add a getrandbits() method.")
   271|         0|            0|            0|  0.00%|            return int(random() * n)
   272|         0|            0|            0|  0.00%|        if n == 0:
   273|         0|            0|            0|  0.00%|            raise ValueError("Boundary cannot be zero")
   274|         0|            0|            0|  0.00%|        rem = maxsize % n
   275|         0|            0|            0|  0.00%|        limit = (maxsize - rem) / maxsize   # int(limit * maxsize) % n == 0
   276|         0|            0|            0|  0.00%|        r = random()
   277|         0|            0|            0|  0.00%|        while r >= limit:
   278|         0|            0|            0|  0.00%|            r = random()
   279|         0|            0|            0|  0.00%|        return int(r*maxsize) % n
   280|         0|            0|            0|  0.00%|
   281|         0|            0|            0|  0.00%|    _randbelow = _randbelow_with_getrandbits
   282|         0|            0|            0|  0.00%|
   283|         0|            0|            0|  0.00%|## -------------------- sequence methods  -------------------
   284|         0|            0|            0|  0.00%|
   285|       320|   0.00144911|  4.52846e-06|  0.00%|    def choice(self, seq):
   286|         0|            0|            0|  0.00%|        """Choose a random element from a non-empty sequence."""
   287|       320|   0.00133491|  4.17158e-06|  0.00%|        try:
   288|       320|   0.00475836|  1.48699e-05|  0.01%|            i = self._randbelow(len(seq))
(call)|       320|    0.0112107|  3.50334e-05|  0.02%|# /opt/conda/lib/python3.8/random.py:250 _randbelow_with_getrandbits
   289|         0|            0|            0|  0.00%|        except ValueError:
   290|         0|            0|            0|  0.00%|            raise IndexError('Cannot choose from an empty sequence') from None
   291|       320|   0.00171685|  5.36516e-06|  0.00%|        return seq[i]
   292|         0|            0|            0|  0.00%|
   293|         0|            0|            0|  0.00%|    def shuffle(self, x, random=None):
   294|         0|            0|            0|  0.00%|        """Shuffle list x in place, and return None.
   295|         0|            0|            0|  0.00%|
   296|         0|            0|            0|  0.00%|        Optional argument random is a 0-argument function returning a
   297|         0|            0|            0|  0.00%|        random float in [0.0, 1.0); if it is the default None, the
   298|         0|            0|            0|  0.00%|        standard random.random will be used.
   299|         0|            0|            0|  0.00%|
   300|         0|            0|            0|  0.00%|        """
   301|         0|            0|            0|  0.00%|
   302|         0|            0|            0|  0.00%|        if random is None:
   303|         0|            0|            0|  0.00%|            randbelow = self._randbelow
   304|         0|            0|            0|  0.00%|            for i in reversed(range(1, len(x))):
   305|         0|            0|            0|  0.00%|                # pick an element in x[:i+1] with which to exchange x[i]
   306|         0|            0|            0|  0.00%|                j = randbelow(i+1)
   307|         0|            0|            0|  0.00%|                x[i], x[j] = x[j], x[i]
   308|         0|            0|            0|  0.00%|        else:
   309|         0|            0|            0|  0.00%|            _int = int
   310|         0|            0|            0|  0.00%|            for i in reversed(range(1, len(x))):
   311|         0|            0|            0|  0.00%|                # pick an element in x[:i+1] with which to exchange x[i]
   312|         0|            0|            0|  0.00%|                j = _int(random() * (i+1))
   313|         0|            0|            0|  0.00%|                x[i], x[j] = x[j], x[i]
   314|         0|            0|            0|  0.00%|
   315|         0|            0|            0|  0.00%|    def sample(self, population, k):
   316|         0|            0|            0|  0.00%|        """Chooses k unique random elements from a population sequence or set.
   317|         0|            0|            0|  0.00%|
   318|         0|            0|            0|  0.00%|        Returns a new list containing elements from the population while
   319|         0|            0|            0|  0.00%|        leaving the original population unchanged.  The resulting list is
   320|         0|            0|            0|  0.00%|        in selection order so that all sub-slices will also be valid random
   321|         0|            0|            0|  0.00%|        samples.  This allows raffle winners (the sample) to be partitioned
   322|         0|            0|            0|  0.00%|        into grand prize and second place winners (the subslices).
   323|         0|            0|            0|  0.00%|
   324|         0|            0|            0|  0.00%|        Members of the population need not be hashable or unique.  If the
   325|         0|            0|            0|  0.00%|        population contains repeats, then each occurrence is a possible
   326|         0|            0|            0|  0.00%|        selection in the sample.
   327|         0|            0|            0|  0.00%|
   328|         0|            0|            0|  0.00%|        To choose a sample in a range of integers, use range as an argument.
   329|         0|            0|            0|  0.00%|        This is especially fast and space efficient for sampling from a
   330|         0|            0|            0|  0.00%|        large population:   sample(range(10000000), 60)
   331|         0|            0|            0|  0.00%|        """
   332|         0|            0|            0|  0.00%|
   333|         0|            0|            0|  0.00%|        # Sampling without replacement entails tracking either potential
   334|         0|            0|            0|  0.00%|        # selections (the pool) in a list or previous selections in a set.
   335|         0|            0|            0|  0.00%|
   336|         0|            0|            0|  0.00%|        # When the number of selections is small compared to the
   337|         0|            0|            0|  0.00%|        # population, then tracking selections is efficient, requiring
   338|         0|            0|            0|  0.00%|        # only a small set and an occasional reselection.  For
   339|         0|            0|            0|  0.00%|        # a larger number of selections, the pool tracking method is
   340|         0|            0|            0|  0.00%|        # preferred since the list takes less space than the
   341|         0|            0|            0|  0.00%|        # set and it doesn't suffer from frequent reselections.
   342|         0|            0|            0|  0.00%|
   343|         0|            0|            0|  0.00%|        # The number of calls to _randbelow() is kept at or near k, the
   344|         0|            0|            0|  0.00%|        # theoretical minimum.  This is important because running time
   345|         0|            0|            0|  0.00%|        # is dominated by _randbelow() and because it extracts the
   346|         0|            0|            0|  0.00%|        # least entropy from the underlying random number generators.
   347|         0|            0|            0|  0.00%|
   348|         0|            0|            0|  0.00%|        # Memory requirements are kept to the smaller of a k-length
   349|         0|            0|            0|  0.00%|        # set or an n-length list.
   350|         0|            0|            0|  0.00%|
   351|         0|            0|            0|  0.00%|        # There are other sampling algorithms that do not require
   352|         0|            0|            0|  0.00%|        # auxiliary memory, but they were rejected because they made
   353|         0|            0|            0|  0.00%|        # too many calls to _randbelow(), making them slower and
   354|         0|            0|            0|  0.00%|        # causing them to eat more entropy than necessary.
   355|         0|            0|            0|  0.00%|
   356|         0|            0|            0|  0.00%|        if isinstance(population, _Set):
   357|         0|            0|            0|  0.00%|            population = tuple(population)
   358|         0|            0|            0|  0.00%|        if not isinstance(population, _Sequence):
   359|         0|            0|            0|  0.00%|            raise TypeError("Population must be a sequence or set.  For dicts, use list(d).")
   360|         0|            0|            0|  0.00%|        randbelow = self._randbelow
   361|         0|            0|            0|  0.00%|        n = len(population)
   362|         0|            0|            0|  0.00%|        if not 0 <= k <= n:
   363|         0|            0|            0|  0.00%|            raise ValueError("Sample larger than population or is negative")
   364|         0|            0|            0|  0.00%|        result = [None] * k
   365|         0|            0|            0|  0.00%|        setsize = 21        # size of a small set minus size of an empty list
   366|         0|            0|            0|  0.00%|        if k > 5:
   367|         0|            0|            0|  0.00%|            setsize += 4 ** _ceil(_log(k * 3, 4)) # table size for big sets
   368|         0|            0|            0|  0.00%|        if n <= setsize:
   369|         0|            0|            0|  0.00%|            # An n-length list is smaller than a k-length set
   370|         0|            0|            0|  0.00%|            pool = list(population)
   371|         0|            0|            0|  0.00%|            for i in range(k):         # invariant:  non-selected at [0,n-i)
   372|         0|            0|            0|  0.00%|                j = randbelow(n-i)
   373|         0|            0|            0|  0.00%|                result[i] = pool[j]
   374|         0|            0|            0|  0.00%|                pool[j] = pool[n-i-1]   # move non-selected item into vacancy
   375|         0|            0|            0|  0.00%|        else:
   376|         0|            0|            0|  0.00%|            selected = set()
   377|         0|            0|            0|  0.00%|            selected_add = selected.add
   378|         0|            0|            0|  0.00%|            for i in range(k):
   379|         0|            0|            0|  0.00%|                j = randbelow(n)
   380|         0|            0|            0|  0.00%|                while j in selected:
   381|         0|            0|            0|  0.00%|                    j = randbelow(n)
   382|         0|            0|            0|  0.00%|                selected_add(j)
   383|         0|            0|            0|  0.00%|                result[i] = population[j]
   384|         0|            0|            0|  0.00%|        return result
   385|         0|            0|            0|  0.00%|
   386|         0|            0|            0|  0.00%|    def choices(self, population, weights=None, *, cum_weights=None, k=1):
   387|         0|            0|            0|  0.00%|        """Return a k sized list of population elements chosen with replacement.
   388|         0|            0|            0|  0.00%|
   389|         0|            0|            0|  0.00%|        If the relative weights or cumulative weights are not specified,
   390|         0|            0|            0|  0.00%|        the selections are made with equal probability.
   391|         0|            0|            0|  0.00%|
   392|         0|            0|            0|  0.00%|        """
   393|         0|            0|            0|  0.00%|        random = self.random
   394|         0|            0|            0|  0.00%|        n = len(population)
   395|         0|            0|            0|  0.00%|        if cum_weights is None:
   396|         0|            0|            0|  0.00%|            if weights is None:
   397|         0|            0|            0|  0.00%|                _int = int
   398|         0|            0|            0|  0.00%|                n += 0.0    # convert to float for a small speed improvement
   399|         0|            0|            0|  0.00%|                return [population[_int(random() * n)] for i in _repeat(None, k)]
   400|         0|            0|            0|  0.00%|            cum_weights = list(_accumulate(weights))
   401|         0|            0|            0|  0.00%|        elif weights is not None:
   402|         0|            0|            0|  0.00%|            raise TypeError('Cannot specify both weights and cumulative weights')
   403|         0|            0|            0|  0.00%|        if len(cum_weights) != n:
   404|         0|            0|            0|  0.00%|            raise ValueError('The number of weights does not match the population')
   405|         0|            0|            0|  0.00%|        bisect = _bisect
   406|         0|            0|            0|  0.00%|        total = cum_weights[-1] + 0.0   # convert to float
   407|         0|            0|            0|  0.00%|        hi = n - 1
   408|         0|            0|            0|  0.00%|        return [population[bisect(cum_weights, random() * total, 0, hi)]
   409|         0|            0|            0|  0.00%|                for i in _repeat(None, k)]
   410|         0|            0|            0|  0.00%|
   411|         0|            0|            0|  0.00%|## -------------------- real-valued distributions  -------------------
   412|         0|            0|            0|  0.00%|
   413|         0|            0|            0|  0.00%|## -------------------- uniform distribution -------------------
   414|         0|            0|            0|  0.00%|
   415|         0|            0|            0|  0.00%|    def uniform(self, a, b):
   416|         0|            0|            0|  0.00%|        "Get a random number in the range [a, b) or [a, b] depending on rounding."
   417|         0|            0|            0|  0.00%|        return a + (b-a) * self.random()
   418|         0|            0|            0|  0.00%|
   419|         0|            0|            0|  0.00%|## -------------------- triangular --------------------
   420|         0|            0|            0|  0.00%|
   421|         0|            0|            0|  0.00%|    def triangular(self, low=0.0, high=1.0, mode=None):
   422|         0|            0|            0|  0.00%|        """Triangular distribution.
   423|         0|            0|            0|  0.00%|
   424|         0|            0|            0|  0.00%|        Continuous distribution bounded by given lower and upper limits,
   425|         0|            0|            0|  0.00%|        and having a given mode value in-between.
   426|         0|            0|            0|  0.00%|
   427|         0|            0|            0|  0.00%|        http://en.wikipedia.org/wiki/Triangular_distribution
   428|         0|            0|            0|  0.00%|
   429|         0|            0|            0|  0.00%|        """
   430|         0|            0|            0|  0.00%|        u = self.random()
   431|         0|            0|            0|  0.00%|        try:
   432|         0|            0|            0|  0.00%|            c = 0.5 if mode is None else (mode - low) / (high - low)
   433|         0|            0|            0|  0.00%|        except ZeroDivisionError:
   434|         0|            0|            0|  0.00%|            return low
   435|         0|            0|            0|  0.00%|        if u > c:
   436|         0|            0|            0|  0.00%|            u = 1.0 - u
   437|         0|            0|            0|  0.00%|            c = 1.0 - c
   438|         0|            0|            0|  0.00%|            low, high = high, low
   439|         0|            0|            0|  0.00%|        return low + (high - low) * _sqrt(u * c)
   440|         0|            0|            0|  0.00%|
   441|         0|            0|            0|  0.00%|## -------------------- normal distribution --------------------
   442|         0|            0|            0|  0.00%|
   443|         0|            0|            0|  0.00%|    def normalvariate(self, mu, sigma):
   444|         0|            0|            0|  0.00%|        """Normal distribution.
   445|         0|            0|            0|  0.00%|
   446|         0|            0|            0|  0.00%|        mu is the mean, and sigma is the standard deviation.
   447|         0|            0|            0|  0.00%|
   448|         0|            0|            0|  0.00%|        """
   449|         0|            0|            0|  0.00%|        # mu = mean, sigma = standard deviation
   450|         0|            0|            0|  0.00%|
   451|         0|            0|            0|  0.00%|        # Uses Kinderman and Monahan method. Reference: Kinderman,
   452|         0|            0|            0|  0.00%|        # A.J. and Monahan, J.F., "Computer generation of random
   453|         0|            0|            0|  0.00%|        # variables using the ratio of uniform deviates", ACM Trans
   454|         0|            0|            0|  0.00%|        # Math Software, 3, (1977), pp257-260.
   455|         0|            0|            0|  0.00%|
   456|         0|            0|            0|  0.00%|        random = self.random
   457|         0|            0|            0|  0.00%|        while 1:
   458|         0|            0|            0|  0.00%|            u1 = random()
   459|         0|            0|            0|  0.00%|            u2 = 1.0 - random()
   460|         0|            0|            0|  0.00%|            z = NV_MAGICCONST*(u1-0.5)/u2
   461|         0|            0|            0|  0.00%|            zz = z*z/4.0
   462|         0|            0|            0|  0.00%|            if zz <= -_log(u2):
   463|         0|            0|            0|  0.00%|                break
   464|         0|            0|            0|  0.00%|        return mu + z*sigma
   465|         0|            0|            0|  0.00%|
   466|         0|            0|            0|  0.00%|## -------------------- lognormal distribution --------------------
   467|         0|            0|            0|  0.00%|
   468|         0|            0|            0|  0.00%|    def lognormvariate(self, mu, sigma):
   469|         0|            0|            0|  0.00%|        """Log normal distribution.
   470|         0|            0|            0|  0.00%|
   471|         0|            0|            0|  0.00%|        If you take the natural logarithm of this distribution, you'll get a
   472|         0|            0|            0|  0.00%|        normal distribution with mean mu and standard deviation sigma.
   473|         0|            0|            0|  0.00%|        mu can have any value, and sigma must be greater than zero.
   474|         0|            0|            0|  0.00%|
   475|         0|            0|            0|  0.00%|        """
   476|         0|            0|            0|  0.00%|        return _exp(self.normalvariate(mu, sigma))
   477|         0|            0|            0|  0.00%|
   478|         0|            0|            0|  0.00%|## -------------------- exponential distribution --------------------
   479|         0|            0|            0|  0.00%|
   480|         0|            0|            0|  0.00%|    def expovariate(self, lambd):
   481|         0|            0|            0|  0.00%|        """Exponential distribution.
   482|         0|            0|            0|  0.00%|
   483|         0|            0|            0|  0.00%|        lambd is 1.0 divided by the desired mean.  It should be
   484|         0|            0|            0|  0.00%|        nonzero.  (The parameter would be called "lambda", but that is
   485|         0|            0|            0|  0.00%|        a reserved word in Python.)  Returned values range from 0 to
   486|         0|            0|            0|  0.00%|        positive infinity if lambd is positive, and from negative
   487|         0|            0|            0|  0.00%|        infinity to 0 if lambd is negative.
   488|         0|            0|            0|  0.00%|
   489|         0|            0|            0|  0.00%|        """
   490|         0|            0|            0|  0.00%|        # lambd: rate lambd = 1/mean
   491|         0|            0|            0|  0.00%|        # ('lambda' is a Python reserved word)
   492|         0|            0|            0|  0.00%|
   493|         0|            0|            0|  0.00%|        # we use 1-random() instead of random() to preclude the
   494|         0|            0|            0|  0.00%|        # possibility of taking the log of zero.
   495|         0|            0|            0|  0.00%|        return -_log(1.0 - self.random())/lambd
   496|         0|            0|            0|  0.00%|
   497|         0|            0|            0|  0.00%|## -------------------- von Mises distribution --------------------
   498|         0|            0|            0|  0.00%|
   499|         0|            0|            0|  0.00%|    def vonmisesvariate(self, mu, kappa):
   500|         0|            0|            0|  0.00%|        """Circular data distribution.
   501|         0|            0|            0|  0.00%|
   502|         0|            0|            0|  0.00%|        mu is the mean angle, expressed in radians between 0 and 2*pi, and
   503|         0|            0|            0|  0.00%|        kappa is the concentration parameter, which must be greater than or
   504|         0|            0|            0|  0.00%|        equal to zero.  If kappa is equal to zero, this distribution reduces
   505|         0|            0|            0|  0.00%|        to a uniform random angle over the range 0 to 2*pi.
   506|         0|            0|            0|  0.00%|
   507|         0|            0|            0|  0.00%|        """
   508|         0|            0|            0|  0.00%|        # mu:    mean angle (in radians between 0 and 2*pi)
   509|         0|            0|            0|  0.00%|        # kappa: concentration parameter kappa (>= 0)
   510|         0|            0|            0|  0.00%|        # if kappa = 0 generate uniform random angle
   511|         0|            0|            0|  0.00%|
   512|         0|            0|            0|  0.00%|        # Based upon an algorithm published in: Fisher, N.I.,
   513|         0|            0|            0|  0.00%|        # "Statistical Analysis of Circular Data", Cambridge
   514|         0|            0|            0|  0.00%|        # University Press, 1993.
   515|         0|            0|            0|  0.00%|
   516|         0|            0|            0|  0.00%|        # Thanks to Magnus Kessler for a correction to the
   517|         0|            0|            0|  0.00%|        # implementation of step 4.
   518|         0|            0|            0|  0.00%|
   519|         0|            0|            0|  0.00%|        random = self.random
   520|         0|            0|            0|  0.00%|        if kappa <= 1e-6:
   521|         0|            0|            0|  0.00%|            return TWOPI * random()
   522|         0|            0|            0|  0.00%|
   523|         0|            0|            0|  0.00%|        s = 0.5 / kappa
   524|         0|            0|            0|  0.00%|        r = s + _sqrt(1.0 + s * s)
   525|         0|            0|            0|  0.00%|
   526|         0|            0|            0|  0.00%|        while 1:
   527|         0|            0|            0|  0.00%|            u1 = random()
   528|         0|            0|            0|  0.00%|            z = _cos(_pi * u1)
   529|         0|            0|            0|  0.00%|
   530|         0|            0|            0|  0.00%|            d = z / (r + z)
   531|         0|            0|            0|  0.00%|            u2 = random()
   532|         0|            0|            0|  0.00%|            if u2 < 1.0 - d * d or u2 <= (1.0 - d) * _exp(d):
   533|         0|            0|            0|  0.00%|                break
   534|         0|            0|            0|  0.00%|
   535|         0|            0|            0|  0.00%|        q = 1.0 / r
   536|         0|            0|            0|  0.00%|        f = (q + z) / (1.0 + q * z)
   537|         0|            0|            0|  0.00%|        u3 = random()
   538|         0|            0|            0|  0.00%|        if u3 > 0.5:
   539|         0|            0|            0|  0.00%|            theta = (mu + _acos(f)) % TWOPI
   540|         0|            0|            0|  0.00%|        else:
   541|         0|            0|            0|  0.00%|            theta = (mu - _acos(f)) % TWOPI
   542|         0|            0|            0|  0.00%|
   543|         0|            0|            0|  0.00%|        return theta
   544|         0|            0|            0|  0.00%|
   545|         0|            0|            0|  0.00%|## -------------------- gamma distribution --------------------
   546|         0|            0|            0|  0.00%|
   547|         0|            0|            0|  0.00%|    def gammavariate(self, alpha, beta):
   548|         0|            0|            0|  0.00%|        """Gamma distribution.  Not the gamma function!
   549|         0|            0|            0|  0.00%|
   550|         0|            0|            0|  0.00%|        Conditions on the parameters are alpha > 0 and beta > 0.
   551|         0|            0|            0|  0.00%|
   552|         0|            0|            0|  0.00%|        The probability distribution function is:
   553|         0|            0|            0|  0.00%|
   554|         0|            0|            0|  0.00%|                    x ** (alpha - 1) * math.exp(-x / beta)
   555|         0|            0|            0|  0.00%|          pdf(x) =  --------------------------------------
   556|         0|            0|            0|  0.00%|                      math.gamma(alpha) * beta ** alpha
   557|         0|            0|            0|  0.00%|
   558|         0|            0|            0|  0.00%|        """
   559|         0|            0|            0|  0.00%|
   560|         0|            0|            0|  0.00%|        # alpha > 0, beta > 0, mean is alpha*beta, variance is alpha*beta**2
   561|         0|            0|            0|  0.00%|
   562|         0|            0|            0|  0.00%|        # Warning: a few older sources define the gamma distribution in terms
   563|         0|            0|            0|  0.00%|        # of alpha > -1.0
   564|         0|            0|            0|  0.00%|        if alpha <= 0.0 or beta <= 0.0:
   565|         0|            0|            0|  0.00%|            raise ValueError('gammavariate: alpha and beta must be > 0.0')
   566|         0|            0|            0|  0.00%|
   567|         0|            0|            0|  0.00%|        random = self.random
   568|         0|            0|            0|  0.00%|        if alpha > 1.0:
   569|         0|            0|            0|  0.00%|
   570|         0|            0|            0|  0.00%|            # Uses R.C.H. Cheng, "The generation of Gamma
   571|         0|            0|            0|  0.00%|            # variables with non-integral shape parameters",
   572|         0|            0|            0|  0.00%|            # Applied Statistics, (1977), 26, No. 1, p71-74
   573|         0|            0|            0|  0.00%|
   574|         0|            0|            0|  0.00%|            ainv = _sqrt(2.0 * alpha - 1.0)
   575|         0|            0|            0|  0.00%|            bbb = alpha - LOG4
   576|         0|            0|            0|  0.00%|            ccc = alpha + ainv
   577|         0|            0|            0|  0.00%|
   578|         0|            0|            0|  0.00%|            while 1:
   579|         0|            0|            0|  0.00%|                u1 = random()
   580|         0|            0|            0|  0.00%|                if not 1e-7 < u1 < .9999999:
   581|         0|            0|            0|  0.00%|                    continue
   582|         0|            0|            0|  0.00%|                u2 = 1.0 - random()
   583|         0|            0|            0|  0.00%|                v = _log(u1/(1.0-u1))/ainv
   584|         0|            0|            0|  0.00%|                x = alpha*_exp(v)
   585|         0|            0|            0|  0.00%|                z = u1*u1*u2
   586|         0|            0|            0|  0.00%|                r = bbb+ccc*v-x
   587|         0|            0|            0|  0.00%|                if r + SG_MAGICCONST - 4.5*z >= 0.0 or r >= _log(z):
   588|         0|            0|            0|  0.00%|                    return x * beta
   589|         0|            0|            0|  0.00%|
   590|         0|            0|            0|  0.00%|        elif alpha == 1.0:
   591|         0|            0|            0|  0.00%|            # expovariate(1/beta)
   592|         0|            0|            0|  0.00%|            return -_log(1.0 - random()) * beta
   593|         0|            0|            0|  0.00%|
   594|         0|            0|            0|  0.00%|        else:   # alpha is between 0 and 1 (exclusive)
   595|         0|            0|            0|  0.00%|
   596|         0|            0|            0|  0.00%|            # Uses ALGORITHM GS of Statistical Computing - Kennedy & Gentle
   597|         0|            0|            0|  0.00%|
   598|         0|            0|            0|  0.00%|            while 1:
   599|         0|            0|            0|  0.00%|                u = random()
   600|         0|            0|            0|  0.00%|                b = (_e + alpha)/_e
   601|         0|            0|            0|  0.00%|                p = b*u
   602|         0|            0|            0|  0.00%|                if p <= 1.0:
   603|         0|            0|            0|  0.00%|                    x = p ** (1.0/alpha)
   604|         0|            0|            0|  0.00%|                else:
   605|         0|            0|            0|  0.00%|                    x = -_log((b-p)/alpha)
   606|         0|            0|            0|  0.00%|                u1 = random()
   607|         0|            0|            0|  0.00%|                if p > 1.0:
   608|         0|            0|            0|  0.00%|                    if u1 <= x ** (alpha - 1.0):
   609|         0|            0|            0|  0.00%|                        break
   610|         0|            0|            0|  0.00%|                elif u1 <= _exp(-x):
   611|         0|            0|            0|  0.00%|                    break
   612|         0|            0|            0|  0.00%|            return x * beta
   613|         0|            0|            0|  0.00%|
   614|         0|            0|            0|  0.00%|## -------------------- Gauss (faster alternative) --------------------
   615|         0|            0|            0|  0.00%|
   616|         0|            0|            0|  0.00%|    def gauss(self, mu, sigma):
   617|         0|            0|            0|  0.00%|        """Gaussian distribution.
   618|         0|            0|            0|  0.00%|
   619|         0|            0|            0|  0.00%|        mu is the mean, and sigma is the standard deviation.  This is
   620|         0|            0|            0|  0.00%|        slightly faster than the normalvariate() function.
   621|         0|            0|            0|  0.00%|
   622|         0|            0|            0|  0.00%|        Not thread-safe without a lock around calls.
   623|         0|            0|            0|  0.00%|
   624|         0|            0|            0|  0.00%|        """
   625|         0|            0|            0|  0.00%|
   626|         0|            0|            0|  0.00%|        # When x and y are two variables from [0, 1), uniformly
   627|         0|            0|            0|  0.00%|        # distributed, then
   628|         0|            0|            0|  0.00%|        #
   629|         0|            0|            0|  0.00%|        #    cos(2*pi*x)*sqrt(-2*log(1-y))
   630|         0|            0|            0|  0.00%|        #    sin(2*pi*x)*sqrt(-2*log(1-y))
   631|         0|            0|            0|  0.00%|        #
   632|         0|            0|            0|  0.00%|        # are two *independent* variables with normal distribution
   633|         0|            0|            0|  0.00%|        # (mu = 0, sigma = 1).
   634|         0|            0|            0|  0.00%|        # (Lambert Meertens)
   635|         0|            0|            0|  0.00%|        # (corrected version; bug discovered by Mike Miller, fixed by LM)
   636|         0|            0|            0|  0.00%|
   637|         0|            0|            0|  0.00%|        # Multithreading note: When two threads call this function
   638|         0|            0|            0|  0.00%|        # simultaneously, it is possible that they will receive the
   639|         0|            0|            0|  0.00%|        # same return value.  The window is very small though.  To
   640|         0|            0|            0|  0.00%|        # avoid this, you have to use a lock around all calls.  (I
   641|         0|            0|            0|  0.00%|        # didn't want to slow this down in the serial case by using a
   642|         0|            0|            0|  0.00%|        # lock here.)
   643|         0|            0|            0|  0.00%|
   644|         0|            0|            0|  0.00%|        random = self.random
   645|         0|            0|            0|  0.00%|        z = self.gauss_next
   646|         0|            0|            0|  0.00%|        self.gauss_next = None
   647|         0|            0|            0|  0.00%|        if z is None:
   648|         0|            0|            0|  0.00%|            x2pi = random() * TWOPI
   649|         0|            0|            0|  0.00%|            g2rad = _sqrt(-2.0 * _log(1.0 - random()))
   650|         0|            0|            0|  0.00%|            z = _cos(x2pi) * g2rad
   651|         0|            0|            0|  0.00%|            self.gauss_next = _sin(x2pi) * g2rad
   652|         0|            0|            0|  0.00%|
   653|         0|            0|            0|  0.00%|        return mu + z*sigma
   654|         0|            0|            0|  0.00%|
   655|         0|            0|            0|  0.00%|## -------------------- beta --------------------
   656|         0|            0|            0|  0.00%|## See
   657|         0|            0|            0|  0.00%|## http://mail.python.org/pipermail/python-bugs-list/2001-January/003752.html
   658|         0|            0|            0|  0.00%|## for Ivan Frohne's insightful analysis of why the original implementation:
   659|         0|            0|            0|  0.00%|##
   660|         0|            0|            0|  0.00%|##    def betavariate(self, alpha, beta):
   661|         0|            0|            0|  0.00%|##        # Discrete Event Simulation in C, pp 87-88.
   662|         0|            0|            0|  0.00%|##
   663|         0|            0|            0|  0.00%|##        y = self.expovariate(alpha)
   664|         0|            0|            0|  0.00%|##        z = self.expovariate(1.0/beta)
   665|         0|            0|            0|  0.00%|##        return z/(y+z)
   666|         0|            0|            0|  0.00%|##
   667|         0|            0|            0|  0.00%|## was dead wrong, and how it probably got that way.
   668|         0|            0|            0|  0.00%|
   669|         0|            0|            0|  0.00%|    def betavariate(self, alpha, beta):
   670|         0|            0|            0|  0.00%|        """Beta distribution.
   671|         0|            0|            0|  0.00%|
   672|         0|            0|            0|  0.00%|        Conditions on the parameters are alpha > 0 and beta > 0.
   673|         0|            0|            0|  0.00%|        Returned values range between 0 and 1.
   674|         0|            0|            0|  0.00%|
   675|         0|            0|            0|  0.00%|        """
   676|         0|            0|            0|  0.00%|
   677|         0|            0|            0|  0.00%|        # This version due to Janne Sinkkonen, and matches all the std
   678|         0|            0|            0|  0.00%|        # texts (e.g., Knuth Vol 2 Ed 3 pg 134 "the beta distribution").
   679|         0|            0|            0|  0.00%|        y = self.gammavariate(alpha, 1.0)
   680|         0|            0|            0|  0.00%|        if y == 0:
   681|         0|            0|            0|  0.00%|            return 0.0
   682|         0|            0|            0|  0.00%|        else:
   683|         0|            0|            0|  0.00%|            return y / (y + self.gammavariate(beta, 1.0))
   684|         0|            0|            0|  0.00%|
   685|         0|            0|            0|  0.00%|## -------------------- Pareto --------------------
   686|         0|            0|            0|  0.00%|
   687|         0|            0|            0|  0.00%|    def paretovariate(self, alpha):
   688|         0|            0|            0|  0.00%|        """Pareto distribution.  alpha is the shape parameter."""
   689|         0|            0|            0|  0.00%|        # Jain, pg. 495
   690|         0|            0|            0|  0.00%|
   691|         0|            0|            0|  0.00%|        u = 1.0 - self.random()
   692|         0|            0|            0|  0.00%|        return 1.0 / u ** (1.0/alpha)
   693|         0|            0|            0|  0.00%|
   694|         0|            0|            0|  0.00%|## -------------------- Weibull --------------------
   695|         0|            0|            0|  0.00%|
   696|         0|            0|            0|  0.00%|    def weibullvariate(self, alpha, beta):
   697|         0|            0|            0|  0.00%|        """Weibull distribution.
   698|         0|            0|            0|  0.00%|
   699|         0|            0|            0|  0.00%|        alpha is the scale parameter and beta is the shape parameter.
   700|         0|            0|            0|  0.00%|
   701|         0|            0|            0|  0.00%|        """
   702|         0|            0|            0|  0.00%|        # Jain, pg. 499; bug fix courtesy Bill Arms
   703|         0|            0|            0|  0.00%|
   704|         0|            0|            0|  0.00%|        u = 1.0 - self.random()
   705|         0|            0|            0|  0.00%|        return alpha * (-_log(u)) ** (1.0/beta)
   706|         0|            0|            0|  0.00%|
   707|         0|            0|            0|  0.00%|## --------------- Operating System Random Source  ------------------
   708|         0|            0|            0|  0.00%|
   709|         0|            0|            0|  0.00%|class SystemRandom(Random):
   710|         0|            0|            0|  0.00%|    """Alternate random number generator using sources provided
   711|         0|            0|            0|  0.00%|    by the operating system (such as /dev/urandom on Unix or
   712|         0|            0|            0|  0.00%|    CryptGenRandom on Windows).
   713|         0|            0|            0|  0.00%|
   714|         0|            0|            0|  0.00%|     Not available on all systems (see os.urandom() for details).
   715|         0|            0|            0|  0.00%|    """
   716|         0|            0|            0|  0.00%|
   717|         0|            0|            0|  0.00%|    def random(self):
   718|         0|            0|            0|  0.00%|        """Get the next random number in the range [0.0, 1.0)."""
   719|         0|            0|            0|  0.00%|        return (int.from_bytes(_urandom(7), 'big') >> 3) * RECIP_BPF
   720|         0|            0|            0|  0.00%|
   721|         0|            0|            0|  0.00%|    def getrandbits(self, k):
   722|         0|            0|            0|  0.00%|        """getrandbits(k) -> x.  Generates an int with k random bits."""
   723|         0|            0|            0|  0.00%|        if k <= 0:
   724|         0|            0|            0|  0.00%|            raise ValueError('number of bits must be greater than zero')
   725|         0|            0|            0|  0.00%|        numbytes = (k + 7) // 8                       # bits / 8 and rounded up
   726|         0|            0|            0|  0.00%|        x = int.from_bytes(_urandom(numbytes), 'big')
   727|         0|            0|            0|  0.00%|        return x >> (numbytes * 8 - k)                # trim excess bits
   728|         0|            0|            0|  0.00%|
   729|         0|            0|            0|  0.00%|    def seed(self, *args, **kwds):
   730|         0|            0|            0|  0.00%|        "Stub method.  Not used for a system random number generator."
   731|         0|            0|            0|  0.00%|        return None
   732|         0|            0|            0|  0.00%|
   733|         0|            0|            0|  0.00%|    def _notimplemented(self, *args, **kwds):
   734|         0|            0|            0|  0.00%|        "Method should not be called for a system random number generator."
   735|         0|            0|            0|  0.00%|        raise NotImplementedError('System entropy source does not have state.')
   736|         0|            0|            0|  0.00%|    getstate = setstate = _notimplemented
   737|         0|            0|            0|  0.00%|
   738|         0|            0|            0|  0.00%|## -------------------- test program --------------------
   739|         0|            0|            0|  0.00%|
   740|         0|            0|            0|  0.00%|def _test_generator(n, func, args):
   741|         0|            0|            0|  0.00%|    import time
   742|         0|            0|            0|  0.00%|    print(n, 'times', func.__name__)
   743|         0|            0|            0|  0.00%|    total = 0.0
   744|         0|            0|            0|  0.00%|    sqsum = 0.0
   745|         0|            0|            0|  0.00%|    smallest = 1e10
   746|         0|            0|            0|  0.00%|    largest = -1e10
   747|         0|            0|            0|  0.00%|    t0 = time.perf_counter()
   748|         0|            0|            0|  0.00%|    for i in range(n):
   749|         0|            0|            0|  0.00%|        x = func(*args)
   750|         0|            0|            0|  0.00%|        total += x
   751|         0|            0|            0|  0.00%|        sqsum = sqsum + x*x
   752|         0|            0|            0|  0.00%|        smallest = min(x, smallest)
   753|         0|            0|            0|  0.00%|        largest = max(x, largest)
   754|         0|            0|            0|  0.00%|    t1 = time.perf_counter()
   755|         0|            0|            0|  0.00%|    print(round(t1-t0, 3), 'sec,', end=' ')
   756|         0|            0|            0|  0.00%|    avg = total/n
   757|         0|            0|            0|  0.00%|    stddev = _sqrt(sqsum/n - avg*avg)
   758|         0|            0|            0|  0.00%|    print('avg %g, stddev %g, min %g, max %g\n' % \
   759|         0|            0|            0|  0.00%|              (avg, stddev, smallest, largest))
   760|         0|            0|            0|  0.00%|
   761|         0|            0|            0|  0.00%|
   762|         0|            0|            0|  0.00%|def _test(N=2000):
   763|         0|            0|            0|  0.00%|    _test_generator(N, random, ())
   764|         0|            0|            0|  0.00%|    _test_generator(N, normalvariate, (0.0, 1.0))
   765|         0|            0|            0|  0.00%|    _test_generator(N, lognormvariate, (0.0, 1.0))
   766|         0|            0|            0|  0.00%|    _test_generator(N, vonmisesvariate, (0.0, 1.0))
   767|         0|            0|            0|  0.00%|    _test_generator(N, gammavariate, (0.01, 1.0))
   768|         0|            0|            0|  0.00%|    _test_generator(N, gammavariate, (0.1, 1.0))
   769|         0|            0|            0|  0.00%|    _test_generator(N, gammavariate, (0.1, 2.0))
   770|         0|            0|            0|  0.00%|    _test_generator(N, gammavariate, (0.5, 1.0))
   771|         0|            0|            0|  0.00%|    _test_generator(N, gammavariate, (0.9, 1.0))
   772|         0|            0|            0|  0.00%|    _test_generator(N, gammavariate, (1.0, 1.0))
   773|         0|            0|            0|  0.00%|    _test_generator(N, gammavariate, (2.0, 1.0))
   774|         0|            0|            0|  0.00%|    _test_generator(N, gammavariate, (20.0, 1.0))
   775|         0|            0|            0|  0.00%|    _test_generator(N, gammavariate, (200.0, 1.0))
   776|         0|            0|            0|  0.00%|    _test_generator(N, gauss, (0.0, 1.0))
   777|         0|            0|            0|  0.00%|    _test_generator(N, betavariate, (3.0, 3.0))
   778|         0|            0|            0|  0.00%|    _test_generator(N, triangular, (0.0, 1.0, 1.0/3.0))
   779|         0|            0|            0|  0.00%|
   780|         0|            0|            0|  0.00%|# Create one instance, seeded from current time, and export its methods
   781|         0|            0|            0|  0.00%|# as module-level functions.  The functions share state across all uses
   782|         0|            0|            0|  0.00%|#(both in the user's code and in the Python libraries), but that's fine
   783|         0|            0|            0|  0.00%|# for most programs and is easier for the casual user than making them
   784|         0|            0|            0|  0.00%|# instantiate their own Random() instance.
   785|         0|            0|            0|  0.00%|
   786|         0|            0|            0|  0.00%|_inst = Random()
   787|         0|            0|            0|  0.00%|seed = _inst.seed
   788|         0|            0|            0|  0.00%|random = _inst.random
   789|         0|            0|            0|  0.00%|uniform = _inst.uniform
   790|         0|            0|            0|  0.00%|triangular = _inst.triangular
   791|         0|            0|            0|  0.00%|randint = _inst.randint
   792|         0|            0|            0|  0.00%|choice = _inst.choice
   793|         0|            0|            0|  0.00%|randrange = _inst.randrange
   794|         0|            0|            0|  0.00%|sample = _inst.sample
   795|         0|            0|            0|  0.00%|shuffle = _inst.shuffle
   796|         0|            0|            0|  0.00%|choices = _inst.choices
   797|         0|            0|            0|  0.00%|normalvariate = _inst.normalvariate
   798|         0|            0|            0|  0.00%|lognormvariate = _inst.lognormvariate
   799|         0|            0|            0|  0.00%|expovariate = _inst.expovariate
   800|         0|            0|            0|  0.00%|vonmisesvariate = _inst.vonmisesvariate
   801|         0|            0|            0|  0.00%|gammavariate = _inst.gammavariate
   802|         0|            0|            0|  0.00%|gauss = _inst.gauss
   803|         0|            0|            0|  0.00%|betavariate = _inst.betavariate
   804|         0|            0|            0|  0.00%|paretovariate = _inst.paretovariate
   805|         0|            0|            0|  0.00%|weibullvariate = _inst.weibullvariate
   806|         0|            0|            0|  0.00%|getstate = _inst.getstate
   807|         0|            0|            0|  0.00%|setstate = _inst.setstate
   808|         0|            0|            0|  0.00%|getrandbits = _inst.getrandbits
   809|         0|            0|            0|  0.00%|
   810|         0|            0|            0|  0.00%|if hasattr(_os, "fork"):
   811|         0|            0|            0|  0.00%|    _os.register_at_fork(after_in_child=_inst.seed)
   812|         0|            0|            0|  0.00%|
   813|         0|            0|            0|  0.00%|
   814|         0|            0|            0|  0.00%|if __name__ == '__main__':
   815|         0|            0|            0|  0.00%|    _test()
File: /opt/conda/lib/python3.8/hashlib.py
File duration: 0.0168066s (0.03%)
Line #|      Hits|         Time| Time per hit|      %|Source code
------+----------+-------------+-------------+-------+-----------
     1|         0|            0|            0|  0.00%|#.  Copyright (C) 2005-2010   Gregory P. Smith (greg@krypto.org)
     2|         0|            0|            0|  0.00%|#  Licensed to PSF under a Contributor Agreement.
     3|         0|            0|            0|  0.00%|#
     4|         0|            0|            0|  0.00%|
     5|         0|            0|            0|  0.00%|__doc__ = """hashlib module - A common interface to many hash functions.
     6|         0|            0|            0|  0.00%|
     7|         0|            0|            0|  0.00%|new(name, data=b'', **kwargs) - returns a new hash object implementing the
     8|         0|            0|            0|  0.00%|                                given hash function; initializing the hash
     9|         0|            0|            0|  0.00%|                                using the given binary data.
    10|         0|            0|            0|  0.00%|
    11|         0|            0|            0|  0.00%|Named constructor functions are also available, these are faster
    12|         0|            0|            0|  0.00%|than using new(name):
    13|         0|            0|            0|  0.00%|
    14|         0|            0|            0|  0.00%|md5(), sha1(), sha224(), sha256(), sha384(), sha512(), blake2b(), blake2s(),
    15|         0|            0|            0|  0.00%|sha3_224, sha3_256, sha3_384, sha3_512, shake_128, and shake_256.
    16|         0|            0|            0|  0.00%|
    17|         0|            0|            0|  0.00%|More algorithms may be available on your platform but the above are guaranteed
    18|         0|            0|            0|  0.00%|to exist.  See the algorithms_guaranteed and algorithms_available attributes
    19|         0|            0|            0|  0.00%|to find out what algorithm names can be passed to new().
    20|         0|            0|            0|  0.00%|
    21|         0|            0|            0|  0.00%|NOTE: If you want the adler32 or crc32 hash functions they are available in
    22|         0|            0|            0|  0.00%|the zlib module.
    23|         0|            0|            0|  0.00%|
    24|         0|            0|            0|  0.00%|Choose your hash function wisely.  Some have known collision weaknesses.
    25|         0|            0|            0|  0.00%|sha384 and sha512 will be slow on 32 bit platforms.
    26|         0|            0|            0|  0.00%|
    27|         0|            0|            0|  0.00%|Hash objects have these methods:
    28|         0|            0|            0|  0.00%| - update(data): Update the hash object with the bytes in data. Repeated calls
    29|         0|            0|            0|  0.00%|                 are equivalent to a single call with the concatenation of all
    30|         0|            0|            0|  0.00%|                 the arguments.
    31|         0|            0|            0|  0.00%| - digest():     Return the digest of the bytes passed to the update() method
    32|         0|            0|            0|  0.00%|                 so far as a bytes object.
    33|         0|            0|            0|  0.00%| - hexdigest():  Like digest() except the digest is returned as a string
    34|         0|            0|            0|  0.00%|                 of double length, containing only hexadecimal digits.
    35|         0|            0|            0|  0.00%| - copy():       Return a copy (clone) of the hash object. This can be used to
    36|         0|            0|            0|  0.00%|                 efficiently compute the digests of datas that share a common
    37|         0|            0|            0|  0.00%|                 initial substring.
    38|         0|            0|            0|  0.00%|
    39|         0|            0|            0|  0.00%|For example, to obtain the digest of the byte string 'Nobody inspects the
    40|         0|            0|            0|  0.00%|spammish repetition':
    41|         0|            0|            0|  0.00%|
    42|         0|            0|            0|  0.00%|    >>> import hashlib
    43|         0|            0|            0|  0.00%|    >>> m = hashlib.md5()
    44|         0|            0|            0|  0.00%|    >>> m.update(b"Nobody inspects")
    45|         0|            0|            0|  0.00%|    >>> m.update(b" the spammish repetition")
    46|         0|            0|            0|  0.00%|    >>> m.digest()
    47|         0|            0|            0|  0.00%|    b'\\xbbd\\x9c\\x83\\xdd\\x1e\\xa5\\xc9\\xd9\\xde\\xc9\\xa1\\x8d\\xf0\\xff\\xe9'
    48|         0|            0|            0|  0.00%|
    49|         0|            0|            0|  0.00%|More condensed:
    50|         0|            0|            0|  0.00%|
    51|         0|            0|            0|  0.00%|    >>> hashlib.sha224(b"Nobody inspects the spammish repetition").hexdigest()
    52|         0|            0|            0|  0.00%|    'a4337bc45a8fc544c03f52dc550cd6e1e87021bc896588bd79e901e2'
    53|         0|            0|            0|  0.00%|
    54|         0|            0|            0|  0.00%|"""
    55|         0|            0|            0|  0.00%|
    56|         0|            0|            0|  0.00%|# This tuple and __get_builtin_constructor() must be modified if a new
    57|         0|            0|            0|  0.00%|# always available algorithm is added.
    58|         0|            0|            0|  0.00%|__always_supported = ('md5', 'sha1', 'sha224', 'sha256', 'sha384', 'sha512',
    59|         0|            0|            0|  0.00%|                      'blake2b', 'blake2s',
    60|         0|            0|            0|  0.00%|                      'sha3_224', 'sha3_256', 'sha3_384', 'sha3_512',
    61|         0|            0|            0|  0.00%|                      'shake_128', 'shake_256')
    62|         0|            0|            0|  0.00%|
    63|         0|            0|            0|  0.00%|
    64|         0|            0|            0|  0.00%|algorithms_guaranteed = set(__always_supported)
    65|         0|            0|            0|  0.00%|algorithms_available = set(__always_supported)
    66|         0|            0|            0|  0.00%|
    67|         0|            0|            0|  0.00%|__all__ = __always_supported + ('new', 'algorithms_guaranteed',
    68|         0|            0|            0|  0.00%|                                'algorithms_available', 'pbkdf2_hmac')
    69|         0|            0|            0|  0.00%|
    70|         0|            0|            0|  0.00%|
    71|         0|            0|            0|  0.00%|__builtin_constructor_cache = {}
    72|         0|            0|            0|  0.00%|
    73|         0|            0|            0|  0.00%|__block_openssl_constructor = {
    74|         0|            0|            0|  0.00%|    'sha3_224', 'sha3_256', 'sha3_384', 'sha3_512',
    75|         0|            0|            0|  0.00%|    'shake_128', 'shake_256',
    76|         0|            0|            0|  0.00%|    'blake2b', 'blake2s',
    77|         0|            0|            0|  0.00%|}
    78|         0|            0|            0|  0.00%|
    79|         0|            0|            0|  0.00%|def __get_builtin_constructor(name):
    80|         0|            0|            0|  0.00%|    cache = __builtin_constructor_cache
    81|         0|            0|            0|  0.00%|    constructor = cache.get(name)
    82|         0|            0|            0|  0.00%|    if constructor is not None:
    83|         0|            0|            0|  0.00%|        return constructor
    84|         0|            0|            0|  0.00%|    try:
    85|         0|            0|            0|  0.00%|        if name in {'SHA1', 'sha1'}:
    86|         0|            0|            0|  0.00%|            import _sha1
    87|         0|            0|            0|  0.00%|            cache['SHA1'] = cache['sha1'] = _sha1.sha1
    88|         0|            0|            0|  0.00%|        elif name in {'MD5', 'md5'}:
    89|         0|            0|            0|  0.00%|            import _md5
    90|         0|            0|            0|  0.00%|            cache['MD5'] = cache['md5'] = _md5.md5
    91|         0|            0|            0|  0.00%|        elif name in {'SHA256', 'sha256', 'SHA224', 'sha224'}:
    92|         0|            0|            0|  0.00%|            import _sha256
    93|         0|            0|            0|  0.00%|            cache['SHA224'] = cache['sha224'] = _sha256.sha224
    94|         0|            0|            0|  0.00%|            cache['SHA256'] = cache['sha256'] = _sha256.sha256
    95|         0|            0|            0|  0.00%|        elif name in {'SHA512', 'sha512', 'SHA384', 'sha384'}:
    96|         0|            0|            0|  0.00%|            import _sha512
    97|         0|            0|            0|  0.00%|            cache['SHA384'] = cache['sha384'] = _sha512.sha384
    98|         0|            0|            0|  0.00%|            cache['SHA512'] = cache['sha512'] = _sha512.sha512
    99|         0|            0|            0|  0.00%|        elif name in {'blake2b', 'blake2s'}:
   100|         0|            0|            0|  0.00%|            import _blake2
   101|         0|            0|            0|  0.00%|            cache['blake2b'] = _blake2.blake2b
   102|         0|            0|            0|  0.00%|            cache['blake2s'] = _blake2.blake2s
   103|         0|            0|            0|  0.00%|        elif name in {'sha3_224', 'sha3_256', 'sha3_384', 'sha3_512'}:
   104|         0|            0|            0|  0.00%|            import _sha3
   105|         0|            0|            0|  0.00%|            cache['sha3_224'] = _sha3.sha3_224
   106|         0|            0|            0|  0.00%|            cache['sha3_256'] = _sha3.sha3_256
   107|         0|            0|            0|  0.00%|            cache['sha3_384'] = _sha3.sha3_384
   108|         0|            0|            0|  0.00%|            cache['sha3_512'] = _sha3.sha3_512
   109|         0|            0|            0|  0.00%|        elif name in {'shake_128', 'shake_256'}:
   110|         0|            0|            0|  0.00%|            import _sha3
   111|         0|            0|            0|  0.00%|            cache['shake_128'] = _sha3.shake_128
   112|         0|            0|            0|  0.00%|            cache['shake_256'] = _sha3.shake_256
   113|         0|            0|            0|  0.00%|    except ImportError:
   114|         0|            0|            0|  0.00%|        pass  # no extension module, this hash is unsupported.
   115|         0|            0|            0|  0.00%|
   116|         0|            0|            0|  0.00%|    constructor = cache.get(name)
   117|         0|            0|            0|  0.00%|    if constructor is not None:
   118|         0|            0|            0|  0.00%|        return constructor
   119|         0|            0|            0|  0.00%|
   120|         0|            0|            0|  0.00%|    raise ValueError('unsupported hash type ' + name)
   121|         0|            0|            0|  0.00%|
   122|         0|            0|            0|  0.00%|
   123|         0|            0|            0|  0.00%|def __get_openssl_constructor(name):
   124|         0|            0|            0|  0.00%|    if name in __block_openssl_constructor:
   125|         0|            0|            0|  0.00%|        # Prefer our blake2 and sha3 implementation.
   126|         0|            0|            0|  0.00%|        return __get_builtin_constructor(name)
   127|         0|            0|            0|  0.00%|    try:
   128|         0|            0|            0|  0.00%|        f = getattr(_hashlib, 'openssl_' + name)
   129|         0|            0|            0|  0.00%|        # Allow the C module to raise ValueError.  The function will be
   130|         0|            0|            0|  0.00%|        # defined but the hash not actually available thanks to OpenSSL.
   131|         0|            0|            0|  0.00%|        f()
   132|         0|            0|            0|  0.00%|        # Use the C function directly (very fast)
   133|         0|            0|            0|  0.00%|        return f
   134|         0|            0|            0|  0.00%|    except (AttributeError, ValueError):
   135|         0|            0|            0|  0.00%|        return __get_builtin_constructor(name)
   136|         0|            0|            0|  0.00%|
   137|         0|            0|            0|  0.00%|
   138|         0|            0|            0|  0.00%|def __py_new(name, data=b'', **kwargs):
   139|         0|            0|            0|  0.00%|    """new(name, data=b'', **kwargs) - Return a new hashing object using the
   140|         0|            0|            0|  0.00%|    named algorithm; optionally initialized with data (which must be
   141|         0|            0|            0|  0.00%|    a bytes-like object).
   142|         0|            0|            0|  0.00%|    """
   143|         0|            0|            0|  0.00%|    return __get_builtin_constructor(name)(data, **kwargs)
   144|         0|            0|            0|  0.00%|
   145|         0|            0|            0|  0.00%|
   146|       800|   0.00347328|   4.3416e-06|  0.01%|def __hash_new(name, data=b'', **kwargs):
   147|         0|            0|            0|  0.00%|    """new(name, data=b'') - Return a new hashing object using the named algorithm;
   148|         0|            0|            0|  0.00%|    optionally initialized with data (which must be a bytes-like object).
   149|         0|            0|            0|  0.00%|    """
   150|       800|   0.00357318|  4.46647e-06|  0.01%|    if name in __block_openssl_constructor:
   151|         0|            0|            0|  0.00%|        # Prefer our blake2 and sha3 implementation
   152|         0|            0|            0|  0.00%|        # OpenSSL 1.1.0 comes with a limited implementation of blake2b/s.
   153|         0|            0|            0|  0.00%|        # It does neither support keyed blake2 nor advanced features like
   154|         0|            0|            0|  0.00%|        # salt, personal, tree hashing or SSE.
   155|         0|            0|            0|  0.00%|        return __get_builtin_constructor(name)(data, **kwargs)
   156|       800|   0.00286126|  3.57658e-06|  0.01%|    try:
   157|       800|   0.00689888|   8.6236e-06|  0.01%|        return _hashlib.new(name, data)
   158|         0|            0|            0|  0.00%|    except ValueError:
   159|         0|            0|            0|  0.00%|        # If the _hashlib module (OpenSSL) doesn't support the named
   160|         0|            0|            0|  0.00%|        # hash, try using our builtin implementations.
   161|         0|            0|            0|  0.00%|        # This allows for SHA224/256 and SHA384/512 support even though
   162|         0|            0|            0|  0.00%|        # the OpenSSL library prior to 0.9.8 doesn't provide them.
   163|         0|            0|            0|  0.00%|        return __get_builtin_constructor(name)(data)
   164|         0|            0|            0|  0.00%|
   165|         0|            0|            0|  0.00%|
   166|         0|            0|            0|  0.00%|try:
   167|         0|            0|            0|  0.00%|    import _hashlib
   168|         0|            0|            0|  0.00%|    new = __hash_new
   169|         0|            0|            0|  0.00%|    __get_hash = __get_openssl_constructor
   170|         0|            0|            0|  0.00%|    algorithms_available = algorithms_available.union(
   171|         0|            0|            0|  0.00%|            _hashlib.openssl_md_meth_names)
   172|         0|            0|            0|  0.00%|except ImportError:
   173|         0|            0|            0|  0.00%|    new = __py_new
   174|         0|            0|            0|  0.00%|    __get_hash = __get_builtin_constructor
   175|         0|            0|            0|  0.00%|
   176|         0|            0|            0|  0.00%|try:
   177|         0|            0|            0|  0.00%|    # OpenSSL's PKCS5_PBKDF2_HMAC requires OpenSSL 1.0+ with HMAC and SHA
   178|         0|            0|            0|  0.00%|    from _hashlib import pbkdf2_hmac
   179|         0|            0|            0|  0.00%|except ImportError:
   180|         0|            0|            0|  0.00%|    _trans_5C = bytes((x ^ 0x5C) for x in range(256))
   181|         0|            0|            0|  0.00%|    _trans_36 = bytes((x ^ 0x36) for x in range(256))
   182|         0|            0|            0|  0.00%|
   183|         0|            0|            0|  0.00%|    def pbkdf2_hmac(hash_name, password, salt, iterations, dklen=None):
   184|         0|            0|            0|  0.00%|        """Password based key derivation function 2 (PKCS #5 v2.0)
   185|         0|            0|            0|  0.00%|
   186|         0|            0|            0|  0.00%|        This Python implementations based on the hmac module about as fast
   187|         0|            0|            0|  0.00%|        as OpenSSL's PKCS5_PBKDF2_HMAC for short passwords and much faster
   188|         0|            0|            0|  0.00%|        for long passwords.
   189|         0|            0|            0|  0.00%|        """
   190|         0|            0|            0|  0.00%|        if not isinstance(hash_name, str):
   191|         0|            0|            0|  0.00%|            raise TypeError(hash_name)
   192|         0|            0|            0|  0.00%|
   193|         0|            0|            0|  0.00%|        if not isinstance(password, (bytes, bytearray)):
   194|         0|            0|            0|  0.00%|            password = bytes(memoryview(password))
   195|         0|            0|            0|  0.00%|        if not isinstance(salt, (bytes, bytearray)):
   196|         0|            0|            0|  0.00%|            salt = bytes(memoryview(salt))
   197|         0|            0|            0|  0.00%|
   198|         0|            0|            0|  0.00%|        # Fast inline HMAC implementation
   199|         0|            0|            0|  0.00%|        inner = new(hash_name)
   200|         0|            0|            0|  0.00%|        outer = new(hash_name)
   201|         0|            0|            0|  0.00%|        blocksize = getattr(inner, 'block_size', 64)
   202|         0|            0|            0|  0.00%|        if len(password) > blocksize:
   203|         0|            0|            0|  0.00%|            password = new(hash_name, password).digest()
   204|         0|            0|            0|  0.00%|        password = password + b'\x00' * (blocksize - len(password))
   205|         0|            0|            0|  0.00%|        inner.update(password.translate(_trans_36))
   206|         0|            0|            0|  0.00%|        outer.update(password.translate(_trans_5C))
   207|         0|            0|            0|  0.00%|
   208|         0|            0|            0|  0.00%|        def prf(msg, inner=inner, outer=outer):
   209|         0|            0|            0|  0.00%|            # PBKDF2_HMAC uses the password as key. We can re-use the same
   210|         0|            0|            0|  0.00%|            # digest objects and just update copies to skip initialization.
   211|         0|            0|            0|  0.00%|            icpy = inner.copy()
   212|         0|            0|            0|  0.00%|            ocpy = outer.copy()
   213|         0|            0|            0|  0.00%|            icpy.update(msg)
   214|         0|            0|            0|  0.00%|            ocpy.update(icpy.digest())
   215|         0|            0|            0|  0.00%|            return ocpy.digest()
   216|         0|            0|            0|  0.00%|
   217|         0|            0|            0|  0.00%|        if iterations < 1:
   218|         0|            0|            0|  0.00%|            raise ValueError(iterations)
   219|         0|            0|            0|  0.00%|        if dklen is None:
   220|         0|            0|            0|  0.00%|            dklen = outer.digest_size
   221|         0|            0|            0|  0.00%|        if dklen < 1:
   222|         0|            0|            0|  0.00%|            raise ValueError(dklen)
   223|         0|            0|            0|  0.00%|
   224|         0|            0|            0|  0.00%|        dkey = b''
   225|         0|            0|            0|  0.00%|        loop = 1
   226|         0|            0|            0|  0.00%|        from_bytes = int.from_bytes
   227|         0|            0|            0|  0.00%|        while len(dkey) < dklen:
   228|         0|            0|            0|  0.00%|            prev = prf(salt + loop.to_bytes(4, 'big'))
   229|         0|            0|            0|  0.00%|            # endianness doesn't matter here as long to / from use the same
   230|         0|            0|            0|  0.00%|            rkey = int.from_bytes(prev, 'big')
   231|         0|            0|            0|  0.00%|            for i in range(iterations - 1):
   232|         0|            0|            0|  0.00%|                prev = prf(prev)
   233|         0|            0|            0|  0.00%|                # rkey = rkey ^ prev
   234|         0|            0|            0|  0.00%|                rkey ^= from_bytes(prev, 'big')
   235|         0|            0|            0|  0.00%|            loop += 1
   236|         0|            0|            0|  0.00%|            dkey += rkey.to_bytes(inner.digest_size, 'big')
   237|         0|            0|            0|  0.00%|
   238|         0|            0|            0|  0.00%|        return dkey[:dklen]
   239|         0|            0|            0|  0.00%|
   240|         0|            0|            0|  0.00%|try:
   241|         0|            0|            0|  0.00%|    # OpenSSL's scrypt requires OpenSSL 1.1+
   242|         0|            0|            0|  0.00%|    from _hashlib import scrypt
   243|         0|            0|            0|  0.00%|except ImportError:
   244|         0|            0|            0|  0.00%|    pass
   245|         0|            0|            0|  0.00%|
   246|         0|            0|            0|  0.00%|
   247|         0|            0|            0|  0.00%|for __func_name in __always_supported:
   248|         0|            0|            0|  0.00%|    # try them all, some may not work due to the OpenSSL
   249|         0|            0|            0|  0.00%|    # version not supporting that algorithm.
   250|         0|            0|            0|  0.00%|    try:
   251|         0|            0|            0|  0.00%|        globals()[__func_name] = __get_hash(__func_name)
   252|         0|            0|            0|  0.00%|    except ValueError:
   253|         0|            0|            0|  0.00%|        import logging
   254|         0|            0|            0|  0.00%|        logging.exception('code for hash %s was not found.', __func_name)
   255|         0|            0|            0|  0.00%|
   256|         0|            0|            0|  0.00%|
   257|         0|            0|            0|  0.00%|# Cleanup locals()
   258|         0|            0|            0|  0.00%|del __always_supported, __func_name, __get_hash
   259|         0|            0|            0|  0.00%|del __py_new, __hash_new, __get_openssl_constructor
File: /opt/conda/lib/python3.8/multiprocessing/synchronize.py
File duration: 0.0159464s (0.03%)
Line #|      Hits|         Time| Time per hit|      %|Source code
------+----------+-------------+-------------+-------+-----------
     1|         0|            0|            0|  0.00%|#
     2|         0|            0|            0|  0.00%|# Module implementing synchronization primitives
     3|         0|            0|            0|  0.00%|#
     4|         0|            0|            0|  0.00%|# multiprocessing/synchronize.py
     5|         0|            0|            0|  0.00%|#
     6|         0|            0|            0|  0.00%|# Copyright (c) 2006-2008, R Oudkerk
     7|         0|            0|            0|  0.00%|# Licensed to PSF under a Contributor Agreement.
     8|         0|            0|            0|  0.00%|#
     9|         0|            0|            0|  0.00%|
    10|         0|            0|            0|  0.00%|__all__ = [
    11|         0|            0|            0|  0.00%|    'Lock', 'RLock', 'Semaphore', 'BoundedSemaphore', 'Condition', 'Event'
    12|         0|            0|            0|  0.00%|    ]
    13|         0|            0|            0|  0.00%|
    14|         0|            0|            0|  0.00%|import threading
    15|         0|            0|            0|  0.00%|import sys
    16|         0|            0|            0|  0.00%|import tempfile
    17|         0|            0|            0|  0.00%|import _multiprocessing
    18|         0|            0|            0|  0.00%|import time
    19|         0|            0|            0|  0.00%|
    20|         0|            0|            0|  0.00%|from . import context
    21|         0|            0|            0|  0.00%|from . import process
    22|         0|            0|            0|  0.00%|from . import util
    23|         0|            0|            0|  0.00%|
    24|         0|            0|            0|  0.00%|# Try to import the mp.synchronize module cleanly, if it fails
    25|         0|            0|            0|  0.00%|# raise ImportError for platforms lacking a working sem_open implementation.
    26|         0|            0|            0|  0.00%|# See issue 3770
    27|         0|            0|            0|  0.00%|try:
    28|         0|            0|            0|  0.00%|    from _multiprocessing import SemLock, sem_unlink
    29|         0|            0|            0|  0.00%|except (ImportError):
    30|         0|            0|            0|  0.00%|    raise ImportError("This platform lacks a functioning sem_open" +
    31|         0|            0|            0|  0.00%|                      " implementation, therefore, the required" +
    32|         0|            0|            0|  0.00%|                      " synchronization primitives needed will not" +
    33|         0|            0|            0|  0.00%|                      " function, see issue 3770.")
    34|         0|            0|            0|  0.00%|
    35|         0|            0|            0|  0.00%|#
    36|         0|            0|            0|  0.00%|# Constants
    37|         0|            0|            0|  0.00%|#
    38|         0|            0|            0|  0.00%|
    39|         0|            0|            0|  0.00%|RECURSIVE_MUTEX, SEMAPHORE = list(range(2))
    40|         0|            0|            0|  0.00%|SEM_VALUE_MAX = _multiprocessing.SemLock.SEM_VALUE_MAX
    41|         0|            0|            0|  0.00%|
    42|         0|            0|            0|  0.00%|#
    43|         0|            0|            0|  0.00%|# Base class for semaphores and mutexes; wraps `_multiprocessing.SemLock`
    44|         0|            0|            0|  0.00%|#
    45|         0|            0|            0|  0.00%|
    46|         0|            0|            0|  0.00%|class SemLock(object):
    47|         0|            0|            0|  0.00%|
    48|         0|            0|            0|  0.00%|    _rand = tempfile._RandomNameSequence()
    49|         0|            0|            0|  0.00%|
    50|        40|  0.000283957|  7.09891e-06|  0.00%|    def __init__(self, kind, value, maxvalue, *, ctx):
    51|        40|  0.000226974|  5.67436e-06|  0.00%|        if ctx is None:
    52|         0|            0|            0|  0.00%|            ctx = context._default_context.get_context()
    53|        40|  0.000735998|     1.84e-05|  0.00%|        name = ctx.get_start_method()
(call)|        40|  0.000389338|  9.73344e-06|  0.00%|# /opt/conda/lib/python3.8/multiprocessing/context.py:197 get_start_method
    54|        40|  0.000313997|  7.84993e-06|  0.00%|        unlink_now = sys.platform == 'win32' or name == 'fork'
    55|        40|   0.00033474|  8.36849e-06|  0.00%|        for i in range(100):
    56|        40|  0.000214815|  5.37038e-06|  0.00%|            try:
    57|        80|   0.00373626|  4.67032e-05|  0.01%|                sl = self._semlock = _multiprocessing.SemLock(
    58|        40|  0.000634193|  1.58548e-05|  0.00%|                    kind, value, maxvalue, self._make_name(),
(call)|        40|    0.0313418|  0.000783545|  0.06%|# /opt/conda/lib/python3.8/multiprocessing/synchronize.py:114 _make_name
    59|        40|  0.000201702|  5.04255e-06|  0.00%|                    unlink_now)
    60|         0|            0|            0|  0.00%|            except FileExistsError:
    61|         0|            0|            0|  0.00%|                pass
    62|         0|            0|            0|  0.00%|            else:
    63|        40|  0.000351191|  8.77976e-06|  0.00%|                break
    64|         0|            0|            0|  0.00%|        else:
    65|         0|            0|            0|  0.00%|            raise FileExistsError('cannot find name for semaphore')
    66|         0|            0|            0|  0.00%|
    67|        40|  0.000878572|  2.19643e-05|  0.00%|        util.debug('created semlock with handle %s' % sl.handle)
(call)|        40|  0.000503302|  1.25825e-05|  0.00%|# /opt/conda/lib/python3.8/multiprocessing/util.py:48 debug
    68|        40|  0.000567913|  1.41978e-05|  0.00%|        self._make_methods()
(call)|        40|   0.00060463|  1.51157e-05|  0.00%|# /opt/conda/lib/python3.8/multiprocessing/synchronize.py:90 _make_methods
    69|         0|            0|            0|  0.00%|
    70|        40|  0.000224829|  5.62072e-06|  0.00%|        if sys.platform != 'win32':
    71|        40|   0.00033474|  8.36849e-06|  0.00%|            def _after_fork(obj):
    72|         0|            0|            0|  0.00%|                obj._semlock._after_fork()
    73|        40|  0.000740528|  1.85132e-05|  0.00%|            util.register_after_fork(self, _after_fork)
(call)|        40|    0.0045321|  0.000113302|  0.01%|# /opt/conda/lib/python3.8/multiprocessing/util.py:171 register_after_fork
    74|         0|            0|            0|  0.00%|
    75|        40|  0.000222445|  5.56111e-06|  0.00%|        if self._semlock.name is not None:
    76|         0|            0|            0|  0.00%|            # We only get here if we are on Unix with forking
    77|         0|            0|            0|  0.00%|            # disabled.  When the object is garbage collected or the
    78|         0|            0|            0|  0.00%|            # process shuts down we unlink the semaphore name
    79|         0|            0|            0|  0.00%|            from .resource_tracker import register
    80|         0|            0|            0|  0.00%|            register(self._semlock.name, "semaphore")
    81|         0|            0|            0|  0.00%|            util.Finalize(self, SemLock._cleanup, (self._semlock.name,),
    82|         0|            0|            0|  0.00%|                          exitpriority=0)
    83|         0|            0|            0|  0.00%|
    84|         0|            0|            0|  0.00%|    @staticmethod
    85|         0|            0|            0|  0.00%|    def _cleanup(name):
    86|         0|            0|            0|  0.00%|        from .resource_tracker import unregister
    87|         0|            0|            0|  0.00%|        sem_unlink(name)
    88|         0|            0|            0|  0.00%|        unregister(name, "semaphore")
    89|         0|            0|            0|  0.00%|
    90|        40|  0.000248432|   6.2108e-06|  0.00%|    def _make_methods(self):
    91|        40|  0.000192404|  4.81009e-06|  0.00%|        self.acquire = self._semlock.acquire
    92|        40|  0.000163794|  4.09484e-06|  0.00%|        self.release = self._semlock.release
    93|         0|            0|            0|  0.00%|
    94|        10|  5.03063e-05|  5.03063e-06|  0.00%|    def __enter__(self):
    95|        10|  0.000129938|  1.29938e-05|  0.00%|        return self._semlock.__enter__()
    96|         0|            0|            0|  0.00%|
    97|        10|  4.19617e-05|  4.19617e-06|  0.00%|    def __exit__(self, *args):
    98|        10|  6.48499e-05|  6.48499e-06|  0.00%|        return self._semlock.__exit__(*args)
    99|         0|            0|            0|  0.00%|
   100|         0|            0|            0|  0.00%|    def __getstate__(self):
   101|         0|            0|            0|  0.00%|        context.assert_spawning(self)
   102|         0|            0|            0|  0.00%|        sl = self._semlock
   103|         0|            0|            0|  0.00%|        if sys.platform == 'win32':
   104|         0|            0|            0|  0.00%|            h = context.get_spawning_popen().duplicate_for_child(sl.handle)
   105|         0|            0|            0|  0.00%|        else:
   106|         0|            0|            0|  0.00%|            h = sl.handle
   107|         0|            0|            0|  0.00%|        return (h, sl.kind, sl.maxvalue, sl.name)
   108|         0|            0|            0|  0.00%|
   109|         0|            0|            0|  0.00%|    def __setstate__(self, state):
   110|         0|            0|            0|  0.00%|        self._semlock = _multiprocessing.SemLock._rebuild(*state)
   111|         0|            0|            0|  0.00%|        util.debug('recreated blocker with handle %r' % state[0])
   112|         0|            0|            0|  0.00%|        self._make_methods()
   113|         0|            0|            0|  0.00%|
   114|        40|  0.000202894|  5.07236e-06|  0.00%|    @staticmethod
   115|         0|            0|            0|  0.00%|    def _make_name():
   116|        80|   0.00113058|  1.41323e-05|  0.00%|        return '%s-%s' % (process.current_process()._config['semprefix'],
(call)|        40|  0.000390053|  9.75132e-06|  0.00%|# /opt/conda/lib/python3.8/multiprocessing/process.py:37 current_process
   117|        40|  0.000889778|  2.22445e-05|  0.00%|                          next(SemLock._rand))
(call)|        40|    0.0287285|  0.000718212|  0.05%|# /opt/conda/lib/python3.8/tempfile.py:144 __next__
   118|         0|            0|            0|  0.00%|
   119|         0|            0|            0|  0.00%|#
   120|         0|            0|            0|  0.00%|# Semaphore
   121|         0|            0|            0|  0.00%|#
   122|         0|            0|            0|  0.00%|
   123|         0|            0|            0|  0.00%|class Semaphore(SemLock):
   124|         0|            0|            0|  0.00%|
   125|         8|  3.71933e-05|  4.64916e-06|  0.00%|    def __init__(self, value=1, *, ctx):
   126|         8|  0.000124693|  1.55866e-05|  0.00%|        SemLock.__init__(self, SEMAPHORE, value, SEM_VALUE_MAX, ctx=ctx)
(call)|         8|   0.00805759|    0.0010072|  0.02%|# /opt/conda/lib/python3.8/multiprocessing/synchronize.py:50 __init__
   127|         0|            0|            0|  0.00%|
   128|         0|            0|            0|  0.00%|    def get_value(self):
   129|         0|            0|            0|  0.00%|        return self._semlock._get_value()
   130|         0|            0|            0|  0.00%|
   131|         0|            0|            0|  0.00%|    def __repr__(self):
   132|         0|            0|            0|  0.00%|        try:
   133|         0|            0|            0|  0.00%|            value = self._semlock._get_value()
   134|         0|            0|            0|  0.00%|        except Exception:
   135|         0|            0|            0|  0.00%|            value = 'unknown'
   136|         0|            0|            0|  0.00%|        return '<%s(value=%s)>' % (self.__class__.__name__, value)
   137|         0|            0|            0|  0.00%|
   138|         0|            0|            0|  0.00%|#
   139|         0|            0|            0|  0.00%|# Bounded semaphore
   140|         0|            0|            0|  0.00%|#
   141|         0|            0|            0|  0.00%|
   142|         0|            0|            0|  0.00%|class BoundedSemaphore(Semaphore):
   143|         0|            0|            0|  0.00%|
   144|        10|  4.81606e-05|  4.81606e-06|  0.00%|    def __init__(self, value=1, *, ctx):
   145|        10|  0.000166893|  1.66893e-05|  0.00%|        SemLock.__init__(self, SEMAPHORE, value, value, ctx=ctx)
(call)|        10|    0.0106957|   0.00106957|  0.02%|# /opt/conda/lib/python3.8/multiprocessing/synchronize.py:50 __init__
   146|         0|            0|            0|  0.00%|
   147|         0|            0|            0|  0.00%|    def __repr__(self):
   148|         0|            0|            0|  0.00%|        try:
   149|         0|            0|            0|  0.00%|            value = self._semlock._get_value()
   150|         0|            0|            0|  0.00%|        except Exception:
   151|         0|            0|            0|  0.00%|            value = 'unknown'
   152|         0|            0|            0|  0.00%|        return '<%s(value=%s, maxvalue=%s)>' % \
   153|         0|            0|            0|  0.00%|               (self.__class__.__name__, value, self._semlock.maxvalue)
   154|         0|            0|            0|  0.00%|
   155|         0|            0|            0|  0.00%|#
   156|         0|            0|            0|  0.00%|# Non-recursive lock
   157|         0|            0|            0|  0.00%|#
   158|         0|            0|            0|  0.00%|
   159|         0|            0|            0|  0.00%|class Lock(SemLock):
   160|         0|            0|            0|  0.00%|
   161|        22|  0.000130892|  5.94963e-06|  0.00%|    def __init__(self, *, ctx):
   162|        22|  0.000830889|  3.77677e-05|  0.00%|        SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx)
(call)|        22|    0.0286207|   0.00130094|  0.05%|# /opt/conda/lib/python3.8/multiprocessing/synchronize.py:50 __init__
   163|         0|            0|            0|  0.00%|
   164|         0|            0|            0|  0.00%|    def __repr__(self):
   165|         0|            0|            0|  0.00%|        try:
   166|         0|            0|            0|  0.00%|            if self._semlock._is_mine():
   167|         0|            0|            0|  0.00%|                name = process.current_process().name
   168|         0|            0|            0|  0.00%|                if threading.current_thread().name != 'MainThread':
   169|         0|            0|            0|  0.00%|                    name += '|' + threading.current_thread().name
   170|         0|            0|            0|  0.00%|            elif self._semlock._get_value() == 1:
   171|         0|            0|            0|  0.00%|                name = 'None'
   172|         0|            0|            0|  0.00%|            elif self._semlock._count() > 0:
   173|         0|            0|            0|  0.00%|                name = 'SomeOtherThread'
   174|         0|            0|            0|  0.00%|            else:
   175|         0|            0|            0|  0.00%|                name = 'SomeOtherProcess'
   176|         0|            0|            0|  0.00%|        except Exception:
   177|         0|            0|            0|  0.00%|            name = 'unknown'
   178|         0|            0|            0|  0.00%|        return '<%s(owner=%s)>' % (self.__class__.__name__, name)
   179|         0|            0|            0|  0.00%|
   180|         0|            0|            0|  0.00%|#
   181|         0|            0|            0|  0.00%|# Recursive lock
   182|         0|            0|            0|  0.00%|#
   183|         0|            0|            0|  0.00%|
   184|         0|            0|            0|  0.00%|class RLock(SemLock):
   185|         0|            0|            0|  0.00%|
   186|         0|            0|            0|  0.00%|    def __init__(self, *, ctx):
   187|         0|            0|            0|  0.00%|        SemLock.__init__(self, RECURSIVE_MUTEX, 1, 1, ctx=ctx)
   188|         0|            0|            0|  0.00%|
   189|         0|            0|            0|  0.00%|    def __repr__(self):
   190|         0|            0|            0|  0.00%|        try:
   191|         0|            0|            0|  0.00%|            if self._semlock._is_mine():
   192|         0|            0|            0|  0.00%|                name = process.current_process().name
   193|         0|            0|            0|  0.00%|                if threading.current_thread().name != 'MainThread':
   194|         0|            0|            0|  0.00%|                    name += '|' + threading.current_thread().name
   195|         0|            0|            0|  0.00%|                count = self._semlock._count()
   196|         0|            0|            0|  0.00%|            elif self._semlock._get_value() == 1:
   197|         0|            0|            0|  0.00%|                name, count = 'None', 0
   198|         0|            0|            0|  0.00%|            elif self._semlock._count() > 0:
   199|         0|            0|            0|  0.00%|                name, count = 'SomeOtherThread', 'nonzero'
   200|         0|            0|            0|  0.00%|            else:
   201|         0|            0|            0|  0.00%|                name, count = 'SomeOtherProcess', 'nonzero'
   202|         0|            0|            0|  0.00%|        except Exception:
   203|         0|            0|            0|  0.00%|            name, count = 'unknown', 'unknown'
   204|         0|            0|            0|  0.00%|        return '<%s(%s, %s)>' % (self.__class__.__name__, name, count)
   205|         0|            0|            0|  0.00%|
   206|         0|            0|            0|  0.00%|#
   207|         0|            0|            0|  0.00%|# Condition variable
   208|         0|            0|            0|  0.00%|#
   209|         0|            0|            0|  0.00%|
   210|         0|            0|            0|  0.00%|class Condition(object):
   211|         0|            0|            0|  0.00%|
   212|         2|  2.02656e-05|  1.01328e-05|  0.00%|    def __init__(self, lock=None, *, ctx):
   213|         2|  1.38283e-05|  6.91414e-06|  0.00%|        self._lock = lock or ctx.RLock()
   214|         2|  3.33786e-05|  1.66893e-05|  0.00%|        self._sleeping_count = ctx.Semaphore(0)
(call)|         2|   0.00227427|   0.00113714|  0.00%|# /opt/conda/lib/python3.8/multiprocessing/context.py:80 Semaphore
   215|         2|  3.05176e-05|  1.52588e-05|  0.00%|        self._woken_count = ctx.Semaphore(0)
(call)|         2|   0.00220394|   0.00110197|  0.00%|# /opt/conda/lib/python3.8/multiprocessing/context.py:80 Semaphore
   216|         2|  3.24249e-05|  1.62125e-05|  0.00%|        self._wait_semaphore = ctx.Semaphore(0)
(call)|         2|   0.00216055|   0.00108027|  0.00%|# /opt/conda/lib/python3.8/multiprocessing/context.py:80 Semaphore
   217|         2|   3.3617e-05|  1.68085e-05|  0.00%|        self._make_methods()
(call)|         2|  3.38554e-05|  1.69277e-05|  0.00%|# /opt/conda/lib/python3.8/multiprocessing/synchronize.py:235 _make_methods
   218|         0|            0|            0|  0.00%|
   219|         0|            0|            0|  0.00%|    def __getstate__(self):
   220|         0|            0|            0|  0.00%|        context.assert_spawning(self)
   221|         0|            0|            0|  0.00%|        return (self._lock, self._sleeping_count,
   222|         0|            0|            0|  0.00%|                self._woken_count, self._wait_semaphore)
   223|         0|            0|            0|  0.00%|
   224|         0|            0|            0|  0.00%|    def __setstate__(self, state):
   225|         0|            0|            0|  0.00%|        (self._lock, self._sleeping_count,
   226|         0|            0|            0|  0.00%|         self._woken_count, self._wait_semaphore) = state
   227|         0|            0|            0|  0.00%|        self._make_methods()
   228|         0|            0|            0|  0.00%|
   229|        10|   6.1512e-05|   6.1512e-06|  0.00%|    def __enter__(self):
   230|        10|  0.000145197|  1.45197e-05|  0.00%|        return self._lock.__enter__()
(call)|        10|  0.000180244|  1.80244e-05|  0.00%|# /opt/conda/lib/python3.8/multiprocessing/synchronize.py:94 __enter__
   231|         0|            0|            0|  0.00%|
   232|        10|  5.53131e-05|  5.53131e-06|  0.00%|    def __exit__(self, *args):
   233|        10|  0.000133276|  1.33276e-05|  0.00%|        return self._lock.__exit__(*args)
(call)|        10|  0.000106812|  1.06812e-05|  0.00%|# /opt/conda/lib/python3.8/multiprocessing/synchronize.py:97 __exit__
   234|         0|            0|            0|  0.00%|
   235|         2|  1.26362e-05|  6.31809e-06|  0.00%|    def _make_methods(self):
   236|         2|  1.14441e-05|  5.72205e-06|  0.00%|        self.acquire = self._lock.acquire
   237|         2|  9.77516e-06|  4.88758e-06|  0.00%|        self.release = self._lock.release
   238|         0|            0|            0|  0.00%|
   239|         0|            0|            0|  0.00%|    def __repr__(self):
   240|         0|            0|            0|  0.00%|        try:
   241|         0|            0|            0|  0.00%|            num_waiters = (self._sleeping_count._semlock._get_value() -
   242|         0|            0|            0|  0.00%|                           self._woken_count._semlock._get_value())
   243|         0|            0|            0|  0.00%|        except Exception:
   244|         0|            0|            0|  0.00%|            num_waiters = 'unknown'
   245|         0|            0|            0|  0.00%|        return '<%s(%s, %s)>' % (self.__class__.__name__, self._lock, num_waiters)
   246|         0|            0|            0|  0.00%|
   247|         0|            0|            0|  0.00%|    def wait(self, timeout=None):
   248|         0|            0|            0|  0.00%|        assert self._lock._semlock._is_mine(), \
   249|         0|            0|            0|  0.00%|               'must acquire() condition before using wait()'
   250|         0|            0|            0|  0.00%|
   251|         0|            0|            0|  0.00%|        # indicate that this thread is going to sleep
   252|         0|            0|            0|  0.00%|        self._sleeping_count.release()
   253|         0|            0|            0|  0.00%|
   254|         0|            0|            0|  0.00%|        # release lock
   255|         0|            0|            0|  0.00%|        count = self._lock._semlock._count()
   256|         0|            0|            0|  0.00%|        for i in range(count):
   257|         0|            0|            0|  0.00%|            self._lock.release()
   258|         0|            0|            0|  0.00%|
   259|         0|            0|            0|  0.00%|        try:
   260|         0|            0|            0|  0.00%|            # wait for notification or timeout
   261|         0|            0|            0|  0.00%|            return self._wait_semaphore.acquire(True, timeout)
   262|         0|            0|            0|  0.00%|        finally:
   263|         0|            0|            0|  0.00%|            # indicate that this thread has woken
   264|         0|            0|            0|  0.00%|            self._woken_count.release()
   265|         0|            0|            0|  0.00%|
   266|         0|            0|            0|  0.00%|            # reacquire lock
   267|         0|            0|            0|  0.00%|            for i in range(count):
   268|         0|            0|            0|  0.00%|                self._lock.acquire()
   269|         0|            0|            0|  0.00%|
   270|         2|  1.69277e-05|  8.46386e-06|  0.00%|    def notify(self, n=1):
   271|         2|  2.26498e-05|  1.13249e-05|  0.00%|        assert self._lock._semlock._is_mine(), 'lock is not owned'
   272|         4|  4.41074e-05|  1.10269e-05|  0.00%|        assert not self._wait_semaphore.acquire(
   273|         2|  1.16825e-05|  5.84126e-06|  0.00%|            False), ('notify: Should not have been able to acquire '
   274|         0|            0|            0|  0.00%|                     + '_wait_semaphore')
   275|         0|            0|            0|  0.00%|
   276|         0|            0|            0|  0.00%|        # to take account of timeouts since last notify*() we subtract
   277|         0|            0|            0|  0.00%|        # woken_count from sleeping_count and rezero woken_count
   278|         2|  2.81334e-05|  1.40667e-05|  0.00%|        while self._woken_count.acquire(False):
   279|         0|            0|            0|  0.00%|            res = self._sleeping_count.acquire(False)
   280|         0|            0|            0|  0.00%|            assert res, ('notify: Bug in sleeping_count.acquire'
   281|         0|            0|            0|  0.00%|                         + '- res should not be False')
   282|         0|            0|            0|  0.00%|
   283|         2|   1.3113e-05|  6.55651e-06|  0.00%|        sleepers = 0
   284|         2|  4.29153e-05|  2.14577e-05|  0.00%|        while sleepers < n and self._sleeping_count.acquire(False):
   285|         0|            0|            0|  0.00%|            self._wait_semaphore.release()        # wake up one sleeper
   286|         0|            0|            0|  0.00%|            sleepers += 1
   287|         0|            0|            0|  0.00%|
   288|         2|  1.33514e-05|  6.67572e-06|  0.00%|        if sleepers:
   289|         0|            0|            0|  0.00%|            for i in range(sleepers):
   290|         0|            0|            0|  0.00%|                self._woken_count.acquire()       # wait for a sleeper to wake
   291|         0|            0|            0|  0.00%|
   292|         0|            0|            0|  0.00%|            # rezero wait_semaphore in case some timeouts just happened
   293|         0|            0|            0|  0.00%|            while self._wait_semaphore.acquire(False):
   294|         0|            0|            0|  0.00%|                pass
   295|         0|            0|            0|  0.00%|
   296|         2|  1.97887e-05|  9.89437e-06|  0.00%|    def notify_all(self):
   297|         2|  6.10352e-05|  3.05176e-05|  0.00%|        self.notify(n=sys.maxsize)
(call)|         2|  0.000192881|  9.64403e-05|  0.00%|# /opt/conda/lib/python3.8/multiprocessing/synchronize.py:270 notify
   298|         0|            0|            0|  0.00%|
   299|         0|            0|            0|  0.00%|    def wait_for(self, predicate, timeout=None):
   300|         0|            0|            0|  0.00%|        result = predicate()
   301|         0|            0|            0|  0.00%|        if result:
   302|         0|            0|            0|  0.00%|            return result
   303|         0|            0|            0|  0.00%|        if timeout is not None:
   304|         0|            0|            0|  0.00%|            endtime = time.monotonic() + timeout
   305|         0|            0|            0|  0.00%|        else:
   306|         0|            0|            0|  0.00%|            endtime = None
   307|         0|            0|            0|  0.00%|            waittime = None
   308|         0|            0|            0|  0.00%|        while not result:
   309|         0|            0|            0|  0.00%|            if endtime is not None:
   310|         0|            0|            0|  0.00%|                waittime = endtime - time.monotonic()
   311|         0|            0|            0|  0.00%|                if waittime <= 0:
   312|         0|            0|            0|  0.00%|                    break
   313|         0|            0|            0|  0.00%|            self.wait(waittime)
   314|         0|            0|            0|  0.00%|            result = predicate()
   315|         0|            0|            0|  0.00%|        return result
   316|         0|            0|            0|  0.00%|
   317|         0|            0|            0|  0.00%|#
   318|         0|            0|            0|  0.00%|# Event
   319|         0|            0|            0|  0.00%|#
   320|         0|            0|            0|  0.00%|
   321|         0|            0|            0|  0.00%|class Event(object):
   322|         0|            0|            0|  0.00%|
   323|         2|  1.21593e-05|  6.07967e-06|  0.00%|    def __init__(self, *, ctx):
   324|         2|  5.45979e-05|  2.72989e-05|  0.00%|        self._cond = ctx.Condition(ctx.Lock())
(call)|         2|    0.0021987|   0.00109935|  0.00%|# /opt/conda/lib/python3.8/multiprocessing/context.py:65 Lock
(call)|         2|   0.00702834|   0.00351417|  0.01%|# /opt/conda/lib/python3.8/multiprocessing/context.py:75 Condition
   325|         2|   3.0756e-05|   1.5378e-05|  0.00%|        self._flag = ctx.Semaphore(0)
(call)|         2|   0.00217652|   0.00108826|  0.00%|# /opt/conda/lib/python3.8/multiprocessing/context.py:80 Semaphore
   326|         0|            0|            0|  0.00%|
   327|         8|  3.62396e-05|  4.52995e-06|  0.00%|    def is_set(self):
   328|         8|  0.000100613|  1.25766e-05|  0.00%|        with self._cond:
(call)|         8|  0.000209332|  2.61664e-05|  0.00%|# /opt/conda/lib/python3.8/multiprocessing/synchronize.py:229 __enter__
   329|         8|  5.36442e-05|  6.70552e-06|  0.00%|            if self._flag.acquire(False):
   330|         8|  4.93526e-05|  6.16908e-06|  0.00%|                self._flag.release()
   331|         8|  9.70364e-05|  1.21295e-05|  0.00%|                return True
(call)|         8|  0.000217438|  2.71797e-05|  0.00%|# /opt/conda/lib/python3.8/multiprocessing/synchronize.py:232 __exit__
   332|         0|            0|            0|  0.00%|            return False
   333|         0|            0|            0|  0.00%|
   334|         2|   1.5974e-05|  7.98702e-06|  0.00%|    def set(self):
   335|         2|  4.86374e-05|  2.43187e-05|  0.00%|        with self._cond:
(call)|         2|  0.000177622|  8.88109e-05|  0.00%|# /opt/conda/lib/python3.8/multiprocessing/synchronize.py:229 __enter__
   336|         2|  4.79221e-05|  2.39611e-05|  0.00%|            self._flag.acquire(False)
   337|         2|  2.00272e-05|  1.00136e-05|  0.00%|            self._flag.release()
   338|         2|  5.60284e-05|  2.80142e-05|  0.00%|            self._cond.notify_all()
(call)|         2|  0.000273705|  0.000136852|  0.00%|# /opt/conda/lib/python3.8/multiprocessing/synchronize.py:296 notify_all
(call)|         2|  7.79629e-05|  3.89814e-05|  0.00%|# /opt/conda/lib/python3.8/multiprocessing/synchronize.py:232 __exit__
   339|         0|            0|            0|  0.00%|
   340|         0|            0|            0|  0.00%|    def clear(self):
   341|         0|            0|            0|  0.00%|        with self._cond:
   342|         0|            0|            0|  0.00%|            self._flag.acquire(False)
   343|         0|            0|            0|  0.00%|
   344|         0|            0|            0|  0.00%|    def wait(self, timeout=None):
   345|         0|            0|            0|  0.00%|        with self._cond:
   346|         0|            0|            0|  0.00%|            if self._flag.acquire(False):
   347|         0|            0|            0|  0.00%|                self._flag.release()
   348|         0|            0|            0|  0.00%|            else:
   349|         0|            0|            0|  0.00%|                self._cond.wait(timeout)
   350|         0|            0|            0|  0.00%|
   351|         0|            0|            0|  0.00%|            if self._flag.acquire(False):
   352|         0|            0|            0|  0.00%|                self._flag.release()
   353|         0|            0|            0|  0.00%|                return True
   354|         0|            0|            0|  0.00%|            return False
   355|         0|            0|            0|  0.00%|
   356|         0|            0|            0|  0.00%|#
   357|         0|            0|            0|  0.00%|# Barrier
   358|         0|            0|            0|  0.00%|#
   359|         0|            0|            0|  0.00%|
   360|         0|            0|            0|  0.00%|class Barrier(threading.Barrier):
   361|         0|            0|            0|  0.00%|
   362|         0|            0|            0|  0.00%|    def __init__(self, parties, action=None, timeout=None, *, ctx):
   363|         0|            0|            0|  0.00%|        import struct
   364|         0|            0|            0|  0.00%|        from .heap import BufferWrapper
   365|         0|            0|            0|  0.00%|        wrapper = BufferWrapper(struct.calcsize('i') * 2)
   366|         0|            0|            0|  0.00%|        cond = ctx.Condition()
   367|         0|            0|            0|  0.00%|        self.__setstate__((parties, action, timeout, cond, wrapper))
   368|         0|            0|            0|  0.00%|        self._state = 0
   369|         0|            0|            0|  0.00%|        self._count = 0
   370|         0|            0|            0|  0.00%|
   371|         0|            0|            0|  0.00%|    def __setstate__(self, state):
   372|         0|            0|            0|  0.00%|        (self._parties, self._action, self._timeout,
   373|         0|            0|            0|  0.00%|         self._cond, self._wrapper) = state
   374|         0|            0|            0|  0.00%|        self._array = self._wrapper.create_memoryview().cast('i')
   375|         0|            0|            0|  0.00%|
   376|         0|            0|            0|  0.00%|    def __getstate__(self):
   377|         0|            0|            0|  0.00%|        return (self._parties, self._action, self._timeout,
   378|         0|            0|            0|  0.00%|                self._cond, self._wrapper)
   379|         0|            0|            0|  0.00%|
   380|         0|            0|            0|  0.00%|    @property
   381|         0|            0|            0|  0.00%|    def _state(self):
   382|         0|            0|            0|  0.00%|        return self._array[0]
   383|         0|            0|            0|  0.00%|
   384|         0|            0|            0|  0.00%|    @_state.setter
   385|         0|            0|            0|  0.00%|    def _state(self, value):
   386|         0|            0|            0|  0.00%|        self._array[0] = value
   387|         0|            0|            0|  0.00%|
   388|         0|            0|            0|  0.00%|    @property
   389|         0|            0|            0|  0.00%|    def _count(self):
   390|         0|            0|            0|  0.00%|        return self._array[1]
   391|         0|            0|            0|  0.00%|
   392|         0|            0|            0|  0.00%|    @_count.setter
   393|         0|            0|            0|  0.00%|    def _count(self, value):
   394|         0|            0|            0|  0.00%|        self._array[1] = value
File: /opt/conda/lib/python3.8/site-packages/torch/utils/data/sampler.py
File duration: 0.0151014s (0.03%)
Line #|      Hits|         Time| Time per hit|      %|Source code
------+----------+-------------+-------------+-------+-----------
     1|         0|            0|            0|  0.00%|import torch
     2|         0|            0|            0|  0.00%|from torch import Tensor
     3|         0|            0|            0|  0.00%|
     4|         0|            0|            0|  0.00%|from typing import Iterator, Optional, Sequence, List, TypeVar, Generic, Sized
     5|         0|            0|            0|  0.00%|
     6|         0|            0|            0|  0.00%|T_co = TypeVar('T_co', covariant=True)
     7|         0|            0|            0|  0.00%|
     8|         0|            0|            0|  0.00%|class Sampler(Generic[T_co]):
     9|         0|            0|            0|  0.00%|    r"""Base class for all Samplers.
    10|         0|            0|            0|  0.00%|
    11|         0|            0|            0|  0.00%|    Every Sampler subclass has to provide an :meth:`__iter__` method, providing a
    12|         0|            0|            0|  0.00%|    way to iterate over indices of dataset elements, and a :meth:`__len__` method
    13|         0|            0|            0|  0.00%|    that returns the length of the returned iterators.
    14|         0|            0|            0|  0.00%|
    15|         0|            0|            0|  0.00%|    .. note:: The :meth:`__len__` method isn't strictly required by
    16|         0|            0|            0|  0.00%|              :class:`~torch.utils.data.DataLoader`, but is expected in any
    17|         0|            0|            0|  0.00%|              calculation involving the length of a :class:`~torch.utils.data.DataLoader`.
    18|         0|            0|            0|  0.00%|    """
    19|         0|            0|            0|  0.00%|
    20|         0|            0|            0|  0.00%|    def __init__(self, data_source: Optional[Sized]) -> None:
    21|         0|            0|            0|  0.00%|        pass
    22|         0|            0|            0|  0.00%|
    23|         0|            0|            0|  0.00%|    def __iter__(self) -> Iterator[T_co]:
    24|         0|            0|            0|  0.00%|        raise NotImplementedError
    25|         0|            0|            0|  0.00%|
    26|         0|            0|            0|  0.00%|    # NOTE [ Lack of Default `__len__` in Python Abstract Base Classes ]
    27|         0|            0|            0|  0.00%|    #
    28|         0|            0|            0|  0.00%|    # Many times we have an abstract class representing a collection/iterable of
    29|         0|            0|            0|  0.00%|    # data, e.g., `torch.utils.data.Sampler`, with its subclasses optionally
    30|         0|            0|            0|  0.00%|    # implementing a `__len__` method. In such cases, we must make sure to not
    31|         0|            0|            0|  0.00%|    # provide a default implementation, because both straightforward default
    32|         0|            0|            0|  0.00%|    # implementations have their issues:
    33|         0|            0|            0|  0.00%|    #
    34|         0|            0|            0|  0.00%|    #   + `return NotImplemented`:
    35|         0|            0|            0|  0.00%|    #     Calling `len(subclass_instance)` raises:
    36|         0|            0|            0|  0.00%|    #       TypeError: 'NotImplementedType' object cannot be interpreted as an integer
    37|         0|            0|            0|  0.00%|    #
    38|         0|            0|            0|  0.00%|    #   + `raise NotImplementedError()`:
    39|         0|            0|            0|  0.00%|    #     This prevents triggering some fallback behavior. E.g., the built-in
    40|         0|            0|            0|  0.00%|    #     `list(X)` tries to call `len(X)` first, and executes a different code
    41|         0|            0|            0|  0.00%|    #     path if the method is not found or `NotImplemented` is returned, while
    42|         0|            0|            0|  0.00%|    #     raising an `NotImplementedError` will propagate and and make the call
    43|         0|            0|            0|  0.00%|    #     fail where it could have use `__iter__` to complete the call.
    44|         0|            0|            0|  0.00%|    #
    45|         0|            0|            0|  0.00%|    # Thus, the only two sensible things to do are
    46|         0|            0|            0|  0.00%|    #
    47|         0|            0|            0|  0.00%|    #   + **not** provide a default `__len__`.
    48|         0|            0|            0|  0.00%|    #
    49|         0|            0|            0|  0.00%|    #   + raise a `TypeError` instead, which is what Python uses when users call
    50|         0|            0|            0|  0.00%|    #     a method that is not defined on an object.
    51|         0|            0|            0|  0.00%|    #     (@ssnl verifies that this works on at least Python 3.7.)
    52|         0|            0|            0|  0.00%|
    53|         0|            0|            0|  0.00%|
    54|         0|            0|            0|  0.00%|class SequentialSampler(Sampler[int]):
    55|         0|            0|            0|  0.00%|    r"""Samples elements sequentially, always in the same order.
    56|         0|            0|            0|  0.00%|
    57|         0|            0|            0|  0.00%|    Args:
    58|         0|            0|            0|  0.00%|        data_source (Dataset): dataset to sample from
    59|         0|            0|            0|  0.00%|    """
    60|         0|            0|            0|  0.00%|    data_source: Sized
    61|         0|            0|            0|  0.00%|
    62|         0|            0|            0|  0.00%|    def __init__(self, data_source: Sized) -> None:
    63|         0|            0|            0|  0.00%|        self.data_source = data_source
    64|         0|            0|            0|  0.00%|
    65|         0|            0|            0|  0.00%|    def __iter__(self) -> Iterator[int]:
    66|         0|            0|            0|  0.00%|        return iter(range(len(self.data_source)))
    67|         0|            0|            0|  0.00%|
    68|         0|            0|            0|  0.00%|    def __len__(self) -> int:
    69|         0|            0|            0|  0.00%|        return len(self.data_source)
    70|         0|            0|            0|  0.00%|
    71|         0|            0|            0|  0.00%|
    72|         0|            0|            0|  0.00%|class RandomSampler(Sampler[int]):
    73|         0|            0|            0|  0.00%|    r"""Samples elements randomly. If without replacement, then sample from a shuffled dataset.
    74|         0|            0|            0|  0.00%|    If with replacement, then user can specify :attr:`num_samples` to draw.
    75|         0|            0|            0|  0.00%|
    76|         0|            0|            0|  0.00%|    Args:
    77|         0|            0|            0|  0.00%|        data_source (Dataset): dataset to sample from
    78|         0|            0|            0|  0.00%|        replacement (bool): samples are drawn on-demand with replacement if ``True``, default=``False``
    79|         0|            0|            0|  0.00%|        num_samples (int): number of samples to draw, default=`len(dataset)`. This argument
    80|         0|            0|            0|  0.00%|            is supposed to be specified only when `replacement` is ``True``.
    81|         0|            0|            0|  0.00%|        generator (Generator): Generator used in sampling.
    82|         0|            0|            0|  0.00%|    """
    83|         0|            0|            0|  0.00%|    data_source: Sized
    84|         0|            0|            0|  0.00%|    replacement: bool
    85|         0|            0|            0|  0.00%|
    86|         0|            0|            0|  0.00%|    def __init__(self, data_source: Sized, replacement: bool = False,
    87|         0|            0|            0|  0.00%|                 num_samples: Optional[int] = None, generator=None) -> None:
    88|         0|            0|            0|  0.00%|        self.data_source = data_source
    89|         0|            0|            0|  0.00%|        self.replacement = replacement
    90|         0|            0|            0|  0.00%|        self._num_samples = num_samples
    91|         0|            0|            0|  0.00%|        self.generator = generator
    92|         0|            0|            0|  0.00%|
    93|         0|            0|            0|  0.00%|        if not isinstance(self.replacement, bool):
    94|         0|            0|            0|  0.00%|            raise TypeError("replacement should be a boolean value, but got "
    95|         0|            0|            0|  0.00%|                            "replacement={}".format(self.replacement))
    96|         0|            0|            0|  0.00%|
    97|         0|            0|            0|  0.00%|        if self._num_samples is not None and not replacement:
    98|         0|            0|            0|  0.00%|            raise ValueError("With replacement=False, num_samples should not be specified, "
    99|         0|            0|            0|  0.00%|                             "since a random permute will be performed.")
   100|         0|            0|            0|  0.00%|
   101|         0|            0|            0|  0.00%|        if not isinstance(self.num_samples, int) or self.num_samples <= 0:
   102|         0|            0|            0|  0.00%|            raise ValueError("num_samples should be a positive integer "
   103|         0|            0|            0|  0.00%|                             "value, but got num_samples={}".format(self.num_samples))
   104|         0|            0|            0|  0.00%|
   105|         0|            0|            0|  0.00%|    @property
   106|         0|            0|            0|  0.00%|    def num_samples(self) -> int:
   107|         0|            0|            0|  0.00%|        # dataset size might change at runtime
   108|         0|            0|            0|  0.00%|        if self._num_samples is None:
   109|         0|            0|            0|  0.00%|            return len(self.data_source)
   110|         0|            0|            0|  0.00%|        return self._num_samples
   111|         0|            0|            0|  0.00%|
   112|         2|  2.88486e-05|  1.44243e-05|  0.00%|    def __iter__(self) -> Iterator[int]:
   113|         2|  0.000135422|  6.77109e-05|  0.00%|        n = len(self.data_source)
(call)|         2|  0.000169992|  8.49962e-05|  0.00%|# /opt/conda/lib/python3.8/site-packages/torchvision/datasets/folder.py:240 __len__
   114|         2|  1.28746e-05|   6.4373e-06|  0.00%|        if self.generator is None:
   115|         2|   0.00014019|  7.00951e-05|  0.00%|            generator = torch.Generator()
   116|         2|  0.000620365|  0.000310183|  0.00%|            generator.manual_seed(int(torch.empty((), dtype=torch.int64).random_().item()))
   117|         0|            0|            0|  0.00%|        else:
   118|         0|            0|            0|  0.00%|            generator = self.generator
   119|         2|  4.62532e-05|  2.31266e-05|  0.00%|        if self.replacement:
   120|         0|            0|            0|  0.00%|            for _ in range(self.num_samples // 32):
   121|         0|            0|            0|  0.00%|                yield from torch.randint(high=n, size=(32,), dtype=torch.int64, generator=generator).tolist()
   122|         0|            0|            0|  0.00%|            yield from torch.randint(high=n, size=(self.num_samples % 32,), dtype=torch.int64, generator=generator).tolist()
   123|         0|            0|            0|  0.00%|        else:
   124|       399|   0.00250459|  6.27716e-06|  0.00%|            yield from torch.randperm(n, generator=generator).tolist()
   125|         0|            0|            0|  0.00%|
   126|         0|            0|            0|  0.00%|    def __len__(self) -> int:
   127|         0|            0|            0|  0.00%|        return self.num_samples
   128|         0|            0|            0|  0.00%|
   129|         0|            0|            0|  0.00%|
   130|         0|            0|            0|  0.00%|class SubsetRandomSampler(Sampler[int]):
   131|         0|            0|            0|  0.00%|    r"""Samples elements randomly from a given list of indices, without replacement.
   132|         0|            0|            0|  0.00%|
   133|         0|            0|            0|  0.00%|    Args:
   134|         0|            0|            0|  0.00%|        indices (sequence): a sequence of indices
   135|         0|            0|            0|  0.00%|        generator (Generator): Generator used in sampling.
   136|         0|            0|            0|  0.00%|    """
   137|         0|            0|            0|  0.00%|    indices: Sequence[int]
   138|         0|            0|            0|  0.00%|
   139|         0|            0|            0|  0.00%|    def __init__(self, indices: Sequence[int], generator=None) -> None:
   140|         0|            0|            0|  0.00%|        self.indices = indices
   141|         0|            0|            0|  0.00%|        self.generator = generator
   142|         0|            0|            0|  0.00%|
   143|         0|            0|            0|  0.00%|    def __iter__(self) -> Iterator[int]:
   144|         0|            0|            0|  0.00%|        return (self.indices[i] for i in torch.randperm(len(self.indices), generator=self.generator))
   145|         0|            0|            0|  0.00%|
   146|         0|            0|            0|  0.00%|    def __len__(self) -> int:
   147|         0|            0|            0|  0.00%|        return len(self.indices)
   148|         0|            0|            0|  0.00%|
   149|         0|            0|            0|  0.00%|
   150|         0|            0|            0|  0.00%|class WeightedRandomSampler(Sampler[int]):
   151|         0|            0|            0|  0.00%|    r"""Samples elements from ``[0,..,len(weights)-1]`` with given probabilities (weights).
   152|         0|            0|            0|  0.00%|
   153|         0|            0|            0|  0.00%|    Args:
   154|         0|            0|            0|  0.00%|        weights (sequence)   : a sequence of weights, not necessary summing up to one
   155|         0|            0|            0|  0.00%|        num_samples (int): number of samples to draw
   156|         0|            0|            0|  0.00%|        replacement (bool): if ``True``, samples are drawn with replacement.
   157|         0|            0|            0|  0.00%|            If not, they are drawn without replacement, which means that when a
   158|         0|            0|            0|  0.00%|            sample index is drawn for a row, it cannot be drawn again for that row.
   159|         0|            0|            0|  0.00%|        generator (Generator): Generator used in sampling.
   160|         0|            0|            0|  0.00%|
   161|         0|            0|            0|  0.00%|    Example:
   162|         0|            0|            0|  0.00%|        >>> list(WeightedRandomSampler([0.1, 0.9, 0.4, 0.7, 3.0, 0.6], 5, replacement=True))
   163|         0|            0|            0|  0.00%|        [4, 4, 1, 4, 5]
   164|         0|            0|            0|  0.00%|        >>> list(WeightedRandomSampler([0.9, 0.4, 0.05, 0.2, 0.3, 0.1], 5, replacement=False))
   165|         0|            0|            0|  0.00%|        [0, 1, 4, 3, 2]
   166|         0|            0|            0|  0.00%|    """
   167|         0|            0|            0|  0.00%|    weights: Tensor
   168|         0|            0|            0|  0.00%|    num_samples: int
   169|         0|            0|            0|  0.00%|    replacement: bool
   170|         0|            0|            0|  0.00%|
   171|         0|            0|            0|  0.00%|    def __init__(self, weights: Sequence[float], num_samples: int,
   172|         0|            0|            0|  0.00%|                 replacement: bool = True, generator=None) -> None:
   173|         0|            0|            0|  0.00%|        if not isinstance(num_samples, int) or isinstance(num_samples, bool) or \
   174|         0|            0|            0|  0.00%|                num_samples <= 0:
   175|         0|            0|            0|  0.00%|            raise ValueError("num_samples should be a positive integer "
   176|         0|            0|            0|  0.00%|                             "value, but got num_samples={}".format(num_samples))
   177|         0|            0|            0|  0.00%|        if not isinstance(replacement, bool):
   178|         0|            0|            0|  0.00%|            raise ValueError("replacement should be a boolean value, but got "
   179|         0|            0|            0|  0.00%|                             "replacement={}".format(replacement))
   180|         0|            0|            0|  0.00%|        self.weights = torch.as_tensor(weights, dtype=torch.double)
   181|         0|            0|            0|  0.00%|        self.num_samples = num_samples
   182|         0|            0|            0|  0.00%|        self.replacement = replacement
   183|         0|            0|            0|  0.00%|        self.generator = generator
   184|         0|            0|            0|  0.00%|
   185|         0|            0|            0|  0.00%|    def __iter__(self) -> Iterator[int]:
   186|         0|            0|            0|  0.00%|        rand_tensor = torch.multinomial(self.weights, self.num_samples, self.replacement, generator=self.generator)
   187|         0|            0|            0|  0.00%|        return iter(rand_tensor.tolist())
   188|         0|            0|            0|  0.00%|
   189|         0|            0|            0|  0.00%|    def __len__(self) -> int:
   190|         0|            0|            0|  0.00%|        return self.num_samples
   191|         0|            0|            0|  0.00%|
   192|         0|            0|            0|  0.00%|
   193|         0|            0|            0|  0.00%|class BatchSampler(Sampler[List[int]]):
   194|         0|            0|            0|  0.00%|    r"""Wraps another sampler to yield a mini-batch of indices.
   195|         0|            0|            0|  0.00%|
   196|         0|            0|            0|  0.00%|    Args:
   197|         0|            0|            0|  0.00%|        sampler (Sampler or Iterable): Base sampler. Can be any iterable object
   198|         0|            0|            0|  0.00%|        batch_size (int): Size of mini-batch.
   199|         0|            0|            0|  0.00%|        drop_last (bool): If ``True``, the sampler will drop the last batch if
   200|         0|            0|            0|  0.00%|            its size would be less than ``batch_size``
   201|         0|            0|            0|  0.00%|
   202|         0|            0|            0|  0.00%|    Example:
   203|         0|            0|            0|  0.00%|        >>> list(BatchSampler(SequentialSampler(range(10)), batch_size=3, drop_last=False))
   204|         0|            0|            0|  0.00%|        [[0, 1, 2], [3, 4, 5], [6, 7, 8], [9]]
   205|         0|            0|            0|  0.00%|        >>> list(BatchSampler(SequentialSampler(range(10)), batch_size=3, drop_last=True))
   206|         0|            0|            0|  0.00%|        [[0, 1, 2], [3, 4, 5], [6, 7, 8]]
   207|         0|            0|            0|  0.00%|    """
   208|         0|            0|            0|  0.00%|
   209|         0|            0|            0|  0.00%|    def __init__(self, sampler: Sampler[int], batch_size: int, drop_last: bool) -> None:
   210|         0|            0|            0|  0.00%|        # Since collections.abc.Iterable does not check for `__getitem__`, which
   211|         0|            0|            0|  0.00%|        # is one way for an object to be an iterable, we don't do an `isinstance`
   212|         0|            0|            0|  0.00%|        # check here.
   213|         0|            0|            0|  0.00%|        if not isinstance(batch_size, int) or isinstance(batch_size, bool) or \
   214|         0|            0|            0|  0.00%|                batch_size <= 0:
   215|         0|            0|            0|  0.00%|            raise ValueError("batch_size should be a positive integer value, "
   216|         0|            0|            0|  0.00%|                             "but got batch_size={}".format(batch_size))
   217|         0|            0|            0|  0.00%|        if not isinstance(drop_last, bool):
   218|         0|            0|            0|  0.00%|            raise ValueError("drop_last should be a boolean value, but got "
   219|         0|            0|            0|  0.00%|                             "drop_last={}".format(drop_last))
   220|         0|            0|            0|  0.00%|        self.sampler = sampler
   221|         0|            0|            0|  0.00%|        self.batch_size = batch_size
   222|         0|            0|            0|  0.00%|        self.drop_last = drop_last
   223|         0|            0|            0|  0.00%|
   224|         4|  6.96182e-05|  1.74046e-05|  0.00%|    def __iter__(self) -> Iterator[List[int]]:
   225|         2|  9.77516e-06|  4.88758e-06|  0.00%|        batch = []
   226|       399|   0.00538158|  1.34877e-05|  0.01%|        for idx in self.sampler:
(call)|       399|   0.00365853|  9.16926e-06|  0.01%|# /opt/conda/lib/python3.8/site-packages/torch/utils/data/sampler.py:112 __iter__
   227|       397|   0.00233459|  5.88059e-06|  0.00%|            batch.append(idx)
   228|       397|    0.0024035|  6.05415e-06|  0.00%|            if len(batch) == self.batch_size:
   229|       198|    0.0008564|  4.32525e-06|  0.00%|                yield batch
   230|        99|  0.000533819|  5.39211e-06|  0.00%|                batch = []
   231|         2|  1.33514e-05|  6.67572e-06|  0.00%|        if len(batch) > 0 and not self.drop_last:
   232|         2|   1.0252e-05|    5.126e-06|  0.00%|            yield batch
   233|         0|            0|            0|  0.00%|
   234|         0|            0|            0|  0.00%|    def __len__(self) -> int:
   235|         0|            0|            0|  0.00%|        # Can only be called if self.sampler has __len__ implemented
   236|         0|            0|            0|  0.00%|        # We cannot enforce this condition, so we turn off typechecking for the
   237|         0|            0|            0|  0.00%|        # implementation below.
   238|         0|            0|            0|  0.00%|        # Somewhat related: see NOTE [ Lack of Default `__len__` in Python Abstract Base Classes ]
   239|         0|            0|            0|  0.00%|        if self.drop_last:
   240|         0|            0|            0|  0.00%|            return len(self.sampler) // self.batch_size  # type: ignore[arg-type]
   241|         0|            0|            0|  0.00%|        else:
   242|         0|            0|            0|  0.00%|            return (len(self.sampler) + self.batch_size - 1) // self.batch_size  # type: ignore[arg-type]
File: /opt/conda/lib/python3.8/site-packages/torch/_jit_internal.py
File duration: 0.0119801s (0.02%)
Line #|      Hits|         Time| Time per hit|      %|Source code
------+----------+-------------+-------------+-------+-----------
     1|         0|            0|            0|  0.00%|"""
     2|         0|            0|            0|  0.00%|The weak_script annotation needs to be here instead of inside torch/jit/ so it
     3|         0|            0|            0|  0.00%|can be used in other places in torch/ (namely torch.nn) without running into
     4|         0|            0|            0|  0.00%|circular dependency problems
     5|         0|            0|            0|  0.00%|"""
     6|         0|            0|            0|  0.00%|
     7|         0|            0|            0|  0.00%|import contextlib
     8|         0|            0|            0|  0.00%|import collections
     9|         0|            0|            0|  0.00%|import enum
    10|         0|            0|            0|  0.00%|import inspect
    11|         0|            0|            0|  0.00%|import ast
    12|         0|            0|            0|  0.00%|import weakref
    13|         0|            0|            0|  0.00%|import warnings
    14|         0|            0|            0|  0.00%|from textwrap import dedent
    15|         0|            0|            0|  0.00%|import torch
    16|         0|            0|            0|  0.00%|import sys
    17|         0|            0|            0|  0.00%|import builtins
    18|         0|            0|            0|  0.00%|import io
    19|         0|            0|            0|  0.00%|import pickle
    20|         0|            0|            0|  0.00%|# This is needed. `torch._jit_internal` is imported before `torch.distributed.__init__`.
    21|         0|            0|            0|  0.00%|# Explicitly ask to import `torch.distributed.__init__` first.
    22|         0|            0|            0|  0.00%|# Otherwise, "AttributeError: module 'torch' has no attribute 'distributed'" is raised.
    23|         0|            0|            0|  0.00%|import torch.distributed.rpc
    24|         0|            0|            0|  0.00%|from torch._utils_internal import get_source_lines_and_file
    25|         0|            0|            0|  0.00%|from torch.futures import Future
    26|         0|            0|            0|  0.00%|import torch.package._mangling as package_mangling
    27|         0|            0|            0|  0.00%|from typing import Any, Callable, Dict, Generic, List, Optional, Tuple, TypeVar, Union  # noqa: F401
    28|         0|            0|            0|  0.00%|
    29|         0|            0|            0|  0.00%|if sys.version_info[:2] > (3, 7):
    30|         0|            0|            0|  0.00%|    from typing import Final
    31|         0|            0|            0|  0.00%|else:
    32|         0|            0|            0|  0.00%|    from typing_extensions import Final
    33|         0|            0|            0|  0.00%|
    34|         0|            0|            0|  0.00%|# Wrapper functions that can call either of 2 functions depending on a boolean
    35|         0|            0|            0|  0.00%|# argument
    36|         0|            0|            0|  0.00%|boolean_dispatched: 'weakref.WeakKeyDictionary[Callable, Dict[str, Callable]]' = weakref.WeakKeyDictionary()  # noqa: T484
    37|         0|            0|            0|  0.00%|
    38|         0|            0|            0|  0.00%|
    39|         0|            0|            0|  0.00%|def createResolutionCallbackFromEnv(lookup_base):
    40|         0|            0|            0|  0.00%|    """
    41|         0|            0|            0|  0.00%|    Creates a resolution callback that will look up qualified names in an
    42|         0|            0|            0|  0.00%|    environment, starting with `lookup_base` for the base of any qualified
    43|         0|            0|            0|  0.00%|    names, then proceeding down the lookup chain with the resolved object.
    44|         0|            0|            0|  0.00%|
    45|         0|            0|            0|  0.00%|    You should not use this directly, it should only be used from the other
    46|         0|            0|            0|  0.00%|    createResolutionCallbackFrom* functions.
    47|         0|            0|            0|  0.00%|    """
    48|         0|            0|            0|  0.00%|    def lookupInModule(qualified_name, module):
    49|         0|            0|            0|  0.00%|        if '.' in qualified_name:
    50|         0|            0|            0|  0.00%|            parts = qualified_name.split('.')
    51|         0|            0|            0|  0.00%|            base = parts[0]
    52|         0|            0|            0|  0.00%|            remaining_pieces = '.'.join(parts[1:])
    53|         0|            0|            0|  0.00%|            module_value = getattr(module, base)
    54|         0|            0|            0|  0.00%|            return lookupInModule(remaining_pieces, module_value)
    55|         0|            0|            0|  0.00%|        else:
    56|         0|            0|            0|  0.00%|            return getattr(module, qualified_name)
    57|         0|            0|            0|  0.00%|
    58|         0|            0|            0|  0.00%|    def parseNestedExpr(expr, module) -> Tuple[Any, int]:
    59|         0|            0|            0|  0.00%|        i = 0
    60|         0|            0|            0|  0.00%|        while i < len(expr) and expr[i] not in (',', '[', ']'):
    61|         0|            0|            0|  0.00%|            i += 1
    62|         0|            0|            0|  0.00%|
    63|         0|            0|            0|  0.00%|        base = lookupInModule(expr[:i].strip(), module)
    64|         0|            0|            0|  0.00%|        assert base is not None, f"Unresolvable type {expr[:i]}"
    65|         0|            0|            0|  0.00%|        if i == len(expr) or expr[i] != '[':
    66|         0|            0|            0|  0.00%|            return base, i
    67|         0|            0|            0|  0.00%|
    68|         0|            0|            0|  0.00%|        assert expr[i] == '['
    69|         0|            0|            0|  0.00%|        parts = []
    70|         0|            0|            0|  0.00%|        while expr[i] != ']':
    71|         0|            0|            0|  0.00%|            part_len = 0
    72|         0|            0|            0|  0.00%|            i += 1
    73|         0|            0|            0|  0.00%|            part, part_len = parseNestedExpr(expr[i:], module)
    74|         0|            0|            0|  0.00%|            parts.append(part)
    75|         0|            0|            0|  0.00%|            i += part_len
    76|         0|            0|            0|  0.00%|        if len(parts) > 1:
    77|         0|            0|            0|  0.00%|            return base[tuple(parts)], i + 1
    78|         0|            0|            0|  0.00%|        else:
    79|         0|            0|            0|  0.00%|            return base[parts[0]], i + 1
    80|         0|            0|            0|  0.00%|
    81|         0|            0|            0|  0.00%|    def parseExpr(expr, module):
    82|         0|            0|            0|  0.00%|        try:
    83|         0|            0|            0|  0.00%|            value, len_parsed = parseNestedExpr(expr, module)
    84|         0|            0|            0|  0.00%|            assert len_parsed == len(expr), "whole expression was not parsed, falling back to c++ parser"
    85|         0|            0|            0|  0.00%|            return value
    86|         0|            0|            0|  0.00%|        except Exception:
    87|         0|            0|            0|  0.00%|            """
    88|         0|            0|            0|  0.00%|            The python resolver fails in several cases in known unit tests, and is intended
    89|         0|            0|            0|  0.00%|            to fall back gracefully to the c++ resolver in general.  For example, python 2 style
    90|         0|            0|            0|  0.00%|            annotations which are frequent in our unit tests often fail with types e.g. int not
    91|         0|            0|            0|  0.00%|            resolvable from the calling frame.
    92|         0|            0|            0|  0.00%|            """
    93|         0|            0|            0|  0.00%|            return None
    94|         0|            0|            0|  0.00%|
    95|         0|            0|            0|  0.00%|    return lambda expr: parseExpr(expr, lookup_base)
    96|         0|            0|            0|  0.00%|
    97|         0|            0|            0|  0.00%|
    98|         0|            0|            0|  0.00%|def createResolutionCallbackFromFrame(frames_up: int = 0):
    99|         0|            0|            0|  0.00%|    """
   100|         0|            0|            0|  0.00%|    Creates a function which, given a string variable name,
   101|         0|            0|            0|  0.00%|    returns the value of the variable in the scope of the caller of
   102|         0|            0|            0|  0.00%|    the function which called createResolutionCallbackFromFrame (by default).
   103|         0|            0|            0|  0.00%|
   104|         0|            0|            0|  0.00%|    This is used to enable access in-scope Python variables inside
   105|         0|            0|            0|  0.00%|    TorchScript fragments.
   106|         0|            0|            0|  0.00%|
   107|         0|            0|            0|  0.00%|    frames_up is number of additional frames to go up on the stack.
   108|         0|            0|            0|  0.00%|    The default value is 0, which correspond to the frame of the caller
   109|         0|            0|            0|  0.00%|    of createResolutionCallbackFromFrame. Also for example, if frames_up is set
   110|         0|            0|            0|  0.00%|    to 1, then the frame of the caller's caller of createResolutionCallbackFromFrame
   111|         0|            0|            0|  0.00%|    will be taken.
   112|         0|            0|            0|  0.00%|
   113|         0|            0|            0|  0.00%|    For example, the following program prints 2::
   114|         0|            0|            0|  0.00%|
   115|         0|            0|            0|  0.00%|        def bar():
   116|         0|            0|            0|  0.00%|            cb = createResolutionCallbackFromFrame(1)
   117|         0|            0|            0|  0.00%|            print(cb("foo"))
   118|         0|            0|            0|  0.00%|
   119|         0|            0|            0|  0.00%|        def baz():
   120|         0|            0|            0|  0.00%|            foo = 2
   121|         0|            0|            0|  0.00%|            bar()
   122|         0|            0|            0|  0.00%|
   123|         0|            0|            0|  0.00%|        baz()
   124|         0|            0|            0|  0.00%|    """
   125|         0|            0|            0|  0.00%|    frame = inspect.currentframe()
   126|         0|            0|            0|  0.00%|    i = 0
   127|         0|            0|            0|  0.00%|    while i < frames_up + 1:
   128|         0|            0|            0|  0.00%|        assert frame is not None
   129|         0|            0|            0|  0.00%|        frame = frame.f_back
   130|         0|            0|            0|  0.00%|        i += 1
   131|         0|            0|            0|  0.00%|
   132|         0|            0|            0|  0.00%|    assert frame is not None
   133|         0|            0|            0|  0.00%|    f_locals = frame.f_locals
   134|         0|            0|            0|  0.00%|    f_globals = frame.f_globals
   135|         0|            0|            0|  0.00%|
   136|         0|            0|            0|  0.00%|    class env(object):
   137|         0|            0|            0|  0.00%|        def __getattr__(self, key):
   138|         0|            0|            0|  0.00%|            if key in f_locals:
   139|         0|            0|            0|  0.00%|                return f_locals[key]
   140|         0|            0|            0|  0.00%|            elif key in f_globals:
   141|         0|            0|            0|  0.00%|                return f_globals[key]
   142|         0|            0|            0|  0.00%|            elif key in dir(builtins):
   143|         0|            0|            0|  0.00%|                return getattr(builtins, key)
   144|         0|            0|            0|  0.00%|
   145|         0|            0|            0|  0.00%|    return createResolutionCallbackFromEnv(env())
   146|         0|            0|            0|  0.00%|
   147|         0|            0|            0|  0.00%|
   148|         0|            0|            0|  0.00%|def get_closure(fn):
   149|         0|            0|            0|  0.00%|    """
   150|         0|            0|            0|  0.00%|    Get a dictionary of closed over variables from a function
   151|         0|            0|            0|  0.00%|    """
   152|         0|            0|            0|  0.00%|    captures = {}
   153|         0|            0|            0|  0.00%|    captures.update(fn.__globals__)
   154|         0|            0|            0|  0.00%|
   155|         0|            0|            0|  0.00%|    for index, captured_name in enumerate(fn.__code__.co_freevars):
   156|         0|            0|            0|  0.00%|        captures[captured_name] = fn.__closure__[index].cell_contents
   157|         0|            0|            0|  0.00%|
   158|         0|            0|            0|  0.00%|    return captures
   159|         0|            0|            0|  0.00%|
   160|         0|            0|            0|  0.00%|# [local resolution in python]
   161|         0|            0|            0|  0.00%|# Depending on where a variable is defined, and where it is used, we may
   162|         0|            0|            0|  0.00%|# or may not be able to recover its value when recursively compiling a
   163|         0|            0|            0|  0.00%|# script function. Remember in the general case, a module or function is
   164|         0|            0|            0|  0.00%|# first defined and then later scripted. This means we do not have a
   165|         0|            0|            0|  0.00%|# chance to capture the active frames when the function is defined. Hence any
   166|         0|            0|            0|  0.00%|# name resolution has to happen later on the created closure. The way
   167|         0|            0|            0|  0.00%|# python captures type annotations restricts what we can recover. The
   168|         0|            0|            0|  0.00%|# follow example illustrates the different cases:
   169|         0|            0|            0|  0.00%|#
   170|         0|            0|            0|  0.00%|#         class MyGlobalClass:
   171|         0|            0|            0|  0.00%|#         ...
   172|         0|            0|            0|  0.00%|#         def my_local_scope():
   173|         0|            0|            0|  0.00%|#             @torch.jit.script
   174|         0|            0|            0|  0.00%|#             class MyClass:
   175|         0|            0|            0|  0.00%|#                 ...
   176|         0|            0|            0|  0.00%|#             @torch.jit.script
   177|         0|            0|            0|  0.00%|#             class MyClassUsedAsVar:
   178|         0|            0|            0|  0.00%|#                 ...
   179|         0|            0|            0|  0.00%|#             def eg(x: MyClass, y: MyGlobalClass):
   180|         0|            0|            0|  0.00%|#                 a_local_capture : Foo
   181|         0|            0|            0|  0.00%|#                 return MyClassUsedAsVar(x)
   182|         0|            0|            0|  0.00%|#
   183|         0|            0|            0|  0.00%|# MyGlobalClass is defined in the __globals__ dictionary of function
   184|         0|            0|            0|  0.00%|# 'eg', so it is always recoverable. my_local_scope introduces a new local
   185|         0|            0|            0|  0.00%|# variable scope in the function. Classes defined here are only visible as
   186|         0|            0|            0|  0.00%|# local variables. For the case of MyClassUsedAsVar, it is captured
   187|         0|            0|            0|  0.00%|# because it is used as a variable inside the body of the function, and we
   188|         0|            0|            0|  0.00%|# can resolve it using the captures returned from `get_closure`. However,
   189|         0|            0|            0|  0.00%|# the type annotations are not captured by the closure. In Python
   190|         0|            0|            0|  0.00%|# 3.0--3.9, the _value_ of MyClass and MyGlobalClass will be available as
   191|         0|            0|            0|  0.00%|# annotations on `eg``, but starting in Python 4.0, they will represented as
   192|         0|            0|            0|  0.00%|# strings and no longer present. Furthermore, since the body of `eg` does
   193|         0|            0|            0|  0.00%|# not reference those names, they do not appear in the list of closed over
   194|         0|            0|            0|  0.00%|# variables. In Python 2.x, type annotations are in comments, leading to a
   195|         0|            0|            0|  0.00%|# similar situation where their definitions are not available. We anticipate
   196|         0|            0|            0|  0.00%|# that most users will not run into this issue because their modules and
   197|         0|            0|            0|  0.00%|# functions will be defined at a global scope like MyGlobalClass. In cases
   198|         0|            0|            0|  0.00%|# where they are not, it is possible to work around issues by declaring the
   199|         0|            0|            0|  0.00%|# values global in the function.
   200|         0|            0|            0|  0.00%|# In Python 3.9 declaring class as global will make it invisible to
   201|         0|            0|            0|  0.00%|# `inspect.getsource`, see https://bugs.python.org/issue42666 .
   202|         0|            0|            0|  0.00%|# This could be worked around by manualy adding it to `global()` dictionary.
   203|         0|            0|            0|  0.00%|
   204|         0|            0|            0|  0.00%|
   205|         0|            0|            0|  0.00%|
   206|         0|            0|            0|  0.00%|def createResolutionCallbackFromClosure(fn):
   207|         0|            0|            0|  0.00%|    """
   208|         0|            0|            0|  0.00%|    Create a resolutionCallback by introspecting the function instead of
   209|         0|            0|            0|  0.00%|    looking up the stack for the enclosing scope
   210|         0|            0|            0|  0.00%|    """
   211|         0|            0|            0|  0.00%|    closure = get_closure(fn)
   212|         0|            0|            0|  0.00%|
   213|         0|            0|            0|  0.00%|    class closure_lookup(object):
   214|         0|            0|            0|  0.00%|        # This is a class since `closure` is a dict and it's easier in
   215|         0|            0|            0|  0.00%|        # `env_helper` if everything just works with `getattr` calls
   216|         0|            0|            0|  0.00%|        def __getattr__(self, key):
   217|         0|            0|            0|  0.00%|            if key in closure:
   218|         0|            0|            0|  0.00%|                return closure[key]
   219|         0|            0|            0|  0.00%|            elif hasattr(builtins, key):
   220|         0|            0|            0|  0.00%|                return getattr(builtins, key)
   221|         0|            0|            0|  0.00%|            return None
   222|         0|            0|            0|  0.00%|
   223|         0|            0|            0|  0.00%|    return createResolutionCallbackFromEnv(closure_lookup())
   224|         0|            0|            0|  0.00%|
   225|         0|            0|            0|  0.00%|
   226|         0|            0|            0|  0.00%|def can_compile_class(cls) -> bool:
   227|         0|            0|            0|  0.00%|    # If any of the functions on a type don't have a code object, this type can't
   228|         0|            0|            0|  0.00%|    # be compiled and is probably a builtin / bound from C
   229|         0|            0|            0|  0.00%|    if is_ignored_fn(cls):
   230|         0|            0|            0|  0.00%|        return False
   231|         0|            0|            0|  0.00%|
   232|         0|            0|            0|  0.00%|    # Ignore the following list of built-in classes.
   233|         0|            0|            0|  0.00%|    ignored_builtin_classes = (torch.nn.Module, tuple, list, Exception)
   234|         0|            0|            0|  0.00%|    if issubclass(cls, ignored_builtin_classes):
   235|         0|            0|            0|  0.00%|        return False
   236|         0|            0|            0|  0.00%|
   237|         0|            0|            0|  0.00%|    names = cls.__dict__
   238|         0|            0|            0|  0.00%|    fns = [getattr(cls, name) for name in names if inspect.isroutine(getattr(cls, name, None))]
   239|         0|            0|            0|  0.00%|    has_code = [hasattr(fn, '__code__') for fn in fns]
   240|         0|            0|            0|  0.00%|    return all(has_code)
   241|         0|            0|            0|  0.00%|
   242|         0|            0|            0|  0.00%|
   243|         0|            0|            0|  0.00%|def get_callable_argument_names(fn) -> List[str]:
   244|         0|            0|            0|  0.00%|    """
   245|         0|            0|            0|  0.00%|    Gets names of all POSITIONAL_OR_KEYWORD arguments for callable `fn`.
   246|         0|            0|            0|  0.00%|    Returns an empty list when other types of arguments are present.
   247|         0|            0|            0|  0.00%|
   248|         0|            0|            0|  0.00%|    This is used by `torch.jit.trace` to assign meaningful argument names to
   249|         0|            0|            0|  0.00%|    traced functions and modules.
   250|         0|            0|            0|  0.00%|
   251|         0|            0|            0|  0.00%|    Args:
   252|         0|            0|            0|  0.00%|        fn: A callable.
   253|         0|            0|            0|  0.00%|    Returns:
   254|         0|            0|            0|  0.00%|        Argument names: List[str]
   255|         0|            0|            0|  0.00%|    """
   256|         0|            0|            0|  0.00%|    # inspect.signature may fail, give up in that case.
   257|         0|            0|            0|  0.00%|    try:
   258|         0|            0|            0|  0.00%|        callable_signature = inspect.signature(fn)
   259|         0|            0|            0|  0.00%|    except Exception:
   260|         0|            0|            0|  0.00%|        return []
   261|         0|            0|            0|  0.00%|
   262|         0|            0|            0|  0.00%|    argument_names = []
   263|         0|            0|            0|  0.00%|    for name, param in callable_signature.parameters.items():
   264|         0|            0|            0|  0.00%|        # All four other types of arguments do not map to individual values
   265|         0|            0|            0|  0.00%|        # with a keyword as name.
   266|         0|            0|            0|  0.00%|        if not param.kind == param.POSITIONAL_OR_KEYWORD:
   267|         0|            0|            0|  0.00%|            return []
   268|         0|            0|            0|  0.00%|
   269|         0|            0|            0|  0.00%|        argument_names.append(name)
   270|         0|            0|            0|  0.00%|
   271|         0|            0|            0|  0.00%|    return argument_names
   272|         0|            0|            0|  0.00%|
   273|         0|            0|            0|  0.00%|
   274|         0|            0|            0|  0.00%|def get_annotation_str(annotation):
   275|         0|            0|            0|  0.00%|    """
   276|         0|            0|            0|  0.00%|    Convert an AST node containing a type annotation to the string present in the source
   277|         0|            0|            0|  0.00%|    that represents the same annotation.
   278|         0|            0|            0|  0.00%|    """
   279|         0|            0|            0|  0.00%|    if isinstance(annotation, ast.Name):
   280|         0|            0|            0|  0.00%|        return annotation.id
   281|         0|            0|            0|  0.00%|    elif isinstance(annotation, ast.Attribute):
   282|         0|            0|            0|  0.00%|        return '.'.join([get_annotation_str(annotation.value), annotation.attr])
   283|         0|            0|            0|  0.00%|    elif isinstance(annotation, ast.Subscript):
   284|         0|            0|            0|  0.00%|        # In Python3.9+ subscript indicies are not wrapped in ast.Index
   285|         0|            0|            0|  0.00%|        subscript_slice = annotation.slice if sys.version_info >= (3, 9) else annotation.slice.value  # type: ignore[attr-defined]
   286|         0|            0|            0|  0.00%|        return f"{get_annotation_str(annotation.value)}[{get_annotation_str(subscript_slice)}]"
   287|         0|            0|            0|  0.00%|    elif isinstance(annotation, ast.Tuple):
   288|         0|            0|            0|  0.00%|        return ','.join([get_annotation_str(elt) for elt in annotation.elts])
   289|         0|            0|            0|  0.00%|    elif isinstance(annotation, ast.Constant) or isinstance(annotation, ast.NameConstant):
   290|         0|            0|            0|  0.00%|        return f"{annotation.value}"
   291|         0|            0|            0|  0.00%|
   292|         0|            0|            0|  0.00%|    # If an AST node is not handled here, it's probably handled in ScriptTypeParser.
   293|         0|            0|            0|  0.00%|    return None
   294|         0|            0|            0|  0.00%|
   295|         0|            0|            0|  0.00%|
   296|         0|            0|            0|  0.00%|def get_type_hint_captures(fn):
   297|         0|            0|            0|  0.00%|    """
   298|         0|            0|            0|  0.00%|    Get a dictionary containing type resolution mappings necessary to resolve types
   299|         0|            0|            0|  0.00%|    for the literal annotations on 'fn'. These are not considered to be closed-over by fn
   300|         0|            0|            0|  0.00%|    and must be obtained separately (e.g. using this function).
   301|         0|            0|            0|  0.00%|
   302|         0|            0|            0|  0.00%|    Args:
   303|         0|            0|            0|  0.00%|        fn: A callable.
   304|         0|            0|            0|  0.00%|    Returns:
   305|         0|            0|            0|  0.00%|        A Dict[str, Any] containing a mapping from the literal annotations used on
   306|         0|            0|            0|  0.00%|        fn to the Python objects they refer to.
   307|         0|            0|            0|  0.00%|    """
   308|         0|            0|            0|  0.00%|    # Gather a dictionary of parameter name -> type, skipping any parameters whose annotated
   309|         0|            0|            0|  0.00%|    # types are strings. These are only understood by TorchScript in the context of a type annotation
   310|         0|            0|            0|  0.00%|    # that refers to a class in its own definition, but trying to include a mapping for this in the result
   311|         0|            0|            0|  0.00%|    # function would cause infinite recursion because the class is currently being compiled.
   312|         0|            0|            0|  0.00%|    # In addition, there is logic in ScriptTypeParser to handle this.
   313|         0|            0|            0|  0.00%|    signature = inspect.signature(fn)
   314|         0|            0|            0|  0.00%|    name_to_type = {
   315|         0|            0|            0|  0.00%|        name: parameter.annotation
   316|         0|            0|            0|  0.00%|        for name, parameter in signature.parameters.items()
   317|         0|            0|            0|  0.00%|        if parameter.annotation is not inspect.Parameter.empty and not isinstance(parameter.annotation, str)
   318|         0|            0|            0|  0.00%|    }
   319|         0|            0|            0|  0.00%|
   320|         0|            0|            0|  0.00%|    # Then, get the literal type annotations from the function declaration
   321|         0|            0|            0|  0.00%|    # by source inspection. This accounts for the case in which aliases are used
   322|         0|            0|            0|  0.00%|    # to annotate the arguments (e.g device_t = torch.device, and then d: device_t).
   323|         0|            0|            0|  0.00%|    src = inspect.getsource(fn)
   324|         0|            0|            0|  0.00%|
   325|         0|            0|            0|  0.00%|    # frontend.py cannot be used here because it includes _jit_internal, so use ast instead.
   326|         0|            0|            0|  0.00%|    a = ast.parse(dedent(src))
   327|         0|            0|            0|  0.00%|    if len(a.body) != 1 or not isinstance(a.body[0], ast.FunctionDef):
   328|         0|            0|            0|  0.00%|        raise RuntimeError(f"Expected {fn} to be a function")
   329|         0|            0|            0|  0.00%|    f = a.body[0]
   330|         0|            0|            0|  0.00%|
   331|         0|            0|            0|  0.00%|    # Prepare a dictionary of source annotation -> type, which will be the final result of this function,
   332|         0|            0|            0|  0.00%|    # by using the parsed AST (f) to reconstruct source annotations as strings for each parameter and mapping
   333|         0|            0|            0|  0.00%|    # them to the type object corresponding to the annotation via name_to_type using the parameter name.
   334|         0|            0|            0|  0.00%|    annotation_to_type = {}
   335|         0|            0|            0|  0.00%|
   336|         0|            0|            0|  0.00%|    for arg in f.args.args:
   337|         0|            0|            0|  0.00%|        # Get the source type annotation string for this argument if possible.
   338|         0|            0|            0|  0.00%|        arg_annotation_str = get_annotation_str(arg.annotation) if arg.annotation else None
   339|         0|            0|            0|  0.00%|
   340|         0|            0|            0|  0.00%|        # If the argument has no annotation or get_annotation_str cannot convert it to a string,
   341|         0|            0|            0|  0.00%|        # arg_annotation_str will be None. Skip this arg; ScriptTypeParser will probably handle
   342|         0|            0|            0|  0.00%|        # this in the latter case.
   343|         0|            0|            0|  0.00%|        if arg_annotation_str is None:
   344|         0|            0|            0|  0.00%|            continue
   345|         0|            0|            0|  0.00%|
   346|         0|            0|            0|  0.00%|        # Insert {arg_annotation_str: type} into annotation_to_type if possible. One reason arg_name may not
   347|         0|            0|            0|  0.00%|        # be present in name_to_type is that the annotation itself is a string and not a type object
   348|         0|            0|            0|  0.00%|        # (common for self-refential annotations in classes). Once again, let ScriptTypeParser handle this.
   349|         0|            0|            0|  0.00%|        arg_name = arg.arg
   350|         0|            0|            0|  0.00%|        if arg_name in name_to_type:
   351|         0|            0|            0|  0.00%|            annotation_to_type[arg_annotation_str] = name_to_type[arg_name]
   352|         0|            0|            0|  0.00%|
   353|         0|            0|            0|  0.00%|    # If there is a valid return annotation, include it in annotation_to_type. As with argument annotations,
   354|         0|            0|            0|  0.00%|    # the literal annotation has to be convertible to a string by get_annotation_str, and the actual type
   355|         0|            0|            0|  0.00%|    # of the annotation cannot be a string.
   356|         0|            0|            0|  0.00%|    literal_return_annotation = get_annotation_str(f.returns)
   357|         0|            0|            0|  0.00%|    valid_literal_annotation = literal_return_annotation is not None
   358|         0|            0|            0|  0.00%|    return_annotation = signature.return_annotation
   359|         0|            0|            0|  0.00%|    valid_return_annotation_type = return_annotation is not inspect.Parameter.empty and not isinstance(return_annotation, str)
   360|         0|            0|            0|  0.00%|    if valid_literal_annotation and valid_return_annotation_type:
   361|         0|            0|            0|  0.00%|        annotation_to_type[literal_return_annotation] = return_annotation
   362|         0|            0|            0|  0.00%|
   363|         0|            0|            0|  0.00%|    return annotation_to_type
   364|         0|            0|            0|  0.00%|
   365|         0|            0|            0|  0.00%|
   366|         0|            0|            0|  0.00%|def createResolutionCallbackForClassMethods(cls):
   367|         0|            0|            0|  0.00%|    """
   368|         0|            0|            0|  0.00%|    This looks at all the methods defined in a class and pulls their closed-over
   369|         0|            0|            0|  0.00%|    variables into a dictionary and uses that to resolve variables.
   370|         0|            0|            0|  0.00%|    """
   371|         0|            0|            0|  0.00%|    # cls is a type here, so `ismethod` is false since the methods on the type
   372|         0|            0|            0|  0.00%|    # aren't bound to anything, so Python treats them as regular functions
   373|         0|            0|            0|  0.00%|    fns = [getattr(cls, name) for name in cls.__dict__ if inspect.isroutine(getattr(cls, name))]
   374|         0|            0|            0|  0.00%|    captures = {}
   375|         0|            0|            0|  0.00%|
   376|         0|            0|            0|  0.00%|    for fn in fns:
   377|         0|            0|            0|  0.00%|        captures.update(get_closure(fn))
   378|         0|            0|            0|  0.00%|        captures.update(get_type_hint_captures(fn))
   379|         0|            0|            0|  0.00%|
   380|         0|            0|            0|  0.00%|    def lookup_in_class(key):
   381|         0|            0|            0|  0.00%|        if key in captures:
   382|         0|            0|            0|  0.00%|            return captures[key]
   383|         0|            0|            0|  0.00%|        else:
   384|         0|            0|            0|  0.00%|            return getattr(builtins, key, None)
   385|         0|            0|            0|  0.00%|
   386|         0|            0|            0|  0.00%|    return lookup_in_class
   387|         0|            0|            0|  0.00%|
   388|         0|            0|            0|  0.00%|
   389|         0|            0|            0|  0.00%|def boolean_dispatch(arg_name, arg_index, default, if_true, if_false, module_name, func_name):
   390|         0|            0|            0|  0.00%|    """
   391|         0|            0|            0|  0.00%|    Dispatches to either of 2 script functions based on a boolean argument.
   392|         0|            0|            0|  0.00%|    In TorchScript, the boolean argument must be constant so that the correct
   393|         0|            0|            0|  0.00%|    function to use can be determined at compile time.
   394|         0|            0|            0|  0.00%|    """
   395|       100|  0.000674486|  6.74486e-06|  0.00%|    def fn(*args, **kwargs):
   396|       100|  0.000596523|  5.96523e-06|  0.00%|        dispatch_flag = False
   397|       100|  0.000489712|  4.89712e-06|  0.00%|        if arg_name in kwargs:
   398|         0|            0|            0|  0.00%|            dispatch_flag = kwargs[arg_name]
   399|       100|  0.000787973|  7.87973e-06|  0.00%|        elif arg_index < len(args):
   400|       100|  0.000466585|  4.66585e-06|  0.00%|            dispatch_flag = args[arg_index]
   401|         0|            0|            0|  0.00%|
   402|       100|  0.000440121|  4.40121e-06|  0.00%|        if dispatch_flag:
   403|         0|            0|            0|  0.00%|            return if_true(*args, **kwargs)
   404|         0|            0|            0|  0.00%|        else:
   405|       100|   0.00314522|  3.14522e-05|  0.01%|            return if_false(*args, **kwargs)
(call)|       100|     0.699395|   0.00699395|  1.31%|# /opt/conda/lib/python3.8/site-packages/torch/nn/functional.py:696 _max_pool2d
   406|         0|            0|            0|  0.00%|
   407|         0|            0|            0|  0.00%|    if if_true.__doc__ is None and if_false.__doc__ is not None:
   408|         0|            0|            0|  0.00%|        doc = if_false.__doc__
   409|         0|            0|            0|  0.00%|        if_true.__doc__ = doc
   410|         0|            0|            0|  0.00%|    elif if_false.__doc__ is None and if_true.__doc__ is not None:
   411|         0|            0|            0|  0.00%|        doc = if_true.__doc__
   412|         0|            0|            0|  0.00%|        if_false.__doc__ = doc
   413|         0|            0|            0|  0.00%|    elif if_false.__doc__ is None and if_true.__doc__ is None:
   414|         0|            0|            0|  0.00%|        # neither function has a docstring
   415|         0|            0|            0|  0.00%|        doc = None
   416|         0|            0|            0|  0.00%|    else:
   417|         0|            0|            0|  0.00%|        raise RuntimeError("only one function can have a docstring")
   418|         0|            0|            0|  0.00%|    fn.__doc__ = doc
   419|         0|            0|            0|  0.00%|
   420|         0|            0|            0|  0.00%|    if module_name is not None:
   421|         0|            0|            0|  0.00%|        fn.__module__ = module_name
   422|         0|            0|            0|  0.00%|    if func_name is not None:
   423|         0|            0|            0|  0.00%|        fn.__name__ = func_name
   424|         0|            0|            0|  0.00%|
   425|         0|            0|            0|  0.00%|    boolean_dispatched[fn] = {
   426|         0|            0|            0|  0.00%|        "if_true": if_true,
   427|         0|            0|            0|  0.00%|        "if_false": if_false,
   428|         0|            0|            0|  0.00%|        "index": arg_index,
   429|         0|            0|            0|  0.00%|        "default": default,
   430|         0|            0|            0|  0.00%|        "arg_name": arg_name
   431|         0|            0|            0|  0.00%|    }
   432|         0|            0|            0|  0.00%|    return fn
   433|         0|            0|            0|  0.00%|
   434|         0|            0|            0|  0.00%|
   435|         0|            0|            0|  0.00%|class FunctionModifiers(object):
   436|         0|            0|            0|  0.00%|    """
   437|         0|            0|            0|  0.00%|    Used to denote the behavior of a function in TorchScript. See export() and
   438|         0|            0|            0|  0.00%|    ignore() for details.
   439|         0|            0|            0|  0.00%|    """
   440|         0|            0|            0|  0.00%|    UNUSED = "unused (ignored and replaced with raising of an exception)"
   441|         0|            0|            0|  0.00%|    IGNORE = "ignore (leave as a call to Python, cannot be torch.jit.save'd)"
   442|         0|            0|            0|  0.00%|    EXPORT = "export (compile this function even if nothing calls it)"
   443|         0|            0|            0|  0.00%|    DEFAULT = "default (compile if called from a exported function / forward)"
   444|         0|            0|            0|  0.00%|    COPY_TO_SCRIPT_WRAPPER = \
   445|         0|            0|            0|  0.00%|        "if this method is not scripted, copy the python method onto the scripted model"
   446|         0|            0|            0|  0.00%|
   447|         0|            0|            0|  0.00%|
   448|         0|            0|            0|  0.00%|def export(fn):
   449|         0|            0|            0|  0.00%|    """
   450|         0|            0|            0|  0.00%|    This decorator indicates that a method on an ``nn.Module`` is used as an entry point into a
   451|         0|            0|            0|  0.00%|    :class:`ScriptModule` and should be compiled.
   452|         0|            0|            0|  0.00%|
   453|         0|            0|            0|  0.00%|    ``forward`` implicitly is assumed to be an entry point, so it does not need this decorator.
   454|         0|            0|            0|  0.00%|    Functions and methods called from ``forward`` are compiled as they are seen
   455|         0|            0|            0|  0.00%|    by the compiler, so they do not need this decorator either.
   456|         0|            0|            0|  0.00%|
   457|         0|            0|            0|  0.00%|    Example (using ``@torch.jit.export`` on a method):
   458|         0|            0|            0|  0.00%|
   459|         0|            0|            0|  0.00%|    .. testcode::
   460|         0|            0|            0|  0.00%|
   461|         0|            0|            0|  0.00%|        import torch
   462|         0|            0|            0|  0.00%|        import torch.nn as nn
   463|         0|            0|            0|  0.00%|
   464|         0|            0|            0|  0.00%|        class MyModule(nn.Module):
   465|         0|            0|            0|  0.00%|            def implicitly_compiled_method(self, x):
   466|         0|            0|            0|  0.00%|                return x + 99
   467|         0|            0|            0|  0.00%|
   468|         0|            0|            0|  0.00%|            # `forward` is implicitly decorated with `@torch.jit.export`,
   469|         0|            0|            0|  0.00%|            # so adding it here would have no effect
   470|         0|            0|            0|  0.00%|            def forward(self, x):
   471|         0|            0|            0|  0.00%|                return x + 10
   472|         0|            0|            0|  0.00%|
   473|         0|            0|            0|  0.00%|            @torch.jit.export
   474|         0|            0|            0|  0.00%|            def another_forward(self, x):
   475|         0|            0|            0|  0.00%|                # When the compiler sees this call, it will compile
   476|         0|            0|            0|  0.00%|                # `implicitly_compiled_method`
   477|         0|            0|            0|  0.00%|                return self.implicitly_compiled_method(x)
   478|         0|            0|            0|  0.00%|
   479|         0|            0|            0|  0.00%|            def unused_method(self, x):
   480|         0|            0|            0|  0.00%|                return x - 20
   481|         0|            0|            0|  0.00%|
   482|         0|            0|            0|  0.00%|        # `m` will contain compiled methods:
   483|         0|            0|            0|  0.00%|        #     `forward`
   484|         0|            0|            0|  0.00%|        #     `another_forward`
   485|         0|            0|            0|  0.00%|        #     `implicitly_compiled_method`
   486|         0|            0|            0|  0.00%|        # `unused_method` will not be compiled since it was not called from
   487|         0|            0|            0|  0.00%|        # any compiled methods and wasn't decorated with `@torch.jit.export`
   488|         0|            0|            0|  0.00%|        m = torch.jit.script(MyModule())
   489|         0|            0|            0|  0.00%|    """
   490|         0|            0|            0|  0.00%|    fn._torchscript_modifier = FunctionModifiers.EXPORT
   491|         0|            0|            0|  0.00%|    return fn
   492|         0|            0|            0|  0.00%|
   493|         0|            0|            0|  0.00%|
   494|         0|            0|            0|  0.00%|def unused(fn):
   495|         0|            0|            0|  0.00%|    """
   496|         0|            0|            0|  0.00%|    This decorator indicates to the compiler that a function or method should
   497|         0|            0|            0|  0.00%|    be ignored and replaced with the raising of an exception. This allows you
   498|         0|            0|            0|  0.00%|    to leave code in your model that is not yet TorchScript compatible and still
   499|         0|            0|            0|  0.00%|    export your model.
   500|         0|            0|            0|  0.00%|
   501|         0|            0|            0|  0.00%|        Example (using ``@torch.jit.unused`` on a method)::
   502|         0|            0|            0|  0.00%|
   503|         0|            0|            0|  0.00%|            import torch
   504|         0|            0|            0|  0.00%|            import torch.nn as nn
   505|         0|            0|            0|  0.00%|
   506|         0|            0|            0|  0.00%|            class MyModule(nn.Module):
   507|         0|            0|            0|  0.00%|                def __init__(self, use_memory_efficient):
   508|         0|            0|            0|  0.00%|                    super(MyModule, self).__init__()
   509|         0|            0|            0|  0.00%|                    self.use_memory_efficient = use_memory_efficient
   510|         0|            0|            0|  0.00%|
   511|         0|            0|            0|  0.00%|                @torch.jit.unused
   512|         0|            0|            0|  0.00%|                def memory_efficient(self, x):
   513|         0|            0|            0|  0.00%|                    import pdb
   514|         0|            0|            0|  0.00%|                    pdb.set_trace()
   515|         0|            0|            0|  0.00%|                    return x + 10
   516|         0|            0|            0|  0.00%|
   517|         0|            0|            0|  0.00%|                def forward(self, x):
   518|         0|            0|            0|  0.00%|                    # Use not-yet-scriptable memory efficient mode
   519|         0|            0|            0|  0.00%|                    if self.use_memory_efficient:
   520|         0|            0|            0|  0.00%|                        return self.memory_efficient(x)
   521|         0|            0|            0|  0.00%|                    else:
   522|         0|            0|            0|  0.00%|                        return x + 10
   523|         0|            0|            0|  0.00%|
   524|         0|            0|            0|  0.00%|            m = torch.jit.script(MyModule(use_memory_efficient=False))
   525|         0|            0|            0|  0.00%|            m.save("m.pt")
   526|         0|            0|            0|  0.00%|
   527|         0|            0|            0|  0.00%|            m = torch.jit.script(MyModule(use_memory_efficient=True))
   528|         0|            0|            0|  0.00%|            # exception raised
   529|         0|            0|            0|  0.00%|            m(torch.rand(100))
   530|         0|            0|            0|  0.00%|    """
   531|         0|            0|            0|  0.00%|    if isinstance(fn, property):
   532|         0|            0|            0|  0.00%|        prop = fn
   533|         0|            0|            0|  0.00%|        setattr(prop.fget, "_torchscript_modifier", FunctionModifiers.UNUSED)  # noqa: B010
   534|         0|            0|            0|  0.00%|
   535|         0|            0|            0|  0.00%|        if prop.fset:
   536|         0|            0|            0|  0.00%|            setattr(prop.fset, "_torchscript_modifier", FunctionModifiers.UNUSED)  # noqa: B010
   537|         0|            0|            0|  0.00%|
   538|         0|            0|            0|  0.00%|        return prop
   539|         0|            0|            0|  0.00%|
   540|         0|            0|            0|  0.00%|    fn._torchscript_modifier = FunctionModifiers.UNUSED
   541|         0|            0|            0|  0.00%|    return fn
   542|         0|            0|            0|  0.00%|
   543|         0|            0|            0|  0.00%|# No op context manager from python side
   544|         0|            0|            0|  0.00%|class _IgnoreContextManager(contextlib.AbstractContextManager):
   545|         0|            0|            0|  0.00%|    def __init__(self, **kwargs):
   546|         0|            0|            0|  0.00%|        pass
   547|         0|            0|            0|  0.00%|
   548|         0|            0|            0|  0.00%|    def __exit__(self, exc_type: Any, exc_value: Any, traceback: Any) -> None:
   549|         0|            0|            0|  0.00%|        pass
   550|         0|            0|            0|  0.00%|
   551|         0|            0|            0|  0.00%|def ignore(drop=False, **kwargs):
   552|         0|            0|            0|  0.00%|    """
   553|         0|            0|            0|  0.00%|    This decorator indicates to the compiler that a function or method should
   554|         0|            0|            0|  0.00%|    be ignored and left as a Python function. This allows you to leave code in
   555|         0|            0|            0|  0.00%|    your model that is not yet TorchScript compatible. If called from TorchScript,
   556|         0|            0|            0|  0.00%|    ignored functions will dispatch the call to the Python interpreter. Models with ignored
   557|         0|            0|            0|  0.00%|    functions cannot be exported; use :func:`@torch.jit.unused <torch.jit.unused>` instead.
   558|         0|            0|            0|  0.00%|
   559|         0|            0|            0|  0.00%|    Example (using ``@torch.jit.ignore`` on a method)::
   560|         0|            0|            0|  0.00%|
   561|         0|            0|            0|  0.00%|        import torch
   562|         0|            0|            0|  0.00%|        import torch.nn as nn
   563|         0|            0|            0|  0.00%|
   564|         0|            0|            0|  0.00%|        class MyModule(nn.Module):
   565|         0|            0|            0|  0.00%|            @torch.jit.ignore
   566|         0|            0|            0|  0.00%|            def debugger(self, x):
   567|         0|            0|            0|  0.00%|                import pdb
   568|         0|            0|            0|  0.00%|                pdb.set_trace()
   569|         0|            0|            0|  0.00%|
   570|         0|            0|            0|  0.00%|            def forward(self, x):
   571|         0|            0|            0|  0.00%|                x += 10
   572|         0|            0|            0|  0.00%|                # The compiler would normally try to compile `debugger`,
   573|         0|            0|            0|  0.00%|                # but since it is `@ignore`d, it will be left as a call
   574|         0|            0|            0|  0.00%|                # to Python
   575|         0|            0|            0|  0.00%|                self.debugger(x)
   576|         0|            0|            0|  0.00%|                return x
   577|         0|            0|            0|  0.00%|
   578|         0|            0|            0|  0.00%|        m = torch.jit.script(MyModule())
   579|         0|            0|            0|  0.00%|
   580|         0|            0|            0|  0.00%|        # Error! The call `debugger` cannot be saved since it calls into Python
   581|         0|            0|            0|  0.00%|        m.save("m.pt")
   582|         0|            0|            0|  0.00%|
   583|         0|            0|            0|  0.00%|    Example (using ``@torch.jit.ignore(drop=True)`` on a method):
   584|         0|            0|            0|  0.00%|
   585|         0|            0|            0|  0.00%|    .. testcode::
   586|         0|            0|            0|  0.00%|
   587|         0|            0|            0|  0.00%|        import torch
   588|         0|            0|            0|  0.00%|        import torch.nn as nn
   589|         0|            0|            0|  0.00%|
   590|         0|            0|            0|  0.00%|        class MyModule(nn.Module):
   591|         0|            0|            0|  0.00%|            @torch.jit.ignore(drop=True)
   592|         0|            0|            0|  0.00%|            def training_method(self, x):
   593|         0|            0|            0|  0.00%|                import pdb
   594|         0|            0|            0|  0.00%|                pdb.set_trace()
   595|         0|            0|            0|  0.00%|
   596|         0|            0|            0|  0.00%|            def forward(self, x):
   597|         0|            0|            0|  0.00%|                if self.training:
   598|         0|            0|            0|  0.00%|                    self.training_method(x)
   599|         0|            0|            0|  0.00%|                return x
   600|         0|            0|            0|  0.00%|
   601|         0|            0|            0|  0.00%|        m = torch.jit.script(MyModule())
   602|         0|            0|            0|  0.00%|
   603|         0|            0|            0|  0.00%|        # This is OK since `training_method` is not saved, the call is replaced
   604|         0|            0|            0|  0.00%|        # with a `raise`.
   605|         0|            0|            0|  0.00%|        m.save("m.pt")
   606|         0|            0|            0|  0.00%|
   607|         0|            0|            0|  0.00%|    .. testcleanup::
   608|         0|            0|            0|  0.00%|
   609|         0|            0|            0|  0.00%|        import os
   610|         0|            0|            0|  0.00%|        os.remove('m.pt')
   611|         0|            0|            0|  0.00%|    """
   612|         0|            0|            0|  0.00%|
   613|         0|            0|            0|  0.00%|    if callable(drop):
   614|         0|            0|            0|  0.00%|        # used without any args, so drop is actually a function
   615|         0|            0|            0|  0.00%|        #   @torch.jit.ignore
   616|         0|            0|            0|  0.00%|        #   def fn(...):
   617|         0|            0|            0|  0.00%|        fn = drop
   618|         0|            0|            0|  0.00%|        fn._torchscript_modifier = FunctionModifiers.IGNORE
   619|         0|            0|            0|  0.00%|        return fn
   620|         0|            0|            0|  0.00%|
   621|         0|            0|            0|  0.00%|    if not isinstance(drop, bool):
   622|         0|            0|            0|  0.00%|        raise RuntimeError("Argument to @torch.jit.ignore must be a bool or "
   623|         0|            0|            0|  0.00%|                           f"a function but got {drop}")
   624|         0|            0|            0|  0.00%|
   625|         0|            0|            0|  0.00%|    # for backwards compat
   626|         0|            0|            0|  0.00%|    drop_on_export = kwargs.pop("drop_on_export", None)
   627|         0|            0|            0|  0.00%|    if drop_on_export:
   628|         0|            0|            0|  0.00%|        warnings.warn("ignore(drop_on_export=True) has been deprecated. TorchScript will now drop the function "
   629|         0|            0|            0|  0.00%|                      "call on compilation. Use torch.jit.unused now. {}", category=FutureWarning)
   630|         0|            0|            0|  0.00%|
   631|         0|            0|            0|  0.00%|        drop = drop_on_export
   632|         0|            0|            0|  0.00%|    elif drop:
   633|         0|            0|            0|  0.00%|        warnings.warn("ignore(True) has been deprecated. TorchScript will now drop the function "
   634|         0|            0|            0|  0.00%|                      "call on compilation. Use torch.jit.unused now. {}", category=FutureWarning)
   635|         0|            0|            0|  0.00%|
   636|         0|            0|            0|  0.00%|    def decorator(fn):
   637|         0|            0|            0|  0.00%|        if drop:
   638|         0|            0|            0|  0.00%|            fn._torchscript_modifier = FunctionModifiers.UNUSED
   639|         0|            0|            0|  0.00%|        else:
   640|         0|            0|            0|  0.00%|            fn._torchscript_modifier = FunctionModifiers.IGNORE
   641|         0|            0|            0|  0.00%|        return fn
   642|         0|            0|            0|  0.00%|    return decorator
   643|         0|            0|            0|  0.00%|
   644|         0|            0|            0|  0.00%|
   645|         0|            0|            0|  0.00%|def _copy_to_script_wrapper(fn):
   646|         0|            0|            0|  0.00%|    fn._torchscript_modifier = FunctionModifiers.COPY_TO_SCRIPT_WRAPPER
   647|         0|            0|            0|  0.00%|    return fn
   648|         0|            0|            0|  0.00%|
   649|         0|            0|            0|  0.00%|def module_has_exports(mod):
   650|         0|            0|            0|  0.00%|    for name in dir(mod):
   651|         0|            0|            0|  0.00%|        if hasattr(mod, name):
   652|         0|            0|            0|  0.00%|            item = getattr(mod, name)
   653|         0|            0|            0|  0.00%|            if callable(item):
   654|         0|            0|            0|  0.00%|                if get_torchscript_modifier(item) is FunctionModifiers.EXPORT:
   655|         0|            0|            0|  0.00%|                    return True
   656|         0|            0|            0|  0.00%|    return False
   657|         0|            0|            0|  0.00%|
   658|         0|            0|            0|  0.00%|def should_drop(fn) -> bool:
   659|         0|            0|            0|  0.00%|    attr = get_torchscript_modifier(fn)
   660|         0|            0|            0|  0.00%|    if attr is None:
   661|         0|            0|            0|  0.00%|        return False
   662|         0|            0|            0|  0.00%|    return attr is FunctionModifiers.UNUSED
   663|         0|            0|            0|  0.00%|
   664|         0|            0|            0|  0.00%|
   665|         0|            0|            0|  0.00%|def is_ignored_fn(fn) -> bool:
   666|         0|            0|            0|  0.00%|    mod = get_torchscript_modifier(fn)
   667|         0|            0|            0|  0.00%|    return mod is FunctionModifiers.UNUSED or mod is FunctionModifiers.IGNORE
   668|         0|            0|            0|  0.00%|
   669|         0|            0|            0|  0.00%|
   670|         0|            0|            0|  0.00%|def is_static_fn(cls, fn) -> bool:
   671|         0|            0|            0|  0.00%|    return isinstance(inspect.getattr_static(cls, fn, default=None), staticmethod)
   672|         0|            0|            0|  0.00%|
   673|         0|            0|            0|  0.00%|def get_static_fn(cls, fn):
   674|         0|            0|            0|  0.00%|    return inspect.getattr_static(cls, fn).__func__
   675|         0|            0|            0|  0.00%|
   676|         0|            0|            0|  0.00%|
   677|         0|            0|            0|  0.00%|def get_torchscript_modifier(fn):
   678|         0|            0|            0|  0.00%|    if not callable(fn):
   679|         0|            0|            0|  0.00%|        return None
   680|         0|            0|            0|  0.00%|    if hasattr(fn, '__func__'):
   681|         0|            0|            0|  0.00%|        fn = fn.__func__
   682|         0|            0|            0|  0.00%|    return getattr(fn, '_torchscript_modifier', FunctionModifiers.DEFAULT)
   683|         0|            0|            0|  0.00%|
   684|         0|            0|            0|  0.00%|def copy_torchscript_modifier(orig, new) -> None:
   685|         0|            0|            0|  0.00%|    attr = get_torchscript_modifier(orig)
   686|         0|            0|            0|  0.00%|    if attr is None:
   687|         0|            0|            0|  0.00%|        return
   688|         0|            0|            0|  0.00%|    new._torchscript_modifier = attr
   689|         0|            0|            0|  0.00%|
   690|         0|            0|            0|  0.00%|# overloading registration
   691|         0|            0|            0|  0.00%|# overloads get registered in this file, and compiled in torch/jit/__init__.py
   692|         0|            0|            0|  0.00%|# so that they can be imported in nn/functional.py without an import cycle
   693|         0|            0|            0|  0.00%|
   694|         0|            0|            0|  0.00%|# qualified_name => list[overload_functions]
   695|         0|            0|            0|  0.00%|_overloaded_fns : Dict[str, List[Callable]] = {}  # noqa: T484
   696|         0|            0|            0|  0.00%|
   697|         0|            0|            0|  0.00%|def _overload(func):
   698|         0|            0|            0|  0.00%|    qual_name = _qualified_name(func)
   699|         0|            0|            0|  0.00%|    global _overloaded_fns
   700|         0|            0|            0|  0.00%|    fn_overload_list = _overloaded_fns.get(qual_name)
   701|         0|            0|            0|  0.00%|    if fn_overload_list is None:
   702|         0|            0|            0|  0.00%|        fn_overload_list = []
   703|         0|            0|            0|  0.00%|        _overloaded_fns[qual_name] = fn_overload_list
   704|         0|            0|            0|  0.00%|    fn_overload_list.append(func)
   705|         0|            0|            0|  0.00%|    return func
   706|         0|            0|            0|  0.00%|
   707|         0|            0|            0|  0.00%|def _get_fn_overloads(qual_name):
   708|         0|            0|            0|  0.00%|    return _overloaded_fns.get(qual_name)
   709|         0|            0|            0|  0.00%|
   710|         0|            0|            0|  0.00%|def _clear_fn_overloads(qual_name) -> None:
   711|         0|            0|            0|  0.00%|    del _overloaded_fns[qual_name]
   712|         0|            0|            0|  0.00%|
   713|         0|            0|            0|  0.00%|def get_class_name_lineno(method) -> Tuple[str, int]:
   714|         0|            0|            0|  0.00%|    current_frame = inspect.currentframe()
   715|         0|            0|            0|  0.00%|
   716|         0|            0|            0|  0.00%|    # one for the get_class_name call, one for _overload_method call
   717|         0|            0|            0|  0.00%|    for i in range(2):
   718|         0|            0|            0|  0.00%|        assert current_frame is not None  # assert current frame is not an Optional[FrameType]
   719|         0|            0|            0|  0.00%|        current_frame = current_frame.f_back
   720|         0|            0|            0|  0.00%|
   721|         0|            0|            0|  0.00%|    assert current_frame is not None  # same here
   722|         0|            0|            0|  0.00%|    class_name = current_frame.f_code.co_name
   723|         0|            0|            0|  0.00%|    line_no = current_frame.f_code.co_firstlineno
   724|         0|            0|            0|  0.00%|    return class_name, line_no
   725|         0|            0|            0|  0.00%|
   726|         0|            0|            0|  0.00%|# At the the point the decorator is applied to class methods the method
   727|         0|            0|            0|  0.00%|# has no reference to its owning class. _qualified_name would not include
   728|         0|            0|            0|  0.00%|# the class it is defined in, so any methods with the same name in the same file
   729|         0|            0|            0|  0.00%|# would have the same _qualified_name, even if they were defined in different
   730|         0|            0|            0|  0.00%|# classes. This problem only exists in python 2.
   731|         0|            0|            0|  0.00%|# We get around this problem by looking at the stack frame and identifying
   732|         0|            0|            0|  0.00%|# the class name, and throwing an error whenever overloads are used
   733|         0|            0|            0|  0.00%|# when modules of the same name are in the same file
   734|         0|            0|            0|  0.00%|
   735|         0|            0|            0|  0.00%|# qualified_name => class name => list[overload_functions]
   736|         0|            0|            0|  0.00%|_overloaded_methods : Dict[str, Dict[str, List[Callable]]] = {}  # noqa: T484
   737|         0|            0|            0|  0.00%|
   738|         0|            0|            0|  0.00%|
   739|         0|            0|            0|  0.00%|# (qualified_name, class name) => class_fileno
   740|         0|            0|            0|  0.00%|_overloaded_method_class_fileno = {}
   741|         0|            0|            0|  0.00%|
   742|         0|            0|            0|  0.00%|def _overload_method(func):
   743|         0|            0|            0|  0.00%|    qual_name = _qualified_name(func)
   744|         0|            0|            0|  0.00%|    global _overloaded_methods
   745|         0|            0|            0|  0.00%|    class_name_map = _overloaded_methods.get(qual_name, None)
   746|         0|            0|            0|  0.00%|    if class_name_map is None:
   747|         0|            0|            0|  0.00%|        class_name_map = {}
   748|         0|            0|            0|  0.00%|        _overloaded_methods[qual_name] = class_name_map
   749|         0|            0|            0|  0.00%|
   750|         0|            0|            0|  0.00%|    class_name, line_no = get_class_name_lineno(func)
   751|         0|            0|            0|  0.00%|    method_overloads = class_name_map.get(class_name, None)
   752|         0|            0|            0|  0.00%|    if method_overloads is None:
   753|         0|            0|            0|  0.00%|        method_overloads = []
   754|         0|            0|            0|  0.00%|        class_name_map[class_name] = method_overloads
   755|         0|            0|            0|  0.00%|        _overloaded_method_class_fileno[(qual_name, class_name)] = line_no
   756|         0|            0|            0|  0.00%|    else:
   757|         0|            0|            0|  0.00%|        existing_lineno = _overloaded_method_class_fileno[(qual_name, class_name)]
   758|         0|            0|            0|  0.00%|        if existing_lineno != line_no:
   759|         0|            0|            0|  0.00%|            raise RuntimeError("Cannot currently overload the same method name in two different"
   760|         0|            0|            0|  0.00%|                               " classes with the same name in the same module")
   761|         0|            0|            0|  0.00%|
   762|         0|            0|            0|  0.00%|    method_overloads.append(func)
   763|         0|            0|            0|  0.00%|    return func
   764|         0|            0|            0|  0.00%|
   765|         0|            0|            0|  0.00%|def _get_overloaded_methods(method, mod_class):
   766|         0|            0|            0|  0.00%|    # TODO: __name__ not set for submodules in recursive script
   767|         0|            0|            0|  0.00%|    if not hasattr(method, "__name__"):
   768|         0|            0|            0|  0.00%|        return None
   769|         0|            0|            0|  0.00%|    qual_name = _qualified_name(method)
   770|         0|            0|            0|  0.00%|    class_name_map = _overloaded_methods.get(qual_name, None)
   771|         0|            0|            0|  0.00%|    if class_name_map is None:
   772|         0|            0|            0|  0.00%|        return None
   773|         0|            0|            0|  0.00%|    overloads = class_name_map.get(mod_class.__name__, None)
   774|         0|            0|            0|  0.00%|    if overloads is None:
   775|         0|            0|            0|  0.00%|        return None
   776|         0|            0|            0|  0.00%|
   777|         0|            0|            0|  0.00%|    method_line_no = get_source_lines_and_file(method)[1]
   778|         0|            0|            0|  0.00%|    mod_class_fileno = get_source_lines_and_file(mod_class)[1]
   779|         0|            0|            0|  0.00%|    mod_end_fileno = mod_class_fileno + len(get_source_lines_and_file(mod_class)[0])
   780|         0|            0|            0|  0.00%|    if not (method_line_no >= mod_class_fileno and method_line_no <= mod_end_fileno):
   781|         0|            0|            0|  0.00%|        raise Exception("Overloads are not useable when a module is redeclared within the same file: " + str(method))
   782|         0|            0|            0|  0.00%|    return overloads
   783|         0|            0|            0|  0.00%|
   784|         0|            0|            0|  0.00%|
   785|         0|            0|            0|  0.00%|def is_tuple(ann) -> bool:
   786|         0|            0|            0|  0.00%|    if ann is Tuple:
   787|         0|            0|            0|  0.00%|        raise_error_container_parameter_missing("Tuple")
   788|         0|            0|            0|  0.00%|
   789|         0|            0|            0|  0.00%|    # For some reason Python 3.7 violates the Type[A, B].__origin__ == Type rule
   790|         0|            0|            0|  0.00%|    if not hasattr(ann, '__module__'):
   791|         0|            0|            0|  0.00%|        return False
   792|         0|            0|            0|  0.00%|    return ann.__module__ == 'typing' and \
   793|         0|            0|            0|  0.00%|        (getattr(ann, '__origin__', None) is Tuple or
   794|         0|            0|            0|  0.00%|            getattr(ann, '__origin__', None) is tuple)
   795|         0|            0|            0|  0.00%|
   796|         0|            0|            0|  0.00%|def is_list(ann) -> bool:
   797|         0|            0|            0|  0.00%|    if ann is List:
   798|         0|            0|            0|  0.00%|        raise_error_container_parameter_missing("List")
   799|         0|            0|            0|  0.00%|
   800|         0|            0|            0|  0.00%|    if not hasattr(ann, '__module__'):
   801|         0|            0|            0|  0.00%|        return False
   802|         0|            0|            0|  0.00%|    return ann.__module__ == 'typing' and \
   803|         0|            0|            0|  0.00%|        (getattr(ann, '__origin__', None) is List or
   804|         0|            0|            0|  0.00%|            getattr(ann, '__origin__', None) is list)
   805|         0|            0|            0|  0.00%|
   806|         0|            0|            0|  0.00%|def is_dict(ann) -> bool:
   807|         0|            0|            0|  0.00%|    if ann is Dict:
   808|         0|            0|            0|  0.00%|        raise_error_container_parameter_missing("Dict")
   809|         0|            0|            0|  0.00%|
   810|         0|            0|            0|  0.00%|    if not hasattr(ann, '__module__'):
   811|         0|            0|            0|  0.00%|        return False
   812|         0|            0|            0|  0.00%|    return ann.__module__ == 'typing' and \
   813|         0|            0|            0|  0.00%|        (getattr(ann, '__origin__', None) is Dict or
   814|         0|            0|            0|  0.00%|            getattr(ann, '__origin__', None) is dict)
   815|         0|            0|            0|  0.00%|
   816|         0|            0|            0|  0.00%|def is_optional(ann) -> bool:
   817|         0|            0|            0|  0.00%|    if ann is Optional:
   818|         0|            0|            0|  0.00%|        raise_error_container_parameter_missing("Optional")
   819|         0|            0|            0|  0.00%|
   820|         0|            0|            0|  0.00%|    # Optional[T] is just shorthand for Union[T, None], so check for both
   821|         0|            0|            0|  0.00%|    def safe_is_subclass(the_type, super_type):
   822|         0|            0|            0|  0.00%|        # Don't throw if `the_type` isn't a class type (e.g. if it is
   823|         0|            0|            0|  0.00%|        # another type annotation instance)
   824|         0|            0|            0|  0.00%|        if not inspect.isclass(the_type):
   825|         0|            0|            0|  0.00%|            return False
   826|         0|            0|            0|  0.00%|        return issubclass(the_type, super_type)
   827|         0|            0|            0|  0.00%|
   828|         0|            0|            0|  0.00%|    if not hasattr(ann, '__module__'):
   829|         0|            0|            0|  0.00%|        return False
   830|         0|            0|            0|  0.00%|
   831|         0|            0|            0|  0.00%|    union_optional = False
   832|         0|            0|            0|  0.00%|    if ann.__module__ == 'typing' and \
   833|         0|            0|            0|  0.00%|       (getattr(ann, '__origin__', None) is Union):
   834|         0|            0|            0|  0.00%|        args = getattr(ann, '__args__', ())
   835|         0|            0|            0|  0.00%|        if len(args) == 2:
   836|         0|            0|            0|  0.00%|            union_optional = (safe_is_subclass(args[1], type(None)) and not safe_is_subclass(args[0], type(None))) \
   837|         0|            0|            0|  0.00%|                or (safe_is_subclass(args[0], type(None)) and not safe_is_subclass(args[1], type(None)))
   838|         0|            0|            0|  0.00%|
   839|         0|            0|            0|  0.00%|    optional = ann.__module__ == 'typing' and \
   840|         0|            0|            0|  0.00%|        (getattr(ann, '__origin__', None) is Optional)
   841|         0|            0|            0|  0.00%|
   842|         0|            0|            0|  0.00%|    return optional or union_optional
   843|         0|            0|            0|  0.00%|
   844|         0|            0|            0|  0.00%|def is_future(ann) -> bool:
   845|         0|            0|            0|  0.00%|    if ann is Future:
   846|         0|            0|            0|  0.00%|        raise RuntimeError(
   847|         0|            0|            0|  0.00%|            "Attempted to use Future without a "
   848|         0|            0|            0|  0.00%|            "contained type. Please add a contained type, e.g. "
   849|         0|            0|            0|  0.00%|            "Future[int]"
   850|         0|            0|            0|  0.00%|        )
   851|         0|            0|            0|  0.00%|    return getattr(ann, "__origin__", None) is Future
   852|         0|            0|            0|  0.00%|
   853|         0|            0|            0|  0.00%|if torch.distributed.rpc.is_available():
   854|         0|            0|            0|  0.00%|    from torch.distributed.rpc import RRef
   855|         0|            0|            0|  0.00%|
   856|         0|            0|            0|  0.00%|    def is_rref(ann) -> bool:
   857|         0|            0|            0|  0.00%|        if ann is RRef:
   858|         0|            0|            0|  0.00%|            raise RuntimeError(
   859|         0|            0|            0|  0.00%|                "Attempted to use RRef without a "
   860|         0|            0|            0|  0.00%|                "contained type. Please add a contained type, e.g. "
   861|         0|            0|            0|  0.00%|                "RRef[int]"
   862|         0|            0|            0|  0.00%|            )
   863|         0|            0|            0|  0.00%|        return getattr(ann, "__origin__", None) is RRef
   864|         0|            0|            0|  0.00%|
   865|         0|            0|            0|  0.00%|def is_final(ann) -> bool:
   866|         0|            0|            0|  0.00%|    return ann.__module__ in {'typing', 'typing_extensions'} and \
   867|         0|            0|            0|  0.00%|        (getattr(ann, '__origin__', None) is Final or isinstance(ann, type(Final)))
   868|         0|            0|            0|  0.00%|
   869|         0|            0|            0|  0.00%|# allows BroadcastingList instance to be subscriptable
   870|         0|            0|            0|  0.00%|class BroadcastingListCls(object):
   871|         0|            0|            0|  0.00%|    def __getitem__(self, types):
   872|         0|            0|            0|  0.00%|        return
   873|         0|            0|            0|  0.00%|
   874|         0|            0|            0|  0.00%|# mypy doesn't support parameters on types, so we have to explicitly type each
   875|         0|            0|            0|  0.00%|# list size
   876|         0|            0|            0|  0.00%|BroadcastingList1 = BroadcastingListCls()
   877|         0|            0|            0|  0.00%|for i in range(2, 7):
   878|         0|            0|            0|  0.00%|    globals()[f"BroadcastingList{i}"] = BroadcastingList1
   879|         0|            0|            0|  0.00%|
   880|         0|            0|            0|  0.00%|
   881|       671|    0.0027914|  4.16007e-06|  0.01%|def is_scripting() -> bool:
   882|         0|            0|            0|  0.00%|    r"""
   883|         0|            0|            0|  0.00%|    Function that returns True when in compilation and False otherwise. This
   884|         0|            0|            0|  0.00%|    is useful especially with the @unused decorator to leave code in your
   885|         0|            0|            0|  0.00%|    model that is not yet TorchScript compatible.
   886|         0|            0|            0|  0.00%|    .. testcode::
   887|         0|            0|            0|  0.00%|
   888|         0|            0|            0|  0.00%|        import torch
   889|         0|            0|            0|  0.00%|
   890|         0|            0|            0|  0.00%|        @torch.jit.unused
   891|         0|            0|            0|  0.00%|        def unsupported_linear_op(x):
   892|         0|            0|            0|  0.00%|            return x
   893|         0|            0|            0|  0.00%|
   894|         0|            0|            0|  0.00%|        def linear(x):
   895|         0|            0|            0|  0.00%|           if torch.jit.is_scripting():
   896|         0|            0|            0|  0.00%|              return torch.linear(x)
   897|         0|            0|            0|  0.00%|           else:
   898|         0|            0|            0|  0.00%|              return unsupported_linear_op(x)
   899|         0|            0|            0|  0.00%|    """
   900|       671|   0.00258803|  3.85698e-06|  0.00%|    return False
   901|         0|            0|            0|  0.00%|
   902|         0|            0|            0|  0.00%|
   903|         0|            0|            0|  0.00%|# Retrieves a fully-qualified name (module hierarchy + classname) for a given obj.
   904|         0|            0|            0|  0.00%|def _qualified_name(obj) -> str:
   905|         0|            0|            0|  0.00%|    # This special case allows us to override the qualified name on a type.
   906|         0|            0|            0|  0.00%|    # It's currently used in conjunction with tracing, where we create a
   907|         0|            0|            0|  0.00%|    # fake module to filter only supported attributes. However, since this
   908|         0|            0|            0|  0.00%|    # new type is defined as a local class, we need a mechanism to override
   909|         0|            0|            0|  0.00%|    # its qualname so it appears correctly in the TorchScript system. This,
   910|         0|            0|            0|  0.00%|    # we set '_jit_override_qualname' with the original traced module's
   911|         0|            0|            0|  0.00%|    # qualified name, which is picked up here
   912|         0|            0|            0|  0.00%|    if hasattr(obj, '_jit_override_qualname'):
   913|         0|            0|            0|  0.00%|        return obj._jit_override_qualname
   914|         0|            0|            0|  0.00%|    # short-circuit in cases where the object already has a known qualified name
   915|         0|            0|            0|  0.00%|    if isinstance(obj, torch._C.ScriptFunction):
   916|         0|            0|            0|  0.00%|        return obj.qualified_name
   917|         0|            0|            0|  0.00%|
   918|         0|            0|            0|  0.00%|    if getattr(obj, "__name__", None):
   919|         0|            0|            0|  0.00%|        name = obj.__name__
   920|         0|            0|            0|  0.00%|    # Enum classes do not have `__name__` attr, instead they have `name`.
   921|         0|            0|            0|  0.00%|    elif isinstance(obj, enum.Enum):
   922|         0|            0|            0|  0.00%|        name = obj.name
   923|         0|            0|            0|  0.00%|    else:
   924|         0|            0|            0|  0.00%|        raise RuntimeError("Could not get name of python class object")
   925|         0|            0|            0|  0.00%|
   926|         0|            0|            0|  0.00%|
   927|         0|            0|            0|  0.00%|    if name == '<lambda>':
   928|         0|            0|            0|  0.00%|        name = '_lambda'  # make name a valid identifier
   929|         0|            0|            0|  0.00%|
   930|         0|            0|            0|  0.00%|    module_name = obj.__module__
   931|         0|            0|            0|  0.00%|
   932|         0|            0|            0|  0.00%|    # If the module is actually a torchbind module, then we should short circuit
   933|         0|            0|            0|  0.00%|    if module_name == "torch._classes":
   934|         0|            0|            0|  0.00%|        return obj.qualified_name
   935|         0|            0|            0|  0.00%|
   936|         0|            0|            0|  0.00%|    # The Python docs are very clear that `__module__` can be None, but I can't
   937|         0|            0|            0|  0.00%|    # figure out when it actually would be.
   938|         0|            0|            0|  0.00%|    if module_name is None:
   939|         0|            0|            0|  0.00%|        raise RuntimeError(f"Could not get qualified name for class '{name}': "
   940|         0|            0|            0|  0.00%|                           "__module__ can't be None.")
   941|         0|            0|            0|  0.00%|
   942|         0|            0|            0|  0.00%|    # if getattr(sys.modules[module_name], name) is not obj:
   943|         0|            0|            0|  0.00%|    #     raise RuntimeError(f"Could not get qualified name for class '{name}': "
   944|         0|            0|            0|  0.00%|    #                        f"the attr {name} on module {module_name} is not the the class")
   945|         0|            0|            0|  0.00%|
   946|         0|            0|            0|  0.00%|    # torch.package and TorchScript have separate mangling schemes to avoid
   947|         0|            0|            0|  0.00%|    # name collisions from multiple packages. To avoid them interfering with
   948|         0|            0|            0|  0.00%|    # each other, remove the package mangling here.
   949|         0|            0|            0|  0.00%|    module_name = package_mangling.demangle(module_name)
   950|         0|            0|            0|  0.00%|
   951|         0|            0|            0|  0.00%|    # __main__ is a builtin module, so rewrite it to "__torch__".
   952|         0|            0|            0|  0.00%|    if module_name == "__main__":
   953|         0|            0|            0|  0.00%|        module_name = "__torch__"
   954|         0|            0|            0|  0.00%|    else:
   955|         0|            0|            0|  0.00%|        # Everything else gets a "__torch__" prefix to avoid name collisions
   956|         0|            0|            0|  0.00%|        # with the names of user values.
   957|         0|            0|            0|  0.00%|        module_name = "__torch__." + module_name
   958|         0|            0|            0|  0.00%|
   959|         0|            0|            0|  0.00%|    if "." in name:
   960|         0|            0|            0|  0.00%|        raise RuntimeError(f"Could not get qualified name for class '{name}': "
   961|         0|            0|            0|  0.00%|                           f"'{name}' is not a valid identifier")
   962|         0|            0|            0|  0.00%|
   963|         0|            0|            0|  0.00%|    return module_name + "." + name
   964|         0|            0|            0|  0.00%|
   965|         0|            0|            0|  0.00%|
   966|         0|            0|            0|  0.00%|# Thin wrapper around SourceRangeFactory to store extra metadata
   967|         0|            0|            0|  0.00%|# about the function-to-be-compiled.
   968|         0|            0|            0|  0.00%|class SourceContext(torch._C._jit_tree_views.SourceRangeFactory):
   969|         0|            0|            0|  0.00%|    def __init__(self, source, filename, file_lineno, leading_whitespace_len, uses_true_division=True):
   970|         0|            0|            0|  0.00%|        super(SourceContext, self).__init__(source, filename, file_lineno, leading_whitespace_len)
   971|         0|            0|            0|  0.00%|        self.uses_true_division = uses_true_division
   972|         0|            0|            0|  0.00%|        self.filename = filename
   973|         0|            0|            0|  0.00%|
   974|         0|            0|            0|  0.00%|
   975|         0|            0|            0|  0.00%|def fake_range():
   976|         0|            0|            0|  0.00%|    return SourceContext('', None, 0, 0).make_raw_range(0, 1)
   977|         0|            0|            0|  0.00%|
   978|         0|            0|            0|  0.00%|
   979|         0|            0|            0|  0.00%|def _try_get_dispatched_fn(fn):
   980|         0|            0|            0|  0.00%|    if not callable(fn):
   981|         0|            0|            0|  0.00%|        return None
   982|         0|            0|            0|  0.00%|    return boolean_dispatched.get(fn)
   983|         0|            0|            0|  0.00%|
   984|         0|            0|            0|  0.00%|
   985|         0|            0|            0|  0.00%|def _get_named_tuple_properties(obj):
   986|         0|            0|            0|  0.00%|    assert issubclass(obj, tuple) and hasattr(obj, '_fields')
   987|         0|            0|            0|  0.00%|    fields = list(obj._fields)
   988|         0|            0|            0|  0.00%|    annotations = []
   989|         0|            0|            0|  0.00%|    has_annotations = hasattr(obj, '__annotations__')
   990|         0|            0|            0|  0.00%|    for field in fields:
   991|         0|            0|            0|  0.00%|        if has_annotations and field in obj.__annotations__:
   992|         0|            0|            0|  0.00%|            the_type = torch.jit.annotations.ann_to_type(obj.__annotations__[field], fake_range())
   993|         0|            0|            0|  0.00%|            annotations.append(the_type)
   994|         0|            0|            0|  0.00%|        else:
   995|         0|            0|            0|  0.00%|            annotations.append(torch._C.TensorType.getInferred())
   996|         0|            0|            0|  0.00%|    return type(obj).__name__, fields, annotations
   997|         0|            0|            0|  0.00%|
   998|         0|            0|            0|  0.00%|
   999|         0|            0|            0|  0.00%|def _create_named_tuple(t, unqual_name: str, field_names: List[str]):
  1000|         0|            0|            0|  0.00%|    # mypy: namedtuple() expects a string literal as the first argument
  1001|         0|            0|            0|  0.00%|    TupleType = collections.namedtuple(unqual_name, field_names)  # type: ignore[misc]
  1002|         0|            0|            0|  0.00%|    return TupleType(*t)
  1003|         0|            0|            0|  0.00%|
  1004|         0|            0|            0|  0.00%|
  1005|         0|            0|            0|  0.00%|@contextlib.contextmanager
  1006|         0|            0|            0|  0.00%|def _disable_emit_hooks():
  1007|         0|            0|            0|  0.00%|    hooks = torch._C._jit_get_emit_hooks()
  1008|         0|            0|            0|  0.00%|    torch._C._jit_set_emit_hooks(None, None)
  1009|         0|            0|            0|  0.00%|    yield
  1010|         0|            0|            0|  0.00%|    torch._C._jit_set_emit_hooks(hooks[0], hooks[1])
  1011|         0|            0|            0|  0.00%|
  1012|         0|            0|            0|  0.00%|
  1013|         0|            0|            0|  0.00%|def _disable_emit_hooks_decorator(_DecoratorContextManager) -> None:  # noqa: F811
  1014|         0|            0|            0|  0.00%|    def __enter__(self) -> None:
  1015|         0|            0|            0|  0.00%|        self.hooks = torch._C._jit_get_emit_hooks()
  1016|         0|            0|            0|  0.00%|        torch._C._jit_set_emit_hooks(None, None)
  1017|         0|            0|            0|  0.00%|
  1018|         0|            0|            0|  0.00%|    def __exit__(self, *args) -> None:
  1019|         0|            0|            0|  0.00%|        torch._C._jit_set_emit_hooks(self.hooks[0], self.hooks[1])
  1020|         0|            0|            0|  0.00%|
  1021|         0|            0|            0|  0.00%|def _is_exception(obj) -> bool:
  1022|         0|            0|            0|  0.00%|    if not inspect.isclass(obj):
  1023|         0|            0|            0|  0.00%|        return False
  1024|         0|            0|            0|  0.00%|    return issubclass(obj, Exception)
  1025|         0|            0|            0|  0.00%|
  1026|         0|            0|            0|  0.00%|def raise_error_container_parameter_missing(target_type) -> None:
  1027|         0|            0|            0|  0.00%|    if target_type == 'Dict':
  1028|         0|            0|            0|  0.00%|        raise RuntimeError(
  1029|         0|            0|            0|  0.00%|            "Attempted to use Dict without "
  1030|         0|            0|            0|  0.00%|            "contained types. Please add contained type, e.g. "
  1031|         0|            0|            0|  0.00%|            "Dict[int, int]"
  1032|         0|            0|            0|  0.00%|        )
  1033|         0|            0|            0|  0.00%|    raise RuntimeError(
  1034|         0|            0|            0|  0.00%|        f"Attempted to use {target_type} without a "
  1035|         0|            0|            0|  0.00%|        "contained type. Please add a contained type, e.g. "
  1036|         0|            0|            0|  0.00%|        f"{target_type}[int]"
  1037|         0|            0|            0|  0.00%|    )
  1038|         0|            0|            0|  0.00%|
  1039|         0|            0|            0|  0.00%|
  1040|         0|            0|            0|  0.00%|def get_origin(target_type):
  1041|         0|            0|            0|  0.00%|    return getattr(target_type, "__origin__", None)
  1042|         0|            0|            0|  0.00%|
  1043|         0|            0|            0|  0.00%|
  1044|         0|            0|            0|  0.00%|def get_args(target_type):
  1045|         0|            0|            0|  0.00%|    return getattr(target_type, "__args__", None)
  1046|         0|            0|            0|  0.00%|
  1047|         0|            0|            0|  0.00%|
  1048|         0|            0|            0|  0.00%|def check_args_exist(target_type) -> None:
  1049|         0|            0|            0|  0.00%|    if target_type is List or target_type is list:
  1050|         0|            0|            0|  0.00%|        raise_error_container_parameter_missing("List")
  1051|         0|            0|            0|  0.00%|    elif target_type is Tuple or target_type is tuple:
  1052|         0|            0|            0|  0.00%|        raise_error_container_parameter_missing("Tuple")
  1053|         0|            0|            0|  0.00%|    elif target_type is Dict or target_type is dict:
  1054|         0|            0|            0|  0.00%|        raise_error_container_parameter_missing("Dict")
  1055|         0|            0|            0|  0.00%|    elif target_type is None or target_type is Optional:
  1056|         0|            0|            0|  0.00%|        raise_error_container_parameter_missing("Optional")
  1057|         0|            0|            0|  0.00%|
  1058|         0|            0|            0|  0.00%|
  1059|         0|            0|            0|  0.00%|# supports List/Dict/Tuple and Optional types
  1060|         0|            0|            0|  0.00%|# TODO support future
  1061|         0|            0|            0|  0.00%|def container_checker(obj, target_type) -> bool:
  1062|         0|            0|            0|  0.00%|    origin_type = get_origin(target_type)
  1063|         0|            0|            0|  0.00%|    check_args_exist(target_type)
  1064|         0|            0|            0|  0.00%|    if origin_type is list or origin_type is List:
  1065|         0|            0|            0|  0.00%|        if not isinstance(obj, list):
  1066|         0|            0|            0|  0.00%|            return False
  1067|         0|            0|            0|  0.00%|        arg_type = get_args(target_type)[0]
  1068|         0|            0|            0|  0.00%|        arg_origin = get_origin(arg_type)
  1069|         0|            0|            0|  0.00%|        for el in obj:
  1070|         0|            0|            0|  0.00%|            # check if nested container, ex: List[List[str]]
  1071|         0|            0|            0|  0.00%|            if arg_origin:  # processes nested container, ex: List[List[str]]
  1072|         0|            0|            0|  0.00%|                if not container_checker(el, arg_type):
  1073|         0|            0|            0|  0.00%|                    return False
  1074|         0|            0|            0|  0.00%|            elif not isinstance(el, arg_type):
  1075|         0|            0|            0|  0.00%|                return False
  1076|         0|            0|            0|  0.00%|        return True
  1077|         0|            0|            0|  0.00%|    elif origin_type is Dict or origin_type is dict:
  1078|         0|            0|            0|  0.00%|        if not isinstance(obj, dict):
  1079|         0|            0|            0|  0.00%|            return False
  1080|         0|            0|            0|  0.00%|        key_type = get_args(target_type)[0]
  1081|         0|            0|            0|  0.00%|        val_type = get_args(target_type)[1]
  1082|         0|            0|            0|  0.00%|        for key, val in obj.items():
  1083|         0|            0|            0|  0.00%|            # check if keys are of right type
  1084|         0|            0|            0|  0.00%|            if not isinstance(key, key_type):
  1085|         0|            0|            0|  0.00%|                return False
  1086|         0|            0|            0|  0.00%|            val_origin = get_origin(val_type)
  1087|         0|            0|            0|  0.00%|            if val_origin:
  1088|         0|            0|            0|  0.00%|                if not container_checker(val, val_type):
  1089|         0|            0|            0|  0.00%|                    return False
  1090|         0|            0|            0|  0.00%|            elif not isinstance(val, val_type):
  1091|         0|            0|            0|  0.00%|                return False
  1092|         0|            0|            0|  0.00%|        return True
  1093|         0|            0|            0|  0.00%|    elif origin_type is Tuple or origin_type is tuple:
  1094|         0|            0|            0|  0.00%|        if not isinstance(obj, tuple):
  1095|         0|            0|            0|  0.00%|            return False
  1096|         0|            0|            0|  0.00%|        arg_types = get_args(target_type)
  1097|         0|            0|            0|  0.00%|        if len(obj) != len(arg_types):
  1098|         0|            0|            0|  0.00%|            return False
  1099|         0|            0|            0|  0.00%|        for el, el_type in zip(obj, arg_types):
  1100|         0|            0|            0|  0.00%|            el_origin = get_origin(el_type)
  1101|         0|            0|            0|  0.00%|            if el_origin:
  1102|         0|            0|            0|  0.00%|                if not container_checker(el, el_type):
  1103|         0|            0|            0|  0.00%|                    return False
  1104|         0|            0|            0|  0.00%|            elif not isinstance(el, el_type):
  1105|         0|            0|            0|  0.00%|                return False
  1106|         0|            0|            0|  0.00%|        return True
  1107|         0|            0|            0|  0.00%|    elif origin_type is Union:  # actually handles Optional Case
  1108|         0|            0|            0|  0.00%|        if obj is None:  # check before recursion because None is always fine
  1109|         0|            0|            0|  0.00%|            return True
  1110|         0|            0|            0|  0.00%|        optional_type = get_args(target_type)[0]
  1111|         0|            0|            0|  0.00%|        optional_origin = get_origin(optional_type)
  1112|         0|            0|            0|  0.00%|        if optional_origin:
  1113|         0|            0|            0|  0.00%|            return container_checker(obj, optional_type)
  1114|         0|            0|            0|  0.00%|        elif isinstance(obj, optional_type):
  1115|         0|            0|            0|  0.00%|            return True
  1116|         0|            0|            0|  0.00%|    return False
  1117|         0|            0|            0|  0.00%|
  1118|         0|            0|            0|  0.00%|
  1119|         0|            0|            0|  0.00%|def _isinstance(obj, target_type) -> bool:
  1120|         0|            0|            0|  0.00%|    origin_type = get_origin(target_type)
  1121|         0|            0|            0|  0.00%|    if origin_type:
  1122|         0|            0|            0|  0.00%|        return container_checker(obj, target_type)
  1123|         0|            0|            0|  0.00%|
  1124|         0|            0|            0|  0.00%|    # Check to handle weird python type behaviors
  1125|         0|            0|            0|  0.00%|    # 1. python 3.6 returns None for origin of containers without
  1126|         0|            0|            0|  0.00%|    #    contained type (intead of returning outer container type)
  1127|         0|            0|            0|  0.00%|    # 2. non-typed optional origin returns as none instead
  1128|         0|            0|            0|  0.00%|    #    of as optional in 3.6-3.8
  1129|         0|            0|            0|  0.00%|    check_args_exist(target_type)
  1130|         0|            0|            0|  0.00%|
  1131|         0|            0|            0|  0.00%|    # handle non-containers
  1132|         0|            0|            0|  0.00%|    return isinstance(obj, target_type)
  1133|         0|            0|            0|  0.00%|
  1134|         0|            0|            0|  0.00%|
  1135|         0|            0|            0|  0.00%|class _TensorExtractor(pickle.Pickler):
  1136|         0|            0|            0|  0.00%|    def __init__(self, *args, tensors: List[torch.Tensor], **kwargs):
  1137|         0|            0|            0|  0.00%|        super().__init__(*args, **kwargs)
  1138|         0|            0|            0|  0.00%|        self.tensors = tensors
  1139|         0|            0|            0|  0.00%|
  1140|         0|            0|            0|  0.00%|    def persistent_id(self, obj):
  1141|         0|            0|            0|  0.00%|        if isinstance(obj, torch.Tensor):
  1142|         0|            0|            0|  0.00%|            self.tensors.append(obj)
  1143|         0|            0|            0|  0.00%|            return ""
  1144|         0|            0|            0|  0.00%|        else:
  1145|         0|            0|            0|  0.00%|            return None
  1146|         0|            0|            0|  0.00%|
  1147|         0|            0|            0|  0.00%|
  1148|         0|            0|            0|  0.00%|def _extract_tensors(obj):
  1149|         0|            0|            0|  0.00%|    r"""
  1150|         0|            0|            0|  0.00%|    This function is exclusively called from C++.
  1151|         0|            0|            0|  0.00%|    See ``torch/csrc/jit/python/python_ivalue.h``.
  1152|         0|            0|            0|  0.00%|
  1153|         0|            0|            0|  0.00%|    It extracts the tensors contained in the given object, through pickling.
  1154|         0|            0|            0|  0.00%|    """
  1155|         0|            0|            0|  0.00%|    tensors: List[torch.Tensor] = []
  1156|         0|            0|            0|  0.00%|    extractor = _TensorExtractor(io.BytesIO(), protocol=-1, tensors=tensors)
  1157|         0|            0|            0|  0.00%|    extractor.dump(obj)
  1158|         0|            0|            0|  0.00%|    return tensors
File: /opt/conda/lib/python3.8/site-packages/torch/cuda/_utils.py
File duration: 0.0118604s (0.02%)
Line #|      Hits|         Time| Time per hit|      %|Source code
------+----------+-------------+-------------+-------+-----------
     1|         0|            0|            0|  0.00%|import torch
     2|         0|            0|            0|  0.00%|from typing import Any
     3|         0|            0|            0|  0.00%|# The _get_device_index has been moved to torch.utils._get_device_index
     4|         0|            0|            0|  0.00%|from torch._utils import _get_device_index as _torch_get_device_index
     5|         0|            0|            0|  0.00%|
     6|         0|            0|            0|  0.00%|
     7|       244|   0.00111103|   4.5534e-06|  0.00%|def _get_device_index(device: Any, optional: bool = False,
     8|         0|            0|            0|  0.00%|                      allow_cpu: bool = False) -> int:
     9|         0|            0|            0|  0.00%|    r"""Gets the device index from :attr:`device`, which can be a torch.device
    10|         0|            0|            0|  0.00%|    object, a Python integer, or ``None``.
    11|         0|            0|            0|  0.00%|
    12|         0|            0|            0|  0.00%|    If :attr:`device` is a torch.device object, returns the device index if it
    13|         0|            0|            0|  0.00%|    is a CUDA device. Note that for a CUDA device without a specified index,
    14|         0|            0|            0|  0.00%|    i.e., ``torch.device('cuda')``, this will return the current default CUDA
    15|         0|            0|            0|  0.00%|    device if :attr:`optional` is ``True``. If :attr:`allow_cpu` is ``True``,
    16|         0|            0|            0|  0.00%|    CPU devices will be accepted and ``-1`` will be returned in this case.
    17|         0|            0|            0|  0.00%|
    18|         0|            0|            0|  0.00%|    If :attr:`device` is a Python integer, it is returned as is.
    19|         0|            0|            0|  0.00%|
    20|         0|            0|            0|  0.00%|    If :attr:`device` is ``None``, this will return the current default CUDA
    21|         0|            0|            0|  0.00%|    device if :attr:`optional` is ``True``.
    22|         0|            0|            0|  0.00%|    """
    23|       244|   0.00166655|  6.83011e-06|  0.00%|    if isinstance(device, str):
    24|         0|            0|            0|  0.00%|        device = torch.device(device)
    25|       244|     0.001477|  6.05329e-06|  0.00%|    if isinstance(device, torch.device):
    26|         0|            0|            0|  0.00%|        if allow_cpu:
    27|         0|            0|            0|  0.00%|            if device.type not in ['cuda', 'cpu']:
    28|         0|            0|            0|  0.00%|                raise ValueError('Expected a cuda or cpu device, but got: {}'.format(device))
    29|         0|            0|            0|  0.00%|        elif device.type != 'cuda':
    30|         0|            0|            0|  0.00%|            raise ValueError('Expected a cuda device, but got: {}'.format(device))
    31|       244|   0.00299978|  1.22942e-05|  0.01%|    if not torch.jit.is_scripting():
(call)|       244|   0.00189281|   7.7574e-06|  0.00%|# /opt/conda/lib/python3.8/site-packages/torch/_jit_internal.py:881 is_scripting
    32|       244|   0.00148821|  6.09922e-06|  0.00%|        if isinstance(device, torch.cuda.device):
    33|         0|            0|            0|  0.00%|            return device.idx
    34|       244|    0.0031178|  1.27779e-05|  0.01%|    return _torch_get_device_index(device, optional, allow_cpu)
(call)|       244|   0.00923204|  3.78362e-05|  0.02%|# /opt/conda/lib/python3.8/site-packages/torch/_utils.py:467 _get_device_index
    35|         0|            0|            0|  0.00%|
    36|         0|            0|            0|  0.00%|
    37|         0|            0|            0|  0.00%|def _dummy_type(name: str) -> type:
    38|         0|            0|            0|  0.00%|    def init_err(self):
    39|         0|            0|            0|  0.00%|        class_name = self.__class__.__name__
    40|         0|            0|            0|  0.00%|        raise RuntimeError(
    41|         0|            0|            0|  0.00%|            "Tried to instantiate dummy base class {}".format(class_name))
    42|         0|            0|            0|  0.00%|    return type(name, (object,), {"__init__": init_err})
File: /opt/conda/lib/python3.8/site-packages/torch/cuda/__init__.py
File duration: 0.0116162s (0.02%)
Line #|      Hits|         Time| Time per hit|      %|Source code
------+----------+-------------+-------------+-------+-----------
     1|         0|            0|            0|  0.00%|r"""
     2|         0|            0|            0|  0.00%|This package adds support for CUDA tensor types, that implement the same
     3|         0|            0|            0|  0.00%|function as CPU tensors, but they utilize GPUs for computation.
     4|         0|            0|            0|  0.00%|
     5|         0|            0|            0|  0.00%|It is lazily initialized, so you can always import it, and use
     6|         0|            0|            0|  0.00%|:func:`is_available()` to determine if your system supports CUDA.
     7|         0|            0|            0|  0.00%|
     8|         0|            0|            0|  0.00%|:ref:`cuda-semantics` has more details about working with CUDA.
     9|         0|            0|            0|  0.00%|"""
    10|         0|            0|            0|  0.00%|
    11|         0|            0|            0|  0.00%|import contextlib
    12|         0|            0|            0|  0.00%|import os
    13|         0|            0|            0|  0.00%|import torch
    14|         0|            0|            0|  0.00%|import traceback
    15|         0|            0|            0|  0.00%|import warnings
    16|         0|            0|            0|  0.00%|import threading
    17|         0|            0|            0|  0.00%|from typing import List, Optional, Tuple, Union, Any
    18|         0|            0|            0|  0.00%|from ._utils import _get_device_index, _dummy_type
    19|         0|            0|            0|  0.00%|from .streams import Stream, Event, _Graph, _graph_pool_handle
    20|         0|            0|            0|  0.00%|from .. import device as _device
    21|         0|            0|            0|  0.00%|import torch._C
    22|         0|            0|            0|  0.00%|
    23|         0|            0|            0|  0.00%|try:
    24|         0|            0|            0|  0.00%|    from torch._C import _cudart  # type: ignore[attr-defined]
    25|         0|            0|            0|  0.00%|except ImportError:
    26|         0|            0|            0|  0.00%|    _cudart = None
    27|         0|            0|            0|  0.00%|
    28|         0|            0|            0|  0.00%|_initialized = False
    29|         0|            0|            0|  0.00%|_tls = threading.local()
    30|         0|            0|            0|  0.00%|_initialization_lock = threading.Lock()
    31|         0|            0|            0|  0.00%|_queued_calls = []  # don't invoke these until initialization occurs
    32|         0|            0|            0|  0.00%|_is_in_bad_fork = getattr(torch._C, "_cuda_isInBadFork", lambda: False)
    33|         0|            0|            0|  0.00%|_device_t = Union[_device, str, int, None]
    34|         0|            0|            0|  0.00%|
    35|         0|            0|            0|  0.00%|# Define dummy _CudaDeviceProperties type if PyTorch was compiled without CUDA
    36|         0|            0|            0|  0.00%|if hasattr(torch._C, '_CudaDeviceProperties'):
    37|         0|            0|            0|  0.00%|    _CudaDeviceProperties = torch._C._CudaDeviceProperties
    38|         0|            0|            0|  0.00%|else:
    39|         0|            0|            0|  0.00%|    _CudaDeviceProperties = _dummy_type('_CudaDeviceProperties')  # type: ignore[assignment, misc]
    40|         0|            0|            0|  0.00%|
    41|         0|            0|            0|  0.00%|# Global variables dynamically populated by native code
    42|         0|            0|            0|  0.00%|has_magma: bool = False
    43|         0|            0|            0|  0.00%|has_half: bool = False
    44|         0|            0|            0|  0.00%|default_generators: Tuple[torch._C.Generator] = ()  # type: ignore[assignment]
    45|         0|            0|            0|  0.00%|
    46|         0|            0|            0|  0.00%|def is_available() -> bool:
    47|         0|            0|            0|  0.00%|    r"""Returns a bool indicating if CUDA is currently available."""
    48|         0|            0|            0|  0.00%|    if not hasattr(torch._C, '_cuda_getDeviceCount'):
    49|         0|            0|            0|  0.00%|        return False
    50|         0|            0|            0|  0.00%|    # This function never throws and returns 0 if driver is missing or can't
    51|         0|            0|            0|  0.00%|    # be initialized
    52|         0|            0|            0|  0.00%|    return torch._C._cuda_getDeviceCount() > 0
    53|         0|            0|            0|  0.00%|
    54|         0|            0|            0|  0.00%|
    55|         0|            0|            0|  0.00%|def _sleep(cycles):
    56|         0|            0|            0|  0.00%|    torch._C._cuda_sleep(cycles)
    57|         0|            0|            0|  0.00%|
    58|         0|            0|            0|  0.00%|
    59|         0|            0|            0|  0.00%|def _check_capability():
    60|         0|            0|            0|  0.00%|    incorrect_binary_warn = """
    61|         0|            0|            0|  0.00%|    Found GPU%d %s which requires CUDA_VERSION >= %d to
    62|         0|            0|            0|  0.00%|     work properly, but your PyTorch was compiled
    63|         0|            0|            0|  0.00%|     with CUDA_VERSION %d. Please install the correct PyTorch binary
    64|         0|            0|            0|  0.00%|     using instructions from https://pytorch.org
    65|         0|            0|            0|  0.00%|    """
    66|         0|            0|            0|  0.00%|
    67|         0|            0|            0|  0.00%|    old_gpu_warn = """
    68|         0|            0|            0|  0.00%|    Found GPU%d %s which is of cuda capability %d.%d.
    69|         0|            0|            0|  0.00%|    PyTorch no longer supports this GPU because it is too old.
    70|         0|            0|            0|  0.00%|    The minimum cuda capability supported by this library is %d.%d.
    71|         0|            0|            0|  0.00%|    """
    72|         0|            0|            0|  0.00%|
    73|         0|            0|            0|  0.00%|    if torch.version.cuda is not None:  # on ROCm we don't want this check
    74|         0|            0|            0|  0.00%|        CUDA_VERSION = torch._C._cuda_getCompiledVersion()
    75|         0|            0|            0|  0.00%|        for d in range(device_count()):
    76|         0|            0|            0|  0.00%|            capability = get_device_capability(d)
    77|         0|            0|            0|  0.00%|            major = capability[0]
    78|         0|            0|            0|  0.00%|            minor = capability[1]
    79|         0|            0|            0|  0.00%|            name = get_device_name(d)
    80|         0|            0|            0|  0.00%|            current_arch = major * 10 + minor
    81|         0|            0|            0|  0.00%|            min_arch = min((int(arch.split("_")[1]) for arch in torch.cuda.get_arch_list()), default=35)
    82|         0|            0|            0|  0.00%|            if current_arch < min_arch:
    83|         0|            0|            0|  0.00%|                warnings.warn(old_gpu_warn.format(d, name, major, minor, min_arch // 10, min_arch % 10))
    84|         0|            0|            0|  0.00%|            elif CUDA_VERSION <= 9000 and major >= 7 and minor >= 5:
    85|         0|            0|            0|  0.00%|                warnings.warn(incorrect_binary_warn % (d, name, 10000, CUDA_VERSION))
    86|         0|            0|            0|  0.00%|
    87|         0|            0|            0|  0.00%|def _check_cubins():
    88|         0|            0|            0|  0.00%|    incompatible_device_warn = """
    89|         0|            0|            0|  0.00%|{} with CUDA capability sm_{} is not compatible with the current PyTorch installation.
    90|         0|            0|            0|  0.00%|The current PyTorch install supports CUDA capabilities {}.
    91|         0|            0|            0|  0.00%|If you want to use the {} GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/
    92|         0|            0|            0|  0.00%|"""
    93|         0|            0|            0|  0.00%|    if torch.version.cuda is None:  # on ROCm we don't want this check
    94|         0|            0|            0|  0.00%|        return
    95|         0|            0|            0|  0.00%|    arch_list = get_arch_list()
    96|         0|            0|            0|  0.00%|    if len(arch_list) == 0:
    97|         0|            0|            0|  0.00%|        return
    98|         0|            0|            0|  0.00%|    supported_sm = [int(arch.split('_')[1]) for arch in arch_list if 'sm_' in arch]
    99|         0|            0|            0|  0.00%|    for idx in range(device_count()):
   100|         0|            0|            0|  0.00%|        cap_major, cap_minor = get_device_capability(idx)
   101|         0|            0|            0|  0.00%|        # NVIDIA GPU compute architectures are backward compatible within major version
   102|         0|            0|            0|  0.00%|        supported = any([sm // 10 == cap_major for sm in supported_sm])
   103|         0|            0|            0|  0.00%|        if not supported:
   104|         0|            0|            0|  0.00%|            device_name = get_device_name(idx)
   105|         0|            0|            0|  0.00%|            capability = cap_major * 10 + cap_minor
   106|         0|            0|            0|  0.00%|            warnings.warn(incompatible_device_warn.format(device_name, capability, " ".join(arch_list), device_name))
   107|         0|            0|            0|  0.00%|
   108|         0|            0|            0|  0.00%|
   109|         0|            0|            0|  0.00%|def is_initialized():
   110|         0|            0|            0|  0.00%|    r"""Returns whether PyTorch's CUDA state has been initialized."""
   111|         0|            0|            0|  0.00%|    return _initialized and not _is_in_bad_fork()
   112|         0|            0|            0|  0.00%|
   113|         0|            0|            0|  0.00%|
   114|         0|            0|            0|  0.00%|def _lazy_call(callable):
   115|         0|            0|            0|  0.00%|    if is_initialized():
   116|         0|            0|            0|  0.00%|        callable()
   117|         0|            0|            0|  0.00%|    else:
   118|         0|            0|            0|  0.00%|        # TODO(torch_deploy): this accesses linecache, which attempts to read the
   119|         0|            0|            0|  0.00%|        # file system to get traceback info. Patch linecache or do something
   120|         0|            0|            0|  0.00%|        # else here if this ends up being important.
   121|         0|            0|            0|  0.00%|
   122|         0|            0|            0|  0.00%|        # Don't store the actual traceback to avoid memory cycle
   123|         0|            0|            0|  0.00%|        _queued_calls.append((callable, traceback.format_stack()))
   124|         0|            0|            0|  0.00%|
   125|         0|            0|            0|  0.00%|_lazy_call(_check_capability)
   126|         0|            0|            0|  0.00%|_lazy_call(_check_cubins)
   127|         0|            0|            0|  0.00%|
   128|         0|            0|            0|  0.00%|
   129|         0|            0|            0|  0.00%|class DeferredCudaCallError(Exception):
   130|         0|            0|            0|  0.00%|    pass
   131|         0|            0|            0|  0.00%|
   132|         0|            0|            0|  0.00%|
   133|         0|            0|            0|  0.00%|def init():
   134|         0|            0|            0|  0.00%|    r"""Initialize PyTorch's CUDA state.  You may need to call
   135|         0|            0|            0|  0.00%|    this explicitly if you are interacting with PyTorch via
   136|         0|            0|            0|  0.00%|    its C API, as Python bindings for CUDA functionality will not
   137|         0|            0|            0|  0.00%|    be available until this initialization takes place.  Ordinary users
   138|         0|            0|            0|  0.00%|    should not need this, as all of PyTorch's CUDA methods
   139|         0|            0|            0|  0.00%|    automatically initialize CUDA state on-demand.
   140|         0|            0|            0|  0.00%|
   141|         0|            0|            0|  0.00%|    Does nothing if the CUDA state is already initialized.
   142|         0|            0|            0|  0.00%|    """
   143|         0|            0|            0|  0.00%|    _lazy_init()
   144|         0|            0|            0|  0.00%|
   145|         0|            0|            0|  0.00%|
   146|         0|            0|            0|  0.00%|def _lazy_init():
   147|         0|            0|            0|  0.00%|    global _initialized, _queued_calls
   148|         0|            0|            0|  0.00%|    if is_initialized() or hasattr(_tls, 'is_initializing'):
   149|         0|            0|            0|  0.00%|        return
   150|         0|            0|            0|  0.00%|    with _initialization_lock:
   151|         0|            0|            0|  0.00%|        # We be double-checked locking, boys!  This is OK because
   152|         0|            0|            0|  0.00%|        # the above test was GIL protected anyway.  The inner test
   153|         0|            0|            0|  0.00%|        # is for when a thread blocked on some other thread which was
   154|         0|            0|            0|  0.00%|        # doing the initialization; when they get the lock, they will
   155|         0|            0|            0|  0.00%|        # find there is nothing left to do.
   156|         0|            0|            0|  0.00%|        if is_initialized():
   157|         0|            0|            0|  0.00%|            return
   158|         0|            0|            0|  0.00%|        # It is important to prevent other threads from entering _lazy_init
   159|         0|            0|            0|  0.00%|        # immediately, while we are still guaranteed to have the GIL, because some
   160|         0|            0|            0|  0.00%|        # of the C calls we make below will release the GIL
   161|         0|            0|            0|  0.00%|        if _is_in_bad_fork():
   162|         0|            0|            0|  0.00%|            raise RuntimeError(
   163|         0|            0|            0|  0.00%|                "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
   164|         0|            0|            0|  0.00%|                "multiprocessing, you must use the 'spawn' start method")
   165|         0|            0|            0|  0.00%|        if not hasattr(torch._C, '_cuda_getDeviceCount'):
   166|         0|            0|            0|  0.00%|            raise AssertionError("Torch not compiled with CUDA enabled")
   167|         0|            0|            0|  0.00%|        if _cudart is None:
   168|         0|            0|            0|  0.00%|            raise AssertionError(
   169|         0|            0|            0|  0.00%|                "libcudart functions unavailable. It looks like you have a broken build?")
   170|         0|            0|            0|  0.00%|        # This function throws if there's a driver initialization error, no GPUs
   171|         0|            0|            0|  0.00%|        # are found or any other error occurs
   172|         0|            0|            0|  0.00%|        torch._C._cuda_init()
   173|         0|            0|            0|  0.00%|        # Some of the queued calls may reentrantly call _lazy_init();
   174|         0|            0|            0|  0.00%|        # we need to just return without initializing in that case.
   175|         0|            0|            0|  0.00%|        # However, we must not let any *other* threads in!
   176|         0|            0|            0|  0.00%|        _tls.is_initializing = True
   177|         0|            0|            0|  0.00%|        try:
   178|         0|            0|            0|  0.00%|            for queued_call, orig_traceback in _queued_calls:
   179|         0|            0|            0|  0.00%|                try:
   180|         0|            0|            0|  0.00%|                    queued_call()
   181|         0|            0|            0|  0.00%|                except Exception as e:
   182|         0|            0|            0|  0.00%|                    msg = (f"CUDA call failed lazily at initialization with error: {str(e)}\n\n"
   183|         0|            0|            0|  0.00%|                           f"CUDA call was originally invoked at:\n\n{orig_traceback}")
   184|         0|            0|            0|  0.00%|                    raise DeferredCudaCallError(msg) from e
   185|         0|            0|            0|  0.00%|        finally:
   186|         0|            0|            0|  0.00%|            delattr(_tls, 'is_initializing')
   187|         0|            0|            0|  0.00%|        _initialized = True
   188|         0|            0|            0|  0.00%|
   189|         0|            0|            0|  0.00%|
   190|         0|            0|            0|  0.00%|def cudart():
   191|         0|            0|            0|  0.00%|    _lazy_init()
   192|         0|            0|            0|  0.00%|    return _cudart
   193|         0|            0|            0|  0.00%|
   194|         0|            0|            0|  0.00%|
   195|         0|            0|            0|  0.00%|class cudaStatus(object):
   196|         0|            0|            0|  0.00%|    SUCCESS: int = 0
   197|         0|            0|            0|  0.00%|    ERROR_NOT_READY: int = 34
   198|         0|            0|            0|  0.00%|
   199|         0|            0|            0|  0.00%|class CudaError(RuntimeError):
   200|         0|            0|            0|  0.00%|    def __init__(self, code: int) -> None:
   201|         0|            0|            0|  0.00%|        msg = _cudart.cudaGetErrorString(_cudart.cudaError(code))
   202|         0|            0|            0|  0.00%|        super(CudaError, self).__init__('{0} ({1})'.format(msg, code))
   203|         0|            0|            0|  0.00%|
   204|         0|            0|            0|  0.00%|
   205|         0|            0|            0|  0.00%|def check_error(res: int) -> None:
   206|         0|            0|            0|  0.00%|    if res != _cudart.cudaError.success:
   207|         0|            0|            0|  0.00%|        raise CudaError(res)
   208|         0|            0|            0|  0.00%|
   209|         0|            0|            0|  0.00%|
   210|         0|            0|            0|  0.00%|class device(object):
   211|         0|            0|            0|  0.00%|    r"""Context-manager that changes the selected device.
   212|         0|            0|            0|  0.00%|
   213|         0|            0|            0|  0.00%|    Args:
   214|         0|            0|            0|  0.00%|        device (torch.device or int): device index to select. It's a no-op if
   215|         0|            0|            0|  0.00%|            this argument is a negative integer or ``None``.
   216|         0|            0|            0|  0.00%|    """
   217|         0|            0|            0|  0.00%|
   218|       244|   0.00106072|  4.34723e-06|  0.00%|    def __init__(self, device: Any):
   219|       244|   0.00329113|  1.34882e-05|  0.01%|        self.idx = _get_device_index(device, optional=True)
(call)|       244|    0.0229852|  9.42017e-05|  0.04%|# /opt/conda/lib/python3.8/site-packages/torch/cuda/_utils.py:7 _get_device_index
   220|       244|  0.000984907|   4.0365e-06|  0.00%|        self.prev_idx = -1
   221|         0|            0|            0|  0.00%|
   222|       244|   0.00101995|  4.18014e-06|  0.00%|    def __enter__(self):
   223|       244|   0.00100398|  4.11467e-06|  0.00%|        if self.idx == -1:
   224|       244|  0.000896931|  3.67595e-06|  0.00%|            return
   225|         0|            0|            0|  0.00%|        self.prev_idx = torch.cuda.current_device()
   226|         0|            0|            0|  0.00%|        if self.prev_idx != self.idx:
   227|         0|            0|            0|  0.00%|            torch.cuda.set_device(self.idx)
   228|         0|            0|            0|  0.00%|        if not torch.jit.is_scripting():
   229|         0|            0|            0|  0.00%|            _lazy_init()
   230|         0|            0|            0|  0.00%|
   231|       244|   0.00126553|  5.18658e-06|  0.00%|    def __exit__(self, type: Any, value: Any, traceback: Any):
   232|       244|   0.00116777|  4.78596e-06|  0.00%|        if self.prev_idx != self.idx:
   233|         0|            0|            0|  0.00%|            torch.cuda.set_device(self.prev_idx)
   234|       244|  0.000925303|  3.79222e-06|  0.00%|        return False
   235|         0|            0|            0|  0.00%|
   236|         0|            0|            0|  0.00%|
   237|         0|            0|            0|  0.00%|class device_of(device):
   238|         0|            0|            0|  0.00%|    r"""Context-manager that changes the current device to that of given object.
   239|         0|            0|            0|  0.00%|
   240|         0|            0|            0|  0.00%|    You can use both tensors and storages as arguments. If a given object is
   241|         0|            0|            0|  0.00%|    not allocated on a GPU, this is a no-op.
   242|         0|            0|            0|  0.00%|
   243|         0|            0|            0|  0.00%|    Args:
   244|         0|            0|            0|  0.00%|        obj (Tensor or Storage): object allocated on the selected device.
   245|         0|            0|            0|  0.00%|    """
   246|         0|            0|            0|  0.00%|
   247|         0|            0|            0|  0.00%|    def __init__(self, obj):
   248|         0|            0|            0|  0.00%|        idx = obj.get_device() if obj.is_cuda else -1
   249|         0|            0|            0|  0.00%|        super(device_of, self).__init__(idx)
   250|         0|            0|            0|  0.00%|
   251|         0|            0|            0|  0.00%|
   252|         0|            0|            0|  0.00%|def set_device(device: _device_t) -> None:
   253|         0|            0|            0|  0.00%|    r"""Sets the current device.
   254|         0|            0|            0|  0.00%|
   255|         0|            0|            0|  0.00%|    Usage of this function is discouraged in favor of :any:`device`. In most
   256|         0|            0|            0|  0.00%|    cases it's better to use ``CUDA_VISIBLE_DEVICES`` environmental variable.
   257|         0|            0|            0|  0.00%|
   258|         0|            0|            0|  0.00%|    Args:
   259|         0|            0|            0|  0.00%|        device (torch.device or int): selected device. This function is a no-op
   260|         0|            0|            0|  0.00%|            if this argument is negative.
   261|         0|            0|            0|  0.00%|    """
   262|         0|            0|            0|  0.00%|    device = _get_device_index(device)
   263|         0|            0|            0|  0.00%|    if device >= 0:
   264|         0|            0|            0|  0.00%|        torch._C._cuda_setDevice(device)
   265|         0|            0|            0|  0.00%|
   266|         0|            0|            0|  0.00%|
   267|         0|            0|            0|  0.00%|def get_device_name(device: Optional[_device_t] = None) -> str:
   268|         0|            0|            0|  0.00%|    r"""Gets the name of a device.
   269|         0|            0|            0|  0.00%|
   270|         0|            0|            0|  0.00%|    Args:
   271|         0|            0|            0|  0.00%|        device (torch.device or int, optional): device for which to return the
   272|         0|            0|            0|  0.00%|            name. This function is a no-op if this argument is a negative
   273|         0|            0|            0|  0.00%|            integer. It uses the current device, given by :func:`~torch.cuda.current_device`,
   274|         0|            0|            0|  0.00%|            if :attr:`device` is ``None`` (default).
   275|         0|            0|            0|  0.00%|
   276|         0|            0|            0|  0.00%|    Returns:
   277|         0|            0|            0|  0.00%|        str: the name of the device
   278|         0|            0|            0|  0.00%|    """
   279|         0|            0|            0|  0.00%|    return get_device_properties(device).name
   280|         0|            0|            0|  0.00%|
   281|         0|            0|            0|  0.00%|
   282|         0|            0|            0|  0.00%|def get_device_capability(device: Optional[_device_t] = None) -> Tuple[int, int]:
   283|         0|            0|            0|  0.00%|    r"""Gets the cuda capability of a device.
   284|         0|            0|            0|  0.00%|
   285|         0|            0|            0|  0.00%|    Args:
   286|         0|            0|            0|  0.00%|        device (torch.device or int, optional): device for which to return the
   287|         0|            0|            0|  0.00%|            device capability. This function is a no-op if this argument is
   288|         0|            0|            0|  0.00%|            a negative integer. It uses the current device, given by
   289|         0|            0|            0|  0.00%|            :func:`~torch.cuda.current_device`, if :attr:`device` is ``None``
   290|         0|            0|            0|  0.00%|            (default).
   291|         0|            0|            0|  0.00%|
   292|         0|            0|            0|  0.00%|    Returns:
   293|         0|            0|            0|  0.00%|        tuple(int, int): the major and minor cuda capability of the device
   294|         0|            0|            0|  0.00%|    """
   295|         0|            0|            0|  0.00%|    prop = get_device_properties(device)
   296|         0|            0|            0|  0.00%|    return prop.major, prop.minor
   297|         0|            0|            0|  0.00%|
   298|         0|            0|            0|  0.00%|
   299|         0|            0|            0|  0.00%|def get_device_properties(device: _device_t) -> _CudaDeviceProperties:
   300|         0|            0|            0|  0.00%|    r"""Gets the properties of a device.
   301|         0|            0|            0|  0.00%|
   302|         0|            0|            0|  0.00%|    Args:
   303|         0|            0|            0|  0.00%|        device (torch.device or int or str): device for which to return the
   304|         0|            0|            0|  0.00%|            properties of the device.
   305|         0|            0|            0|  0.00%|
   306|         0|            0|            0|  0.00%|    Returns:
   307|         0|            0|            0|  0.00%|        _CudaDeviceProperties: the properties of the device
   308|         0|            0|            0|  0.00%|    """
   309|         0|            0|            0|  0.00%|    _lazy_init()  # will define _get_device_properties
   310|         0|            0|            0|  0.00%|    device = _get_device_index(device, optional=True)
   311|         0|            0|            0|  0.00%|    if device < 0 or device >= device_count():
   312|         0|            0|            0|  0.00%|        raise AssertionError("Invalid device id")
   313|         0|            0|            0|  0.00%|    return _get_device_properties(device)  # type: ignore[name-defined]
   314|         0|            0|            0|  0.00%|
   315|         0|            0|            0|  0.00%|def can_device_access_peer(device: _device_t, peer_device: _device_t) -> bool:
   316|         0|            0|            0|  0.00%|    r"""Checks if peer access between two devices is possible.
   317|         0|            0|            0|  0.00%|    """
   318|         0|            0|            0|  0.00%|    _lazy_init()
   319|         0|            0|            0|  0.00%|    device = _get_device_index(device, optional=True)
   320|         0|            0|            0|  0.00%|    peer_device = _get_device_index(peer_device)
   321|         0|            0|            0|  0.00%|    if device < 0 or device >= device_count():
   322|         0|            0|            0|  0.00%|        raise AssertionError("Invalid device id")
   323|         0|            0|            0|  0.00%|    if peer_device < 0 or peer_device >= device_count():
   324|         0|            0|            0|  0.00%|        raise AssertionError("Invalid peer device id")
   325|         0|            0|            0|  0.00%|    return torch._C._cuda_canDeviceAccessPeer(device, peer_device)
   326|         0|            0|            0|  0.00%|
   327|         0|            0|            0|  0.00%|
   328|         0|            0|            0|  0.00%|class StreamContext(object):
   329|         0|            0|            0|  0.00%|    r"""Context-manager that selects a given stream.
   330|         0|            0|            0|  0.00%|
   331|         0|            0|            0|  0.00%|    All CUDA kernels queued within its context will be enqueued on a selected
   332|         0|            0|            0|  0.00%|    stream.
   333|         0|            0|            0|  0.00%|
   334|         0|            0|            0|  0.00%|    Args:
   335|         0|            0|            0|  0.00%|        Stream (Stream): selected stream. This manager is a no-op if it's
   336|         0|            0|            0|  0.00%|            ``None``.
   337|         0|            0|            0|  0.00%|    .. note:: Streams are per-device.
   338|         0|            0|            0|  0.00%|    """
   339|         0|            0|            0|  0.00%|    cur_stream : Optional['torch.cuda.Stream']
   340|         0|            0|            0|  0.00%|
   341|         0|            0|            0|  0.00%|    def __init__(self, stream: Optional['torch.cuda.Stream']):
   342|         0|            0|            0|  0.00%|        self.stream = stream
   343|         0|            0|            0|  0.00%|        self.idx = _get_device_index(None, True)
   344|         0|            0|            0|  0.00%|        if not torch.jit.is_scripting():
   345|         0|            0|            0|  0.00%|            if self.idx is None:
   346|         0|            0|            0|  0.00%|                self.idx = -1
   347|         0|            0|            0|  0.00%|
   348|         0|            0|            0|  0.00%|        self.src_prev_stream = None if not torch.jit.is_scripting() else torch.cuda.default_stream(None)
   349|         0|            0|            0|  0.00%|        self.dst_prev_stream = None if not torch.jit.is_scripting() else torch.cuda.default_stream(None)
   350|         0|            0|            0|  0.00%|
   351|         0|            0|            0|  0.00%|    def __enter__(self):
   352|         0|            0|            0|  0.00%|        # Local cur_stream variable for type refinement
   353|         0|            0|            0|  0.00%|        cur_stream = self.stream
   354|         0|            0|            0|  0.00%|        # Return if stream is None or CUDA device not available
   355|         0|            0|            0|  0.00%|        if cur_stream is None or self.idx == -1:
   356|         0|            0|            0|  0.00%|            return
   357|         0|            0|            0|  0.00%|        self.src_prev_stream = torch.cuda.current_stream(None)
   358|         0|            0|            0|  0.00%|
   359|         0|            0|            0|  0.00%|        # If the stream is not on the current device, then
   360|         0|            0|            0|  0.00%|        # set the current stream on the device
   361|         0|            0|            0|  0.00%|        if self.src_prev_stream.device != cur_stream.device:
   362|         0|            0|            0|  0.00%|            with device(cur_stream.device):
   363|         0|            0|            0|  0.00%|                self.dst_prev_stream = torch.cuda.current_stream(cur_stream.device)
   364|         0|            0|            0|  0.00%|        torch.cuda.set_stream(cur_stream)
   365|         0|            0|            0|  0.00%|
   366|         0|            0|            0|  0.00%|    def __exit__(self, type: Any, value: Any, traceback: Any):
   367|         0|            0|            0|  0.00%|        # Local cur_stream variable for type refinement
   368|         0|            0|            0|  0.00%|        cur_stream = self.stream
   369|         0|            0|            0|  0.00%|        # If stream is None or no CUDA device available, return
   370|         0|            0|            0|  0.00%|        if cur_stream is None or self.idx == -1:
   371|         0|            0|            0|  0.00%|            return
   372|         0|            0|            0|  0.00%|
   373|         0|            0|            0|  0.00%|        # Reset the stream on the original device
   374|         0|            0|            0|  0.00%|        # and destination device
   375|         0|            0|            0|  0.00%|        if self.src_prev_stream.device != cur_stream.device:  # type: ignore[union-attr]
   376|         0|            0|            0|  0.00%|            torch.cuda.set_stream(self.dst_prev_stream)  # type: ignore[arg-type]
   377|         0|            0|            0|  0.00%|        torch.cuda.set_stream(self.src_prev_stream)  # type: ignore[arg-type]
   378|         0|            0|            0|  0.00%|
   379|         0|            0|            0|  0.00%|def stream(stream: Optional['torch.cuda.Stream']) -> StreamContext:
   380|         0|            0|            0|  0.00%|    r"""Wrapper around the Context-manager StreamContext that
   381|         0|            0|            0|  0.00%|    selects a given stream.
   382|         0|            0|            0|  0.00%|
   383|         0|            0|            0|  0.00%|    Arguments:
   384|         0|            0|            0|  0.00%|        stream (Stream): selected stream. This manager is a no-op if it's
   385|         0|            0|            0|  0.00%|            ``None``.
   386|         0|            0|            0|  0.00%|    ..Note:: In eager mode stream is of type Stream class while in JIT it is
   387|         0|            0|            0|  0.00%|    an object of the custom class ``torch.classes.cuda.Stream``.
   388|         0|            0|            0|  0.00%|    """
   389|         0|            0|            0|  0.00%|    return StreamContext(stream)
   390|         0|            0|            0|  0.00%|
   391|         0|            0|            0|  0.00%|def set_stream(stream: Stream):
   392|         0|            0|            0|  0.00%|    r"""Sets the current stream.This is a wrapper API to set the stream.
   393|         0|            0|            0|  0.00%|        Usage of this function is discouraged in favor of the ``stream``
   394|         0|            0|            0|  0.00%|        context manager.
   395|         0|            0|            0|  0.00%|
   396|         0|            0|            0|  0.00%|    Args:
   397|         0|            0|            0|  0.00%|        stream (Stream): selected stream. This function is a no-op
   398|         0|            0|            0|  0.00%|            if this argument is ``None``.
   399|         0|            0|            0|  0.00%|    """
   400|         0|            0|            0|  0.00%|    if stream is None:
   401|         0|            0|            0|  0.00%|        return
   402|         0|            0|            0|  0.00%|    torch._C._cuda_setStream(stream._cdata)
   403|         0|            0|            0|  0.00%|
   404|         0|            0|            0|  0.00%|def device_count() -> int:
   405|         0|            0|            0|  0.00%|    r"""Returns the number of GPUs available."""
   406|         0|            0|            0|  0.00%|    if is_available():
   407|         0|            0|            0|  0.00%|        return torch._C._cuda_getDeviceCount()
   408|         0|            0|            0|  0.00%|    else:
   409|         0|            0|            0|  0.00%|        return 0
   410|         0|            0|            0|  0.00%|
   411|         0|            0|            0|  0.00%|def get_arch_list() -> List[str]:
   412|         0|            0|            0|  0.00%|    r"""Returns list CUDA architectures this library was compiled for."""
   413|         0|            0|            0|  0.00%|    if not is_available():
   414|         0|            0|            0|  0.00%|        return []
   415|         0|            0|            0|  0.00%|    arch_flags = torch._C._cuda_getArchFlags()
   416|         0|            0|            0|  0.00%|    if arch_flags is None:
   417|         0|            0|            0|  0.00%|        return []
   418|         0|            0|            0|  0.00%|    return arch_flags.split()
   419|         0|            0|            0|  0.00%|
   420|         0|            0|            0|  0.00%|def get_gencode_flags() -> str:
   421|         0|            0|            0|  0.00%|    r"""Returns NVCC gencode flags this library was compiled with."""
   422|         0|            0|            0|  0.00%|    arch_list = get_arch_list()
   423|         0|            0|            0|  0.00%|    if len(arch_list) == 0:
   424|         0|            0|            0|  0.00%|        return ""
   425|         0|            0|            0|  0.00%|    arch_list_ = [arch.split("_") for arch in arch_list]
   426|         0|            0|            0|  0.00%|    return " ".join([f"-gencode compute=compute_{arch},code={kind}_{arch}" for (kind, arch) in arch_list_])
   427|         0|            0|            0|  0.00%|
   428|         0|            0|            0|  0.00%|
   429|         0|            0|            0|  0.00%|
   430|         0|            0|            0|  0.00%|def current_device() -> int:
   431|         0|            0|            0|  0.00%|    r"""Returns the index of a currently selected device."""
   432|         0|            0|            0|  0.00%|    _lazy_init()
   433|         0|            0|            0|  0.00%|    return torch._C._cuda_getDevice()
   434|         0|            0|            0|  0.00%|
   435|         0|            0|            0|  0.00%|
   436|         0|            0|            0|  0.00%|def synchronize(device: _device_t = None) -> None:
   437|         0|            0|            0|  0.00%|    r"""Waits for all kernels in all streams on a CUDA device to complete.
   438|         0|            0|            0|  0.00%|
   439|         0|            0|            0|  0.00%|    Args:
   440|         0|            0|            0|  0.00%|        device (torch.device or int, optional): device for which to synchronize.
   441|         0|            0|            0|  0.00%|            It uses the current device, given by :func:`~torch.cuda.current_device`,
   442|         0|            0|            0|  0.00%|            if :attr:`device` is ``None`` (default).
   443|         0|            0|            0|  0.00%|    """
   444|         0|            0|            0|  0.00%|    _lazy_init()
   445|         0|            0|            0|  0.00%|    with torch.cuda.device(device):
   446|         0|            0|            0|  0.00%|        return torch._C._cuda_synchronize()
   447|         0|            0|            0|  0.00%|
   448|         0|            0|            0|  0.00%|
   449|         0|            0|            0|  0.00%|def ipc_collect():
   450|         0|            0|            0|  0.00%|    r"""Force collects GPU memory after it has been released by CUDA IPC.
   451|         0|            0|            0|  0.00%|
   452|         0|            0|            0|  0.00%|    .. note::
   453|         0|            0|            0|  0.00%|        Checks if any sent CUDA tensors could be cleaned from the memory. Force
   454|         0|            0|            0|  0.00%|        closes shared memory file used for reference counting if there is no
   455|         0|            0|            0|  0.00%|        active counters. Useful when the producer process stopped actively sending
   456|         0|            0|            0|  0.00%|        tensors and want to release unused memory.
   457|         0|            0|            0|  0.00%|    """
   458|         0|            0|            0|  0.00%|    _lazy_init()
   459|         0|            0|            0|  0.00%|    return torch._C._cuda_ipc_collect()
   460|         0|            0|            0|  0.00%|
   461|         0|            0|            0|  0.00%|
   462|         0|            0|            0|  0.00%|def current_stream(device: Optional[_device_t] = None) -> Stream:
   463|         0|            0|            0|  0.00%|    r"""Returns the currently selected :class:`Stream` for a given device.
   464|         0|            0|            0|  0.00%|
   465|         0|            0|            0|  0.00%|    Args:
   466|         0|            0|            0|  0.00%|        device (torch.device or int, optional): selected device. Returns
   467|         0|            0|            0|  0.00%|            the currently selected :class:`Stream` for the current device, given
   468|         0|            0|            0|  0.00%|            by :func:`~torch.cuda.current_device`, if :attr:`device` is ``None``
   469|         0|            0|            0|  0.00%|            (default).
   470|         0|            0|            0|  0.00%|    """
   471|         0|            0|            0|  0.00%|    _lazy_init()
   472|         0|            0|            0|  0.00%|    return Stream(_cdata=torch._C._cuda_getCurrentStream(
   473|         0|            0|            0|  0.00%|        _get_device_index(device, optional=True)))
   474|         0|            0|            0|  0.00%|
   475|         0|            0|            0|  0.00%|
   476|         0|            0|            0|  0.00%|def default_stream(device: Optional[_device_t] = None) -> Stream:
   477|         0|            0|            0|  0.00%|    r"""Returns the default :class:`Stream` for a given device.
   478|         0|            0|            0|  0.00%|
   479|         0|            0|            0|  0.00%|    Args:
   480|         0|            0|            0|  0.00%|        device (torch.device or int, optional): selected device. Returns
   481|         0|            0|            0|  0.00%|            the default :class:`Stream` for the current device, given by
   482|         0|            0|            0|  0.00%|            :func:`~torch.cuda.current_device`, if :attr:`device` is ``None``
   483|         0|            0|            0|  0.00%|            (default).
   484|         0|            0|            0|  0.00%|    """
   485|         0|            0|            0|  0.00%|    _lazy_init()
   486|         0|            0|            0|  0.00%|    return Stream(_cdata=torch._C._cuda_getDefaultStream(
   487|         0|            0|            0|  0.00%|        _get_device_index(device, optional=True)))
   488|         0|            0|            0|  0.00%|
   489|         0|            0|            0|  0.00%|
   490|         0|            0|            0|  0.00%|def current_blas_handle():
   491|         0|            0|            0|  0.00%|    r"""Returns cublasHandle_t pointer to current cuBLAS handle"""
   492|         0|            0|            0|  0.00%|    _lazy_init()
   493|         0|            0|            0|  0.00%|    return torch._C._cuda_getCurrentBlasHandle()
   494|         0|            0|            0|  0.00%|
   495|         0|            0|            0|  0.00%|
   496|         0|            0|            0|  0.00%|from .memory import *  # noqa: F403
   497|         0|            0|            0|  0.00%|
   498|         0|            0|            0|  0.00%|
   499|         0|            0|            0|  0.00%|from .random import *  # noqa: F403
   500|         0|            0|            0|  0.00%|
   501|         0|            0|            0|  0.00%|################################################################################
   502|         0|            0|            0|  0.00%|# Define Storage and Tensor classes
   503|         0|            0|            0|  0.00%|################################################################################
   504|         0|            0|            0|  0.00%|
   505|         0|            0|            0|  0.00%|
   506|         0|            0|            0|  0.00%|from ..storage import _StorageBase
   507|         0|            0|            0|  0.00%|
   508|         0|            0|            0|  0.00%|
   509|         0|            0|            0|  0.00%|if not hasattr(torch._C, 'CudaDoubleStorageBase'):
   510|         0|            0|            0|  0.00%|    # Define dummy base classes
   511|         0|            0|            0|  0.00%|    for t in ['Double', 'Float', 'Long', 'Int', 'Short', 'Char', 'Byte', 'Half', 'Bool', 'BFloat16',
   512|         0|            0|            0|  0.00%|              'ComplexDouble', 'ComplexFloat']:
   513|         0|            0|            0|  0.00%|        storage_name = 'Cuda{0}StorageBase'.format(t)
   514|         0|            0|            0|  0.00%|        tensor_name = 'Cuda{0}TensorBase'.format(t)
   515|         0|            0|            0|  0.00%|
   516|         0|            0|            0|  0.00%|        torch._C.__dict__[storage_name] = _dummy_type(storage_name)
   517|         0|            0|            0|  0.00%|        torch._C.__dict__[tensor_name] = _dummy_type(tensor_name)
   518|         0|            0|            0|  0.00%|
   519|         0|            0|            0|  0.00%|    torch._C.__dict__['_CudaStreamBase'] = _dummy_type('CudaStreamBase')
   520|         0|            0|            0|  0.00%|    torch._C.__dict__['_CudaEventBase'] = _dummy_type('CudaEventBase')
   521|         0|            0|            0|  0.00%|
   522|         0|            0|            0|  0.00%|
   523|         0|            0|            0|  0.00%|@staticmethod  # type: ignore[misc]
   524|         0|            0|            0|  0.00%|def _lazy_new(cls, *args, **kwargs):
   525|         0|            0|            0|  0.00%|    _lazy_init()
   526|         0|            0|            0|  0.00%|    # We may need to call lazy init again if we are a forked child
   527|         0|            0|            0|  0.00%|    # del _CudaBase.__new__
   528|         0|            0|            0|  0.00%|    return super(_CudaBase, cls).__new__(cls, *args, **kwargs)
   529|         0|            0|            0|  0.00%|
   530|         0|            0|            0|  0.00%|
   531|         0|            0|            0|  0.00%|class _CudaBase(object):
   532|         0|            0|            0|  0.00%|    is_cuda = True
   533|         0|            0|            0|  0.00%|    is_sparse = False
   534|         0|            0|            0|  0.00%|
   535|         0|            0|            0|  0.00%|    def type(self, *args, **kwargs):
   536|         0|            0|            0|  0.00%|        # We could use a Protocol here to tell mypy that self has `get_device` method
   537|         0|            0|            0|  0.00%|        # but it is only available in the typing module on Python >= 3.8
   538|         0|            0|            0|  0.00%|        # or on typing_extensions module on Python >= 3.6
   539|         0|            0|            0|  0.00%|        with device(self.get_device()):  # type: ignore[attr-defined]
   540|         0|            0|            0|  0.00%|            return super(_CudaBase, self).type(*args, **kwargs)  # type: ignore[misc]
   541|         0|            0|            0|  0.00%|
   542|         0|            0|            0|  0.00%|    __new__ = _lazy_new
   543|         0|            0|            0|  0.00%|
   544|         0|            0|            0|  0.00%|
   545|         0|            0|            0|  0.00%|class DoubleStorage(_CudaBase, torch._C.CudaDoubleStorageBase, _StorageBase):
   546|         0|            0|            0|  0.00%|    pass
   547|         0|            0|            0|  0.00%|
   548|         0|            0|            0|  0.00%|
   549|         0|            0|            0|  0.00%|class FloatStorage(_CudaBase, torch._C.CudaFloatStorageBase, _StorageBase):
   550|         0|            0|            0|  0.00%|    pass
   551|         0|            0|            0|  0.00%|
   552|         0|            0|            0|  0.00%|
   553|         0|            0|            0|  0.00%|class LongStorage(_CudaBase, torch._C.CudaLongStorageBase, _StorageBase):
   554|         0|            0|            0|  0.00%|    pass
   555|         0|            0|            0|  0.00%|
   556|         0|            0|            0|  0.00%|
   557|         0|            0|            0|  0.00%|class IntStorage(_CudaBase, torch._C.CudaIntStorageBase, _StorageBase):
   558|         0|            0|            0|  0.00%|    pass
   559|         0|            0|            0|  0.00%|
   560|         0|            0|            0|  0.00%|
   561|         0|            0|            0|  0.00%|class ShortStorage(_CudaBase, torch._C.CudaShortStorageBase, _StorageBase):
   562|         0|            0|            0|  0.00%|    pass
   563|         0|            0|            0|  0.00%|
   564|         0|            0|            0|  0.00%|
   565|         0|            0|            0|  0.00%|class CharStorage(_CudaBase, torch._C.CudaCharStorageBase, _StorageBase):
   566|         0|            0|            0|  0.00%|    pass
   567|         0|            0|            0|  0.00%|
   568|         0|            0|            0|  0.00%|
   569|         0|            0|            0|  0.00%|class ByteStorage(_CudaBase, torch._C.CudaByteStorageBase, _StorageBase):
   570|         0|            0|            0|  0.00%|    pass
   571|         0|            0|            0|  0.00%|
   572|         0|            0|            0|  0.00%|
   573|         0|            0|            0|  0.00%|class HalfStorage(_CudaBase, torch._C.CudaHalfStorageBase, _StorageBase):
   574|         0|            0|            0|  0.00%|    pass
   575|         0|            0|            0|  0.00%|
   576|         0|            0|            0|  0.00%|
   577|         0|            0|            0|  0.00%|class BoolStorage(_CudaBase, torch._C.CudaBoolStorageBase, _StorageBase):
   578|         0|            0|            0|  0.00%|    pass
   579|         0|            0|            0|  0.00%|
   580|         0|            0|            0|  0.00%|
   581|         0|            0|            0|  0.00%|class BFloat16Storage(_CudaBase, torch._C.CudaBFloat16StorageBase, _StorageBase):
   582|         0|            0|            0|  0.00%|    pass
   583|         0|            0|            0|  0.00%|
   584|         0|            0|            0|  0.00%|class ComplexDoubleStorage(_CudaBase, torch._C.CudaComplexDoubleStorageBase, _StorageBase):
   585|         0|            0|            0|  0.00%|    pass
   586|         0|            0|            0|  0.00%|
   587|         0|            0|            0|  0.00%|
   588|         0|            0|            0|  0.00%|class ComplexFloatStorage(_CudaBase, torch._C.CudaComplexFloatStorageBase, _StorageBase):
   589|         0|            0|            0|  0.00%|    pass
   590|         0|            0|            0|  0.00%|
   591|         0|            0|            0|  0.00%|torch._storage_classes.add(DoubleStorage)
   592|         0|            0|            0|  0.00%|torch._storage_classes.add(FloatStorage)
   593|         0|            0|            0|  0.00%|torch._storage_classes.add(LongStorage)
   594|         0|            0|            0|  0.00%|torch._storage_classes.add(IntStorage)
   595|         0|            0|            0|  0.00%|torch._storage_classes.add(ShortStorage)
   596|         0|            0|            0|  0.00%|torch._storage_classes.add(CharStorage)
   597|         0|            0|            0|  0.00%|torch._storage_classes.add(ByteStorage)
   598|         0|            0|            0|  0.00%|torch._storage_classes.add(HalfStorage)
   599|         0|            0|            0|  0.00%|torch._storage_classes.add(BoolStorage)
   600|         0|            0|            0|  0.00%|torch._storage_classes.add(BFloat16Storage)
   601|         0|            0|            0|  0.00%|torch._storage_classes.add(ComplexDoubleStorage)
   602|         0|            0|            0|  0.00%|torch._storage_classes.add(ComplexFloatStorage)
   603|         0|            0|            0|  0.00%|
   604|         0|            0|            0|  0.00%|from . import sparse
   605|         0|            0|            0|  0.00%|from . import profiler
   606|         0|            0|            0|  0.00%|from . import nvtx
   607|         0|            0|            0|  0.00%|from . import amp
File: /opt/conda/lib/python3.8/multiprocessing/process.py
File duration: 0.0105557s (0.02%)
Line #|      Hits|         Time| Time per hit|      %|Source code
------+----------+-------------+-------------+-------+-----------
     1|         0|            0|            0|  0.00%|#
     2|         0|            0|            0|  0.00%|# Module providing the `Process` class which emulates `threading.Thread`
     3|         0|            0|            0|  0.00%|#
     4|         0|            0|            0|  0.00%|# multiprocessing/process.py
     5|         0|            0|            0|  0.00%|#
     6|         0|            0|            0|  0.00%|# Copyright (c) 2006-2008, R Oudkerk
     7|         0|            0|            0|  0.00%|# Licensed to PSF under a Contributor Agreement.
     8|         0|            0|            0|  0.00%|#
     9|         0|            0|            0|  0.00%|
    10|         0|            0|            0|  0.00%|__all__ = ['BaseProcess', 'current_process', 'active_children',
    11|         0|            0|            0|  0.00%|           'parent_process']
    12|         0|            0|            0|  0.00%|
    13|         0|            0|            0|  0.00%|#
    14|         0|            0|            0|  0.00%|# Imports
    15|         0|            0|            0|  0.00%|#
    16|         0|            0|            0|  0.00%|
    17|         0|            0|            0|  0.00%|import os
    18|         0|            0|            0|  0.00%|import sys
    19|         0|            0|            0|  0.00%|import signal
    20|         0|            0|            0|  0.00%|import itertools
    21|         0|            0|            0|  0.00%|import threading
    22|         0|            0|            0|  0.00%|from _weakrefset import WeakSet
    23|         0|            0|            0|  0.00%|
    24|         0|            0|            0|  0.00%|#
    25|         0|            0|            0|  0.00%|#
    26|         0|            0|            0|  0.00%|#
    27|         0|            0|            0|  0.00%|
    28|         0|            0|            0|  0.00%|try:
    29|         0|            0|            0|  0.00%|    ORIGINAL_DIR = os.path.abspath(os.getcwd())
    30|         0|            0|            0|  0.00%|except OSError:
    31|         0|            0|            0|  0.00%|    ORIGINAL_DIR = None
    32|         0|            0|            0|  0.00%|
    33|         0|            0|            0|  0.00%|#
    34|         0|            0|            0|  0.00%|# Public functions
    35|         0|            0|            0|  0.00%|#
    36|         0|            0|            0|  0.00%|
    37|       240|   0.00124049|  5.16872e-06|  0.00%|def current_process():
    38|         0|            0|            0|  0.00%|    '''
    39|         0|            0|            0|  0.00%|    Return process object representing the current process
    40|         0|            0|            0|  0.00%|    '''
    41|       240|   0.00108647|  4.52697e-06|  0.00%|    return _current_process
    42|         0|            0|            0|  0.00%|
    43|         0|            0|            0|  0.00%|def active_children():
    44|         0|            0|            0|  0.00%|    '''
    45|         0|            0|            0|  0.00%|    Return list of process objects corresponding to live child processes
    46|         0|            0|            0|  0.00%|    '''
    47|         0|            0|            0|  0.00%|    _cleanup()
    48|         0|            0|            0|  0.00%|    return list(_children)
    49|         0|            0|            0|  0.00%|
    50|         0|            0|            0|  0.00%|
    51|         0|            0|            0|  0.00%|def parent_process():
    52|         0|            0|            0|  0.00%|    '''
    53|         0|            0|            0|  0.00%|    Return process object representing the parent process
    54|         0|            0|            0|  0.00%|    '''
    55|         0|            0|            0|  0.00%|    return _parent_process
    56|         0|            0|            0|  0.00%|
    57|         0|            0|            0|  0.00%|#
    58|         0|            0|            0|  0.00%|#
    59|         0|            0|            0|  0.00%|#
    60|         0|            0|            0|  0.00%|
    61|         8|    4.673e-05|  5.84126e-06|  0.00%|def _cleanup():
    62|         0|            0|            0|  0.00%|    # check for processes which have finished
    63|        20|  0.000133276|   6.6638e-06|  0.00%|    for p in list(_children):
    64|        12|  0.000320673|  2.67227e-05|  0.00%|        if p._popen.poll() is not None:
(call)|        12|  0.000464201|  3.86834e-05|  0.00%|# /opt/conda/lib/python3.8/multiprocessing/popen_fork.py:24 poll
    65|         0|            0|            0|  0.00%|            _children.discard(p)
    66|         0|            0|            0|  0.00%|
    67|         0|            0|            0|  0.00%|#
    68|         0|            0|            0|  0.00%|# The `Process` class
    69|         0|            0|            0|  0.00%|#
    70|         0|            0|            0|  0.00%|
    71|         0|            0|            0|  0.00%|class BaseProcess(object):
    72|         0|            0|            0|  0.00%|    '''
    73|         0|            0|            0|  0.00%|    Process objects represent activity that is run in a separate process
    74|         0|            0|            0|  0.00%|
    75|         0|            0|            0|  0.00%|    The class is analogous to `threading.Thread`
    76|         0|            0|            0|  0.00%|    '''
    77|         0|            0|            0|  0.00%|    def _Popen(self):
    78|         0|            0|            0|  0.00%|        raise NotImplementedError
    79|         0|            0|            0|  0.00%|
    80|         8|  7.00951e-05|  8.76188e-06|  0.00%|    def __init__(self, group=None, target=None, name=None, args=(), kwargs={},
    81|         0|            0|            0|  0.00%|                 *, daemon=None):
    82|         8|  4.69685e-05|  5.87106e-06|  0.00%|        assert group is None, 'group argument must be None for now'
    83|         8|  6.00815e-05|  7.51019e-06|  0.00%|        count = next(_process_counter)
    84|         8|  5.31673e-05|  6.64592e-06|  0.00%|        self._identity = _current_process._identity + (count,)
    85|         8|  0.000220537|  2.75671e-05|  0.00%|        self._config = _current_process._config.copy()
    86|         8|  7.96318e-05|  9.95398e-06|  0.00%|        self._parent_pid = os.getpid()
    87|         8|   0.00016427|  2.05338e-05|  0.00%|        self._parent_name = _current_process.name
(call)|         8|  8.91685e-05|  1.11461e-05|  0.00%|# /opt/conda/lib/python3.8/multiprocessing/process.py:189 name
    88|         8|  3.95775e-05|  4.94719e-06|  0.00%|        self._popen = None
    89|         8|    4.673e-05|  5.84126e-06|  0.00%|        self._closed = False
    90|         8|  5.36442e-05|  6.70552e-06|  0.00%|        self._target = target
    91|         8|    4.673e-05|  5.84126e-06|  0.00%|        self._args = tuple(args)
    92|         8|  0.000155687|  1.94609e-05|  0.00%|        self._kwargs = dict(kwargs)
    93|        16|  0.000122547|   7.6592e-06|  0.00%|        self._name = name or type(self).__name__ + '-' + \
    94|        40|  0.000607491|  1.51873e-05|  0.00%|                     ':'.join(str(i) for i in self._identity)
(call)|        16|  0.000209093|  1.30683e-05|  0.00%|# /opt/conda/lib/python3.8/multiprocessing/process.py:94 <genexpr>
    95|         8|  3.55244e-05|  4.44055e-06|  0.00%|        if daemon is not None:
    96|         0|            0|            0|  0.00%|            self.daemon = daemon
    97|         8|  0.000222683|  2.78354e-05|  0.00%|        _dangling.add(self)
(call)|         8|  0.000236511|  2.95639e-05|  0.00%|# /opt/conda/lib/python3.8/_weakrefset.py:81 add
    98|         0|            0|            0|  0.00%|
    99|        32|  0.000225067|  7.03335e-06|  0.00%|    def _check_closed(self):
   100|        32|  0.000164509|   5.1409e-06|  0.00%|        if self._closed:
   101|         0|            0|            0|  0.00%|            raise ValueError("process object is closed")
   102|         0|            0|            0|  0.00%|
   103|         0|            0|            0|  0.00%|    def run(self):
   104|         0|            0|            0|  0.00%|        '''
   105|         0|            0|            0|  0.00%|        Method to be run in sub-process; can be overridden in sub-class
   106|         0|            0|            0|  0.00%|        '''
   107|         0|            0|            0|  0.00%|        if self._target:
   108|         0|            0|            0|  0.00%|            self._target(*self._args, **self._kwargs)
   109|         0|            0|            0|  0.00%|
   110|         8|   5.8651e-05|  7.33137e-06|  0.00%|    def start(self):
   111|         0|            0|            0|  0.00%|        '''
   112|         0|            0|            0|  0.00%|        Start child process
   113|         0|            0|            0|  0.00%|        '''
   114|         8|  0.000110149|  1.37687e-05|  0.00%|        self._check_closed()
(call)|         8|   0.00015521|  1.94013e-05|  0.00%|# /opt/conda/lib/python3.8/multiprocessing/process.py:99 _check_closed
   115|         8|  3.57628e-05|  4.47035e-06|  0.00%|        assert self._popen is None, 'cannot start a process twice'
   116|         8|  6.19888e-05|   7.7486e-06|  0.00%|        assert self._parent_pid == os.getpid(), \
   117|         0|            0|            0|  0.00%|               'can only start a process object created by current process'
   118|         8|  5.67436e-05|  7.09295e-06|  0.00%|        assert not _current_process._config.get('daemon'), \
   119|         0|            0|            0|  0.00%|               'daemonic processes are not allowed to have children'
   120|         8|  0.000132084|  1.65105e-05|  0.00%|        _cleanup()
(call)|         8|   0.00096488|   0.00012061|  0.00%|# /opt/conda/lib/python3.8/multiprocessing/process.py:61 _cleanup
   121|         8|  0.000475168|   5.9396e-05|  0.00%|        self._popen = self._Popen(self)
(call)|         8|      1.01805|     0.127256|  1.90%|# /opt/conda/lib/python3.8/multiprocessing/context.py:222 _Popen
   122|         8|  0.000249624|   3.1203e-05|  0.00%|        self._sentinel = self._popen.sentinel
   123|         0|            0|            0|  0.00%|        # Avoid a refcycle if the target function holds an indirect
   124|         0|            0|            0|  0.00%|        # reference to the process object (see bpo-30775)
   125|         8|   0.00031662|  3.95775e-05|  0.00%|        del self._target, self._args, self._kwargs
   126|         8|  0.000327826|  4.09782e-05|  0.00%|        _children.add(self)
   127|         0|            0|            0|  0.00%|
   128|         0|            0|            0|  0.00%|    def terminate(self):
   129|         0|            0|            0|  0.00%|        '''
   130|         0|            0|            0|  0.00%|        Terminate process; sends SIGTERM signal or uses TerminateProcess()
   131|         0|            0|            0|  0.00%|        '''
   132|         0|            0|            0|  0.00%|        self._check_closed()
   133|         0|            0|            0|  0.00%|        self._popen.terminate()
   134|         0|            0|            0|  0.00%|
   135|         0|            0|            0|  0.00%|    def kill(self):
   136|         0|            0|            0|  0.00%|        '''
   137|         0|            0|            0|  0.00%|        Terminate process; sends SIGKILL signal or uses TerminateProcess()
   138|         0|            0|            0|  0.00%|        '''
   139|         0|            0|            0|  0.00%|        self._check_closed()
   140|         0|            0|            0|  0.00%|        self._popen.kill()
   141|         0|            0|            0|  0.00%|
   142|         8|  8.41618e-05|  1.05202e-05|  0.00%|    def join(self, timeout=None):
   143|         0|            0|            0|  0.00%|        '''
   144|         0|            0|            0|  0.00%|        Wait until child process terminates
   145|         0|            0|            0|  0.00%|        '''
   146|         8|   0.00011754|  1.46925e-05|  0.00%|        self._check_closed()
(call)|         8|  7.39098e-05|  9.23872e-06|  0.00%|# /opt/conda/lib/python3.8/multiprocessing/process.py:99 _check_closed
   147|         8|  9.91821e-05|  1.23978e-05|  0.00%|        assert self._parent_pid == os.getpid(), 'can only join a child process'
   148|         8|  3.98159e-05|  4.97699e-06|  0.00%|        assert self._popen is not None, 'can only join a started process'
   149|         8|  0.000137568|  1.71959e-05|  0.00%|        res = self._popen.wait(timeout)
(call)|         8|     0.126284|    0.0157855|  0.24%|# /opt/conda/lib/python3.8/multiprocessing/popen_fork.py:40 wait
   150|         8|   3.6478e-05|  4.55976e-06|  0.00%|        if res is not None:
   151|         8|  6.55651e-05|  8.19564e-06|  0.00%|            _children.discard(self)
   152|         0|            0|            0|  0.00%|
   153|         8|  4.19617e-05|  5.24521e-06|  0.00%|    def is_alive(self):
   154|         0|            0|            0|  0.00%|        '''
   155|         0|            0|            0|  0.00%|        Return whether process is alive
   156|         0|            0|            0|  0.00%|        '''
   157|         8|   0.00010252|   1.2815e-05|  0.00%|        self._check_closed()
(call)|         8|  6.60419e-05|  8.25524e-06|  0.00%|# /opt/conda/lib/python3.8/multiprocessing/process.py:99 _check_closed
   158|         8|  3.57628e-05|  4.47035e-06|  0.00%|        if self is _current_process:
   159|         0|            0|            0|  0.00%|            return True
   160|         8|  5.91278e-05|  7.39098e-06|  0.00%|        assert self._parent_pid == os.getpid(), 'can only test a child process'
   161|         0|            0|            0|  0.00%|
   162|         8|  3.52859e-05|  4.41074e-06|  0.00%|        if self._popen is None:
   163|         0|            0|            0|  0.00%|            return False
   164|         0|            0|            0|  0.00%|
   165|         8|  0.000104666|  1.30832e-05|  0.00%|        returncode = self._popen.poll()
(call)|         8|  0.000109911|  1.37389e-05|  0.00%|# /opt/conda/lib/python3.8/multiprocessing/popen_fork.py:24 poll
   166|         8|  4.36306e-05|  5.45382e-06|  0.00%|        if returncode is None:
   167|         0|            0|            0|  0.00%|            return True
   168|         0|            0|            0|  0.00%|        else:
   169|         8|  5.07832e-05|  6.34789e-06|  0.00%|            _children.discard(self)
   170|         8|  3.31402e-05|  4.14252e-06|  0.00%|            return False
   171|         0|            0|            0|  0.00%|
   172|         0|            0|            0|  0.00%|    def close(self):
   173|         0|            0|            0|  0.00%|        '''
   174|         0|            0|            0|  0.00%|        Close the Process object.
   175|         0|            0|            0|  0.00%|
   176|         0|            0|            0|  0.00%|        This method releases resources held by the Process object.  It is
   177|         0|            0|            0|  0.00%|        an error to call this method if the child process is still running.
   178|         0|            0|            0|  0.00%|        '''
   179|         0|            0|            0|  0.00%|        if self._popen is not None:
   180|         0|            0|            0|  0.00%|            if self._popen.poll() is None:
   181|         0|            0|            0|  0.00%|                raise ValueError("Cannot close a process while it is still running. "
   182|         0|            0|            0|  0.00%|                                 "You should first call join() or terminate().")
   183|         0|            0|            0|  0.00%|            self._popen.close()
   184|         0|            0|            0|  0.00%|            self._popen = None
   185|         0|            0|            0|  0.00%|            del self._sentinel
   186|         0|            0|            0|  0.00%|            _children.discard(self)
   187|         0|            0|            0|  0.00%|        self._closed = True
   188|         0|            0|            0|  0.00%|
   189|         8|  5.26905e-05|  6.58631e-06|  0.00%|    @property
   190|         0|            0|            0|  0.00%|    def name(self):
   191|         8|   3.6478e-05|  4.55976e-06|  0.00%|        return self._name
   192|         0|            0|            0|  0.00%|
   193|         0|            0|            0|  0.00%|    @name.setter
   194|         0|            0|            0|  0.00%|    def name(self, name):
   195|         0|            0|            0|  0.00%|        assert isinstance(name, str), 'name must be a string'
   196|         0|            0|            0|  0.00%|        self._name = name
   197|         0|            0|            0|  0.00%|
   198|         0|            0|            0|  0.00%|    @property
   199|         0|            0|            0|  0.00%|    def daemon(self):
   200|         0|            0|            0|  0.00%|        '''
   201|         0|            0|            0|  0.00%|        Return whether process is a daemon
   202|         0|            0|            0|  0.00%|        '''
   203|         0|            0|            0|  0.00%|        return self._config.get('daemon', False)
   204|         0|            0|            0|  0.00%|
   205|         8|  4.76837e-05|  5.96046e-06|  0.00%|    @daemon.setter
   206|         0|            0|            0|  0.00%|    def daemon(self, daemonic):
   207|         0|            0|            0|  0.00%|        '''
   208|         0|            0|            0|  0.00%|        Set whether process is a daemon
   209|         0|            0|            0|  0.00%|        '''
   210|         8|  3.76701e-05|  4.70877e-06|  0.00%|        assert self._popen is None, 'process has already started'
   211|         8|   3.6478e-05|  4.55976e-06|  0.00%|        self._config['daemon'] = daemonic
   212|         0|            0|            0|  0.00%|
   213|       200|  0.000901222|  4.50611e-06|  0.00%|    @property
   214|         0|            0|            0|  0.00%|    def authkey(self):
   215|       200|  0.000980139|  4.90069e-06|  0.00%|        return self._config['authkey']
   216|         0|            0|            0|  0.00%|
   217|         0|            0|            0|  0.00%|    @authkey.setter
   218|         0|            0|            0|  0.00%|    def authkey(self, authkey):
   219|         0|            0|            0|  0.00%|        '''
   220|         0|            0|            0|  0.00%|        Set authorization key of process
   221|         0|            0|            0|  0.00%|        '''
   222|         0|            0|            0|  0.00%|        self._config['authkey'] = AuthenticationString(authkey)
   223|         0|            0|            0|  0.00%|
   224|         0|            0|            0|  0.00%|    @property
   225|         0|            0|            0|  0.00%|    def exitcode(self):
   226|         0|            0|            0|  0.00%|        '''
   227|         0|            0|            0|  0.00%|        Return exit code of process or `None` if it has yet to stop
   228|         0|            0|            0|  0.00%|        '''
   229|         0|            0|            0|  0.00%|        self._check_closed()
   230|         0|            0|            0|  0.00%|        if self._popen is None:
   231|         0|            0|            0|  0.00%|            return self._popen
   232|         0|            0|            0|  0.00%|        return self._popen.poll()
   233|         0|            0|            0|  0.00%|
   234|         8|  5.45979e-05|  6.82473e-06|  0.00%|    @property
   235|         0|            0|            0|  0.00%|    def ident(self):
   236|         0|            0|            0|  0.00%|        '''
   237|         0|            0|            0|  0.00%|        Return identifier (PID) of process or `None` if it has yet to start
   238|         0|            0|            0|  0.00%|        '''
   239|         8|   0.00013113|  1.63913e-05|  0.00%|        self._check_closed()
(call)|         8|  9.44138e-05|  1.18017e-05|  0.00%|# /opt/conda/lib/python3.8/multiprocessing/process.py:99 _check_closed
   240|         8|  5.29289e-05|  6.61612e-06|  0.00%|        if self is _current_process:
   241|         0|            0|            0|  0.00%|            return os.getpid()
   242|         0|            0|            0|  0.00%|        else:
   243|         8|  7.03335e-05|  8.79169e-06|  0.00%|            return self._popen and self._popen.pid
   244|         0|            0|            0|  0.00%|
   245|         0|            0|            0|  0.00%|    pid = ident
   246|         0|            0|            0|  0.00%|
   247|         0|            0|            0|  0.00%|    @property
   248|         0|            0|            0|  0.00%|    def sentinel(self):
   249|         0|            0|            0|  0.00%|        '''
   250|         0|            0|            0|  0.00%|        Return a file descriptor (Unix) or handle (Windows) suitable for
   251|         0|            0|            0|  0.00%|        waiting for process termination.
   252|         0|            0|            0|  0.00%|        '''
   253|         0|            0|            0|  0.00%|        self._check_closed()
   254|         0|            0|            0|  0.00%|        try:
   255|         0|            0|            0|  0.00%|            return self._sentinel
   256|         0|            0|            0|  0.00%|        except AttributeError:
   257|         0|            0|            0|  0.00%|            raise ValueError("process not started") from None
   258|         0|            0|            0|  0.00%|
   259|         0|            0|            0|  0.00%|    def __repr__(self):
   260|         0|            0|            0|  0.00%|        exitcode = None
   261|         0|            0|            0|  0.00%|        if self is _current_process:
   262|         0|            0|            0|  0.00%|            status = 'started'
   263|         0|            0|            0|  0.00%|        elif self._closed:
   264|         0|            0|            0|  0.00%|            status = 'closed'
   265|         0|            0|            0|  0.00%|        elif self._parent_pid != os.getpid():
   266|         0|            0|            0|  0.00%|            status = 'unknown'
   267|         0|            0|            0|  0.00%|        elif self._popen is None:
   268|         0|            0|            0|  0.00%|            status = 'initial'
   269|         0|            0|            0|  0.00%|        else:
   270|         0|            0|            0|  0.00%|            exitcode = self._popen.poll()
   271|         0|            0|            0|  0.00%|            if exitcode is not None:
   272|         0|            0|            0|  0.00%|                status = 'stopped'
   273|         0|            0|            0|  0.00%|            else:
   274|         0|            0|            0|  0.00%|                status = 'started'
   275|         0|            0|            0|  0.00%|
   276|         0|            0|            0|  0.00%|        info = [type(self).__name__, 'name=%r' % self._name]
   277|         0|            0|            0|  0.00%|        if self._popen is not None:
   278|         0|            0|            0|  0.00%|            info.append('pid=%s' % self._popen.pid)
   279|         0|            0|            0|  0.00%|        info.append('parent=%s' % self._parent_pid)
   280|         0|            0|            0|  0.00%|        info.append(status)
   281|         0|            0|            0|  0.00%|        if exitcode is not None:
   282|         0|            0|            0|  0.00%|            exitcode = _exitcode_to_name.get(exitcode, exitcode)
   283|         0|            0|            0|  0.00%|            info.append('exitcode=%s' % exitcode)
   284|         0|            0|            0|  0.00%|        if self.daemon:
   285|         0|            0|            0|  0.00%|            info.append('daemon')
   286|         0|            0|            0|  0.00%|        return '<%s>' % ' '.join(info)
   287|         0|            0|            0|  0.00%|
   288|         0|            0|            0|  0.00%|    ##
   289|         0|            0|            0|  0.00%|
   290|         0|            0|            0|  0.00%|    def _bootstrap(self, parent_sentinel=None):
   291|         0|            0|            0|  0.00%|        from . import util, context
   292|         0|            0|            0|  0.00%|        global _current_process, _parent_process, _process_counter, _children
   293|         0|            0|            0|  0.00%|
   294|         0|            0|            0|  0.00%|        try:
   295|         0|            0|            0|  0.00%|            if self._start_method is not None:
   296|         0|            0|            0|  0.00%|                context._force_start_method(self._start_method)
   297|         0|            0|            0|  0.00%|            _process_counter = itertools.count(1)
   298|         0|            0|            0|  0.00%|            _children = set()
   299|         0|            0|            0|  0.00%|            util._close_stdin()
   300|         0|            0|            0|  0.00%|            old_process = _current_process
   301|         0|            0|            0|  0.00%|            _current_process = self
   302|         0|            0|            0|  0.00%|            _parent_process = _ParentProcess(
   303|         0|            0|            0|  0.00%|                self._parent_name, self._parent_pid, parent_sentinel)
   304|         0|            0|            0|  0.00%|            if threading._HAVE_THREAD_NATIVE_ID:
   305|         0|            0|            0|  0.00%|                threading.main_thread()._set_native_id()
   306|         0|            0|            0|  0.00%|            try:
   307|         0|            0|            0|  0.00%|                util._finalizer_registry.clear()
   308|         0|            0|            0|  0.00%|                util._run_after_forkers()
   309|         0|            0|            0|  0.00%|            finally:
   310|         0|            0|            0|  0.00%|                # delay finalization of the old process object until after
   311|         0|            0|            0|  0.00%|                # _run_after_forkers() is executed
   312|         0|            0|            0|  0.00%|                del old_process
   313|         0|            0|            0|  0.00%|            util.info('child process calling self.run()')
   314|         0|            0|            0|  0.00%|            try:
   315|         0|            0|            0|  0.00%|                self.run()
   316|         0|            0|            0|  0.00%|                exitcode = 0
   317|         0|            0|            0|  0.00%|            finally:
   318|         0|            0|            0|  0.00%|                util._exit_function()
   319|         0|            0|            0|  0.00%|        except SystemExit as e:
   320|         0|            0|            0|  0.00%|            if not e.args:
   321|         0|            0|            0|  0.00%|                exitcode = 1
   322|         0|            0|            0|  0.00%|            elif isinstance(e.args[0], int):
   323|         0|            0|            0|  0.00%|                exitcode = e.args[0]
   324|         0|            0|            0|  0.00%|            else:
   325|         0|            0|            0|  0.00%|                sys.stderr.write(str(e.args[0]) + '\n')
   326|         0|            0|            0|  0.00%|                exitcode = 1
   327|         0|            0|            0|  0.00%|        except:
   328|         0|            0|            0|  0.00%|            exitcode = 1
   329|         0|            0|            0|  0.00%|            import traceback
   330|         0|            0|            0|  0.00%|            sys.stderr.write('Process %s:\n' % self.name)
   331|         0|            0|            0|  0.00%|            traceback.print_exc()
   332|         0|            0|            0|  0.00%|        finally:
   333|         0|            0|            0|  0.00%|            threading._shutdown()
   334|         0|            0|            0|  0.00%|            util.info('process exiting with exitcode %d' % exitcode)
   335|         0|            0|            0|  0.00%|            util._flush_std_streams()
   336|         0|            0|            0|  0.00%|
   337|         0|            0|            0|  0.00%|        return exitcode
   338|         0|            0|            0|  0.00%|
   339|         0|            0|            0|  0.00%|#
   340|         0|            0|            0|  0.00%|# We subclass bytes to avoid accidental transmission of auth keys over network
   341|         0|            0|            0|  0.00%|#
   342|         0|            0|            0|  0.00%|
   343|         0|            0|            0|  0.00%|class AuthenticationString(bytes):
   344|         0|            0|            0|  0.00%|    def __reduce__(self):
   345|         0|            0|            0|  0.00%|        from .context import get_spawning_popen
   346|         0|            0|            0|  0.00%|        if get_spawning_popen() is None:
   347|         0|            0|            0|  0.00%|            raise TypeError(
   348|         0|            0|            0|  0.00%|                'Pickling an AuthenticationString object is '
   349|         0|            0|            0|  0.00%|                'disallowed for security reasons'
   350|         0|            0|            0|  0.00%|                )
   351|         0|            0|            0|  0.00%|        return AuthenticationString, (bytes(self),)
   352|         0|            0|            0|  0.00%|
   353|         0|            0|            0|  0.00%|
   354|         0|            0|            0|  0.00%|#
   355|         0|            0|            0|  0.00%|# Create object representing the parent process
   356|         0|            0|            0|  0.00%|#
   357|         0|            0|            0|  0.00%|
   358|         0|            0|            0|  0.00%|class _ParentProcess(BaseProcess):
   359|         0|            0|            0|  0.00%|
   360|         0|            0|            0|  0.00%|    def __init__(self, name, pid, sentinel):
   361|         0|            0|            0|  0.00%|        self._identity = ()
   362|         0|            0|            0|  0.00%|        self._name = name
   363|         0|            0|            0|  0.00%|        self._pid = pid
   364|         0|            0|            0|  0.00%|        self._parent_pid = None
   365|         0|            0|            0|  0.00%|        self._popen = None
   366|         0|            0|            0|  0.00%|        self._closed = False
   367|         0|            0|            0|  0.00%|        self._sentinel = sentinel
   368|         0|            0|            0|  0.00%|        self._config = {}
   369|         0|            0|            0|  0.00%|
   370|         0|            0|            0|  0.00%|    def is_alive(self):
   371|         0|            0|            0|  0.00%|        from multiprocessing.connection import wait
   372|         0|            0|            0|  0.00%|        return not wait([self._sentinel], timeout=0)
   373|         0|            0|            0|  0.00%|
   374|         0|            0|            0|  0.00%|    @property
   375|         0|            0|            0|  0.00%|    def ident(self):
   376|         0|            0|            0|  0.00%|        return self._pid
   377|         0|            0|            0|  0.00%|
   378|         0|            0|            0|  0.00%|    def join(self, timeout=None):
   379|         0|            0|            0|  0.00%|        '''
   380|         0|            0|            0|  0.00%|        Wait until parent process terminates
   381|         0|            0|            0|  0.00%|        '''
   382|         0|            0|            0|  0.00%|        from multiprocessing.connection import wait
   383|         0|            0|            0|  0.00%|        wait([self._sentinel], timeout=timeout)
   384|         0|            0|            0|  0.00%|
   385|         0|            0|            0|  0.00%|    pid = ident
   386|         0|            0|            0|  0.00%|
   387|         0|            0|            0|  0.00%|#
   388|         0|            0|            0|  0.00%|# Create object representing the main process
   389|         0|            0|            0|  0.00%|#
   390|         0|            0|            0|  0.00%|
   391|         0|            0|            0|  0.00%|class _MainProcess(BaseProcess):
   392|         0|            0|            0|  0.00%|
   393|         0|            0|            0|  0.00%|    def __init__(self):
   394|         0|            0|            0|  0.00%|        self._identity = ()
   395|         0|            0|            0|  0.00%|        self._name = 'MainProcess'
   396|         0|            0|            0|  0.00%|        self._parent_pid = None
   397|         0|            0|            0|  0.00%|        self._popen = None
   398|         0|            0|            0|  0.00%|        self._closed = False
   399|         0|            0|            0|  0.00%|        self._config = {'authkey': AuthenticationString(os.urandom(32)),
   400|         0|            0|            0|  0.00%|                        'semprefix': '/mp'}
   401|         0|            0|            0|  0.00%|        # Note that some versions of FreeBSD only allow named
   402|         0|            0|            0|  0.00%|        # semaphores to have names of up to 14 characters.  Therefore
   403|         0|            0|            0|  0.00%|        # we choose a short prefix.
   404|         0|            0|            0|  0.00%|        #
   405|         0|            0|            0|  0.00%|        # On MacOSX in a sandbox it may be necessary to use a
   406|         0|            0|            0|  0.00%|        # different prefix -- see #19478.
   407|         0|            0|            0|  0.00%|        #
   408|         0|            0|            0|  0.00%|        # Everything in self._config will be inherited by descendant
   409|         0|            0|            0|  0.00%|        # processes.
   410|         0|            0|            0|  0.00%|
   411|         0|            0|            0|  0.00%|    def close(self):
   412|         0|            0|            0|  0.00%|        pass
   413|         0|            0|            0|  0.00%|
   414|         0|            0|            0|  0.00%|
   415|         0|            0|            0|  0.00%|_parent_process = None
   416|         0|            0|            0|  0.00%|_current_process = _MainProcess()
   417|         0|            0|            0|  0.00%|_process_counter = itertools.count(1)
   418|         0|            0|            0|  0.00%|_children = set()
   419|         0|            0|            0|  0.00%|del _MainProcess
   420|         0|            0|            0|  0.00%|
   421|         0|            0|            0|  0.00%|#
   422|         0|            0|            0|  0.00%|# Give names to some return codes
   423|         0|            0|            0|  0.00%|#
   424|         0|            0|            0|  0.00%|
   425|         0|            0|            0|  0.00%|_exitcode_to_name = {}
   426|         0|            0|            0|  0.00%|
   427|         0|            0|            0|  0.00%|for name, signum in list(signal.__dict__.items()):
   428|         0|            0|            0|  0.00%|    if name[:3]=='SIG' and '_' not in name:
   429|         0|            0|            0|  0.00%|        _exitcode_to_name[-signum] = f'-{name}'
   430|         0|            0|            0|  0.00%|
   431|         0|            0|            0|  0.00%|# For debug and leak testing
   432|         0|            0|            0|  0.00%|_dangling = WeakSet()
File: /opt/conda/lib/python3.8/tempfile.py
File duration: 0.00825858s (0.02%)
Line #|      Hits|         Time| Time per hit|      %|Source code
------+----------+-------------+-------------+-------+-----------
     1|         0|            0|            0|  0.00%|"""Temporary files.
     2|         0|            0|            0|  0.00%|
     3|         0|            0|            0|  0.00%|This module provides generic, low- and high-level interfaces for
     4|         0|            0|            0|  0.00%|creating temporary files and directories.  All of the interfaces
     5|         0|            0|            0|  0.00%|provided by this module can be used without fear of race conditions
     6|         0|            0|            0|  0.00%|except for 'mktemp'.  'mktemp' is subject to race conditions and
     7|         0|            0|            0|  0.00%|should not be used; it is provided for backward compatibility only.
     8|         0|            0|            0|  0.00%|
     9|         0|            0|            0|  0.00%|The default path names are returned as str.  If you supply bytes as
    10|         0|            0|            0|  0.00%|input, all return values will be in bytes.  Ex:
    11|         0|            0|            0|  0.00%|
    12|         0|            0|            0|  0.00%|    >>> tempfile.mkstemp()
    13|         0|            0|            0|  0.00%|    (4, '/tmp/tmptpu9nin8')
    14|         0|            0|            0|  0.00%|    >>> tempfile.mkdtemp(suffix=b'')
    15|         0|            0|            0|  0.00%|    b'/tmp/tmppbi8f0hy'
    16|         0|            0|            0|  0.00%|
    17|         0|            0|            0|  0.00%|This module also provides some data items to the user:
    18|         0|            0|            0|  0.00%|
    19|         0|            0|            0|  0.00%|  TMP_MAX  - maximum number of names that will be tried before
    20|         0|            0|            0|  0.00%|             giving up.
    21|         0|            0|            0|  0.00%|  tempdir  - If this is set to a string before the first use of
    22|         0|            0|            0|  0.00%|             any routine from this module, it will be considered as
    23|         0|            0|            0|  0.00%|             another candidate location to store temporary files.
    24|         0|            0|            0|  0.00%|"""
    25|         0|            0|            0|  0.00%|
    26|         0|            0|            0|  0.00%|__all__ = [
    27|         0|            0|            0|  0.00%|    "NamedTemporaryFile", "TemporaryFile", # high level safe interfaces
    28|         0|            0|            0|  0.00%|    "SpooledTemporaryFile", "TemporaryDirectory",
    29|         0|            0|            0|  0.00%|    "mkstemp", "mkdtemp",                  # low level safe interfaces
    30|         0|            0|            0|  0.00%|    "mktemp",                              # deprecated unsafe interface
    31|         0|            0|            0|  0.00%|    "TMP_MAX", "gettempprefix",            # constants
    32|         0|            0|            0|  0.00%|    "tempdir", "gettempdir",
    33|         0|            0|            0|  0.00%|    "gettempprefixb", "gettempdirb",
    34|         0|            0|            0|  0.00%|   ]
    35|         0|            0|            0|  0.00%|
    36|         0|            0|            0|  0.00%|
    37|         0|            0|            0|  0.00%|# Imports.
    38|         0|            0|            0|  0.00%|
    39|         0|            0|            0|  0.00%|import functools as _functools
    40|         0|            0|            0|  0.00%|import warnings as _warnings
    41|         0|            0|            0|  0.00%|import io as _io
    42|         0|            0|            0|  0.00%|import os as _os
    43|         0|            0|            0|  0.00%|import shutil as _shutil
    44|         0|            0|            0|  0.00%|import errno as _errno
    45|         0|            0|            0|  0.00%|from random import Random as _Random
    46|         0|            0|            0|  0.00%|import sys as _sys
    47|         0|            0|            0|  0.00%|import weakref as _weakref
    48|         0|            0|            0|  0.00%|import _thread
    49|         0|            0|            0|  0.00%|_allocate_lock = _thread.allocate_lock
    50|         0|            0|            0|  0.00%|
    51|         0|            0|            0|  0.00%|_text_openflags = _os.O_RDWR | _os.O_CREAT | _os.O_EXCL
    52|         0|            0|            0|  0.00%|if hasattr(_os, 'O_NOFOLLOW'):
    53|         0|            0|            0|  0.00%|    _text_openflags |= _os.O_NOFOLLOW
    54|         0|            0|            0|  0.00%|
    55|         0|            0|            0|  0.00%|_bin_openflags = _text_openflags
    56|         0|            0|            0|  0.00%|if hasattr(_os, 'O_BINARY'):
    57|         0|            0|            0|  0.00%|    _bin_openflags |= _os.O_BINARY
    58|         0|            0|            0|  0.00%|
    59|         0|            0|            0|  0.00%|if hasattr(_os, 'TMP_MAX'):
    60|         0|            0|            0|  0.00%|    TMP_MAX = _os.TMP_MAX
    61|         0|            0|            0|  0.00%|else:
    62|         0|            0|            0|  0.00%|    TMP_MAX = 10000
    63|         0|            0|            0|  0.00%|
    64|         0|            0|            0|  0.00%|# This variable _was_ unused for legacy reasons, see issue 10354.
    65|         0|            0|            0|  0.00%|# But as of 3.5 we actually use it at runtime so changing it would
    66|         0|            0|            0|  0.00%|# have a possibly desirable side effect...  But we do not want to support
    67|         0|            0|            0|  0.00%|# that as an API.  It is undocumented on purpose.  Do not depend on this.
    68|         0|            0|            0|  0.00%|template = "tmp"
    69|         0|            0|            0|  0.00%|
    70|         0|            0|            0|  0.00%|# Internal routines.
    71|         0|            0|            0|  0.00%|
    72|         0|            0|            0|  0.00%|_once_lock = _allocate_lock()
    73|         0|            0|            0|  0.00%|
    74|         0|            0|            0|  0.00%|
    75|         0|            0|            0|  0.00%|def _exists(fn):
    76|         0|            0|            0|  0.00%|    try:
    77|         0|            0|            0|  0.00%|        _os.lstat(fn)
    78|         0|            0|            0|  0.00%|    except OSError:
    79|         0|            0|            0|  0.00%|        return False
    80|         0|            0|            0|  0.00%|    else:
    81|         0|            0|            0|  0.00%|        return True
    82|         0|            0|            0|  0.00%|
    83|         0|            0|            0|  0.00%|
    84|         0|            0|            0|  0.00%|def _infer_return_type(*args):
    85|         0|            0|            0|  0.00%|    """Look at the type of all args and divine their implied return type."""
    86|         0|            0|            0|  0.00%|    return_type = None
    87|         0|            0|            0|  0.00%|    for arg in args:
    88|         0|            0|            0|  0.00%|        if arg is None:
    89|         0|            0|            0|  0.00%|            continue
    90|         0|            0|            0|  0.00%|        if isinstance(arg, bytes):
    91|         0|            0|            0|  0.00%|            if return_type is str:
    92|         0|            0|            0|  0.00%|                raise TypeError("Can't mix bytes and non-bytes in "
    93|         0|            0|            0|  0.00%|                                "path components.")
    94|         0|            0|            0|  0.00%|            return_type = bytes
    95|         0|            0|            0|  0.00%|        else:
    96|         0|            0|            0|  0.00%|            if return_type is bytes:
    97|         0|            0|            0|  0.00%|                raise TypeError("Can't mix bytes and non-bytes in "
    98|         0|            0|            0|  0.00%|                                "path components.")
    99|         0|            0|            0|  0.00%|            return_type = str
   100|         0|            0|            0|  0.00%|    if return_type is None:
   101|         0|            0|            0|  0.00%|        return str  # tempfile APIs return a str by default.
   102|         0|            0|            0|  0.00%|    return return_type
   103|         0|            0|            0|  0.00%|
   104|         0|            0|            0|  0.00%|
   105|         0|            0|            0|  0.00%|def _sanitize_params(prefix, suffix, dir):
   106|         0|            0|            0|  0.00%|    """Common parameter processing for most APIs in this module."""
   107|         0|            0|            0|  0.00%|    output_type = _infer_return_type(prefix, suffix, dir)
   108|         0|            0|            0|  0.00%|    if suffix is None:
   109|         0|            0|            0|  0.00%|        suffix = output_type()
   110|         0|            0|            0|  0.00%|    if prefix is None:
   111|         0|            0|            0|  0.00%|        if output_type is str:
   112|         0|            0|            0|  0.00%|            prefix = template
   113|         0|            0|            0|  0.00%|        else:
   114|         0|            0|            0|  0.00%|            prefix = _os.fsencode(template)
   115|         0|            0|            0|  0.00%|    if dir is None:
   116|         0|            0|            0|  0.00%|        if output_type is str:
   117|         0|            0|            0|  0.00%|            dir = gettempdir()
   118|         0|            0|            0|  0.00%|        else:
   119|         0|            0|            0|  0.00%|            dir = gettempdirb()
   120|         0|            0|            0|  0.00%|    return prefix, suffix, dir, output_type
   121|         0|            0|            0|  0.00%|
   122|         0|            0|            0|  0.00%|
   123|         0|            0|            0|  0.00%|class _RandomNameSequence:
   124|         0|            0|            0|  0.00%|    """An instance of _RandomNameSequence generates an endless
   125|         0|            0|            0|  0.00%|    sequence of unpredictable strings which can safely be incorporated
   126|         0|            0|            0|  0.00%|    into file names.  Each string is eight characters long.  Multiple
   127|         0|            0|            0|  0.00%|    threads can safely use the same instance at the same time.
   128|         0|            0|            0|  0.00%|
   129|         0|            0|            0|  0.00%|    _RandomNameSequence is an iterator."""
   130|         0|            0|            0|  0.00%|
   131|         0|            0|            0|  0.00%|    characters = "abcdefghijklmnopqrstuvwxyz0123456789_"
   132|         0|            0|            0|  0.00%|
   133|        40|  0.000201702|  5.04255e-06|  0.00%|    @property
   134|         0|            0|            0|  0.00%|    def rng(self):
   135|        40|  0.000302315|  7.55787e-06|  0.00%|        cur_pid = _os.getpid()
   136|        40|  0.000335455|  8.38637e-06|  0.00%|        if cur_pid != getattr(self, '_rng_pid', None):
   137|         0|            0|            0|  0.00%|            self._rng = _Random()
   138|         0|            0|            0|  0.00%|            self._rng_pid = cur_pid
   139|        40|  0.000301361|  7.53403e-06|  0.00%|        return self._rng
   140|         0|            0|            0|  0.00%|
   141|         0|            0|            0|  0.00%|    def __iter__(self):
   142|         0|            0|            0|  0.00%|        return self
   143|         0|            0|            0|  0.00%|
   144|        40|  0.000248194|  6.20484e-06|  0.00%|    def __next__(self):
   145|        40|  0.000276327|  6.90818e-06|  0.00%|        c = self.characters
   146|        40|  0.000895262|  2.23815e-05|  0.00%|        choose = self.rng.choice
(call)|        40|   0.00114083|  2.85208e-05|  0.00%|# /opt/conda/lib/python3.8/tempfile.py:133 rng
   147|       440|   0.00540566|  1.22856e-05|  0.01%|        letters = [choose(c) for dummy in range(8)]
(call)|       320|    0.0204699|  6.39684e-05|  0.04%|# /opt/conda/lib/python3.8/random.py:285 choice
(call)|        40|     0.025002|   0.00062505|  0.05%|# /opt/conda/lib/python3.8/tempfile.py:147 <listcomp>
   148|        40|  0.000292301|  7.30753e-06|  0.00%|        return ''.join(letters)
   149|         0|            0|            0|  0.00%|
   150|         0|            0|            0|  0.00%|def _candidate_tempdir_list():
   151|         0|            0|            0|  0.00%|    """Generate a list of candidate temporary directories which
   152|         0|            0|            0|  0.00%|    _get_default_tempdir will try."""
   153|         0|            0|            0|  0.00%|
   154|         0|            0|            0|  0.00%|    dirlist = []
   155|         0|            0|            0|  0.00%|
   156|         0|            0|            0|  0.00%|    # First, try the environment.
   157|         0|            0|            0|  0.00%|    for envname in 'TMPDIR', 'TEMP', 'TMP':
   158|         0|            0|            0|  0.00%|        dirname = _os.getenv(envname)
   159|         0|            0|            0|  0.00%|        if dirname: dirlist.append(dirname)
   160|         0|            0|            0|  0.00%|
   161|         0|            0|            0|  0.00%|    # Failing that, try OS-specific locations.
   162|         0|            0|            0|  0.00%|    if _os.name == 'nt':
   163|         0|            0|            0|  0.00%|        dirlist.extend([ _os.path.expanduser(r'~\AppData\Local\Temp'),
   164|         0|            0|            0|  0.00%|                         _os.path.expandvars(r'%SYSTEMROOT%\Temp'),
   165|         0|            0|            0|  0.00%|                         r'c:\temp', r'c:\tmp', r'\temp', r'\tmp' ])
   166|         0|            0|            0|  0.00%|    else:
   167|         0|            0|            0|  0.00%|        dirlist.extend([ '/tmp', '/var/tmp', '/usr/tmp' ])
   168|         0|            0|            0|  0.00%|
   169|         0|            0|            0|  0.00%|    # As a last resort, the current directory.
   170|         0|            0|            0|  0.00%|    try:
   171|         0|            0|            0|  0.00%|        dirlist.append(_os.getcwd())
   172|         0|            0|            0|  0.00%|    except (AttributeError, OSError):
   173|         0|            0|            0|  0.00%|        dirlist.append(_os.curdir)
   174|         0|            0|            0|  0.00%|
   175|         0|            0|            0|  0.00%|    return dirlist
   176|         0|            0|            0|  0.00%|
   177|         0|            0|            0|  0.00%|def _get_default_tempdir():
   178|         0|            0|            0|  0.00%|    """Calculate the default directory to use for temporary files.
   179|         0|            0|            0|  0.00%|    This routine should be called exactly once.
   180|         0|            0|            0|  0.00%|
   181|         0|            0|            0|  0.00%|    We determine whether or not a candidate temp dir is usable by
   182|         0|            0|            0|  0.00%|    trying to create and write to a file in that directory.  If this
   183|         0|            0|            0|  0.00%|    is successful, the test file is deleted.  To prevent denial of
   184|         0|            0|            0|  0.00%|    service, the name of the test file must be randomized."""
   185|         0|            0|            0|  0.00%|
   186|         0|            0|            0|  0.00%|    namer = _RandomNameSequence()
   187|         0|            0|            0|  0.00%|    dirlist = _candidate_tempdir_list()
   188|         0|            0|            0|  0.00%|
   189|         0|            0|            0|  0.00%|    for dir in dirlist:
   190|         0|            0|            0|  0.00%|        if dir != _os.curdir:
   191|         0|            0|            0|  0.00%|            dir = _os.path.abspath(dir)
   192|         0|            0|            0|  0.00%|        # Try only a few names per directory.
   193|         0|            0|            0|  0.00%|        for seq in range(100):
   194|         0|            0|            0|  0.00%|            name = next(namer)
   195|         0|            0|            0|  0.00%|            filename = _os.path.join(dir, name)
   196|         0|            0|            0|  0.00%|            try:
   197|         0|            0|            0|  0.00%|                fd = _os.open(filename, _bin_openflags, 0o600)
   198|         0|            0|            0|  0.00%|                try:
   199|         0|            0|            0|  0.00%|                    try:
   200|         0|            0|            0|  0.00%|                        with _io.open(fd, 'wb', closefd=False) as fp:
   201|         0|            0|            0|  0.00%|                            fp.write(b'blat')
   202|         0|            0|            0|  0.00%|                    finally:
   203|         0|            0|            0|  0.00%|                        _os.close(fd)
   204|         0|            0|            0|  0.00%|                finally:
   205|         0|            0|            0|  0.00%|                    _os.unlink(filename)
   206|         0|            0|            0|  0.00%|                return dir
   207|         0|            0|            0|  0.00%|            except FileExistsError:
   208|         0|            0|            0|  0.00%|                pass
   209|         0|            0|            0|  0.00%|            except PermissionError:
   210|         0|            0|            0|  0.00%|                # This exception is thrown when a directory with the chosen name
   211|         0|            0|            0|  0.00%|                # already exists on windows.
   212|         0|            0|            0|  0.00%|                if (_os.name == 'nt' and _os.path.isdir(dir) and
   213|         0|            0|            0|  0.00%|                    _os.access(dir, _os.W_OK)):
   214|         0|            0|            0|  0.00%|                    continue
   215|         0|            0|            0|  0.00%|                break   # no point trying more names in this directory
   216|         0|            0|            0|  0.00%|            except OSError:
   217|         0|            0|            0|  0.00%|                break   # no point trying more names in this directory
   218|         0|            0|            0|  0.00%|    raise FileNotFoundError(_errno.ENOENT,
   219|         0|            0|            0|  0.00%|                            "No usable temporary directory found in %s" %
   220|         0|            0|            0|  0.00%|                            dirlist)
   221|         0|            0|            0|  0.00%|
   222|         0|            0|            0|  0.00%|_name_sequence = None
   223|         0|            0|            0|  0.00%|
   224|         0|            0|            0|  0.00%|def _get_candidate_names():
   225|         0|            0|            0|  0.00%|    """Common setup sequence for all user-callable interfaces."""
   226|         0|            0|            0|  0.00%|
   227|         0|            0|            0|  0.00%|    global _name_sequence
   228|         0|            0|            0|  0.00%|    if _name_sequence is None:
   229|         0|            0|            0|  0.00%|        _once_lock.acquire()
   230|         0|            0|            0|  0.00%|        try:
   231|         0|            0|            0|  0.00%|            if _name_sequence is None:
   232|         0|            0|            0|  0.00%|                _name_sequence = _RandomNameSequence()
   233|         0|            0|            0|  0.00%|        finally:
   234|         0|            0|            0|  0.00%|            _once_lock.release()
   235|         0|            0|            0|  0.00%|    return _name_sequence
   236|         0|            0|            0|  0.00%|
   237|         0|            0|            0|  0.00%|
   238|         0|            0|            0|  0.00%|def _mkstemp_inner(dir, pre, suf, flags, output_type):
   239|         0|            0|            0|  0.00%|    """Code common to mkstemp, TemporaryFile, and NamedTemporaryFile."""
   240|         0|            0|            0|  0.00%|
   241|         0|            0|            0|  0.00%|    names = _get_candidate_names()
   242|         0|            0|            0|  0.00%|    if output_type is bytes:
   243|         0|            0|            0|  0.00%|        names = map(_os.fsencode, names)
   244|         0|            0|            0|  0.00%|
   245|         0|            0|            0|  0.00%|    for seq in range(TMP_MAX):
   246|         0|            0|            0|  0.00%|        name = next(names)
   247|         0|            0|            0|  0.00%|        file = _os.path.join(dir, pre + name + suf)
   248|         0|            0|            0|  0.00%|        _sys.audit("tempfile.mkstemp", file)
   249|         0|            0|            0|  0.00%|        try:
   250|         0|            0|            0|  0.00%|            fd = _os.open(file, flags, 0o600)
   251|         0|            0|            0|  0.00%|        except FileExistsError:
   252|         0|            0|            0|  0.00%|            continue    # try again
   253|         0|            0|            0|  0.00%|        except PermissionError:
   254|         0|            0|            0|  0.00%|            # This exception is thrown when a directory with the chosen name
   255|         0|            0|            0|  0.00%|            # already exists on windows.
   256|         0|            0|            0|  0.00%|            if (_os.name == 'nt' and _os.path.isdir(dir) and
   257|         0|            0|            0|  0.00%|                _os.access(dir, _os.W_OK)):
   258|         0|            0|            0|  0.00%|                continue
   259|         0|            0|            0|  0.00%|            else:
   260|         0|            0|            0|  0.00%|                raise
   261|         0|            0|            0|  0.00%|        return (fd, _os.path.abspath(file))
   262|         0|            0|            0|  0.00%|
   263|         0|            0|            0|  0.00%|    raise FileExistsError(_errno.EEXIST,
   264|         0|            0|            0|  0.00%|                          "No usable temporary file name found")
   265|         0|            0|            0|  0.00%|
   266|         0|            0|            0|  0.00%|
   267|         0|            0|            0|  0.00%|# User visible interfaces.
   268|         0|            0|            0|  0.00%|
   269|         0|            0|            0|  0.00%|def gettempprefix():
   270|         0|            0|            0|  0.00%|    """The default prefix for temporary directories."""
   271|         0|            0|            0|  0.00%|    return template
   272|         0|            0|            0|  0.00%|
   273|         0|            0|            0|  0.00%|def gettempprefixb():
   274|         0|            0|            0|  0.00%|    """The default prefix for temporary directories as bytes."""
   275|         0|            0|            0|  0.00%|    return _os.fsencode(gettempprefix())
   276|         0|            0|            0|  0.00%|
   277|         0|            0|            0|  0.00%|tempdir = None
   278|         0|            0|            0|  0.00%|
   279|         0|            0|            0|  0.00%|def gettempdir():
   280|         0|            0|            0|  0.00%|    """Accessor for tempfile.tempdir."""
   281|         0|            0|            0|  0.00%|    global tempdir
   282|         0|            0|            0|  0.00%|    if tempdir is None:
   283|         0|            0|            0|  0.00%|        _once_lock.acquire()
   284|         0|            0|            0|  0.00%|        try:
   285|         0|            0|            0|  0.00%|            if tempdir is None:
   286|         0|            0|            0|  0.00%|                tempdir = _get_default_tempdir()
   287|         0|            0|            0|  0.00%|        finally:
   288|         0|            0|            0|  0.00%|            _once_lock.release()
   289|         0|            0|            0|  0.00%|    return tempdir
   290|         0|            0|            0|  0.00%|
   291|         0|            0|            0|  0.00%|def gettempdirb():
   292|         0|            0|            0|  0.00%|    """A bytes version of tempfile.gettempdir()."""
   293|         0|            0|            0|  0.00%|    return _os.fsencode(gettempdir())
   294|         0|            0|            0|  0.00%|
   295|         0|            0|            0|  0.00%|def mkstemp(suffix=None, prefix=None, dir=None, text=False):
   296|         0|            0|            0|  0.00%|    """User-callable function to create and return a unique temporary
   297|         0|            0|            0|  0.00%|    file.  The return value is a pair (fd, name) where fd is the
   298|         0|            0|            0|  0.00%|    file descriptor returned by os.open, and name is the filename.
   299|         0|            0|            0|  0.00%|
   300|         0|            0|            0|  0.00%|    If 'suffix' is not None, the file name will end with that suffix,
   301|         0|            0|            0|  0.00%|    otherwise there will be no suffix.
   302|         0|            0|            0|  0.00%|
   303|         0|            0|            0|  0.00%|    If 'prefix' is not None, the file name will begin with that prefix,
   304|         0|            0|            0|  0.00%|    otherwise a default prefix is used.
   305|         0|            0|            0|  0.00%|
   306|         0|            0|            0|  0.00%|    If 'dir' is not None, the file will be created in that directory,
   307|         0|            0|            0|  0.00%|    otherwise a default directory is used.
   308|         0|            0|            0|  0.00%|
   309|         0|            0|            0|  0.00%|    If 'text' is specified and true, the file is opened in text
   310|         0|            0|            0|  0.00%|    mode.  Else (the default) the file is opened in binary mode.
   311|         0|            0|            0|  0.00%|
   312|         0|            0|            0|  0.00%|    If any of 'suffix', 'prefix' and 'dir' are not None, they must be the
   313|         0|            0|            0|  0.00%|    same type.  If they are bytes, the returned name will be bytes; str
   314|         0|            0|            0|  0.00%|    otherwise.
   315|         0|            0|            0|  0.00%|
   316|         0|            0|            0|  0.00%|    The file is readable and writable only by the creating user ID.
   317|         0|            0|            0|  0.00%|    If the operating system uses permission bits to indicate whether a
   318|         0|            0|            0|  0.00%|    file is executable, the file is executable by no one. The file
   319|         0|            0|            0|  0.00%|    descriptor is not inherited by children of this process.
   320|         0|            0|            0|  0.00%|
   321|         0|            0|            0|  0.00%|    Caller is responsible for deleting the file when done with it.
   322|         0|            0|            0|  0.00%|    """
   323|         0|            0|            0|  0.00%|
   324|         0|            0|            0|  0.00%|    prefix, suffix, dir, output_type = _sanitize_params(prefix, suffix, dir)
   325|         0|            0|            0|  0.00%|
   326|         0|            0|            0|  0.00%|    if text:
   327|         0|            0|            0|  0.00%|        flags = _text_openflags
   328|         0|            0|            0|  0.00%|    else:
   329|         0|            0|            0|  0.00%|        flags = _bin_openflags
   330|         0|            0|            0|  0.00%|
   331|         0|            0|            0|  0.00%|    return _mkstemp_inner(dir, prefix, suffix, flags, output_type)
   332|         0|            0|            0|  0.00%|
   333|         0|            0|            0|  0.00%|
   334|         0|            0|            0|  0.00%|def mkdtemp(suffix=None, prefix=None, dir=None):
   335|         0|            0|            0|  0.00%|    """User-callable function to create and return a unique temporary
   336|         0|            0|            0|  0.00%|    directory.  The return value is the pathname of the directory.
   337|         0|            0|            0|  0.00%|
   338|         0|            0|            0|  0.00%|    Arguments are as for mkstemp, except that the 'text' argument is
   339|         0|            0|            0|  0.00%|    not accepted.
   340|         0|            0|            0|  0.00%|
   341|         0|            0|            0|  0.00%|    The directory is readable, writable, and searchable only by the
   342|         0|            0|            0|  0.00%|    creating user.
   343|         0|            0|            0|  0.00%|
   344|         0|            0|            0|  0.00%|    Caller is responsible for deleting the directory when done with it.
   345|         0|            0|            0|  0.00%|    """
   346|         0|            0|            0|  0.00%|
   347|         0|            0|            0|  0.00%|    prefix, suffix, dir, output_type = _sanitize_params(prefix, suffix, dir)
   348|         0|            0|            0|  0.00%|
   349|         0|            0|            0|  0.00%|    names = _get_candidate_names()
   350|         0|            0|            0|  0.00%|    if output_type is bytes:
   351|         0|            0|            0|  0.00%|        names = map(_os.fsencode, names)
   352|         0|            0|            0|  0.00%|
   353|         0|            0|            0|  0.00%|    for seq in range(TMP_MAX):
   354|         0|            0|            0|  0.00%|        name = next(names)
   355|         0|            0|            0|  0.00%|        file = _os.path.join(dir, prefix + name + suffix)
   356|         0|            0|            0|  0.00%|        _sys.audit("tempfile.mkdtemp", file)
   357|         0|            0|            0|  0.00%|        try:
   358|         0|            0|            0|  0.00%|            _os.mkdir(file, 0o700)
   359|         0|            0|            0|  0.00%|        except FileExistsError:
   360|         0|            0|            0|  0.00%|            continue    # try again
   361|         0|            0|            0|  0.00%|        except PermissionError:
   362|         0|            0|            0|  0.00%|            # This exception is thrown when a directory with the chosen name
   363|         0|            0|            0|  0.00%|            # already exists on windows.
   364|         0|            0|            0|  0.00%|            if (_os.name == 'nt' and _os.path.isdir(dir) and
   365|         0|            0|            0|  0.00%|                _os.access(dir, _os.W_OK)):
   366|         0|            0|            0|  0.00%|                continue
   367|         0|            0|            0|  0.00%|            else:
   368|         0|            0|            0|  0.00%|                raise
   369|         0|            0|            0|  0.00%|        return file
   370|         0|            0|            0|  0.00%|
   371|         0|            0|            0|  0.00%|    raise FileExistsError(_errno.EEXIST,
   372|         0|            0|            0|  0.00%|                          "No usable temporary directory name found")
   373|         0|            0|            0|  0.00%|
   374|         0|            0|            0|  0.00%|def mktemp(suffix="", prefix=template, dir=None):
   375|         0|            0|            0|  0.00%|    """User-callable function to return a unique temporary file name.  The
   376|         0|            0|            0|  0.00%|    file is not created.
   377|         0|            0|            0|  0.00%|
   378|         0|            0|            0|  0.00%|    Arguments are similar to mkstemp, except that the 'text' argument is
   379|         0|            0|            0|  0.00%|    not accepted, and suffix=None, prefix=None and bytes file names are not
   380|         0|            0|            0|  0.00%|    supported.
   381|         0|            0|            0|  0.00%|
   382|         0|            0|            0|  0.00%|    THIS FUNCTION IS UNSAFE AND SHOULD NOT BE USED.  The file name may
   383|         0|            0|            0|  0.00%|    refer to a file that did not exist at some point, but by the time
   384|         0|            0|            0|  0.00%|    you get around to creating it, someone else may have beaten you to
   385|         0|            0|            0|  0.00%|    the punch.
   386|         0|            0|            0|  0.00%|    """
   387|         0|            0|            0|  0.00%|
   388|         0|            0|            0|  0.00%|##    from warnings import warn as _warn
   389|         0|            0|            0|  0.00%|##    _warn("mktemp is a potential security risk to your program",
   390|         0|            0|            0|  0.00%|##          RuntimeWarning, stacklevel=2)
   391|         0|            0|            0|  0.00%|
   392|         0|            0|            0|  0.00%|    if dir is None:
   393|         0|            0|            0|  0.00%|        dir = gettempdir()
   394|         0|            0|            0|  0.00%|
   395|         0|            0|            0|  0.00%|    names = _get_candidate_names()
   396|         0|            0|            0|  0.00%|    for seq in range(TMP_MAX):
   397|         0|            0|            0|  0.00%|        name = next(names)
   398|         0|            0|            0|  0.00%|        file = _os.path.join(dir, prefix + name + suffix)
   399|         0|            0|            0|  0.00%|        if not _exists(file):
   400|         0|            0|            0|  0.00%|            return file
   401|         0|            0|            0|  0.00%|
   402|         0|            0|            0|  0.00%|    raise FileExistsError(_errno.EEXIST,
   403|         0|            0|            0|  0.00%|                          "No usable temporary filename found")
   404|         0|            0|            0|  0.00%|
   405|         0|            0|            0|  0.00%|
   406|         0|            0|            0|  0.00%|class _TemporaryFileCloser:
   407|         0|            0|            0|  0.00%|    """A separate object allowing proper closing of a temporary file's
   408|         0|            0|            0|  0.00%|    underlying file object, without adding a __del__ method to the
   409|         0|            0|            0|  0.00%|    temporary file."""
   410|         0|            0|            0|  0.00%|
   411|         0|            0|            0|  0.00%|    file = None  # Set here since __del__ checks it
   412|         0|            0|            0|  0.00%|    close_called = False
   413|         0|            0|            0|  0.00%|
   414|         0|            0|            0|  0.00%|    def __init__(self, file, name, delete=True):
   415|         0|            0|            0|  0.00%|        self.file = file
   416|         0|            0|            0|  0.00%|        self.name = name
   417|         0|            0|            0|  0.00%|        self.delete = delete
   418|         0|            0|            0|  0.00%|
   419|         0|            0|            0|  0.00%|    # NT provides delete-on-close as a primitive, so we don't need
   420|         0|            0|            0|  0.00%|    # the wrapper to do anything special.  We still use it so that
   421|         0|            0|            0|  0.00%|    # file.name is useful (i.e. not "(fdopen)") with NamedTemporaryFile.
   422|         0|            0|            0|  0.00%|    if _os.name != 'nt':
   423|         0|            0|            0|  0.00%|        # Cache the unlinker so we don't get spurious errors at
   424|         0|            0|            0|  0.00%|        # shutdown when the module-level "os" is None'd out.  Note
   425|         0|            0|            0|  0.00%|        # that this must be referenced as self.unlink, because the
   426|         0|            0|            0|  0.00%|        # name TemporaryFileWrapper may also get None'd out before
   427|         0|            0|            0|  0.00%|        # __del__ is called.
   428|         0|            0|            0|  0.00%|
   429|         0|            0|            0|  0.00%|        def close(self, unlink=_os.unlink):
   430|         0|            0|            0|  0.00%|            if not self.close_called and self.file is not None:
   431|         0|            0|            0|  0.00%|                self.close_called = True
   432|         0|            0|            0|  0.00%|                try:
   433|         0|            0|            0|  0.00%|                    self.file.close()
   434|         0|            0|            0|  0.00%|                finally:
   435|         0|            0|            0|  0.00%|                    if self.delete:
   436|         0|            0|            0|  0.00%|                        unlink(self.name)
   437|         0|            0|            0|  0.00%|
   438|         0|            0|            0|  0.00%|        # Need to ensure the file is deleted on __del__
   439|         0|            0|            0|  0.00%|        def __del__(self):
   440|         0|            0|            0|  0.00%|            self.close()
   441|         0|            0|            0|  0.00%|
   442|         0|            0|            0|  0.00%|    else:
   443|         0|            0|            0|  0.00%|        def close(self):
   444|         0|            0|            0|  0.00%|            if not self.close_called:
   445|         0|            0|            0|  0.00%|                self.close_called = True
   446|         0|            0|            0|  0.00%|                self.file.close()
   447|         0|            0|            0|  0.00%|
   448|         0|            0|            0|  0.00%|
   449|         0|            0|            0|  0.00%|class _TemporaryFileWrapper:
   450|         0|            0|            0|  0.00%|    """Temporary file wrapper
   451|         0|            0|            0|  0.00%|
   452|         0|            0|            0|  0.00%|    This class provides a wrapper around files opened for
   453|         0|            0|            0|  0.00%|    temporary use.  In particular, it seeks to automatically
   454|         0|            0|            0|  0.00%|    remove the file when it is no longer needed.
   455|         0|            0|            0|  0.00%|    """
   456|         0|            0|            0|  0.00%|
   457|         0|            0|            0|  0.00%|    def __init__(self, file, name, delete=True):
   458|         0|            0|            0|  0.00%|        self.file = file
   459|         0|            0|            0|  0.00%|        self.name = name
   460|         0|            0|            0|  0.00%|        self.delete = delete
   461|         0|            0|            0|  0.00%|        self._closer = _TemporaryFileCloser(file, name, delete)
   462|         0|            0|            0|  0.00%|
   463|         0|            0|            0|  0.00%|    def __getattr__(self, name):
   464|         0|            0|            0|  0.00%|        # Attribute lookups are delegated to the underlying file
   465|         0|            0|            0|  0.00%|        # and cached for non-numeric results
   466|         0|            0|            0|  0.00%|        # (i.e. methods are cached, closed and friends are not)
   467|         0|            0|            0|  0.00%|        file = self.__dict__['file']
   468|         0|            0|            0|  0.00%|        a = getattr(file, name)
   469|         0|            0|            0|  0.00%|        if hasattr(a, '__call__'):
   470|         0|            0|            0|  0.00%|            func = a
   471|         0|            0|            0|  0.00%|            @_functools.wraps(func)
   472|         0|            0|            0|  0.00%|            def func_wrapper(*args, **kwargs):
   473|         0|            0|            0|  0.00%|                return func(*args, **kwargs)
   474|         0|            0|            0|  0.00%|            # Avoid closing the file as long as the wrapper is alive,
   475|         0|            0|            0|  0.00%|            # see issue #18879.
   476|         0|            0|            0|  0.00%|            func_wrapper._closer = self._closer
   477|         0|            0|            0|  0.00%|            a = func_wrapper
   478|         0|            0|            0|  0.00%|        if not isinstance(a, int):
   479|         0|            0|            0|  0.00%|            setattr(self, name, a)
   480|         0|            0|            0|  0.00%|        return a
   481|         0|            0|            0|  0.00%|
   482|         0|            0|            0|  0.00%|    # The underlying __enter__ method returns the wrong object
   483|         0|            0|            0|  0.00%|    # (self.file) so override it to return the wrapper
   484|         0|            0|            0|  0.00%|    def __enter__(self):
   485|         0|            0|            0|  0.00%|        self.file.__enter__()
   486|         0|            0|            0|  0.00%|        return self
   487|         0|            0|            0|  0.00%|
   488|         0|            0|            0|  0.00%|    # Need to trap __exit__ as well to ensure the file gets
   489|         0|            0|            0|  0.00%|    # deleted when used in a with statement
   490|         0|            0|            0|  0.00%|    def __exit__(self, exc, value, tb):
   491|         0|            0|            0|  0.00%|        result = self.file.__exit__(exc, value, tb)
   492|         0|            0|            0|  0.00%|        self.close()
   493|         0|            0|            0|  0.00%|        return result
   494|         0|            0|            0|  0.00%|
   495|         0|            0|            0|  0.00%|    def close(self):
   496|         0|            0|            0|  0.00%|        """
   497|         0|            0|            0|  0.00%|        Close the temporary file, possibly deleting it.
   498|         0|            0|            0|  0.00%|        """
   499|         0|            0|            0|  0.00%|        self._closer.close()
   500|         0|            0|            0|  0.00%|
   501|         0|            0|            0|  0.00%|    # iter() doesn't use __getattr__ to find the __iter__ method
   502|         0|            0|            0|  0.00%|    def __iter__(self):
   503|         0|            0|            0|  0.00%|        # Don't return iter(self.file), but yield from it to avoid closing
   504|         0|            0|            0|  0.00%|        # file as long as it's being used as iterator (see issue #23700).  We
   505|         0|            0|            0|  0.00%|        # can't use 'yield from' here because iter(file) returns the file
   506|         0|            0|            0|  0.00%|        # object itself, which has a close method, and thus the file would get
   507|         0|            0|            0|  0.00%|        # closed when the generator is finalized, due to PEP380 semantics.
   508|         0|            0|            0|  0.00%|        for line in self.file:
   509|         0|            0|            0|  0.00%|            yield line
   510|         0|            0|            0|  0.00%|
   511|         0|            0|            0|  0.00%|
   512|         0|            0|            0|  0.00%|def NamedTemporaryFile(mode='w+b', buffering=-1, encoding=None,
   513|         0|            0|            0|  0.00%|                       newline=None, suffix=None, prefix=None,
   514|         0|            0|            0|  0.00%|                       dir=None, delete=True, *, errors=None):
   515|         0|            0|            0|  0.00%|    """Create and return a temporary file.
   516|         0|            0|            0|  0.00%|    Arguments:
   517|         0|            0|            0|  0.00%|    'prefix', 'suffix', 'dir' -- as for mkstemp.
   518|         0|            0|            0|  0.00%|    'mode' -- the mode argument to io.open (default "w+b").
   519|         0|            0|            0|  0.00%|    'buffering' -- the buffer size argument to io.open (default -1).
   520|         0|            0|            0|  0.00%|    'encoding' -- the encoding argument to io.open (default None)
   521|         0|            0|            0|  0.00%|    'newline' -- the newline argument to io.open (default None)
   522|         0|            0|            0|  0.00%|    'delete' -- whether the file is deleted on close (default True).
   523|         0|            0|            0|  0.00%|    'errors' -- the errors argument to io.open (default None)
   524|         0|            0|            0|  0.00%|    The file is created as mkstemp() would do it.
   525|         0|            0|            0|  0.00%|
   526|         0|            0|            0|  0.00%|    Returns an object with a file-like interface; the name of the file
   527|         0|            0|            0|  0.00%|    is accessible as its 'name' attribute.  The file will be automatically
   528|         0|            0|            0|  0.00%|    deleted when it is closed unless the 'delete' argument is set to False.
   529|         0|            0|            0|  0.00%|    """
   530|         0|            0|            0|  0.00%|
   531|         0|            0|            0|  0.00%|    prefix, suffix, dir, output_type = _sanitize_params(prefix, suffix, dir)
   532|         0|            0|            0|  0.00%|
   533|         0|            0|            0|  0.00%|    flags = _bin_openflags
   534|         0|            0|            0|  0.00%|
   535|         0|            0|            0|  0.00%|    # Setting O_TEMPORARY in the flags causes the OS to delete
   536|         0|            0|            0|  0.00%|    # the file when it is closed.  This is only supported by Windows.
   537|         0|            0|            0|  0.00%|    if _os.name == 'nt' and delete:
   538|         0|            0|            0|  0.00%|        flags |= _os.O_TEMPORARY
   539|         0|            0|            0|  0.00%|
   540|         0|            0|            0|  0.00%|    (fd, name) = _mkstemp_inner(dir, prefix, suffix, flags, output_type)
   541|         0|            0|            0|  0.00%|    try:
   542|         0|            0|            0|  0.00%|        file = _io.open(fd, mode, buffering=buffering,
   543|         0|            0|            0|  0.00%|                        newline=newline, encoding=encoding, errors=errors)
   544|         0|            0|            0|  0.00%|
   545|         0|            0|            0|  0.00%|        return _TemporaryFileWrapper(file, name, delete)
   546|         0|            0|            0|  0.00%|    except BaseException:
   547|         0|            0|            0|  0.00%|        _os.unlink(name)
   548|         0|            0|            0|  0.00%|        _os.close(fd)
   549|         0|            0|            0|  0.00%|        raise
   550|         0|            0|            0|  0.00%|
   551|         0|            0|            0|  0.00%|if _os.name != 'posix' or _sys.platform == 'cygwin':
   552|         0|            0|            0|  0.00%|    # On non-POSIX and Cygwin systems, assume that we cannot unlink a file
   553|         0|            0|            0|  0.00%|    # while it is open.
   554|         0|            0|            0|  0.00%|    TemporaryFile = NamedTemporaryFile
   555|         0|            0|            0|  0.00%|
   556|         0|            0|            0|  0.00%|else:
   557|         0|            0|            0|  0.00%|    # Is the O_TMPFILE flag available and does it work?
   558|         0|            0|            0|  0.00%|    # The flag is set to False if os.open(dir, os.O_TMPFILE) raises an
   559|         0|            0|            0|  0.00%|    # IsADirectoryError exception
   560|         0|            0|            0|  0.00%|    _O_TMPFILE_WORKS = hasattr(_os, 'O_TMPFILE')
   561|         0|            0|            0|  0.00%|
   562|         0|            0|            0|  0.00%|    def TemporaryFile(mode='w+b', buffering=-1, encoding=None,
   563|         0|            0|            0|  0.00%|                      newline=None, suffix=None, prefix=None,
   564|         0|            0|            0|  0.00%|                      dir=None, *, errors=None):
   565|         0|            0|            0|  0.00%|        """Create and return a temporary file.
   566|         0|            0|            0|  0.00%|        Arguments:
   567|         0|            0|            0|  0.00%|        'prefix', 'suffix', 'dir' -- as for mkstemp.
   568|         0|            0|            0|  0.00%|        'mode' -- the mode argument to io.open (default "w+b").
   569|         0|            0|            0|  0.00%|        'buffering' -- the buffer size argument to io.open (default -1).
   570|         0|            0|            0|  0.00%|        'encoding' -- the encoding argument to io.open (default None)
   571|         0|            0|            0|  0.00%|        'newline' -- the newline argument to io.open (default None)
   572|         0|            0|            0|  0.00%|        'errors' -- the errors argument to io.open (default None)
   573|         0|            0|            0|  0.00%|        The file is created as mkstemp() would do it.
   574|         0|            0|            0|  0.00%|
   575|         0|            0|            0|  0.00%|        Returns an object with a file-like interface.  The file has no
   576|         0|            0|            0|  0.00%|        name, and will cease to exist when it is closed.
   577|         0|            0|            0|  0.00%|        """
   578|         0|            0|            0|  0.00%|        global _O_TMPFILE_WORKS
   579|         0|            0|            0|  0.00%|
   580|         0|            0|            0|  0.00%|        prefix, suffix, dir, output_type = _sanitize_params(prefix, suffix, dir)
   581|         0|            0|            0|  0.00%|
   582|         0|            0|            0|  0.00%|        flags = _bin_openflags
   583|         0|            0|            0|  0.00%|        if _O_TMPFILE_WORKS:
   584|         0|            0|            0|  0.00%|            try:
   585|         0|            0|            0|  0.00%|                flags2 = (flags | _os.O_TMPFILE) & ~_os.O_CREAT
   586|         0|            0|            0|  0.00%|                fd = _os.open(dir, flags2, 0o600)
   587|         0|            0|            0|  0.00%|            except IsADirectoryError:
   588|         0|            0|            0|  0.00%|                # Linux kernel older than 3.11 ignores the O_TMPFILE flag:
   589|         0|            0|            0|  0.00%|                # O_TMPFILE is read as O_DIRECTORY. Trying to open a directory
   590|         0|            0|            0|  0.00%|                # with O_RDWR|O_DIRECTORY fails with IsADirectoryError, a
   591|         0|            0|            0|  0.00%|                # directory cannot be open to write. Set flag to False to not
   592|         0|            0|            0|  0.00%|                # try again.
   593|         0|            0|            0|  0.00%|                _O_TMPFILE_WORKS = False
   594|         0|            0|            0|  0.00%|            except OSError:
   595|         0|            0|            0|  0.00%|                # The filesystem of the directory does not support O_TMPFILE.
   596|         0|            0|            0|  0.00%|                # For example, OSError(95, 'Operation not supported').
   597|         0|            0|            0|  0.00%|                #
   598|         0|            0|            0|  0.00%|                # On Linux kernel older than 3.11, trying to open a regular
   599|         0|            0|            0|  0.00%|                # file (or a symbolic link to a regular file) with O_TMPFILE
   600|         0|            0|            0|  0.00%|                # fails with NotADirectoryError, because O_TMPFILE is read as
   601|         0|            0|            0|  0.00%|                # O_DIRECTORY.
   602|         0|            0|            0|  0.00%|                pass
   603|         0|            0|            0|  0.00%|            else:
   604|         0|            0|            0|  0.00%|                try:
   605|         0|            0|            0|  0.00%|                    return _io.open(fd, mode, buffering=buffering,
   606|         0|            0|            0|  0.00%|                                    newline=newline, encoding=encoding,
   607|         0|            0|            0|  0.00%|                                    errors=errors)
   608|         0|            0|            0|  0.00%|                except:
   609|         0|            0|            0|  0.00%|                    _os.close(fd)
   610|         0|            0|            0|  0.00%|                    raise
   611|         0|            0|            0|  0.00%|            # Fallback to _mkstemp_inner().
   612|         0|            0|            0|  0.00%|
   613|         0|            0|            0|  0.00%|        (fd, name) = _mkstemp_inner(dir, prefix, suffix, flags, output_type)
   614|         0|            0|            0|  0.00%|        try:
   615|         0|            0|            0|  0.00%|            _os.unlink(name)
   616|         0|            0|            0|  0.00%|            return _io.open(fd, mode, buffering=buffering,
   617|         0|            0|            0|  0.00%|                            newline=newline, encoding=encoding, errors=errors)
   618|         0|            0|            0|  0.00%|        except:
   619|         0|            0|            0|  0.00%|            _os.close(fd)
   620|         0|            0|            0|  0.00%|            raise
   621|         0|            0|            0|  0.00%|
   622|         0|            0|            0|  0.00%|class SpooledTemporaryFile:
   623|         0|            0|            0|  0.00%|    """Temporary file wrapper, specialized to switch from BytesIO
   624|         0|            0|            0|  0.00%|    or StringIO to a real file when it exceeds a certain size or
   625|         0|            0|            0|  0.00%|    when a fileno is needed.
   626|         0|            0|            0|  0.00%|    """
   627|         0|            0|            0|  0.00%|    _rolled = False
   628|         0|            0|            0|  0.00%|
   629|         0|            0|            0|  0.00%|    def __init__(self, max_size=0, mode='w+b', buffering=-1,
   630|         0|            0|            0|  0.00%|                 encoding=None, newline=None,
   631|         0|            0|            0|  0.00%|                 suffix=None, prefix=None, dir=None, *, errors=None):
   632|         0|            0|            0|  0.00%|        if 'b' in mode:
   633|         0|            0|            0|  0.00%|            self._file = _io.BytesIO()
   634|         0|            0|            0|  0.00%|        else:
   635|         0|            0|            0|  0.00%|            self._file = _io.TextIOWrapper(_io.BytesIO(),
   636|         0|            0|            0|  0.00%|                            encoding=encoding, errors=errors,
   637|         0|            0|            0|  0.00%|                            newline=newline)
   638|         0|            0|            0|  0.00%|        self._max_size = max_size
   639|         0|            0|            0|  0.00%|        self._rolled = False
   640|         0|            0|            0|  0.00%|        self._TemporaryFileArgs = {'mode': mode, 'buffering': buffering,
   641|         0|            0|            0|  0.00%|                                   'suffix': suffix, 'prefix': prefix,
   642|         0|            0|            0|  0.00%|                                   'encoding': encoding, 'newline': newline,
   643|         0|            0|            0|  0.00%|                                   'dir': dir, 'errors': errors}
   644|         0|            0|            0|  0.00%|
   645|         0|            0|            0|  0.00%|    def _check(self, file):
   646|         0|            0|            0|  0.00%|        if self._rolled: return
   647|         0|            0|            0|  0.00%|        max_size = self._max_size
   648|         0|            0|            0|  0.00%|        if max_size and file.tell() > max_size:
   649|         0|            0|            0|  0.00%|            self.rollover()
   650|         0|            0|            0|  0.00%|
   651|         0|            0|            0|  0.00%|    def rollover(self):
   652|         0|            0|            0|  0.00%|        if self._rolled: return
   653|         0|            0|            0|  0.00%|        file = self._file
   654|         0|            0|            0|  0.00%|        newfile = self._file = TemporaryFile(**self._TemporaryFileArgs)
   655|         0|            0|            0|  0.00%|        del self._TemporaryFileArgs
   656|         0|            0|            0|  0.00%|
   657|         0|            0|            0|  0.00%|        pos = file.tell()
   658|         0|            0|            0|  0.00%|        if hasattr(newfile, 'buffer'):
   659|         0|            0|            0|  0.00%|            newfile.buffer.write(file.detach().getvalue())
   660|         0|            0|            0|  0.00%|        else:
   661|         0|            0|            0|  0.00%|            newfile.write(file.getvalue())
   662|         0|            0|            0|  0.00%|        newfile.seek(pos, 0)
   663|         0|            0|            0|  0.00%|
   664|         0|            0|            0|  0.00%|        self._rolled = True
   665|         0|            0|            0|  0.00%|
   666|         0|            0|            0|  0.00%|    # The method caching trick from NamedTemporaryFile
   667|         0|            0|            0|  0.00%|    # won't work here, because _file may change from a
   668|         0|            0|            0|  0.00%|    # BytesIO/StringIO instance to a real file. So we list
   669|         0|            0|            0|  0.00%|    # all the methods directly.
   670|         0|            0|            0|  0.00%|
   671|         0|            0|            0|  0.00%|    # Context management protocol
   672|         0|            0|            0|  0.00%|    def __enter__(self):
   673|         0|            0|            0|  0.00%|        if self._file.closed:
   674|         0|            0|            0|  0.00%|            raise ValueError("Cannot enter context with closed file")
   675|         0|            0|            0|  0.00%|        return self
   676|         0|            0|            0|  0.00%|
   677|         0|            0|            0|  0.00%|    def __exit__(self, exc, value, tb):
   678|         0|            0|            0|  0.00%|        self._file.close()
   679|         0|            0|            0|  0.00%|
   680|         0|            0|            0|  0.00%|    # file protocol
   681|         0|            0|            0|  0.00%|    def __iter__(self):
   682|         0|            0|            0|  0.00%|        return self._file.__iter__()
   683|         0|            0|            0|  0.00%|
   684|         0|            0|            0|  0.00%|    def close(self):
   685|         0|            0|            0|  0.00%|        self._file.close()
   686|         0|            0|            0|  0.00%|
   687|         0|            0|            0|  0.00%|    @property
   688|         0|            0|            0|  0.00%|    def closed(self):
   689|         0|            0|            0|  0.00%|        return self._file.closed
   690|         0|            0|            0|  0.00%|
   691|         0|            0|            0|  0.00%|    @property
   692|         0|            0|            0|  0.00%|    def encoding(self):
   693|         0|            0|            0|  0.00%|        return self._file.encoding
   694|         0|            0|            0|  0.00%|
   695|         0|            0|            0|  0.00%|    @property
   696|         0|            0|            0|  0.00%|    def errors(self):
   697|         0|            0|            0|  0.00%|        return self._file.errors
   698|         0|            0|            0|  0.00%|
   699|         0|            0|            0|  0.00%|    def fileno(self):
   700|         0|            0|            0|  0.00%|        self.rollover()
   701|         0|            0|            0|  0.00%|        return self._file.fileno()
   702|         0|            0|            0|  0.00%|
   703|         0|            0|            0|  0.00%|    def flush(self):
   704|         0|            0|            0|  0.00%|        self._file.flush()
   705|         0|            0|            0|  0.00%|
   706|         0|            0|            0|  0.00%|    def isatty(self):
   707|         0|            0|            0|  0.00%|        return self._file.isatty()
   708|         0|            0|            0|  0.00%|
   709|         0|            0|            0|  0.00%|    @property
   710|         0|            0|            0|  0.00%|    def mode(self):
   711|         0|            0|            0|  0.00%|        try:
   712|         0|            0|            0|  0.00%|            return self._file.mode
   713|         0|            0|            0|  0.00%|        except AttributeError:
   714|         0|            0|            0|  0.00%|            return self._TemporaryFileArgs['mode']
   715|         0|            0|            0|  0.00%|
   716|         0|            0|            0|  0.00%|    @property
   717|         0|            0|            0|  0.00%|    def name(self):
   718|         0|            0|            0|  0.00%|        try:
   719|         0|            0|            0|  0.00%|            return self._file.name
   720|         0|            0|            0|  0.00%|        except AttributeError:
   721|         0|            0|            0|  0.00%|            return None
   722|         0|            0|            0|  0.00%|
   723|         0|            0|            0|  0.00%|    @property
   724|         0|            0|            0|  0.00%|    def newlines(self):
   725|         0|            0|            0|  0.00%|        return self._file.newlines
   726|         0|            0|            0|  0.00%|
   727|         0|            0|            0|  0.00%|    def read(self, *args):
   728|         0|            0|            0|  0.00%|        return self._file.read(*args)
   729|         0|            0|            0|  0.00%|
   730|         0|            0|            0|  0.00%|    def readline(self, *args):
   731|         0|            0|            0|  0.00%|        return self._file.readline(*args)
   732|         0|            0|            0|  0.00%|
   733|         0|            0|            0|  0.00%|    def readlines(self, *args):
   734|         0|            0|            0|  0.00%|        return self._file.readlines(*args)
   735|         0|            0|            0|  0.00%|
   736|         0|            0|            0|  0.00%|    def seek(self, *args):
   737|         0|            0|            0|  0.00%|        return self._file.seek(*args)
   738|         0|            0|            0|  0.00%|
   739|         0|            0|            0|  0.00%|    @property
   740|         0|            0|            0|  0.00%|    def softspace(self):
   741|         0|            0|            0|  0.00%|        return self._file.softspace
   742|         0|            0|            0|  0.00%|
   743|         0|            0|            0|  0.00%|    def tell(self):
   744|         0|            0|            0|  0.00%|        return self._file.tell()
   745|         0|            0|            0|  0.00%|
   746|         0|            0|            0|  0.00%|    def truncate(self, size=None):
   747|         0|            0|            0|  0.00%|        if size is None:
   748|         0|            0|            0|  0.00%|            self._file.truncate()
   749|         0|            0|            0|  0.00%|        else:
   750|         0|            0|            0|  0.00%|            if size > self._max_size:
   751|         0|            0|            0|  0.00%|                self.rollover()
   752|         0|            0|            0|  0.00%|            self._file.truncate(size)
   753|         0|            0|            0|  0.00%|
   754|         0|            0|            0|  0.00%|    def write(self, s):
   755|         0|            0|            0|  0.00%|        file = self._file
   756|         0|            0|            0|  0.00%|        rv = file.write(s)
   757|         0|            0|            0|  0.00%|        self._check(file)
   758|         0|            0|            0|  0.00%|        return rv
   759|         0|            0|            0|  0.00%|
   760|         0|            0|            0|  0.00%|    def writelines(self, iterable):
   761|         0|            0|            0|  0.00%|        file = self._file
   762|         0|            0|            0|  0.00%|        rv = file.writelines(iterable)
   763|         0|            0|            0|  0.00%|        self._check(file)
   764|         0|            0|            0|  0.00%|        return rv
   765|         0|            0|            0|  0.00%|
   766|         0|            0|            0|  0.00%|
   767|         0|            0|            0|  0.00%|class TemporaryDirectory(object):
   768|         0|            0|            0|  0.00%|    """Create and return a temporary directory.  This has the same
   769|         0|            0|            0|  0.00%|    behavior as mkdtemp but can be used as a context manager.  For
   770|         0|            0|            0|  0.00%|    example:
   771|         0|            0|            0|  0.00%|
   772|         0|            0|            0|  0.00%|        with TemporaryDirectory() as tmpdir:
   773|         0|            0|            0|  0.00%|            ...
   774|         0|            0|            0|  0.00%|
   775|         0|            0|            0|  0.00%|    Upon exiting the context, the directory and everything contained
   776|         0|            0|            0|  0.00%|    in it are removed.
   777|         0|            0|            0|  0.00%|    """
   778|         0|            0|            0|  0.00%|
   779|         0|            0|            0|  0.00%|    def __init__(self, suffix=None, prefix=None, dir=None):
   780|         0|            0|            0|  0.00%|        self.name = mkdtemp(suffix, prefix, dir)
   781|         0|            0|            0|  0.00%|        self._finalizer = _weakref.finalize(
   782|         0|            0|            0|  0.00%|            self, self._cleanup, self.name,
   783|         0|            0|            0|  0.00%|            warn_message="Implicitly cleaning up {!r}".format(self))
   784|         0|            0|            0|  0.00%|
   785|         0|            0|            0|  0.00%|    @classmethod
   786|         0|            0|            0|  0.00%|    def _rmtree(cls, name):
   787|         0|            0|            0|  0.00%|        def onerror(func, path, exc_info):
   788|         0|            0|            0|  0.00%|            if issubclass(exc_info[0], PermissionError):
   789|         0|            0|            0|  0.00%|                def resetperms(path):
   790|         0|            0|            0|  0.00%|                    try:
   791|         0|            0|            0|  0.00%|                        _os.chflags(path, 0)
   792|         0|            0|            0|  0.00%|                    except AttributeError:
   793|         0|            0|            0|  0.00%|                        pass
   794|         0|            0|            0|  0.00%|                    _os.chmod(path, 0o700)
   795|         0|            0|            0|  0.00%|
   796|         0|            0|            0|  0.00%|                try:
   797|         0|            0|            0|  0.00%|                    if path != name:
   798|         0|            0|            0|  0.00%|                        resetperms(_os.path.dirname(path))
   799|         0|            0|            0|  0.00%|                    resetperms(path)
   800|         0|            0|            0|  0.00%|
   801|         0|            0|            0|  0.00%|                    try:
   802|         0|            0|            0|  0.00%|                        _os.unlink(path)
   803|         0|            0|            0|  0.00%|                    # PermissionError is raised on FreeBSD for directories
   804|         0|            0|            0|  0.00%|                    except (IsADirectoryError, PermissionError):
   805|         0|            0|            0|  0.00%|                        cls._rmtree(path)
   806|         0|            0|            0|  0.00%|                except FileNotFoundError:
   807|         0|            0|            0|  0.00%|                    pass
   808|         0|            0|            0|  0.00%|            elif issubclass(exc_info[0], FileNotFoundError):
   809|         0|            0|            0|  0.00%|                pass
   810|         0|            0|            0|  0.00%|            else:
   811|         0|            0|            0|  0.00%|                raise
   812|         0|            0|            0|  0.00%|
   813|         0|            0|            0|  0.00%|        _shutil.rmtree(name, onerror=onerror)
   814|         0|            0|            0|  0.00%|
   815|         0|            0|            0|  0.00%|    @classmethod
   816|         0|            0|            0|  0.00%|    def _cleanup(cls, name, warn_message):
   817|         0|            0|            0|  0.00%|        cls._rmtree(name)
   818|         0|            0|            0|  0.00%|        _warnings.warn(warn_message, ResourceWarning)
   819|         0|            0|            0|  0.00%|
   820|         0|            0|            0|  0.00%|    def __repr__(self):
   821|         0|            0|            0|  0.00%|        return "<{} {!r}>".format(self.__class__.__name__, self.name)
   822|         0|            0|            0|  0.00%|
   823|         0|            0|            0|  0.00%|    def __enter__(self):
   824|         0|            0|            0|  0.00%|        return self.name
   825|         0|            0|            0|  0.00%|
   826|         0|            0|            0|  0.00%|    def __exit__(self, exc, value, tb):
   827|         0|            0|            0|  0.00%|        self.cleanup()
   828|         0|            0|            0|  0.00%|
   829|         0|            0|            0|  0.00%|    def cleanup(self):
   830|         0|            0|            0|  0.00%|        if self._finalizer.detach():
   831|         0|            0|            0|  0.00%|            self._rmtree(self.name)
File: /opt/conda/lib/python3.8/multiprocessing/context.py
File duration: 0.00794291s (0.01%)
Line #|      Hits|         Time| Time per hit|      %|Source code
------+----------+-------------+-------------+-------+-----------
     1|         0|            0|            0|  0.00%|import os
     2|         0|            0|            0|  0.00%|import sys
     3|         0|            0|            0|  0.00%|import threading
     4|         0|            0|            0|  0.00%|
     5|         0|            0|            0|  0.00%|from . import process
     6|         0|            0|            0|  0.00%|from . import reduction
     7|         0|            0|            0|  0.00%|
     8|         0|            0|            0|  0.00%|__all__ = ()
     9|         0|            0|            0|  0.00%|
    10|         0|            0|            0|  0.00%|#
    11|         0|            0|            0|  0.00%|# Exceptions
    12|         0|            0|            0|  0.00%|#
    13|         0|            0|            0|  0.00%|
    14|         0|            0|            0|  0.00%|class ProcessError(Exception):
    15|         0|            0|            0|  0.00%|    pass
    16|         0|            0|            0|  0.00%|
    17|         0|            0|            0|  0.00%|class BufferTooShort(ProcessError):
    18|         0|            0|            0|  0.00%|    pass
    19|         0|            0|            0|  0.00%|
    20|         0|            0|            0|  0.00%|class TimeoutError(ProcessError):
    21|         0|            0|            0|  0.00%|    pass
    22|         0|            0|            0|  0.00%|
    23|         0|            0|            0|  0.00%|class AuthenticationError(ProcessError):
    24|         0|            0|            0|  0.00%|    pass
    25|         0|            0|            0|  0.00%|
    26|         0|            0|            0|  0.00%|#
    27|         0|            0|            0|  0.00%|# Base type for contexts. Bound methods of an instance of this type are included in __all__ of __init__.py
    28|         0|            0|            0|  0.00%|#
    29|         0|            0|            0|  0.00%|
    30|         0|            0|            0|  0.00%|class BaseContext(object):
    31|         0|            0|            0|  0.00%|
    32|         0|            0|            0|  0.00%|    ProcessError = ProcessError
    33|         0|            0|            0|  0.00%|    BufferTooShort = BufferTooShort
    34|         0|            0|            0|  0.00%|    TimeoutError = TimeoutError
    35|         0|            0|            0|  0.00%|    AuthenticationError = AuthenticationError
    36|         0|            0|            0|  0.00%|
    37|         0|            0|            0|  0.00%|    current_process = staticmethod(process.current_process)
    38|         0|            0|            0|  0.00%|    parent_process = staticmethod(process.parent_process)
    39|         0|            0|            0|  0.00%|    active_children = staticmethod(process.active_children)
    40|         0|            0|            0|  0.00%|
    41|         0|            0|            0|  0.00%|    def cpu_count(self):
    42|         0|            0|            0|  0.00%|        '''Returns the number of CPUs in the system'''
    43|         0|            0|            0|  0.00%|        num = os.cpu_count()
    44|         0|            0|            0|  0.00%|        if num is None:
    45|         0|            0|            0|  0.00%|            raise NotImplementedError('cannot determine number of cpus')
    46|         0|            0|            0|  0.00%|        else:
    47|         0|            0|            0|  0.00%|            return num
    48|         0|            0|            0|  0.00%|
    49|         0|            0|            0|  0.00%|    def Manager(self):
    50|         0|            0|            0|  0.00%|        '''Returns a manager associated with a running server process
    51|         0|            0|            0|  0.00%|
    52|         0|            0|            0|  0.00%|        The managers methods such as `Lock()`, `Condition()` and `Queue()`
    53|         0|            0|            0|  0.00%|        can be used to create shared objects.
    54|         0|            0|            0|  0.00%|        '''
    55|         0|            0|            0|  0.00%|        from .managers import SyncManager
    56|         0|            0|            0|  0.00%|        m = SyncManager(ctx=self.get_context())
    57|         0|            0|            0|  0.00%|        m.start()
    58|         0|            0|            0|  0.00%|        return m
    59|         0|            0|            0|  0.00%|
    60|         0|            0|            0|  0.00%|    def Pipe(self, duplex=True):
    61|         0|            0|            0|  0.00%|        '''Returns two connection object connected by a pipe'''
    62|         0|            0|            0|  0.00%|        from .connection import Pipe
    63|         0|            0|            0|  0.00%|        return Pipe(duplex)
    64|         0|            0|            0|  0.00%|
    65|        22|  0.000132799|  6.03632e-06|  0.00%|    def Lock(self):
    66|         0|            0|            0|  0.00%|        '''Returns a non-recursive lock object'''
    67|        22|   0.00050354|  2.28882e-05|  0.00%|        from .synchronize import Lock
(call)|        22|  0.000417709|  1.89868e-05|  0.00%|# <frozen importlib._bootstrap>:389 parent
    68|        22|  0.000649929|  2.95422e-05|  0.00%|        return Lock(ctx=self.get_context())
(call)|        22|   0.00030899|   1.4045e-05|  0.00%|# /opt/conda/lib/python3.8/multiprocessing/context.py:187 get_context
(call)|        22|    0.0295825|   0.00134466|  0.06%|# /opt/conda/lib/python3.8/multiprocessing/synchronize.py:161 __init__
    69|         0|            0|            0|  0.00%|
    70|         0|            0|            0|  0.00%|    def RLock(self):
    71|         0|            0|            0|  0.00%|        '''Returns a recursive lock object'''
    72|         0|            0|            0|  0.00%|        from .synchronize import RLock
    73|         0|            0|            0|  0.00%|        return RLock(ctx=self.get_context())
    74|         0|            0|            0|  0.00%|
    75|         2|   2.5034e-05|   1.2517e-05|  0.00%|    def Condition(self, lock=None):
    76|         0|            0|            0|  0.00%|        '''Returns a condition object'''
    77|         2|  4.26769e-05|  2.13385e-05|  0.00%|        from .synchronize import Condition
(call)|         2|  3.19481e-05|   1.5974e-05|  0.00%|# <frozen importlib._bootstrap>:389 parent
    78|         2|  6.53267e-05|  3.26633e-05|  0.00%|        return Condition(lock, ctx=self.get_context())
(call)|         2|  2.67029e-05|  1.33514e-05|  0.00%|# /opt/conda/lib/python3.8/multiprocessing/context.py:187 get_context
(call)|         2|   0.00683665|   0.00341833|  0.01%|# /opt/conda/lib/python3.8/multiprocessing/synchronize.py:212 __init__
    79|         0|            0|            0|  0.00%|
    80|         8|  4.12464e-05|   5.1558e-06|  0.00%|    def Semaphore(self, value=1):
    81|         0|            0|            0|  0.00%|        '''Returns a semaphore object'''
    82|         8|  0.000142813|  1.78516e-05|  0.00%|        from .synchronize import Semaphore
(call)|         8|  0.000119209|  1.49012e-05|  0.00%|# <frozen importlib._bootstrap>:389 parent
    83|         8|  0.000191689|  2.39611e-05|  0.00%|        return Semaphore(value, ctx=self.get_context())
(call)|         8|  0.000100851|  1.26064e-05|  0.00%|# /opt/conda/lib/python3.8/multiprocessing/context.py:187 get_context
(call)|         8|   0.00821948|   0.00102744|  0.02%|# /opt/conda/lib/python3.8/multiprocessing/synchronize.py:125 __init__
    84|         0|            0|            0|  0.00%|
    85|        10|  0.000187159|  1.87159e-05|  0.00%|    def BoundedSemaphore(self, value=1):
    86|         0|            0|            0|  0.00%|        '''Returns a bounded semaphore object'''
    87|        10|  0.000281096|  2.81096e-05|  0.00%|        from .synchronize import BoundedSemaphore
(call)|        10|  0.000167608|  1.67608e-05|  0.00%|# <frozen importlib._bootstrap>:389 parent
    88|        10|  0.000264406|  2.64406e-05|  0.00%|        return BoundedSemaphore(value, ctx=self.get_context())
(call)|        10|  0.000142097|  1.42097e-05|  0.00%|# /opt/conda/lib/python3.8/multiprocessing/context.py:187 get_context
(call)|        10|    0.0109107|   0.00109107|  0.02%|# /opt/conda/lib/python3.8/multiprocessing/synchronize.py:144 __init__
    89|         0|            0|            0|  0.00%|
    90|         2|  1.28746e-05|   6.4373e-06|  0.00%|    def Event(self):
    91|         0|            0|            0|  0.00%|        '''Returns an event object'''
    92|         2|  4.05312e-05|  2.02656e-05|  0.00%|        from .synchronize import Event
(call)|         2|  3.17097e-05|  1.58548e-05|  0.00%|# <frozen importlib._bootstrap>:389 parent
    93|         2|  5.60284e-05|  2.80142e-05|  0.00%|        return Event(ctx=self.get_context())
(call)|         2|  3.38554e-05|  1.69277e-05|  0.00%|# /opt/conda/lib/python3.8/multiprocessing/context.py:233 get_context
(call)|         2|    0.0115011|   0.00575054|  0.02%|# /opt/conda/lib/python3.8/multiprocessing/synchronize.py:323 __init__
    94|         0|            0|            0|  0.00%|
    95|         0|            0|            0|  0.00%|    def Barrier(self, parties, action=None, timeout=None):
    96|         0|            0|            0|  0.00%|        '''Returns a barrier object'''
    97|         0|            0|            0|  0.00%|        from .synchronize import Barrier
    98|         0|            0|            0|  0.00%|        return Barrier(parties, action, timeout, ctx=self.get_context())
    99|         0|            0|            0|  0.00%|
   100|        10|  0.000230551|  2.30551e-05|  0.00%|    def Queue(self, maxsize=0):
   101|         0|            0|            0|  0.00%|        '''Returns a queue object'''
   102|        10|   0.00108242|  0.000108242|  0.00%|        from .queues import Queue
(call)|        10|  0.000362873|  3.62873e-05|  0.00%|# <frozen importlib._bootstrap>:389 parent
   103|        10|  0.000864506|  8.64506e-05|  0.00%|        return Queue(maxsize, ctx=self.get_context())
(call)|        10|  0.000260115|  2.60115e-05|  0.00%|# /opt/conda/lib/python3.8/multiprocessing/context.py:233 get_context
(call)|        10|    0.0509214|   0.00509214|  0.10%|# /opt/conda/lib/python3.8/multiprocessing/queues.py:36 __init__
   104|         0|            0|            0|  0.00%|
   105|         0|            0|            0|  0.00%|    def JoinableQueue(self, maxsize=0):
   106|         0|            0|            0|  0.00%|        '''Returns a queue object'''
   107|         0|            0|            0|  0.00%|        from .queues import JoinableQueue
   108|         0|            0|            0|  0.00%|        return JoinableQueue(maxsize, ctx=self.get_context())
   109|         0|            0|            0|  0.00%|
   110|         0|            0|            0|  0.00%|    def SimpleQueue(self):
   111|         0|            0|            0|  0.00%|        '''Returns a queue object'''
   112|         0|            0|            0|  0.00%|        from .queues import SimpleQueue
   113|         0|            0|            0|  0.00%|        return SimpleQueue(ctx=self.get_context())
   114|         0|            0|            0|  0.00%|
   115|         0|            0|            0|  0.00%|    def Pool(self, processes=None, initializer=None, initargs=(),
   116|         0|            0|            0|  0.00%|             maxtasksperchild=None):
   117|         0|            0|            0|  0.00%|        '''Returns a process pool object'''
   118|         0|            0|            0|  0.00%|        from .pool import Pool
   119|         0|            0|            0|  0.00%|        return Pool(processes, initializer, initargs, maxtasksperchild,
   120|         0|            0|            0|  0.00%|                    context=self.get_context())
   121|         0|            0|            0|  0.00%|
   122|         0|            0|            0|  0.00%|    def RawValue(self, typecode_or_type, *args):
   123|         0|            0|            0|  0.00%|        '''Returns a shared object'''
   124|         0|            0|            0|  0.00%|        from .sharedctypes import RawValue
   125|         0|            0|            0|  0.00%|        return RawValue(typecode_or_type, *args)
   126|         0|            0|            0|  0.00%|
   127|         0|            0|            0|  0.00%|    def RawArray(self, typecode_or_type, size_or_initializer):
   128|         0|            0|            0|  0.00%|        '''Returns a shared array'''
   129|         0|            0|            0|  0.00%|        from .sharedctypes import RawArray
   130|         0|            0|            0|  0.00%|        return RawArray(typecode_or_type, size_or_initializer)
   131|         0|            0|            0|  0.00%|
   132|         0|            0|            0|  0.00%|    def Value(self, typecode_or_type, *args, lock=True):
   133|         0|            0|            0|  0.00%|        '''Returns a synchronized shared object'''
   134|         0|            0|            0|  0.00%|        from .sharedctypes import Value
   135|         0|            0|            0|  0.00%|        return Value(typecode_or_type, *args, lock=lock,
   136|         0|            0|            0|  0.00%|                     ctx=self.get_context())
   137|         0|            0|            0|  0.00%|
   138|         0|            0|            0|  0.00%|    def Array(self, typecode_or_type, size_or_initializer, *, lock=True):
   139|         0|            0|            0|  0.00%|        '''Returns a synchronized shared array'''
   140|         0|            0|            0|  0.00%|        from .sharedctypes import Array
   141|         0|            0|            0|  0.00%|        return Array(typecode_or_type, size_or_initializer, lock=lock,
   142|         0|            0|            0|  0.00%|                     ctx=self.get_context())
   143|         0|            0|            0|  0.00%|
   144|         0|            0|            0|  0.00%|    def freeze_support(self):
   145|         0|            0|            0|  0.00%|        '''Check whether this is a fake forked process in a frozen executable.
   146|         0|            0|            0|  0.00%|        If so then run code specified by commandline and exit.
   147|         0|            0|            0|  0.00%|        '''
   148|         0|            0|            0|  0.00%|        if sys.platform == 'win32' and getattr(sys, 'frozen', False):
   149|         0|            0|            0|  0.00%|            from .spawn import freeze_support
   150|         0|            0|            0|  0.00%|            freeze_support()
   151|         0|            0|            0|  0.00%|
   152|         0|            0|            0|  0.00%|    def get_logger(self):
   153|         0|            0|            0|  0.00%|        '''Return package logger -- if it does not already exist then
   154|         0|            0|            0|  0.00%|        it is created.
   155|         0|            0|            0|  0.00%|        '''
   156|         0|            0|            0|  0.00%|        from .util import get_logger
   157|         0|            0|            0|  0.00%|        return get_logger()
   158|         0|            0|            0|  0.00%|
   159|         0|            0|            0|  0.00%|    def log_to_stderr(self, level=None):
   160|         0|            0|            0|  0.00%|        '''Turn on logging and add a handler which prints to stderr'''
   161|         0|            0|            0|  0.00%|        from .util import log_to_stderr
   162|         0|            0|            0|  0.00%|        return log_to_stderr(level)
   163|         0|            0|            0|  0.00%|
   164|         0|            0|            0|  0.00%|    def allow_connection_pickling(self):
   165|         0|            0|            0|  0.00%|        '''Install support for sending connections and sockets
   166|         0|            0|            0|  0.00%|        between processes
   167|         0|            0|            0|  0.00%|        '''
   168|         0|            0|            0|  0.00%|        # This is undocumented.  In previous versions of multiprocessing
   169|         0|            0|            0|  0.00%|        # its only effect was to make socket objects inheritable on Windows.
   170|         0|            0|            0|  0.00%|        from . import connection
   171|         0|            0|            0|  0.00%|
   172|         0|            0|            0|  0.00%|    def set_executable(self, executable):
   173|         0|            0|            0|  0.00%|        '''Sets the path to a python.exe or pythonw.exe binary used to run
   174|         0|            0|            0|  0.00%|        child processes instead of sys.executable when using the 'spawn'
   175|         0|            0|            0|  0.00%|        start method.  Useful for people embedding Python.
   176|         0|            0|            0|  0.00%|        '''
   177|         0|            0|            0|  0.00%|        from .spawn import set_executable
   178|         0|            0|            0|  0.00%|        set_executable(executable)
   179|         0|            0|            0|  0.00%|
   180|         0|            0|            0|  0.00%|    def set_forkserver_preload(self, module_names):
   181|         0|            0|            0|  0.00%|        '''Set list of module names to try to load in forkserver process.
   182|         0|            0|            0|  0.00%|        This is really just a hint.
   183|         0|            0|            0|  0.00%|        '''
   184|         0|            0|            0|  0.00%|        from .forkserver import set_forkserver_preload
   185|         0|            0|            0|  0.00%|        set_forkserver_preload(module_names)
   186|         0|            0|            0|  0.00%|
   187|        42|  0.000230312|  5.48363e-06|  0.00%|    def get_context(self, method=None):
   188|        42|  0.000182867|  4.35398e-06|  0.00%|        if method is None:
   189|        42|  0.000165462|  3.93958e-06|  0.00%|            return self
   190|         0|            0|            0|  0.00%|        try:
   191|         0|            0|            0|  0.00%|            ctx = _concrete_contexts[method]
   192|         0|            0|            0|  0.00%|        except KeyError:
   193|         0|            0|            0|  0.00%|            raise ValueError('cannot find context for %r' % method) from None
   194|         0|            0|            0|  0.00%|        ctx._check_available()
   195|         0|            0|            0|  0.00%|        return ctx
   196|         0|            0|            0|  0.00%|
   197|        40|  0.000222921|  5.57303e-06|  0.00%|    def get_start_method(self, allow_none=False):
   198|        40|  0.000166416|   4.1604e-06|  0.00%|        return self._name
   199|         0|            0|            0|  0.00%|
   200|         0|            0|            0|  0.00%|    def set_start_method(self, method, force=False):
   201|         0|            0|            0|  0.00%|        raise ValueError('cannot set start method of concrete context')
   202|         0|            0|            0|  0.00%|
   203|         0|            0|            0|  0.00%|    @property
   204|         0|            0|            0|  0.00%|    def reducer(self):
   205|         0|            0|            0|  0.00%|        '''Controls how objects will be reduced to a form that can be
   206|         0|            0|            0|  0.00%|        shared with other processes.'''
   207|         0|            0|            0|  0.00%|        return globals().get('reduction')
   208|         0|            0|            0|  0.00%|
   209|         0|            0|            0|  0.00%|    @reducer.setter
   210|         0|            0|            0|  0.00%|    def reducer(self, reduction):
   211|         0|            0|            0|  0.00%|        globals()['reduction'] = reduction
   212|         0|            0|            0|  0.00%|
   213|         0|            0|            0|  0.00%|    def _check_available(self):
   214|         0|            0|            0|  0.00%|        pass
   215|         0|            0|            0|  0.00%|
   216|         0|            0|            0|  0.00%|#
   217|         0|            0|            0|  0.00%|# Type of default context -- underlying context can be set at most once
   218|         0|            0|            0|  0.00%|#
   219|         0|            0|            0|  0.00%|
   220|         0|            0|            0|  0.00%|class Process(process.BaseProcess):
   221|         0|            0|            0|  0.00%|    _start_method = None
   222|         8|  3.91006e-05|  4.88758e-06|  0.00%|    @staticmethod
   223|         0|            0|            0|  0.00%|    def _Popen(process_obj):
   224|         8|  0.000516176|   6.4522e-05|  0.00%|        return _default_context.get_context().Process._Popen(process_obj)
(call)|         8|  0.000126362|  1.57952e-05|  0.00%|# /opt/conda/lib/python3.8/multiprocessing/context.py:233 get_context
(call)|         8|      1.01737|     0.127171|  1.90%|# /opt/conda/lib/python3.8/multiprocessing/context.py:274 _Popen
   225|         0|            0|            0|  0.00%|
   226|         0|            0|            0|  0.00%|class DefaultContext(BaseContext):
   227|         0|            0|            0|  0.00%|    Process = Process
   228|         0|            0|            0|  0.00%|
   229|         0|            0|            0|  0.00%|    def __init__(self, context):
   230|         0|            0|            0|  0.00%|        self._default_context = context
   231|         0|            0|            0|  0.00%|        self._actual_context = None
   232|         0|            0|            0|  0.00%|
   233|        20|  0.000164032|   8.2016e-06|  0.00%|    def get_context(self, method=None):
   234|        20|  9.05991e-05|  4.52995e-06|  0.00%|        if method is None:
   235|        20|  7.89165e-05|  3.94583e-06|  0.00%|            if self._actual_context is None:
   236|         0|            0|            0|  0.00%|                self._actual_context = self._default_context
   237|        20|  8.67844e-05|  4.33922e-06|  0.00%|            return self._actual_context
   238|         0|            0|            0|  0.00%|        else:
   239|         0|            0|            0|  0.00%|            return super().get_context(method)
   240|         0|            0|            0|  0.00%|
   241|         0|            0|            0|  0.00%|    def set_start_method(self, method, force=False):
   242|         0|            0|            0|  0.00%|        if self._actual_context is not None and not force:
   243|         0|            0|            0|  0.00%|            raise RuntimeError('context has already been set')
   244|         0|            0|            0|  0.00%|        if method is None and force:
   245|         0|            0|            0|  0.00%|            self._actual_context = None
   246|         0|            0|            0|  0.00%|            return
   247|         0|            0|            0|  0.00%|        self._actual_context = self.get_context(method)
   248|         0|            0|            0|  0.00%|
   249|         0|            0|            0|  0.00%|    def get_start_method(self, allow_none=False):
   250|         0|            0|            0|  0.00%|        if self._actual_context is None:
   251|         0|            0|            0|  0.00%|            if allow_none:
   252|         0|            0|            0|  0.00%|                return None
   253|         0|            0|            0|  0.00%|            self._actual_context = self._default_context
   254|         0|            0|            0|  0.00%|        return self._actual_context._name
   255|         0|            0|            0|  0.00%|
   256|         0|            0|            0|  0.00%|    def get_all_start_methods(self):
   257|         0|            0|            0|  0.00%|        if sys.platform == 'win32':
   258|         0|            0|            0|  0.00%|            return ['spawn']
   259|         0|            0|            0|  0.00%|        else:
   260|         0|            0|            0|  0.00%|            methods = ['spawn', 'fork'] if sys.platform == 'darwin' else ['fork', 'spawn']
   261|         0|            0|            0|  0.00%|            if reduction.HAVE_SEND_HANDLE:
   262|         0|            0|            0|  0.00%|                methods.append('forkserver')
   263|         0|            0|            0|  0.00%|            return methods
   264|         0|            0|            0|  0.00%|
   265|         0|            0|            0|  0.00%|
   266|         0|            0|            0|  0.00%|#
   267|         0|            0|            0|  0.00%|# Context types for fixed start method
   268|         0|            0|            0|  0.00%|#
   269|         0|            0|            0|  0.00%|
   270|         0|            0|            0|  0.00%|if sys.platform != 'win32':
   271|         0|            0|            0|  0.00%|
   272|         0|            0|            0|  0.00%|    class ForkProcess(process.BaseProcess):
   273|         0|            0|            0|  0.00%|        _start_method = 'fork'
   274|         8|   3.8147e-05|  4.76837e-06|  0.00%|        @staticmethod
   275|         0|            0|            0|  0.00%|        def _Popen(process_obj):
   276|         8|  0.000320911|  4.01139e-05|  0.00%|            from .popen_fork import Popen
(call)|         8|  0.000156641|  1.95801e-05|  0.00%|# <frozen importlib._bootstrap>:389 parent
   277|         8|  0.000825644|  0.000103205|  0.00%|            return Popen(process_obj)
(call)|         8|      1.01603|     0.127003|  1.90%|# /opt/conda/lib/python3.8/multiprocessing/popen_fork.py:15 __init__
   278|         0|            0|            0|  0.00%|
   279|         0|            0|            0|  0.00%|    class SpawnProcess(process.BaseProcess):
   280|         0|            0|            0|  0.00%|        _start_method = 'spawn'
   281|         0|            0|            0|  0.00%|        @staticmethod
   282|         0|            0|            0|  0.00%|        def _Popen(process_obj):
   283|         0|            0|            0|  0.00%|            from .popen_spawn_posix import Popen
   284|         0|            0|            0|  0.00%|            return Popen(process_obj)
   285|         0|            0|            0|  0.00%|
   286|         0|            0|            0|  0.00%|    class ForkServerProcess(process.BaseProcess):
   287|         0|            0|            0|  0.00%|        _start_method = 'forkserver'
   288|         0|            0|            0|  0.00%|        @staticmethod
   289|         0|            0|            0|  0.00%|        def _Popen(process_obj):
   290|         0|            0|            0|  0.00%|            from .popen_forkserver import Popen
   291|         0|            0|            0|  0.00%|            return Popen(process_obj)
   292|         0|            0|            0|  0.00%|
   293|         0|            0|            0|  0.00%|    class ForkContext(BaseContext):
   294|         0|            0|            0|  0.00%|        _name = 'fork'
   295|         0|            0|            0|  0.00%|        Process = ForkProcess
   296|         0|            0|            0|  0.00%|
   297|         0|            0|            0|  0.00%|    class SpawnContext(BaseContext):
   298|         0|            0|            0|  0.00%|        _name = 'spawn'
   299|         0|            0|            0|  0.00%|        Process = SpawnProcess
   300|         0|            0|            0|  0.00%|
   301|         0|            0|            0|  0.00%|    class ForkServerContext(BaseContext):
   302|         0|            0|            0|  0.00%|        _name = 'forkserver'
   303|         0|            0|            0|  0.00%|        Process = ForkServerProcess
   304|         0|            0|            0|  0.00%|        def _check_available(self):
   305|         0|            0|            0|  0.00%|            if not reduction.HAVE_SEND_HANDLE:
   306|         0|            0|            0|  0.00%|                raise ValueError('forkserver start method not available')
   307|         0|            0|            0|  0.00%|
   308|         0|            0|            0|  0.00%|    _concrete_contexts = {
   309|         0|            0|            0|  0.00%|        'fork': ForkContext(),
   310|         0|            0|            0|  0.00%|        'spawn': SpawnContext(),
   311|         0|            0|            0|  0.00%|        'forkserver': ForkServerContext(),
   312|         0|            0|            0|  0.00%|    }
   313|         0|            0|            0|  0.00%|    if sys.platform == 'darwin':
   314|         0|            0|            0|  0.00%|        # bpo-33725: running arbitrary code after fork() is no longer reliable
   315|         0|            0|            0|  0.00%|        # on macOS since macOS 10.14 (Mojave). Use spawn by default instead.
   316|         0|            0|            0|  0.00%|        _default_context = DefaultContext(_concrete_contexts['spawn'])
   317|         0|            0|            0|  0.00%|    else:
   318|         0|            0|            0|  0.00%|        _default_context = DefaultContext(_concrete_contexts['fork'])
   319|         0|            0|            0|  0.00%|
   320|         0|            0|            0|  0.00%|else:
   321|         0|            0|            0|  0.00%|
   322|         0|            0|            0|  0.00%|    class SpawnProcess(process.BaseProcess):
   323|         0|            0|            0|  0.00%|        _start_method = 'spawn'
   324|         0|            0|            0|  0.00%|        @staticmethod
   325|         0|            0|            0|  0.00%|        def _Popen(process_obj):
   326|         0|            0|            0|  0.00%|            from .popen_spawn_win32 import Popen
   327|         0|            0|            0|  0.00%|            return Popen(process_obj)
   328|         0|            0|            0|  0.00%|
   329|         0|            0|            0|  0.00%|    class SpawnContext(BaseContext):
   330|         0|            0|            0|  0.00%|        _name = 'spawn'
   331|         0|            0|            0|  0.00%|        Process = SpawnProcess
   332|         0|            0|            0|  0.00%|
   333|         0|            0|            0|  0.00%|    _concrete_contexts = {
   334|         0|            0|            0|  0.00%|        'spawn': SpawnContext(),
   335|         0|            0|            0|  0.00%|    }
   336|         0|            0|            0|  0.00%|    _default_context = DefaultContext(_concrete_contexts['spawn'])
   337|         0|            0|            0|  0.00%|
   338|         0|            0|            0|  0.00%|#
   339|         0|            0|            0|  0.00%|# Force the start method
   340|         0|            0|            0|  0.00%|#
   341|         0|            0|            0|  0.00%|
   342|         0|            0|            0|  0.00%|def _force_start_method(method):
   343|         0|            0|            0|  0.00%|    _default_context._actual_context = _concrete_contexts[method]
   344|         0|            0|            0|  0.00%|
   345|         0|            0|            0|  0.00%|#
   346|         0|            0|            0|  0.00%|# Check that the current thread is spawning a child process
   347|         0|            0|            0|  0.00%|#
   348|         0|            0|            0|  0.00%|
   349|         0|            0|            0|  0.00%|_tls = threading.local()
   350|         0|            0|            0|  0.00%|
   351|         0|            0|            0|  0.00%|def get_spawning_popen():
   352|         0|            0|            0|  0.00%|    return getattr(_tls, 'spawning_popen', None)
   353|         0|            0|            0|  0.00%|
   354|         0|            0|            0|  0.00%|def set_spawning_popen(popen):
   355|         0|            0|            0|  0.00%|    _tls.spawning_popen = popen
   356|         0|            0|            0|  0.00%|
   357|         0|            0|            0|  0.00%|def assert_spawning(obj):
   358|         0|            0|            0|  0.00%|    if get_spawning_popen() is None:
   359|         0|            0|            0|  0.00%|        raise RuntimeError(
   360|         0|            0|            0|  0.00%|            '%s objects should only be shared between processes'
   361|         0|            0|            0|  0.00%|            ' through inheritance' % type(obj).__name__
   362|         0|            0|            0|  0.00%|            )
File: /opt/conda/lib/python3.8/site-packages/torch/nn/modules/pooling.py
File duration: 0.00763297s (0.01%)
Line #|      Hits|         Time| Time per hit|      %|Source code
------+----------+-------------+-------------+-------+-----------
     1|         0|            0|            0|  0.00%|from typing import List, Optional
     2|         0|            0|            0|  0.00%|
     3|         0|            0|            0|  0.00%|from torch import Tensor
     4|         0|            0|            0|  0.00%|from .module import Module
     5|         0|            0|            0|  0.00%|from .utils import _single, _pair, _triple
     6|         0|            0|            0|  0.00%|from .. import functional as F
     7|         0|            0|            0|  0.00%|
     8|         0|            0|            0|  0.00%|from ..common_types import (_size_any_t, _size_1_t, _size_2_t, _size_3_t,
     9|         0|            0|            0|  0.00%|                            _ratio_3_t, _ratio_2_t, _size_any_opt_t, _size_2_opt_t, _size_3_opt_t)
    10|         0|            0|            0|  0.00%|
    11|         0|            0|            0|  0.00%|
    12|         0|            0|            0|  0.00%|class _MaxPoolNd(Module):
    13|         0|            0|            0|  0.00%|    __constants__ = ['kernel_size', 'stride', 'padding', 'dilation',
    14|         0|            0|            0|  0.00%|                     'return_indices', 'ceil_mode']
    15|         0|            0|            0|  0.00%|    return_indices: bool
    16|         0|            0|            0|  0.00%|    ceil_mode: bool
    17|         0|            0|            0|  0.00%|
    18|         0|            0|            0|  0.00%|    def __init__(self, kernel_size: _size_any_t, stride: Optional[_size_any_t] = None,
    19|         0|            0|            0|  0.00%|                 padding: _size_any_t = 0, dilation: _size_any_t = 1,
    20|         0|            0|            0|  0.00%|                 return_indices: bool = False, ceil_mode: bool = False) -> None:
    21|         0|            0|            0|  0.00%|        super(_MaxPoolNd, self).__init__()
    22|         0|            0|            0|  0.00%|        self.kernel_size = kernel_size
    23|         0|            0|            0|  0.00%|        self.stride = stride if (stride is not None) else kernel_size
    24|         0|            0|            0|  0.00%|        self.padding = padding
    25|         0|            0|            0|  0.00%|        self.dilation = dilation
    26|         0|            0|            0|  0.00%|        self.return_indices = return_indices
    27|         0|            0|            0|  0.00%|        self.ceil_mode = ceil_mode
    28|         0|            0|            0|  0.00%|
    29|         0|            0|            0|  0.00%|    def extra_repr(self) -> str:
    30|         0|            0|            0|  0.00%|        return 'kernel_size={kernel_size}, stride={stride}, padding={padding}' \
    31|         0|            0|            0|  0.00%|            ', dilation={dilation}, ceil_mode={ceil_mode}'.format(**self.__dict__)
    32|         0|            0|            0|  0.00%|
    33|         0|            0|            0|  0.00%|
    34|         0|            0|            0|  0.00%|class MaxPool1d(_MaxPoolNd):
    35|         0|            0|            0|  0.00%|    r"""Applies a 1D max pooling over an input signal composed of several input
    36|         0|            0|            0|  0.00%|    planes.
    37|         0|            0|            0|  0.00%|
    38|         0|            0|            0|  0.00%|    In the simplest case, the output value of the layer with input size :math:`(N, C, L)`
    39|         0|            0|            0|  0.00%|    and output :math:`(N, C, L_{out})` can be precisely described as:
    40|         0|            0|            0|  0.00%|
    41|         0|            0|            0|  0.00%|    .. math::
    42|         0|            0|            0|  0.00%|        out(N_i, C_j, k) = \max_{m=0, \ldots, \text{kernel\_size} - 1}
    43|         0|            0|            0|  0.00%|                input(N_i, C_j, stride \times k + m)
    44|         0|            0|            0|  0.00%|
    45|         0|            0|            0|  0.00%|    If :attr:`padding` is non-zero, then the input is implicitly padded with negative infinity on both sides
    46|         0|            0|            0|  0.00%|    for :attr:`padding` number of points. :attr:`dilation` is the stride between the elements within the
    47|         0|            0|            0|  0.00%|    sliding window. This `link`_ has a nice visualization of the pooling parameters.
    48|         0|            0|            0|  0.00%|
    49|         0|            0|            0|  0.00%|    Note:
    50|         0|            0|            0|  0.00%|        When ceil_mode=True, sliding windows are allowed to go off-bounds if they start within the left padding
    51|         0|            0|            0|  0.00%|        or the input. Sliding windows that would start in the right padded region are ignored.
    52|         0|            0|            0|  0.00%|
    53|         0|            0|            0|  0.00%|    Args:
    54|         0|            0|            0|  0.00%|        kernel_size: The size of the sliding window, must be > 0.
    55|         0|            0|            0|  0.00%|        stride: The stride of the sliding window, must be > 0. Default value is :attr:`kernel_size`.
    56|         0|            0|            0|  0.00%|        padding: Implicit negative infinity padding to be added on both sides, must be >= 0 and <= kernel_size / 2.
    57|         0|            0|            0|  0.00%|        dilation: The stride between elements within a sliding window, must be > 0.
    58|         0|            0|            0|  0.00%|        return_indices: If ``True``, will return the argmax along with the max values.
    59|         0|            0|            0|  0.00%|                        Useful for :class:`torch.nn.MaxUnpool1d` later
    60|         0|            0|            0|  0.00%|        ceil_mode: If ``True``, will use `ceil` instead of `floor` to compute the output shape. This
    61|         0|            0|            0|  0.00%|                   ensures that every element in the input tensor is covered by a sliding window.
    62|         0|            0|            0|  0.00%|
    63|         0|            0|            0|  0.00%|    Shape:
    64|         0|            0|            0|  0.00%|        - Input: :math:`(N, C, L_{in})`
    65|         0|            0|            0|  0.00%|        - Output: :math:`(N, C, L_{out})`, where
    66|         0|            0|            0|  0.00%|
    67|         0|            0|            0|  0.00%|          .. math::
    68|         0|            0|            0|  0.00%|              L_{out} = \left\lfloor \frac{L_{in} + 2 \times \text{padding} - \text{dilation}
    69|         0|            0|            0|  0.00%|                    \times (\text{kernel\_size} - 1) - 1}{\text{stride}} + 1\right\rfloor
    70|         0|            0|            0|  0.00%|
    71|         0|            0|            0|  0.00%|    Examples::
    72|         0|            0|            0|  0.00%|
    73|         0|            0|            0|  0.00%|        >>> # pool of size=3, stride=2
    74|         0|            0|            0|  0.00%|        >>> m = nn.MaxPool1d(3, stride=2)
    75|         0|            0|            0|  0.00%|        >>> input = torch.randn(20, 16, 50)
    76|         0|            0|            0|  0.00%|        >>> output = m(input)
    77|         0|            0|            0|  0.00%|
    78|         0|            0|            0|  0.00%|    .. _link:
    79|         0|            0|            0|  0.00%|        https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md
    80|         0|            0|            0|  0.00%|    """
    81|         0|            0|            0|  0.00%|
    82|         0|            0|            0|  0.00%|    kernel_size: _size_1_t
    83|         0|            0|            0|  0.00%|    stride: _size_1_t
    84|         0|            0|            0|  0.00%|    padding: _size_1_t
    85|         0|            0|            0|  0.00%|    dilation: _size_1_t
    86|         0|            0|            0|  0.00%|
    87|         0|            0|            0|  0.00%|    def forward(self, input: Tensor) -> Tensor:
    88|         0|            0|            0|  0.00%|        return F.max_pool1d(input, self.kernel_size, self.stride,
    89|         0|            0|            0|  0.00%|                            self.padding, self.dilation, self.ceil_mode,
    90|         0|            0|            0|  0.00%|                            self.return_indices)
    91|         0|            0|            0|  0.00%|
    92|         0|            0|            0|  0.00%|
    93|         0|            0|            0|  0.00%|class MaxPool2d(_MaxPoolNd):
    94|         0|            0|            0|  0.00%|    r"""Applies a 2D max pooling over an input signal composed of several input
    95|         0|            0|            0|  0.00%|    planes.
    96|         0|            0|            0|  0.00%|
    97|         0|            0|            0|  0.00%|    In the simplest case, the output value of the layer with input size :math:`(N, C, H, W)`,
    98|         0|            0|            0|  0.00%|    output :math:`(N, C, H_{out}, W_{out})` and :attr:`kernel_size` :math:`(kH, kW)`
    99|         0|            0|            0|  0.00%|    can be precisely described as:
   100|         0|            0|            0|  0.00%|
   101|         0|            0|            0|  0.00%|    .. math::
   102|         0|            0|            0|  0.00%|        \begin{aligned}
   103|         0|            0|            0|  0.00%|            out(N_i, C_j, h, w) ={} & \max_{m=0, \ldots, kH-1} \max_{n=0, \ldots, kW-1} \\
   104|         0|            0|            0|  0.00%|                                    & \text{input}(N_i, C_j, \text{stride[0]} \times h + m,
   105|         0|            0|            0|  0.00%|                                                   \text{stride[1]} \times w + n)
   106|         0|            0|            0|  0.00%|        \end{aligned}
   107|         0|            0|            0|  0.00%|
   108|         0|            0|            0|  0.00%|    If :attr:`padding` is non-zero, then the input is implicitly zero-padded on both sides
   109|         0|            0|            0|  0.00%|    for :attr:`padding` number of points. :attr:`dilation` controls the spacing between the kernel points.
   110|         0|            0|            0|  0.00%|    It is harder to describe, but this `link`_ has a nice visualization of what :attr:`dilation` does.
   111|         0|            0|            0|  0.00%|
   112|         0|            0|            0|  0.00%|    Note:
   113|         0|            0|            0|  0.00%|        When ceil_mode=True, sliding windows are allowed to go off-bounds if they start within the left padding
   114|         0|            0|            0|  0.00%|        or the input. Sliding windows that would start in the right padded region are ignored.
   115|         0|            0|            0|  0.00%|
   116|         0|            0|            0|  0.00%|    The parameters :attr:`kernel_size`, :attr:`stride`, :attr:`padding`, :attr:`dilation` can either be:
   117|         0|            0|            0|  0.00%|
   118|         0|            0|            0|  0.00%|        - a single ``int`` -- in which case the same value is used for the height and width dimension
   119|         0|            0|            0|  0.00%|        - a ``tuple`` of two ints -- in which case, the first `int` is used for the height dimension,
   120|         0|            0|            0|  0.00%|          and the second `int` for the width dimension
   121|         0|            0|            0|  0.00%|
   122|         0|            0|            0|  0.00%|    Args:
   123|         0|            0|            0|  0.00%|        kernel_size: the size of the window to take a max over
   124|         0|            0|            0|  0.00%|        stride: the stride of the window. Default value is :attr:`kernel_size`
   125|         0|            0|            0|  0.00%|        padding: implicit zero padding to be added on both sides
   126|         0|            0|            0|  0.00%|        dilation: a parameter that controls the stride of elements in the window
   127|         0|            0|            0|  0.00%|        return_indices: if ``True``, will return the max indices along with the outputs.
   128|         0|            0|            0|  0.00%|                        Useful for :class:`torch.nn.MaxUnpool2d` later
   129|         0|            0|            0|  0.00%|        ceil_mode: when True, will use `ceil` instead of `floor` to compute the output shape
   130|         0|            0|            0|  0.00%|
   131|         0|            0|            0|  0.00%|    Shape:
   132|         0|            0|            0|  0.00%|        - Input: :math:`(N, C, H_{in}, W_{in})`
   133|         0|            0|            0|  0.00%|        - Output: :math:`(N, C, H_{out}, W_{out})`, where
   134|         0|            0|            0|  0.00%|
   135|         0|            0|            0|  0.00%|          .. math::
   136|         0|            0|            0|  0.00%|              H_{out} = \left\lfloor\frac{H_{in} + 2 * \text{padding[0]} - \text{dilation[0]}
   137|         0|            0|            0|  0.00%|                    \times (\text{kernel\_size[0]} - 1) - 1}{\text{stride[0]}} + 1\right\rfloor
   138|         0|            0|            0|  0.00%|
   139|         0|            0|            0|  0.00%|          .. math::
   140|         0|            0|            0|  0.00%|              W_{out} = \left\lfloor\frac{W_{in} + 2 * \text{padding[1]} - \text{dilation[1]}
   141|         0|            0|            0|  0.00%|                    \times (\text{kernel\_size[1]} - 1) - 1}{\text{stride[1]}} + 1\right\rfloor
   142|         0|            0|            0|  0.00%|
   143|         0|            0|            0|  0.00%|    Examples::
   144|         0|            0|            0|  0.00%|
   145|         0|            0|            0|  0.00%|        >>> # pool of square window of size=3, stride=2
   146|         0|            0|            0|  0.00%|        >>> m = nn.MaxPool2d(3, stride=2)
   147|         0|            0|            0|  0.00%|        >>> # pool of non-square window
   148|         0|            0|            0|  0.00%|        >>> m = nn.MaxPool2d((3, 2), stride=(2, 1))
   149|         0|            0|            0|  0.00%|        >>> input = torch.randn(20, 16, 50, 32)
   150|         0|            0|            0|  0.00%|        >>> output = m(input)
   151|         0|            0|            0|  0.00%|
   152|         0|            0|            0|  0.00%|    .. _link:
   153|         0|            0|            0|  0.00%|        https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md
   154|         0|            0|            0|  0.00%|    """
   155|         0|            0|            0|  0.00%|
   156|         0|            0|            0|  0.00%|    kernel_size: _size_2_t
   157|         0|            0|            0|  0.00%|    stride: _size_2_t
   158|         0|            0|            0|  0.00%|    padding: _size_2_t
   159|         0|            0|            0|  0.00%|    dilation: _size_2_t
   160|         0|            0|            0|  0.00%|
   161|       100|  0.000679016|  6.79016e-06|  0.00%|    def forward(self, input: Tensor) -> Tensor:
   162|       200|   0.00285459|  1.42729e-05|  0.01%|        return F.max_pool2d(input, self.kernel_size, self.stride,
(call)|       100|     0.705995|   0.00705995|  1.32%|# /opt/conda/lib/python3.8/site-packages/torch/_jit_internal.py:395 fn
   163|       100|  0.000587463|  5.87463e-06|  0.00%|                            self.padding, self.dilation, self.ceil_mode,
   164|       100|  0.000495672|  4.95672e-06|  0.00%|                            self.return_indices)
   165|         0|            0|            0|  0.00%|
   166|         0|            0|            0|  0.00%|
   167|         0|            0|            0|  0.00%|class MaxPool3d(_MaxPoolNd):
   168|         0|            0|            0|  0.00%|    r"""Applies a 3D max pooling over an input signal composed of several input
   169|         0|            0|            0|  0.00%|    planes.
   170|         0|            0|            0|  0.00%|
   171|         0|            0|            0|  0.00%|    In the simplest case, the output value of the layer with input size :math:`(N, C, D, H, W)`,
   172|         0|            0|            0|  0.00%|    output :math:`(N, C, D_{out}, H_{out}, W_{out})` and :attr:`kernel_size` :math:`(kD, kH, kW)`
   173|         0|            0|            0|  0.00%|    can be precisely described as:
   174|         0|            0|            0|  0.00%|
   175|         0|            0|            0|  0.00%|    .. math::
   176|         0|            0|            0|  0.00%|        \begin{aligned}
   177|         0|            0|            0|  0.00%|            \text{out}(N_i, C_j, d, h, w) ={} & \max_{k=0, \ldots, kD-1} \max_{m=0, \ldots, kH-1} \max_{n=0, \ldots, kW-1} \\
   178|         0|            0|            0|  0.00%|                                              & \text{input}(N_i, C_j, \text{stride[0]} \times d + k,
   179|         0|            0|            0|  0.00%|                                                             \text{stride[1]} \times h + m, \text{stride[2]} \times w + n)
   180|         0|            0|            0|  0.00%|        \end{aligned}
   181|         0|            0|            0|  0.00%|
   182|         0|            0|            0|  0.00%|    If :attr:`padding` is non-zero, then the input is implicitly zero-padded on both sides
   183|         0|            0|            0|  0.00%|    for :attr:`padding` number of points. :attr:`dilation` controls the spacing between the kernel points.
   184|         0|            0|            0|  0.00%|    It is harder to describe, but this `link`_ has a nice visualization of what :attr:`dilation` does.
   185|         0|            0|            0|  0.00%|
   186|         0|            0|            0|  0.00%|    Note:
   187|         0|            0|            0|  0.00%|        When ceil_mode=True, sliding windows are allowed to go off-bounds if they start within the left padding
   188|         0|            0|            0|  0.00%|        or the input. Sliding windows that would start in the right padded region are ignored.
   189|         0|            0|            0|  0.00%|
   190|         0|            0|            0|  0.00%|    The parameters :attr:`kernel_size`, :attr:`stride`, :attr:`padding`, :attr:`dilation` can either be:
   191|         0|            0|            0|  0.00%|
   192|         0|            0|            0|  0.00%|        - a single ``int`` -- in which case the same value is used for the depth, height and width dimension
   193|         0|            0|            0|  0.00%|        - a ``tuple`` of three ints -- in which case, the first `int` is used for the depth dimension,
   194|         0|            0|            0|  0.00%|          the second `int` for the height dimension and the third `int` for the width dimension
   195|         0|            0|            0|  0.00%|
   196|         0|            0|            0|  0.00%|    Args:
   197|         0|            0|            0|  0.00%|        kernel_size: the size of the window to take a max over
   198|         0|            0|            0|  0.00%|        stride: the stride of the window. Default value is :attr:`kernel_size`
   199|         0|            0|            0|  0.00%|        padding: implicit zero padding to be added on all three sides
   200|         0|            0|            0|  0.00%|        dilation: a parameter that controls the stride of elements in the window
   201|         0|            0|            0|  0.00%|        return_indices: if ``True``, will return the max indices along with the outputs.
   202|         0|            0|            0|  0.00%|                        Useful for :class:`torch.nn.MaxUnpool3d` later
   203|         0|            0|            0|  0.00%|        ceil_mode: when True, will use `ceil` instead of `floor` to compute the output shape
   204|         0|            0|            0|  0.00%|
   205|         0|            0|            0|  0.00%|    Shape:
   206|         0|            0|            0|  0.00%|        - Input: :math:`(N, C, D_{in}, H_{in}, W_{in})`
   207|         0|            0|            0|  0.00%|        - Output: :math:`(N, C, D_{out}, H_{out}, W_{out})`, where
   208|         0|            0|            0|  0.00%|
   209|         0|            0|            0|  0.00%|          .. math::
   210|         0|            0|            0|  0.00%|              D_{out} = \left\lfloor\frac{D_{in} + 2 \times \text{padding}[0] - \text{dilation}[0] \times
   211|         0|            0|            0|  0.00%|                (\text{kernel\_size}[0] - 1) - 1}{\text{stride}[0]} + 1\right\rfloor
   212|         0|            0|            0|  0.00%|
   213|         0|            0|            0|  0.00%|          .. math::
   214|         0|            0|            0|  0.00%|              H_{out} = \left\lfloor\frac{H_{in} + 2 \times \text{padding}[1] - \text{dilation}[1] \times
   215|         0|            0|            0|  0.00%|                (\text{kernel\_size}[1] - 1) - 1}{\text{stride}[1]} + 1\right\rfloor
   216|         0|            0|            0|  0.00%|
   217|         0|            0|            0|  0.00%|          .. math::
   218|         0|            0|            0|  0.00%|              W_{out} = \left\lfloor\frac{W_{in} + 2 \times \text{padding}[2] - \text{dilation}[2] \times
   219|         0|            0|            0|  0.00%|                (\text{kernel\_size}[2] - 1) - 1}{\text{stride}[2]} + 1\right\rfloor
   220|         0|            0|            0|  0.00%|
   221|         0|            0|            0|  0.00%|    Examples::
   222|         0|            0|            0|  0.00%|
   223|         0|            0|            0|  0.00%|        >>> # pool of square window of size=3, stride=2
   224|         0|            0|            0|  0.00%|        >>> m = nn.MaxPool3d(3, stride=2)
   225|         0|            0|            0|  0.00%|        >>> # pool of non-square window
   226|         0|            0|            0|  0.00%|        >>> m = nn.MaxPool3d((3, 2, 2), stride=(2, 1, 2))
   227|         0|            0|            0|  0.00%|        >>> input = torch.randn(20, 16, 50,44, 31)
   228|         0|            0|            0|  0.00%|        >>> output = m(input)
   229|         0|            0|            0|  0.00%|
   230|         0|            0|            0|  0.00%|    .. _link:
   231|         0|            0|            0|  0.00%|        https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md
   232|         0|            0|            0|  0.00%|    """  # noqa: E501
   233|         0|            0|            0|  0.00%|
   234|         0|            0|            0|  0.00%|    kernel_size: _size_3_t
   235|         0|            0|            0|  0.00%|    stride: _size_3_t
   236|         0|            0|            0|  0.00%|    padding: _size_3_t
   237|         0|            0|            0|  0.00%|    dilation: _size_3_t
   238|         0|            0|            0|  0.00%|
   239|         0|            0|            0|  0.00%|    def forward(self, input: Tensor) -> Tensor:
   240|         0|            0|            0|  0.00%|        return F.max_pool3d(input, self.kernel_size, self.stride,
   241|         0|            0|            0|  0.00%|                            self.padding, self.dilation, self.ceil_mode,
   242|         0|            0|            0|  0.00%|                            self.return_indices)
   243|         0|            0|            0|  0.00%|
   244|         0|            0|            0|  0.00%|
   245|         0|            0|            0|  0.00%|class _MaxUnpoolNd(Module):
   246|         0|            0|            0|  0.00%|
   247|         0|            0|            0|  0.00%|    def extra_repr(self) -> str:
   248|         0|            0|            0|  0.00%|        return 'kernel_size={}, stride={}, padding={}'.format(
   249|         0|            0|            0|  0.00%|            self.kernel_size, self.stride, self.padding
   250|         0|            0|            0|  0.00%|        )
   251|         0|            0|            0|  0.00%|
   252|         0|            0|            0|  0.00%|
   253|         0|            0|            0|  0.00%|class MaxUnpool1d(_MaxUnpoolNd):
   254|         0|            0|            0|  0.00%|    r"""Computes a partial inverse of :class:`MaxPool1d`.
   255|         0|            0|            0|  0.00%|
   256|         0|            0|            0|  0.00%|    :class:`MaxPool1d` is not fully invertible, since the non-maximal values are lost.
   257|         0|            0|            0|  0.00%|
   258|         0|            0|            0|  0.00%|    :class:`MaxUnpool1d` takes in as input the output of :class:`MaxPool1d`
   259|         0|            0|            0|  0.00%|    including the indices of the maximal values and computes a partial inverse
   260|         0|            0|            0|  0.00%|    in which all non-maximal values are set to zero.
   261|         0|            0|            0|  0.00%|
   262|         0|            0|            0|  0.00%|    .. note:: :class:`MaxPool1d` can map several input sizes to the same output
   263|         0|            0|            0|  0.00%|              sizes. Hence, the inversion process can get ambiguous.
   264|         0|            0|            0|  0.00%|              To accommodate this, you can provide the needed output size
   265|         0|            0|            0|  0.00%|              as an additional argument :attr:`output_size` in the forward call.
   266|         0|            0|            0|  0.00%|              See the Inputs and Example below.
   267|         0|            0|            0|  0.00%|
   268|         0|            0|            0|  0.00%|    Args:
   269|         0|            0|            0|  0.00%|        kernel_size (int or tuple): Size of the max pooling window.
   270|         0|            0|            0|  0.00%|        stride (int or tuple): Stride of the max pooling window.
   271|         0|            0|            0|  0.00%|            It is set to :attr:`kernel_size` by default.
   272|         0|            0|            0|  0.00%|        padding (int or tuple): Padding that was added to the input
   273|         0|            0|            0|  0.00%|
   274|         0|            0|            0|  0.00%|    Inputs:
   275|         0|            0|            0|  0.00%|        - `input`: the input Tensor to invert
   276|         0|            0|            0|  0.00%|        - `indices`: the indices given out by :class:`~torch.nn.MaxPool1d`
   277|         0|            0|            0|  0.00%|        - `output_size` (optional): the targeted output size
   278|         0|            0|            0|  0.00%|
   279|         0|            0|            0|  0.00%|    Shape:
   280|         0|            0|            0|  0.00%|        - Input: :math:`(N, C, H_{in})`
   281|         0|            0|            0|  0.00%|        - Output: :math:`(N, C, H_{out})`, where
   282|         0|            0|            0|  0.00%|
   283|         0|            0|            0|  0.00%|          .. math::
   284|         0|            0|            0|  0.00%|              H_{out} = (H_{in} - 1) \times \text{stride}[0] - 2 \times \text{padding}[0] + \text{kernel\_size}[0]
   285|         0|            0|            0|  0.00%|
   286|         0|            0|            0|  0.00%|          or as given by :attr:`output_size` in the call operator
   287|         0|            0|            0|  0.00%|
   288|         0|            0|            0|  0.00%|    Example::
   289|         0|            0|            0|  0.00%|
   290|         0|            0|            0|  0.00%|        >>> pool = nn.MaxPool1d(2, stride=2, return_indices=True)
   291|         0|            0|            0|  0.00%|        >>> unpool = nn.MaxUnpool1d(2, stride=2)
   292|         0|            0|            0|  0.00%|        >>> input = torch.tensor([[[1., 2, 3, 4, 5, 6, 7, 8]]])
   293|         0|            0|            0|  0.00%|        >>> output, indices = pool(input)
   294|         0|            0|            0|  0.00%|        >>> unpool(output, indices)
   295|         0|            0|            0|  0.00%|        tensor([[[ 0.,  2.,  0.,  4.,  0.,  6.,  0., 8.]]])
   296|         0|            0|            0|  0.00%|
   297|         0|            0|            0|  0.00%|        >>> # Example showcasing the use of output_size
   298|         0|            0|            0|  0.00%|        >>> input = torch.tensor([[[1., 2, 3, 4, 5, 6, 7, 8, 9]]])
   299|         0|            0|            0|  0.00%|        >>> output, indices = pool(input)
   300|         0|            0|            0|  0.00%|        >>> unpool(output, indices, output_size=input.size())
   301|         0|            0|            0|  0.00%|        tensor([[[ 0.,  2.,  0.,  4.,  0.,  6.,  0., 8.,  0.]]])
   302|         0|            0|            0|  0.00%|
   303|         0|            0|            0|  0.00%|        >>> unpool(output, indices)
   304|         0|            0|            0|  0.00%|        tensor([[[ 0.,  2.,  0.,  4.,  0.,  6.,  0., 8.]]])
   305|         0|            0|            0|  0.00%|    """
   306|         0|            0|            0|  0.00%|
   307|         0|            0|            0|  0.00%|    kernel_size: _size_1_t
   308|         0|            0|            0|  0.00%|    stride: _size_1_t
   309|         0|            0|            0|  0.00%|    padding: _size_1_t
   310|         0|            0|            0|  0.00%|
   311|         0|            0|            0|  0.00%|    def __init__(self, kernel_size: _size_1_t, stride: Optional[_size_1_t] = None, padding: _size_1_t = 0) -> None:
   312|         0|            0|            0|  0.00%|        super(MaxUnpool1d, self).__init__()
   313|         0|            0|            0|  0.00%|        self.kernel_size = _single(kernel_size)
   314|         0|            0|            0|  0.00%|        self.stride = _single(stride if (stride is not None) else kernel_size)
   315|         0|            0|            0|  0.00%|        self.padding = _single(padding)
   316|         0|            0|            0|  0.00%|
   317|         0|            0|            0|  0.00%|    def forward(self, input: Tensor, indices: Tensor, output_size: Optional[List[int]] = None) -> Tensor:
   318|         0|            0|            0|  0.00%|        return F.max_unpool1d(input, indices, self.kernel_size, self.stride,
   319|         0|            0|            0|  0.00%|                              self.padding, output_size)
   320|         0|            0|            0|  0.00%|
   321|         0|            0|            0|  0.00%|
   322|         0|            0|            0|  0.00%|class MaxUnpool2d(_MaxUnpoolNd):
   323|         0|            0|            0|  0.00%|    r"""Computes a partial inverse of :class:`MaxPool2d`.
   324|         0|            0|            0|  0.00%|
   325|         0|            0|            0|  0.00%|    :class:`MaxPool2d` is not fully invertible, since the non-maximal values are lost.
   326|         0|            0|            0|  0.00%|
   327|         0|            0|            0|  0.00%|    :class:`MaxUnpool2d` takes in as input the output of :class:`MaxPool2d`
   328|         0|            0|            0|  0.00%|    including the indices of the maximal values and computes a partial inverse
   329|         0|            0|            0|  0.00%|    in which all non-maximal values are set to zero.
   330|         0|            0|            0|  0.00%|
   331|         0|            0|            0|  0.00%|    .. note:: :class:`MaxPool2d` can map several input sizes to the same output
   332|         0|            0|            0|  0.00%|              sizes. Hence, the inversion process can get ambiguous.
   333|         0|            0|            0|  0.00%|              To accommodate this, you can provide the needed output size
   334|         0|            0|            0|  0.00%|              as an additional argument :attr:`output_size` in the forward call.
   335|         0|            0|            0|  0.00%|              See the Inputs and Example below.
   336|         0|            0|            0|  0.00%|
   337|         0|            0|            0|  0.00%|    Args:
   338|         0|            0|            0|  0.00%|        kernel_size (int or tuple): Size of the max pooling window.
   339|         0|            0|            0|  0.00%|        stride (int or tuple): Stride of the max pooling window.
   340|         0|            0|            0|  0.00%|            It is set to :attr:`kernel_size` by default.
   341|         0|            0|            0|  0.00%|        padding (int or tuple): Padding that was added to the input
   342|         0|            0|            0|  0.00%|
   343|         0|            0|            0|  0.00%|    Inputs:
   344|         0|            0|            0|  0.00%|        - `input`: the input Tensor to invert
   345|         0|            0|            0|  0.00%|        - `indices`: the indices given out by :class:`~torch.nn.MaxPool2d`
   346|         0|            0|            0|  0.00%|        - `output_size` (optional): the targeted output size
   347|         0|            0|            0|  0.00%|
   348|         0|            0|            0|  0.00%|    Shape:
   349|         0|            0|            0|  0.00%|        - Input: :math:`(N, C, H_{in}, W_{in})`
   350|         0|            0|            0|  0.00%|        - Output: :math:`(N, C, H_{out}, W_{out})`, where
   351|         0|            0|            0|  0.00%|
   352|         0|            0|            0|  0.00%|          .. math::
   353|         0|            0|            0|  0.00%|            H_{out} = (H_{in} - 1) \times \text{stride[0]} - 2 \times \text{padding[0]} + \text{kernel\_size[0]}
   354|         0|            0|            0|  0.00%|
   355|         0|            0|            0|  0.00%|          .. math::
   356|         0|            0|            0|  0.00%|            W_{out} = (W_{in} - 1) \times \text{stride[1]} - 2 \times \text{padding[1]} + \text{kernel\_size[1]}
   357|         0|            0|            0|  0.00%|
   358|         0|            0|            0|  0.00%|          or as given by :attr:`output_size` in the call operator
   359|         0|            0|            0|  0.00%|
   360|         0|            0|            0|  0.00%|    Example::
   361|         0|            0|            0|  0.00%|
   362|         0|            0|            0|  0.00%|        >>> pool = nn.MaxPool2d(2, stride=2, return_indices=True)
   363|         0|            0|            0|  0.00%|        >>> unpool = nn.MaxUnpool2d(2, stride=2)
   364|         0|            0|            0|  0.00%|        >>> input = torch.tensor([[[[ 1.,  2,  3,  4],
   365|         0|            0|            0|  0.00%|                                    [ 5,  6,  7,  8],
   366|         0|            0|            0|  0.00%|                                    [ 9, 10, 11, 12],
   367|         0|            0|            0|  0.00%|                                    [13, 14, 15, 16]]]])
   368|         0|            0|            0|  0.00%|        >>> output, indices = pool(input)
   369|         0|            0|            0|  0.00%|        >>> unpool(output, indices)
   370|         0|            0|            0|  0.00%|        tensor([[[[  0.,   0.,   0.,   0.],
   371|         0|            0|            0|  0.00%|                  [  0.,   6.,   0.,   8.],
   372|         0|            0|            0|  0.00%|                  [  0.,   0.,   0.,   0.],
   373|         0|            0|            0|  0.00%|                  [  0.,  14.,   0.,  16.]]]])
   374|         0|            0|            0|  0.00%|
   375|         0|            0|            0|  0.00%|        >>> # specify a different output size than input size
   376|         0|            0|            0|  0.00%|        >>> unpool(output, indices, output_size=torch.Size([1, 1, 5, 5]))
   377|         0|            0|            0|  0.00%|        tensor([[[[  0.,   0.,   0.,   0.,   0.],
   378|         0|            0|            0|  0.00%|                  [  6.,   0.,   8.,   0.,   0.],
   379|         0|            0|            0|  0.00%|                  [  0.,   0.,   0.,  14.,   0.],
   380|         0|            0|            0|  0.00%|                  [ 16.,   0.,   0.,   0.,   0.],
   381|         0|            0|            0|  0.00%|                  [  0.,   0.,   0.,   0.,   0.]]]])
   382|         0|            0|            0|  0.00%|    """
   383|         0|            0|            0|  0.00%|
   384|         0|            0|            0|  0.00%|    kernel_size: _size_2_t
   385|         0|            0|            0|  0.00%|    stride: _size_2_t
   386|         0|            0|            0|  0.00%|    padding: _size_2_t
   387|         0|            0|            0|  0.00%|
   388|         0|            0|            0|  0.00%|    def __init__(self, kernel_size: _size_2_t, stride: Optional[_size_2_t] = None, padding: _size_2_t = 0) -> None:
   389|         0|            0|            0|  0.00%|        super(MaxUnpool2d, self).__init__()
   390|         0|            0|            0|  0.00%|        self.kernel_size = _pair(kernel_size)
   391|         0|            0|            0|  0.00%|        self.stride = _pair(stride if (stride is not None) else kernel_size)
   392|         0|            0|            0|  0.00%|        self.padding = _pair(padding)
   393|         0|            0|            0|  0.00%|
   394|         0|            0|            0|  0.00%|    def forward(self, input: Tensor, indices: Tensor, output_size: Optional[List[int]] = None) -> Tensor:
   395|         0|            0|            0|  0.00%|        return F.max_unpool2d(input, indices, self.kernel_size, self.stride,
   396|         0|            0|            0|  0.00%|                              self.padding, output_size)
   397|         0|            0|            0|  0.00%|
   398|         0|            0|            0|  0.00%|
   399|         0|            0|            0|  0.00%|class MaxUnpool3d(_MaxUnpoolNd):
   400|         0|            0|            0|  0.00%|    r"""Computes a partial inverse of :class:`MaxPool3d`.
   401|         0|            0|            0|  0.00%|
   402|         0|            0|            0|  0.00%|    :class:`MaxPool3d` is not fully invertible, since the non-maximal values are lost.
   403|         0|            0|            0|  0.00%|    :class:`MaxUnpool3d` takes in as input the output of :class:`MaxPool3d`
   404|         0|            0|            0|  0.00%|    including the indices of the maximal values and computes a partial inverse
   405|         0|            0|            0|  0.00%|    in which all non-maximal values are set to zero.
   406|         0|            0|            0|  0.00%|
   407|         0|            0|            0|  0.00%|    .. note:: :class:`MaxPool3d` can map several input sizes to the same output
   408|         0|            0|            0|  0.00%|              sizes. Hence, the inversion process can get ambiguous.
   409|         0|            0|            0|  0.00%|              To accommodate this, you can provide the needed output size
   410|         0|            0|            0|  0.00%|              as an additional argument :attr:`output_size` in the forward call.
   411|         0|            0|            0|  0.00%|              See the Inputs section below.
   412|         0|            0|            0|  0.00%|
   413|         0|            0|            0|  0.00%|    Args:
   414|         0|            0|            0|  0.00%|        kernel_size (int or tuple): Size of the max pooling window.
   415|         0|            0|            0|  0.00%|        stride (int or tuple): Stride of the max pooling window.
   416|         0|            0|            0|  0.00%|            It is set to :attr:`kernel_size` by default.
   417|         0|            0|            0|  0.00%|        padding (int or tuple): Padding that was added to the input
   418|         0|            0|            0|  0.00%|
   419|         0|            0|            0|  0.00%|    Inputs:
   420|         0|            0|            0|  0.00%|        - `input`: the input Tensor to invert
   421|         0|            0|            0|  0.00%|        - `indices`: the indices given out by :class:`~torch.nn.MaxPool3d`
   422|         0|            0|            0|  0.00%|        - `output_size` (optional): the targeted output size
   423|         0|            0|            0|  0.00%|
   424|         0|            0|            0|  0.00%|    Shape:
   425|         0|            0|            0|  0.00%|        - Input: :math:`(N, C, D_{in}, H_{in}, W_{in})`
   426|         0|            0|            0|  0.00%|        - Output: :math:`(N, C, D_{out}, H_{out}, W_{out})`, where
   427|         0|            0|            0|  0.00%|
   428|         0|            0|            0|  0.00%|          .. math::
   429|         0|            0|            0|  0.00%|              D_{out} = (D_{in} - 1) \times \text{stride[0]} - 2 \times \text{padding[0]} + \text{kernel\_size[0]}
   430|         0|            0|            0|  0.00%|
   431|         0|            0|            0|  0.00%|          .. math::
   432|         0|            0|            0|  0.00%|              H_{out} = (H_{in} - 1) \times \text{stride[1]} - 2 \times \text{padding[1]} + \text{kernel\_size[1]}
   433|         0|            0|            0|  0.00%|
   434|         0|            0|            0|  0.00%|          .. math::
   435|         0|            0|            0|  0.00%|              W_{out} = (W_{in} - 1) \times \text{stride[2]} - 2 \times \text{padding[2]} + \text{kernel\_size[2]}
   436|         0|            0|            0|  0.00%|
   437|         0|            0|            0|  0.00%|          or as given by :attr:`output_size` in the call operator
   438|         0|            0|            0|  0.00%|
   439|         0|            0|            0|  0.00%|    Example::
   440|         0|            0|            0|  0.00%|
   441|         0|            0|            0|  0.00%|        >>> # pool of square window of size=3, stride=2
   442|         0|            0|            0|  0.00%|        >>> pool = nn.MaxPool3d(3, stride=2, return_indices=True)
   443|         0|            0|            0|  0.00%|        >>> unpool = nn.MaxUnpool3d(3, stride=2)
   444|         0|            0|            0|  0.00%|        >>> output, indices = pool(torch.randn(20, 16, 51, 33, 15))
   445|         0|            0|            0|  0.00%|        >>> unpooled_output = unpool(output, indices)
   446|         0|            0|            0|  0.00%|        >>> unpooled_output.size()
   447|         0|            0|            0|  0.00%|        torch.Size([20, 16, 51, 33, 15])
   448|         0|            0|            0|  0.00%|    """
   449|         0|            0|            0|  0.00%|
   450|         0|            0|            0|  0.00%|    kernel_size: _size_3_t
   451|         0|            0|            0|  0.00%|    stride: _size_3_t
   452|         0|            0|            0|  0.00%|    padding: _size_3_t
   453|         0|            0|            0|  0.00%|
   454|         0|            0|            0|  0.00%|    def __init__(self, kernel_size: _size_3_t, stride: Optional[_size_3_t] = None, padding: _size_3_t = 0) -> None:
   455|         0|            0|            0|  0.00%|        super(MaxUnpool3d, self).__init__()
   456|         0|            0|            0|  0.00%|        self.kernel_size = _triple(kernel_size)
   457|         0|            0|            0|  0.00%|        self.stride = _triple(stride if (stride is not None) else kernel_size)
   458|         0|            0|            0|  0.00%|        self.padding = _triple(padding)
   459|         0|            0|            0|  0.00%|
   460|         0|            0|            0|  0.00%|    def forward(self, input: Tensor, indices: Tensor, output_size: Optional[List[int]] = None) -> Tensor:
   461|         0|            0|            0|  0.00%|        return F.max_unpool3d(input, indices, self.kernel_size, self.stride,
   462|         0|            0|            0|  0.00%|                              self.padding, output_size)
   463|         0|            0|            0|  0.00%|
   464|         0|            0|            0|  0.00%|
   465|         0|            0|            0|  0.00%|class _AvgPoolNd(Module):
   466|         0|            0|            0|  0.00%|    __constants__ = ['kernel_size', 'stride', 'padding', 'ceil_mode', 'count_include_pad']
   467|         0|            0|            0|  0.00%|
   468|         0|            0|            0|  0.00%|    def extra_repr(self) -> str:
   469|         0|            0|            0|  0.00%|        return 'kernel_size={}, stride={}, padding={}'.format(
   470|         0|            0|            0|  0.00%|            self.kernel_size, self.stride, self.padding
   471|         0|            0|            0|  0.00%|        )
   472|         0|            0|            0|  0.00%|
   473|         0|            0|            0|  0.00%|
   474|         0|            0|            0|  0.00%|class AvgPool1d(_AvgPoolNd):
   475|         0|            0|            0|  0.00%|    r"""Applies a 1D average pooling over an input signal composed of several
   476|         0|            0|            0|  0.00%|    input planes.
   477|         0|            0|            0|  0.00%|
   478|         0|            0|            0|  0.00%|    In the simplest case, the output value of the layer with input size :math:`(N, C, L)`,
   479|         0|            0|            0|  0.00%|    output :math:`(N, C, L_{out})` and :attr:`kernel_size` :math:`k`
   480|         0|            0|            0|  0.00%|    can be precisely described as:
   481|         0|            0|            0|  0.00%|
   482|         0|            0|            0|  0.00%|    .. math::
   483|         0|            0|            0|  0.00%|
   484|         0|            0|            0|  0.00%|        \text{out}(N_i, C_j, l) = \frac{1}{k} \sum_{m=0}^{k-1}
   485|         0|            0|            0|  0.00%|                               \text{input}(N_i, C_j, \text{stride} \times l + m)
   486|         0|            0|            0|  0.00%|
   487|         0|            0|            0|  0.00%|    If :attr:`padding` is non-zero, then the input is implicitly zero-padded on both sides
   488|         0|            0|            0|  0.00%|    for :attr:`padding` number of points.
   489|         0|            0|            0|  0.00%|
   490|         0|            0|            0|  0.00%|    Note:
   491|         0|            0|            0|  0.00%|        When ceil_mode=True, sliding windows are allowed to go off-bounds if they start within the left padding
   492|         0|            0|            0|  0.00%|        or the input. Sliding windows that would start in the right padded region are ignored.
   493|         0|            0|            0|  0.00%|
   494|         0|            0|            0|  0.00%|    The parameters :attr:`kernel_size`, :attr:`stride`, :attr:`padding` can each be
   495|         0|            0|            0|  0.00%|    an ``int`` or a one-element tuple.
   496|         0|            0|            0|  0.00%|
   497|         0|            0|            0|  0.00%|    Args:
   498|         0|            0|            0|  0.00%|        kernel_size: the size of the window
   499|         0|            0|            0|  0.00%|        stride: the stride of the window. Default value is :attr:`kernel_size`
   500|         0|            0|            0|  0.00%|        padding: implicit zero padding to be added on both sides
   501|         0|            0|            0|  0.00%|        ceil_mode: when True, will use `ceil` instead of `floor` to compute the output shape
   502|         0|            0|            0|  0.00%|        count_include_pad: when True, will include the zero-padding in the averaging calculation
   503|         0|            0|            0|  0.00%|
   504|         0|            0|            0|  0.00%|    Shape:
   505|         0|            0|            0|  0.00%|        - Input: :math:`(N, C, L_{in})`
   506|         0|            0|            0|  0.00%|        - Output: :math:`(N, C, L_{out})`, where
   507|         0|            0|            0|  0.00%|
   508|         0|            0|            0|  0.00%|          .. math::
   509|         0|            0|            0|  0.00%|              L_{out} = \left\lfloor \frac{L_{in} +
   510|         0|            0|            0|  0.00%|              2 \times \text{padding} - \text{kernel\_size}}{\text{stride}} + 1\right\rfloor
   511|         0|            0|            0|  0.00%|
   512|         0|            0|            0|  0.00%|    Examples::
   513|         0|            0|            0|  0.00%|
   514|         0|            0|            0|  0.00%|        >>> # pool with window of size=3, stride=2
   515|         0|            0|            0|  0.00%|        >>> m = nn.AvgPool1d(3, stride=2)
   516|         0|            0|            0|  0.00%|        >>> m(torch.tensor([[[1.,2,3,4,5,6,7]]]))
   517|         0|            0|            0|  0.00%|        tensor([[[ 2.,  4.,  6.]]])
   518|         0|            0|            0|  0.00%|    """
   519|         0|            0|            0|  0.00%|
   520|         0|            0|            0|  0.00%|    kernel_size: _size_1_t
   521|         0|            0|            0|  0.00%|    stride: _size_1_t
   522|         0|            0|            0|  0.00%|    padding: _size_1_t
   523|         0|            0|            0|  0.00%|    ceil_mode: bool
   524|         0|            0|            0|  0.00%|    count_include_pad: bool
   525|         0|            0|            0|  0.00%|
   526|         0|            0|            0|  0.00%|    def __init__(self, kernel_size: _size_1_t, stride: _size_1_t = None, padding: _size_1_t = 0, ceil_mode: bool = False,
   527|         0|            0|            0|  0.00%|                 count_include_pad: bool = True) -> None:
   528|         0|            0|            0|  0.00%|        super(AvgPool1d, self).__init__()
   529|         0|            0|            0|  0.00%|        self.kernel_size = _single(kernel_size)
   530|         0|            0|            0|  0.00%|        self.stride = _single(stride if stride is not None else kernel_size)
   531|         0|            0|            0|  0.00%|        self.padding = _single(padding)
   532|         0|            0|            0|  0.00%|        self.ceil_mode = ceil_mode
   533|         0|            0|            0|  0.00%|        self.count_include_pad = count_include_pad
   534|         0|            0|            0|  0.00%|
   535|         0|            0|            0|  0.00%|    def forward(self, input: Tensor) -> Tensor:
   536|         0|            0|            0|  0.00%|        return F.avg_pool1d(
   537|         0|            0|            0|  0.00%|            input, self.kernel_size, self.stride, self.padding, self.ceil_mode,
   538|         0|            0|            0|  0.00%|            self.count_include_pad)
   539|         0|            0|            0|  0.00%|
   540|         0|            0|            0|  0.00%|
   541|         0|            0|            0|  0.00%|class AvgPool2d(_AvgPoolNd):
   542|         0|            0|            0|  0.00%|    r"""Applies a 2D average pooling over an input signal composed of several input
   543|         0|            0|            0|  0.00%|    planes.
   544|         0|            0|            0|  0.00%|
   545|         0|            0|            0|  0.00%|    In the simplest case, the output value of the layer with input size :math:`(N, C, H, W)`,
   546|         0|            0|            0|  0.00%|    output :math:`(N, C, H_{out}, W_{out})` and :attr:`kernel_size` :math:`(kH, kW)`
   547|         0|            0|            0|  0.00%|    can be precisely described as:
   548|         0|            0|            0|  0.00%|
   549|         0|            0|            0|  0.00%|    .. math::
   550|         0|            0|            0|  0.00%|
   551|         0|            0|            0|  0.00%|        out(N_i, C_j, h, w)  = \frac{1}{kH * kW} \sum_{m=0}^{kH-1} \sum_{n=0}^{kW-1}
   552|         0|            0|            0|  0.00%|                               input(N_i, C_j, stride[0] \times h + m, stride[1] \times w + n)
   553|         0|            0|            0|  0.00%|
   554|         0|            0|            0|  0.00%|    If :attr:`padding` is non-zero, then the input is implicitly zero-padded on both sides
   555|         0|            0|            0|  0.00%|    for :attr:`padding` number of points.
   556|         0|            0|            0|  0.00%|
   557|         0|            0|            0|  0.00%|    Note:
   558|         0|            0|            0|  0.00%|        When ceil_mode=True, sliding windows are allowed to go off-bounds if they start within the left padding
   559|         0|            0|            0|  0.00%|        or the input. Sliding windows that would start in the right padded region are ignored.
   560|         0|            0|            0|  0.00%|
   561|         0|            0|            0|  0.00%|    The parameters :attr:`kernel_size`, :attr:`stride`, :attr:`padding` can either be:
   562|         0|            0|            0|  0.00%|
   563|         0|            0|            0|  0.00%|        - a single ``int`` -- in which case the same value is used for the height and width dimension
   564|         0|            0|            0|  0.00%|        - a ``tuple`` of two ints -- in which case, the first `int` is used for the height dimension,
   565|         0|            0|            0|  0.00%|          and the second `int` for the width dimension
   566|         0|            0|            0|  0.00%|
   567|         0|            0|            0|  0.00%|    Args:
   568|         0|            0|            0|  0.00%|        kernel_size: the size of the window
   569|         0|            0|            0|  0.00%|        stride: the stride of the window. Default value is :attr:`kernel_size`
   570|         0|            0|            0|  0.00%|        padding: implicit zero padding to be added on both sides
   571|         0|            0|            0|  0.00%|        ceil_mode: when True, will use `ceil` instead of `floor` to compute the output shape
   572|         0|            0|            0|  0.00%|        count_include_pad: when True, will include the zero-padding in the averaging calculation
   573|         0|            0|            0|  0.00%|        divisor_override: if specified, it will be used as divisor, otherwise :attr:`kernel_size` will be used
   574|         0|            0|            0|  0.00%|
   575|         0|            0|            0|  0.00%|    Shape:
   576|         0|            0|            0|  0.00%|        - Input: :math:`(N, C, H_{in}, W_{in})`
   577|         0|            0|            0|  0.00%|        - Output: :math:`(N, C, H_{out}, W_{out})`, where
   578|         0|            0|            0|  0.00%|
   579|         0|            0|            0|  0.00%|          .. math::
   580|         0|            0|            0|  0.00%|              H_{out} = \left\lfloor\frac{H_{in}  + 2 \times \text{padding}[0] -
   581|         0|            0|            0|  0.00%|                \text{kernel\_size}[0]}{\text{stride}[0]} + 1\right\rfloor
   582|         0|            0|            0|  0.00%|
   583|         0|            0|            0|  0.00%|          .. math::
   584|         0|            0|            0|  0.00%|              W_{out} = \left\lfloor\frac{W_{in}  + 2 \times \text{padding}[1] -
   585|         0|            0|            0|  0.00%|                \text{kernel\_size}[1]}{\text{stride}[1]} + 1\right\rfloor
   586|         0|            0|            0|  0.00%|
   587|         0|            0|            0|  0.00%|    Examples::
   588|         0|            0|            0|  0.00%|
   589|         0|            0|            0|  0.00%|        >>> # pool of square window of size=3, stride=2
   590|         0|            0|            0|  0.00%|        >>> m = nn.AvgPool2d(3, stride=2)
   591|         0|            0|            0|  0.00%|        >>> # pool of non-square window
   592|         0|            0|            0|  0.00%|        >>> m = nn.AvgPool2d((3, 2), stride=(2, 1))
   593|         0|            0|            0|  0.00%|        >>> input = torch.randn(20, 16, 50, 32)
   594|         0|            0|            0|  0.00%|        >>> output = m(input)
   595|         0|            0|            0|  0.00%|    """
   596|         0|            0|            0|  0.00%|    __constants__ = ['kernel_size', 'stride', 'padding', 'ceil_mode', 'count_include_pad', 'divisor_override']
   597|         0|            0|            0|  0.00%|
   598|         0|            0|            0|  0.00%|    kernel_size: _size_2_t
   599|         0|            0|            0|  0.00%|    stride: _size_2_t
   600|         0|            0|            0|  0.00%|    padding: _size_2_t
   601|         0|            0|            0|  0.00%|    ceil_mode: bool
   602|         0|            0|            0|  0.00%|    count_include_pad: bool
   603|         0|            0|            0|  0.00%|
   604|         0|            0|            0|  0.00%|    def __init__(self, kernel_size: _size_2_t, stride: Optional[_size_2_t] = None, padding: _size_2_t = 0,
   605|         0|            0|            0|  0.00%|                 ceil_mode: bool = False, count_include_pad: bool = True, divisor_override: Optional[int] = None) -> None:
   606|         0|            0|            0|  0.00%|        super(AvgPool2d, self).__init__()
   607|         0|            0|            0|  0.00%|        self.kernel_size = kernel_size
   608|         0|            0|            0|  0.00%|        self.stride = stride if (stride is not None) else kernel_size
   609|         0|            0|            0|  0.00%|        self.padding = padding
   610|         0|            0|            0|  0.00%|        self.ceil_mode = ceil_mode
   611|         0|            0|            0|  0.00%|        self.count_include_pad = count_include_pad
   612|         0|            0|            0|  0.00%|        self.divisor_override = divisor_override
   613|         0|            0|            0|  0.00%|
   614|         0|            0|            0|  0.00%|    def forward(self, input: Tensor) -> Tensor:
   615|         0|            0|            0|  0.00%|        return F.avg_pool2d(input, self.kernel_size, self.stride,
   616|         0|            0|            0|  0.00%|                            self.padding, self.ceil_mode, self.count_include_pad, self.divisor_override)
   617|         0|            0|            0|  0.00%|
   618|         0|            0|            0|  0.00%|
   619|         0|            0|            0|  0.00%|class AvgPool3d(_AvgPoolNd):
   620|         0|            0|            0|  0.00%|    r"""Applies a 3D average pooling over an input signal composed of several input
   621|         0|            0|            0|  0.00%|    planes.
   622|         0|            0|            0|  0.00%|
   623|         0|            0|            0|  0.00%|    In the simplest case, the output value of the layer with input size :math:`(N, C, D, H, W)`,
   624|         0|            0|            0|  0.00%|    output :math:`(N, C, D_{out}, H_{out}, W_{out})` and :attr:`kernel_size` :math:`(kD, kH, kW)`
   625|         0|            0|            0|  0.00%|    can be precisely described as:
   626|         0|            0|            0|  0.00%|
   627|         0|            0|            0|  0.00%|    .. math::
   628|         0|            0|            0|  0.00%|        \begin{aligned}
   629|         0|            0|            0|  0.00%|            \text{out}(N_i, C_j, d, h, w) ={} & \sum_{k=0}^{kD-1} \sum_{m=0}^{kH-1} \sum_{n=0}^{kW-1} \\
   630|         0|            0|            0|  0.00%|                                              & \frac{\text{input}(N_i, C_j, \text{stride}[0] \times d + k,
   631|         0|            0|            0|  0.00%|                                                      \text{stride}[1] \times h + m, \text{stride}[2] \times w + n)}
   632|         0|            0|            0|  0.00%|                                                     {kD \times kH \times kW}
   633|         0|            0|            0|  0.00%|        \end{aligned}
   634|         0|            0|            0|  0.00%|
   635|         0|            0|            0|  0.00%|    If :attr:`padding` is non-zero, then the input is implicitly zero-padded on all three sides
   636|         0|            0|            0|  0.00%|    for :attr:`padding` number of points.
   637|         0|            0|            0|  0.00%|
   638|         0|            0|            0|  0.00%|    Note:
   639|         0|            0|            0|  0.00%|        When ceil_mode=True, sliding windows are allowed to go off-bounds if they start within the left padding
   640|         0|            0|            0|  0.00%|        or the input. Sliding windows that would start in the right padded region are ignored.
   641|         0|            0|            0|  0.00%|
   642|         0|            0|            0|  0.00%|    The parameters :attr:`kernel_size`, :attr:`stride` can either be:
   643|         0|            0|            0|  0.00%|
   644|         0|            0|            0|  0.00%|        - a single ``int`` -- in which case the same value is used for the depth, height and width dimension
   645|         0|            0|            0|  0.00%|        - a ``tuple`` of three ints -- in which case, the first `int` is used for the depth dimension,
   646|         0|            0|            0|  0.00%|          the second `int` for the height dimension and the third `int` for the width dimension
   647|         0|            0|            0|  0.00%|
   648|         0|            0|            0|  0.00%|    Args:
   649|         0|            0|            0|  0.00%|        kernel_size: the size of the window
   650|         0|            0|            0|  0.00%|        stride: the stride of the window. Default value is :attr:`kernel_size`
   651|         0|            0|            0|  0.00%|        padding: implicit zero padding to be added on all three sides
   652|         0|            0|            0|  0.00%|        ceil_mode: when True, will use `ceil` instead of `floor` to compute the output shape
   653|         0|            0|            0|  0.00%|        count_include_pad: when True, will include the zero-padding in the averaging calculation
   654|         0|            0|            0|  0.00%|        divisor_override: if specified, it will be used as divisor, otherwise :attr:`kernel_size` will be used
   655|         0|            0|            0|  0.00%|
   656|         0|            0|            0|  0.00%|    Shape:
   657|         0|            0|            0|  0.00%|        - Input: :math:`(N, C, D_{in}, H_{in}, W_{in})`
   658|         0|            0|            0|  0.00%|        - Output: :math:`(N, C, D_{out}, H_{out}, W_{out})`, where
   659|         0|            0|            0|  0.00%|
   660|         0|            0|            0|  0.00%|          .. math::
   661|         0|            0|            0|  0.00%|              D_{out} = \left\lfloor\frac{D_{in} + 2 \times \text{padding}[0] -
   662|         0|            0|            0|  0.00%|                    \text{kernel\_size}[0]}{\text{stride}[0]} + 1\right\rfloor
   663|         0|            0|            0|  0.00%|
   664|         0|            0|            0|  0.00%|          .. math::
   665|         0|            0|            0|  0.00%|              H_{out} = \left\lfloor\frac{H_{in} + 2 \times \text{padding}[1] -
   666|         0|            0|            0|  0.00%|                    \text{kernel\_size}[1]}{\text{stride}[1]} + 1\right\rfloor
   667|         0|            0|            0|  0.00%|
   668|         0|            0|            0|  0.00%|          .. math::
   669|         0|            0|            0|  0.00%|              W_{out} = \left\lfloor\frac{W_{in} + 2 \times \text{padding}[2] -
   670|         0|            0|            0|  0.00%|                    \text{kernel\_size}[2]}{\text{stride}[2]} + 1\right\rfloor
   671|         0|            0|            0|  0.00%|
   672|         0|            0|            0|  0.00%|    Examples::
   673|         0|            0|            0|  0.00%|
   674|         0|            0|            0|  0.00%|        >>> # pool of square window of size=3, stride=2
   675|         0|            0|            0|  0.00%|        >>> m = nn.AvgPool3d(3, stride=2)
   676|         0|            0|            0|  0.00%|        >>> # pool of non-square window
   677|         0|            0|            0|  0.00%|        >>> m = nn.AvgPool3d((3, 2, 2), stride=(2, 1, 2))
   678|         0|            0|            0|  0.00%|        >>> input = torch.randn(20, 16, 50,44, 31)
   679|         0|            0|            0|  0.00%|        >>> output = m(input)
   680|         0|            0|            0|  0.00%|    """
   681|         0|            0|            0|  0.00%|    __constants__ = ['kernel_size', 'stride', 'padding', 'ceil_mode', 'count_include_pad', 'divisor_override']
   682|         0|            0|            0|  0.00%|
   683|         0|            0|            0|  0.00%|    kernel_size: _size_3_t
   684|         0|            0|            0|  0.00%|    stride: _size_3_t
   685|         0|            0|            0|  0.00%|    padding: _size_3_t
   686|         0|            0|            0|  0.00%|    ceil_mode: bool
   687|         0|            0|            0|  0.00%|    count_include_pad: bool
   688|         0|            0|            0|  0.00%|
   689|         0|            0|            0|  0.00%|    def __init__(self, kernel_size: _size_3_t, stride: Optional[_size_3_t] = None, padding: _size_3_t = 0,
   690|         0|            0|            0|  0.00%|                 ceil_mode: bool = False, count_include_pad: bool = True, divisor_override: Optional[int] = None) -> None:
   691|         0|            0|            0|  0.00%|        super(AvgPool3d, self).__init__()
   692|         0|            0|            0|  0.00%|        self.kernel_size = kernel_size
   693|         0|            0|            0|  0.00%|        self.stride = stride if (stride is not None) else kernel_size
   694|         0|            0|            0|  0.00%|        self.padding = padding
   695|         0|            0|            0|  0.00%|        self.ceil_mode = ceil_mode
   696|         0|            0|            0|  0.00%|        self.count_include_pad = count_include_pad
   697|         0|            0|            0|  0.00%|        self.divisor_override = divisor_override
   698|         0|            0|            0|  0.00%|
   699|         0|            0|            0|  0.00%|    def forward(self, input: Tensor) -> Tensor:
   700|         0|            0|            0|  0.00%|        return F.avg_pool3d(input, self.kernel_size, self.stride,
   701|         0|            0|            0|  0.00%|                            self.padding, self.ceil_mode, self.count_include_pad, self.divisor_override)
   702|         0|            0|            0|  0.00%|
   703|         0|            0|            0|  0.00%|    def __setstate__(self, d):
   704|         0|            0|            0|  0.00%|        super(AvgPool3d, self).__setstate__(d)
   705|         0|            0|            0|  0.00%|        self.__dict__.setdefault('padding', 0)
   706|         0|            0|            0|  0.00%|        self.__dict__.setdefault('ceil_mode', False)
   707|         0|            0|            0|  0.00%|        self.__dict__.setdefault('count_include_pad', True)
   708|         0|            0|            0|  0.00%|
   709|         0|            0|            0|  0.00%|
   710|         0|            0|            0|  0.00%|class FractionalMaxPool2d(Module):
   711|         0|            0|            0|  0.00%|    r"""Applies a 2D fractional max pooling over an input signal composed of several input planes.
   712|         0|            0|            0|  0.00%|
   713|         0|            0|            0|  0.00%|    Fractional MaxPooling is described in detail in the paper `Fractional MaxPooling`_ by Ben Graham
   714|         0|            0|            0|  0.00%|
   715|         0|            0|            0|  0.00%|    The max-pooling operation is applied in :math:`kH \times kW` regions by a stochastic
   716|         0|            0|            0|  0.00%|    step size determined by the target output size.
   717|         0|            0|            0|  0.00%|    The number of output features is equal to the number of input planes.
   718|         0|            0|            0|  0.00%|
   719|         0|            0|            0|  0.00%|    Args:
   720|         0|            0|            0|  0.00%|        kernel_size: the size of the window to take a max over.
   721|         0|            0|            0|  0.00%|                     Can be a single number k (for a square kernel of k x k) or a tuple `(kh, kw)`
   722|         0|            0|            0|  0.00%|        output_size: the target output size of the image of the form `oH x oW`.
   723|         0|            0|            0|  0.00%|                     Can be a tuple `(oH, oW)` or a single number oH for a square image `oH x oH`
   724|         0|            0|            0|  0.00%|        output_ratio: If one wants to have an output size as a ratio of the input size, this option can be given.
   725|         0|            0|            0|  0.00%|                      This has to be a number or tuple in the range (0, 1)
   726|         0|            0|            0|  0.00%|        return_indices: if ``True``, will return the indices along with the outputs.
   727|         0|            0|            0|  0.00%|                        Useful to pass to :meth:`nn.MaxUnpool2d`. Default: ``False``
   728|         0|            0|            0|  0.00%|
   729|         0|            0|            0|  0.00%|    Examples:
   730|         0|            0|            0|  0.00%|        >>> # pool of square window of size=3, and target output size 13x12
   731|         0|            0|            0|  0.00%|        >>> m = nn.FractionalMaxPool2d(3, output_size=(13, 12))
   732|         0|            0|            0|  0.00%|        >>> # pool of square window and target output size being half of input image size
   733|         0|            0|            0|  0.00%|        >>> m = nn.FractionalMaxPool2d(3, output_ratio=(0.5, 0.5))
   734|         0|            0|            0|  0.00%|        >>> input = torch.randn(20, 16, 50, 32)
   735|         0|            0|            0|  0.00%|        >>> output = m(input)
   736|         0|            0|            0|  0.00%|
   737|         0|            0|            0|  0.00%|    .. _Fractional MaxPooling:
   738|         0|            0|            0|  0.00%|        https://arxiv.org/abs/1412.6071
   739|         0|            0|            0|  0.00%|    """
   740|         0|            0|            0|  0.00%|    __constants__ = ['kernel_size', 'return_indices', 'output_size',
   741|         0|            0|            0|  0.00%|                     'output_ratio']
   742|         0|            0|            0|  0.00%|
   743|         0|            0|            0|  0.00%|    kernel_size: _size_2_t
   744|         0|            0|            0|  0.00%|    return_indices: bool
   745|         0|            0|            0|  0.00%|    output_size: _size_2_t
   746|         0|            0|            0|  0.00%|    output_ratio: _ratio_2_t
   747|         0|            0|            0|  0.00%|
   748|         0|            0|            0|  0.00%|    def __init__(self, kernel_size: _size_2_t, output_size: Optional[_size_2_t] = None,
   749|         0|            0|            0|  0.00%|                 output_ratio: Optional[_ratio_2_t] = None,
   750|         0|            0|            0|  0.00%|                 return_indices: bool = False, _random_samples=None) -> None:
   751|         0|            0|            0|  0.00%|        super(FractionalMaxPool2d, self).__init__()
   752|         0|            0|            0|  0.00%|        self.kernel_size = _pair(kernel_size)
   753|         0|            0|            0|  0.00%|        self.return_indices = return_indices
   754|         0|            0|            0|  0.00%|        self.register_buffer('_random_samples', _random_samples)
   755|         0|            0|            0|  0.00%|        self.output_size = _pair(output_size) if output_size is not None else None
   756|         0|            0|            0|  0.00%|        self.output_ratio = _pair(output_ratio) if output_ratio is not None else None
   757|         0|            0|            0|  0.00%|        if output_size is None and output_ratio is None:
   758|         0|            0|            0|  0.00%|            raise ValueError("FractionalMaxPool2d requires specifying either "
   759|         0|            0|            0|  0.00%|                             "an output size, or a pooling ratio")
   760|         0|            0|            0|  0.00%|        if output_size is not None and output_ratio is not None:
   761|         0|            0|            0|  0.00%|            raise ValueError("only one of output_size and output_ratio may be specified")
   762|         0|            0|            0|  0.00%|        if self.output_ratio is not None:
   763|         0|            0|            0|  0.00%|            if not (0 < self.output_ratio[0] < 1 and 0 < self.output_ratio[1] < 1):
   764|         0|            0|            0|  0.00%|                raise ValueError("output_ratio must be between 0 and 1 (got {})"
   765|         0|            0|            0|  0.00%|                                 .format(output_ratio))
   766|         0|            0|            0|  0.00%|
   767|         0|            0|            0|  0.00%|    def forward(self, input: Tensor) -> Tensor:
   768|         0|            0|            0|  0.00%|        return F.fractional_max_pool2d(
   769|         0|            0|            0|  0.00%|            input, self.kernel_size, self.output_size, self.output_ratio,
   770|         0|            0|            0|  0.00%|            self.return_indices,
   771|         0|            0|            0|  0.00%|            _random_samples=self._random_samples)
   772|         0|            0|            0|  0.00%|
   773|         0|            0|            0|  0.00%|
   774|         0|            0|            0|  0.00%|class FractionalMaxPool3d(Module):
   775|         0|            0|            0|  0.00%|    r"""Applies a 3D fractional max pooling over an input signal composed of several input planes.
   776|         0|            0|            0|  0.00%|
   777|         0|            0|            0|  0.00%|    Fractional MaxPooling is described in detail in the paper `Fractional MaxPooling`_ by Ben Graham
   778|         0|            0|            0|  0.00%|
   779|         0|            0|            0|  0.00%|    The max-pooling operation is applied in :math:`kTxkHxkW` regions by a stochastic
   780|         0|            0|            0|  0.00%|    step size determined by the target output size.
   781|         0|            0|            0|  0.00%|    The number of output features is equal to the number of input planes.
   782|         0|            0|            0|  0.00%|
   783|         0|            0|            0|  0.00%|    Args:
   784|         0|            0|            0|  0.00%|        kernel_size: the size of the window to take a max over.
   785|         0|            0|            0|  0.00%|                     Can be a single number k (for a square kernel of k x k x k) or a tuple `(kt x kh x kw)`
   786|         0|            0|            0|  0.00%|        output_size: the target output size of the image of the form `oT x oH x oW`.
   787|         0|            0|            0|  0.00%|                     Can be a tuple `(oT, oH, oW)` or a single number oH for a square image `oH x oH x oH`
   788|         0|            0|            0|  0.00%|        output_ratio: If one wants to have an output size as a ratio of the input size, this option can be given.
   789|         0|            0|            0|  0.00%|                      This has to be a number or tuple in the range (0, 1)
   790|         0|            0|            0|  0.00%|        return_indices: if ``True``, will return the indices along with the outputs.
   791|         0|            0|            0|  0.00%|                        Useful to pass to :meth:`nn.MaxUnpool3d`. Default: ``False``
   792|         0|            0|            0|  0.00%|
   793|         0|            0|            0|  0.00%|    Examples:
   794|         0|            0|            0|  0.00%|        >>> # pool of cubic window of size=3, and target output size 13x12x11
   795|         0|            0|            0|  0.00%|        >>> m = nn.FractionalMaxPool3d(3, output_size=(13, 12, 11))
   796|         0|            0|            0|  0.00%|        >>> # pool of cubic window and target output size being half of input size
   797|         0|            0|            0|  0.00%|        >>> m = nn.FractionalMaxPool3d(3, output_ratio=(0.5, 0.5, 0.5))
   798|         0|            0|            0|  0.00%|        >>> input = torch.randn(20, 16, 50, 32, 16)
   799|         0|            0|            0|  0.00%|        >>> output = m(input)
   800|         0|            0|            0|  0.00%|
   801|         0|            0|            0|  0.00%|    .. _Fractional MaxPooling:
   802|         0|            0|            0|  0.00%|        https://arxiv.org/abs/1412.6071
   803|         0|            0|            0|  0.00%|    """
   804|         0|            0|            0|  0.00%|    __constants__ = ['kernel_size', 'return_indices', 'output_size',
   805|         0|            0|            0|  0.00%|                     'output_ratio']
   806|         0|            0|            0|  0.00%|    kernel_size: _size_3_t
   807|         0|            0|            0|  0.00%|    return_indices: bool
   808|         0|            0|            0|  0.00%|    output_size: _size_3_t
   809|         0|            0|            0|  0.00%|    output_ratio: _ratio_3_t
   810|         0|            0|            0|  0.00%|
   811|         0|            0|            0|  0.00%|    def __init__(self, kernel_size: _size_3_t, output_size: Optional[_size_3_t] = None,
   812|         0|            0|            0|  0.00%|                 output_ratio: Optional[_ratio_3_t] = None,
   813|         0|            0|            0|  0.00%|                 return_indices: bool = False, _random_samples=None) -> None:
   814|         0|            0|            0|  0.00%|        super(FractionalMaxPool3d, self).__init__()
   815|         0|            0|            0|  0.00%|        self.kernel_size = _triple(kernel_size)
   816|         0|            0|            0|  0.00%|        self.return_indices = return_indices
   817|         0|            0|            0|  0.00%|        self.register_buffer('_random_samples', _random_samples)
   818|         0|            0|            0|  0.00%|        self.output_size = _triple(output_size) if output_size is not None else None
   819|         0|            0|            0|  0.00%|        self.output_ratio = _triple(output_ratio) if output_ratio is not None else None
   820|         0|            0|            0|  0.00%|        if output_size is None and output_ratio is None:
   821|         0|            0|            0|  0.00%|            raise ValueError("FractionalMaxPool3d requires specifying either "
   822|         0|            0|            0|  0.00%|                             "an output size, or a pooling ratio")
   823|         0|            0|            0|  0.00%|        if output_size is not None and output_ratio is not None:
   824|         0|            0|            0|  0.00%|            raise ValueError("only one of output_size and output_ratio may be specified")
   825|         0|            0|            0|  0.00%|        if self.output_ratio is not None:
   826|         0|            0|            0|  0.00%|            if not (0 < self.output_ratio[0] < 1 and 0 < self.output_ratio[1] < 1 and 0 < self.output_ratio[2] < 1):
   827|         0|            0|            0|  0.00%|                raise ValueError("output_ratio must be between 0 and 1 (got {})"
   828|         0|            0|            0|  0.00%|                                 .format(output_ratio))
   829|         0|            0|            0|  0.00%|
   830|         0|            0|            0|  0.00%|    def forward(self, input: Tensor) -> Tensor:
   831|         0|            0|            0|  0.00%|        return F.fractional_max_pool3d(
   832|         0|            0|            0|  0.00%|            input, self.kernel_size, self.output_size, self.output_ratio,
   833|         0|            0|            0|  0.00%|            self.return_indices,
   834|         0|            0|            0|  0.00%|            _random_samples=self._random_samples)
   835|         0|            0|            0|  0.00%|
   836|         0|            0|            0|  0.00%|
   837|         0|            0|            0|  0.00%|class _LPPoolNd(Module):
   838|         0|            0|            0|  0.00%|    __constants__ = ['norm_type', 'kernel_size', 'stride', 'ceil_mode']
   839|         0|            0|            0|  0.00%|
   840|         0|            0|            0|  0.00%|    norm_type: float
   841|         0|            0|            0|  0.00%|    ceil_mode: bool
   842|         0|            0|            0|  0.00%|
   843|         0|            0|            0|  0.00%|    def __init__(self, norm_type: float, kernel_size: _size_any_t, stride: Optional[_size_any_t] = None,
   844|         0|            0|            0|  0.00%|                 ceil_mode: bool = False) -> None:
   845|         0|            0|            0|  0.00%|        super(_LPPoolNd, self).__init__()
   846|         0|            0|            0|  0.00%|        self.norm_type = norm_type
   847|         0|            0|            0|  0.00%|        self.kernel_size = kernel_size
   848|         0|            0|            0|  0.00%|        self.stride = stride
   849|         0|            0|            0|  0.00%|        self.ceil_mode = ceil_mode
   850|         0|            0|            0|  0.00%|
   851|         0|            0|            0|  0.00%|    def extra_repr(self) -> str:
   852|         0|            0|            0|  0.00%|        return 'norm_type={norm_type}, kernel_size={kernel_size}, stride={stride}, ' \
   853|         0|            0|            0|  0.00%|            'ceil_mode={ceil_mode}'.format(**self.__dict__)
   854|         0|            0|            0|  0.00%|
   855|         0|            0|            0|  0.00%|
   856|         0|            0|            0|  0.00%|class LPPool1d(_LPPoolNd):
   857|         0|            0|            0|  0.00%|    r"""Applies a 1D power-average pooling over an input signal composed of several input
   858|         0|            0|            0|  0.00%|    planes.
   859|         0|            0|            0|  0.00%|
   860|         0|            0|            0|  0.00%|    On each window, the function computed is:
   861|         0|            0|            0|  0.00%|
   862|         0|            0|            0|  0.00%|    .. math::
   863|         0|            0|            0|  0.00%|        f(X) = \sqrt[p]{\sum_{x \in X} x^{p}}
   864|         0|            0|            0|  0.00%|
   865|         0|            0|            0|  0.00%|    - At p = :math:`\infty`, one gets Max Pooling
   866|         0|            0|            0|  0.00%|    - At p = 1, one gets Sum Pooling (which is proportional to Average Pooling)
   867|         0|            0|            0|  0.00%|
   868|         0|            0|            0|  0.00%|    .. note:: If the sum to the power of `p` is zero, the gradient of this function is
   869|         0|            0|            0|  0.00%|              not defined. This implementation will set the gradient to zero in this case.
   870|         0|            0|            0|  0.00%|
   871|         0|            0|            0|  0.00%|    Args:
   872|         0|            0|            0|  0.00%|        kernel_size: a single int, the size of the window
   873|         0|            0|            0|  0.00%|        stride: a single int, the stride of the window. Default value is :attr:`kernel_size`
   874|         0|            0|            0|  0.00%|        ceil_mode: when True, will use `ceil` instead of `floor` to compute the output shape
   875|         0|            0|            0|  0.00%|
   876|         0|            0|            0|  0.00%|    Shape:
   877|         0|            0|            0|  0.00%|        - Input: :math:`(N, C, L_{in})`
   878|         0|            0|            0|  0.00%|        - Output: :math:`(N, C, L_{out})`, where
   879|         0|            0|            0|  0.00%|
   880|         0|            0|            0|  0.00%|          .. math::
   881|         0|            0|            0|  0.00%|              L_{out} = \left\lfloor\frac{L_{in} - \text{kernel\_size}}{\text{stride}} + 1\right\rfloor
   882|         0|            0|            0|  0.00%|
   883|         0|            0|            0|  0.00%|    Examples::
   884|         0|            0|            0|  0.00%|        >>> # power-2 pool of window of length 3, with stride 2.
   885|         0|            0|            0|  0.00%|        >>> m = nn.LPPool1d(2, 3, stride=2)
   886|         0|            0|            0|  0.00%|        >>> input = torch.randn(20, 16, 50)
   887|         0|            0|            0|  0.00%|        >>> output = m(input)
   888|         0|            0|            0|  0.00%|    """
   889|         0|            0|            0|  0.00%|
   890|         0|            0|            0|  0.00%|    kernel_size: _size_1_t
   891|         0|            0|            0|  0.00%|    stride: _size_1_t
   892|         0|            0|            0|  0.00%|
   893|         0|            0|            0|  0.00%|    def forward(self, input: Tensor) -> Tensor:
   894|         0|            0|            0|  0.00%|        return F.lp_pool1d(input, float(self.norm_type), self.kernel_size,
   895|         0|            0|            0|  0.00%|                           self.stride, self.ceil_mode)
   896|         0|            0|            0|  0.00%|
   897|         0|            0|            0|  0.00%|
   898|         0|            0|            0|  0.00%|class LPPool2d(_LPPoolNd):
   899|         0|            0|            0|  0.00%|    r"""Applies a 2D power-average pooling over an input signal composed of several input
   900|         0|            0|            0|  0.00%|    planes.
   901|         0|            0|            0|  0.00%|
   902|         0|            0|            0|  0.00%|    On each window, the function computed is:
   903|         0|            0|            0|  0.00%|
   904|         0|            0|            0|  0.00%|    .. math::
   905|         0|            0|            0|  0.00%|        f(X) = \sqrt[p]{\sum_{x \in X} x^{p}}
   906|         0|            0|            0|  0.00%|
   907|         0|            0|            0|  0.00%|    - At p = :math:`\infty`, one gets Max Pooling
   908|         0|            0|            0|  0.00%|    - At p = 1, one gets Sum Pooling (which is proportional to average pooling)
   909|         0|            0|            0|  0.00%|
   910|         0|            0|            0|  0.00%|    The parameters :attr:`kernel_size`, :attr:`stride` can either be:
   911|         0|            0|            0|  0.00%|
   912|         0|            0|            0|  0.00%|        - a single ``int`` -- in which case the same value is used for the height and width dimension
   913|         0|            0|            0|  0.00%|        - a ``tuple`` of two ints -- in which case, the first `int` is used for the height dimension,
   914|         0|            0|            0|  0.00%|          and the second `int` for the width dimension
   915|         0|            0|            0|  0.00%|
   916|         0|            0|            0|  0.00%|    .. note:: If the sum to the power of `p` is zero, the gradient of this function is
   917|         0|            0|            0|  0.00%|              not defined. This implementation will set the gradient to zero in this case.
   918|         0|            0|            0|  0.00%|
   919|         0|            0|            0|  0.00%|    Args:
   920|         0|            0|            0|  0.00%|        kernel_size: the size of the window
   921|         0|            0|            0|  0.00%|        stride: the stride of the window. Default value is :attr:`kernel_size`
   922|         0|            0|            0|  0.00%|        ceil_mode: when True, will use `ceil` instead of `floor` to compute the output shape
   923|         0|            0|            0|  0.00%|
   924|         0|            0|            0|  0.00%|    Shape:
   925|         0|            0|            0|  0.00%|        - Input: :math:`(N, C, H_{in}, W_{in})`
   926|         0|            0|            0|  0.00%|        - Output: :math:`(N, C, H_{out}, W_{out})`, where
   927|         0|            0|            0|  0.00%|
   928|         0|            0|            0|  0.00%|          .. math::
   929|         0|            0|            0|  0.00%|              H_{out} = \left\lfloor\frac{H_{in} - \text{kernel\_size}[0]}{\text{stride}[0]} + 1\right\rfloor
   930|         0|            0|            0|  0.00%|
   931|         0|            0|            0|  0.00%|          .. math::
   932|         0|            0|            0|  0.00%|              W_{out} = \left\lfloor\frac{W_{in} - \text{kernel\_size}[1]}{\text{stride}[1]} + 1\right\rfloor
   933|         0|            0|            0|  0.00%|
   934|         0|            0|            0|  0.00%|    Examples::
   935|         0|            0|            0|  0.00%|
   936|         0|            0|            0|  0.00%|        >>> # power-2 pool of square window of size=3, stride=2
   937|         0|            0|            0|  0.00%|        >>> m = nn.LPPool2d(2, 3, stride=2)
   938|         0|            0|            0|  0.00%|        >>> # pool of non-square window of power 1.2
   939|         0|            0|            0|  0.00%|        >>> m = nn.LPPool2d(1.2, (3, 2), stride=(2, 1))
   940|         0|            0|            0|  0.00%|        >>> input = torch.randn(20, 16, 50, 32)
   941|         0|            0|            0|  0.00%|        >>> output = m(input)
   942|         0|            0|            0|  0.00%|
   943|         0|            0|            0|  0.00%|    """
   944|         0|            0|            0|  0.00%|
   945|         0|            0|            0|  0.00%|    kernel_size: _size_2_t
   946|         0|            0|            0|  0.00%|    stride: _size_2_t
   947|         0|            0|            0|  0.00%|
   948|         0|            0|            0|  0.00%|    def forward(self, input: Tensor) -> Tensor:
   949|         0|            0|            0|  0.00%|        return F.lp_pool2d(input, float(self.norm_type), self.kernel_size,
   950|         0|            0|            0|  0.00%|                           self.stride, self.ceil_mode)
   951|         0|            0|            0|  0.00%|
   952|         0|            0|            0|  0.00%|
   953|         0|            0|            0|  0.00%|class _AdaptiveMaxPoolNd(Module):
   954|         0|            0|            0|  0.00%|    __constants__ = ['output_size', 'return_indices']
   955|         0|            0|            0|  0.00%|    return_indices: bool
   956|         0|            0|            0|  0.00%|
   957|         0|            0|            0|  0.00%|    def __init__(self, output_size: _size_any_opt_t, return_indices: bool = False) -> None:
   958|         0|            0|            0|  0.00%|        super(_AdaptiveMaxPoolNd, self).__init__()
   959|         0|            0|            0|  0.00%|        self.output_size = output_size
   960|         0|            0|            0|  0.00%|        self.return_indices = return_indices
   961|         0|            0|            0|  0.00%|
   962|         0|            0|            0|  0.00%|    def extra_repr(self) -> str:
   963|         0|            0|            0|  0.00%|        return 'output_size={}'.format(self.output_size)
   964|         0|            0|            0|  0.00%|
   965|         0|            0|            0|  0.00%|# FIXME (by @ssnl): Improve adaptive pooling docs: specify what the input and
   966|         0|            0|            0|  0.00%|#   output shapes are, and how the operation computes output.
   967|         0|            0|            0|  0.00%|
   968|         0|            0|            0|  0.00%|
   969|         0|            0|            0|  0.00%|class AdaptiveMaxPool1d(_AdaptiveMaxPoolNd):
   970|         0|            0|            0|  0.00%|    r"""Applies a 1D adaptive max pooling over an input signal composed of several input planes.
   971|         0|            0|            0|  0.00%|
   972|         0|            0|            0|  0.00%|    The output size is H, for any input size.
   973|         0|            0|            0|  0.00%|    The number of output features is equal to the number of input planes.
   974|         0|            0|            0|  0.00%|
   975|         0|            0|            0|  0.00%|    Args:
   976|         0|            0|            0|  0.00%|        output_size: the target output size H
   977|         0|            0|            0|  0.00%|        return_indices: if ``True``, will return the indices along with the outputs.
   978|         0|            0|            0|  0.00%|                        Useful to pass to nn.MaxUnpool1d. Default: ``False``
   979|         0|            0|            0|  0.00%|
   980|         0|            0|            0|  0.00%|    Examples:
   981|         0|            0|            0|  0.00%|        >>> # target output size of 5
   982|         0|            0|            0|  0.00%|        >>> m = nn.AdaptiveMaxPool1d(5)
   983|         0|            0|            0|  0.00%|        >>> input = torch.randn(1, 64, 8)
   984|         0|            0|            0|  0.00%|        >>> output = m(input)
   985|         0|            0|            0|  0.00%|
   986|         0|            0|            0|  0.00%|    """
   987|         0|            0|            0|  0.00%|
   988|         0|            0|            0|  0.00%|    output_size: _size_1_t
   989|         0|            0|            0|  0.00%|
   990|         0|            0|            0|  0.00%|    def forward(self, input: Tensor) -> Tensor:
   991|         0|            0|            0|  0.00%|        return F.adaptive_max_pool1d(input, self.output_size, self.return_indices)
   992|         0|            0|            0|  0.00%|
   993|         0|            0|            0|  0.00%|
   994|         0|            0|            0|  0.00%|class AdaptiveMaxPool2d(_AdaptiveMaxPoolNd):
   995|         0|            0|            0|  0.00%|    r"""Applies a 2D adaptive max pooling over an input signal composed of several input planes.
   996|         0|            0|            0|  0.00%|
   997|         0|            0|            0|  0.00%|    The output is of size H x W, for any input size.
   998|         0|            0|            0|  0.00%|    The number of output features is equal to the number of input planes.
   999|         0|            0|            0|  0.00%|
  1000|         0|            0|            0|  0.00%|    Args:
  1001|         0|            0|            0|  0.00%|        output_size: the target output size of the image of the form H x W.
  1002|         0|            0|            0|  0.00%|                     Can be a tuple (H, W) or a single H for a square image H x H.
  1003|         0|            0|            0|  0.00%|                     H and W can be either a ``int``, or ``None`` which means the size will
  1004|         0|            0|            0|  0.00%|                     be the same as that of the input.
  1005|         0|            0|            0|  0.00%|        return_indices: if ``True``, will return the indices along with the outputs.
  1006|         0|            0|            0|  0.00%|                        Useful to pass to nn.MaxUnpool2d. Default: ``False``
  1007|         0|            0|            0|  0.00%|
  1008|         0|            0|            0|  0.00%|    Examples:
  1009|         0|            0|            0|  0.00%|        >>> # target output size of 5x7
  1010|         0|            0|            0|  0.00%|        >>> m = nn.AdaptiveMaxPool2d((5,7))
  1011|         0|            0|            0|  0.00%|        >>> input = torch.randn(1, 64, 8, 9)
  1012|         0|            0|            0|  0.00%|        >>> output = m(input)
  1013|         0|            0|            0|  0.00%|        >>> # target output size of 7x7 (square)
  1014|         0|            0|            0|  0.00%|        >>> m = nn.AdaptiveMaxPool2d(7)
  1015|         0|            0|            0|  0.00%|        >>> input = torch.randn(1, 64, 10, 9)
  1016|         0|            0|            0|  0.00%|        >>> output = m(input)
  1017|         0|            0|            0|  0.00%|        >>> # target output size of 10x7
  1018|         0|            0|            0|  0.00%|        >>> m = nn.AdaptiveMaxPool2d((None, 7))
  1019|         0|            0|            0|  0.00%|        >>> input = torch.randn(1, 64, 10, 9)
  1020|         0|            0|            0|  0.00%|        >>> output = m(input)
  1021|         0|            0|            0|  0.00%|
  1022|         0|            0|            0|  0.00%|    """
  1023|         0|            0|            0|  0.00%|
  1024|         0|            0|            0|  0.00%|    output_size: _size_2_opt_t
  1025|         0|            0|            0|  0.00%|
  1026|         0|            0|            0|  0.00%|    def forward(self, input: Tensor) -> Tensor:
  1027|         0|            0|            0|  0.00%|        return F.adaptive_max_pool2d(input, self.output_size, self.return_indices)
  1028|         0|            0|            0|  0.00%|
  1029|         0|            0|            0|  0.00%|
  1030|         0|            0|            0|  0.00%|class AdaptiveMaxPool3d(_AdaptiveMaxPoolNd):
  1031|         0|            0|            0|  0.00%|    r"""Applies a 3D adaptive max pooling over an input signal composed of several input planes.
  1032|         0|            0|            0|  0.00%|
  1033|         0|            0|            0|  0.00%|    The output is of size D x H x W, for any input size.
  1034|         0|            0|            0|  0.00%|    The number of output features is equal to the number of input planes.
  1035|         0|            0|            0|  0.00%|
  1036|         0|            0|            0|  0.00%|    Args:
  1037|         0|            0|            0|  0.00%|        output_size: the target output size of the image of the form D x H x W.
  1038|         0|            0|            0|  0.00%|                     Can be a tuple (D, H, W) or a single D for a cube D x D x D.
  1039|         0|            0|            0|  0.00%|                     D, H and W can be either a ``int``, or ``None`` which means the size will
  1040|         0|            0|            0|  0.00%|                     be the same as that of the input.
  1041|         0|            0|            0|  0.00%|
  1042|         0|            0|            0|  0.00%|        return_indices: if ``True``, will return the indices along with the outputs.
  1043|         0|            0|            0|  0.00%|                        Useful to pass to nn.MaxUnpool3d. Default: ``False``
  1044|         0|            0|            0|  0.00%|
  1045|         0|            0|            0|  0.00%|    Examples:
  1046|         0|            0|            0|  0.00%|        >>> # target output size of 5x7x9
  1047|         0|            0|            0|  0.00%|        >>> m = nn.AdaptiveMaxPool3d((5,7,9))
  1048|         0|            0|            0|  0.00%|        >>> input = torch.randn(1, 64, 8, 9, 10)
  1049|         0|            0|            0|  0.00%|        >>> output = m(input)
  1050|         0|            0|            0|  0.00%|        >>> # target output size of 7x7x7 (cube)
  1051|         0|            0|            0|  0.00%|        >>> m = nn.AdaptiveMaxPool3d(7)
  1052|         0|            0|            0|  0.00%|        >>> input = torch.randn(1, 64, 10, 9, 8)
  1053|         0|            0|            0|  0.00%|        >>> output = m(input)
  1054|         0|            0|            0|  0.00%|        >>> # target output size of 7x9x8
  1055|         0|            0|            0|  0.00%|        >>> m = nn.AdaptiveMaxPool3d((7, None, None))
  1056|         0|            0|            0|  0.00%|        >>> input = torch.randn(1, 64, 10, 9, 8)
  1057|         0|            0|            0|  0.00%|        >>> output = m(input)
  1058|         0|            0|            0|  0.00%|
  1059|         0|            0|            0|  0.00%|    """
  1060|         0|            0|            0|  0.00%|
  1061|         0|            0|            0|  0.00%|    output_size: _size_3_opt_t
  1062|         0|            0|            0|  0.00%|
  1063|         0|            0|            0|  0.00%|    def forward(self, input: Tensor) -> Tensor:
  1064|         0|            0|            0|  0.00%|        return F.adaptive_max_pool3d(input, self.output_size, self.return_indices)
  1065|         0|            0|            0|  0.00%|
  1066|         0|            0|            0|  0.00%|
  1067|         0|            0|            0|  0.00%|class _AdaptiveAvgPoolNd(Module):
  1068|         0|            0|            0|  0.00%|    __constants__ = ['output_size']
  1069|         0|            0|            0|  0.00%|
  1070|         0|            0|            0|  0.00%|    def __init__(self, output_size: _size_any_opt_t) -> None:
  1071|         0|            0|            0|  0.00%|        super(_AdaptiveAvgPoolNd, self).__init__()
  1072|         0|            0|            0|  0.00%|        self.output_size = output_size
  1073|         0|            0|            0|  0.00%|
  1074|         0|            0|            0|  0.00%|    def extra_repr(self) -> str:
  1075|         0|            0|            0|  0.00%|        return 'output_size={}'.format(self.output_size)
  1076|         0|            0|            0|  0.00%|
  1077|         0|            0|            0|  0.00%|
  1078|         0|            0|            0|  0.00%|class AdaptiveAvgPool1d(_AdaptiveAvgPoolNd):
  1079|         0|            0|            0|  0.00%|    r"""Applies a 1D adaptive average pooling over an input signal composed of several input planes.
  1080|         0|            0|            0|  0.00%|
  1081|         0|            0|            0|  0.00%|    The output size is H, for any input size.
  1082|         0|            0|            0|  0.00%|    The number of output features is equal to the number of input planes.
  1083|         0|            0|            0|  0.00%|
  1084|         0|            0|            0|  0.00%|    Args:
  1085|         0|            0|            0|  0.00%|        output_size: the target output size H
  1086|         0|            0|            0|  0.00%|
  1087|         0|            0|            0|  0.00%|    Examples:
  1088|         0|            0|            0|  0.00%|        >>> # target output size of 5
  1089|         0|            0|            0|  0.00%|        >>> m = nn.AdaptiveAvgPool1d(5)
  1090|         0|            0|            0|  0.00%|        >>> input = torch.randn(1, 64, 8)
  1091|         0|            0|            0|  0.00%|        >>> output = m(input)
  1092|         0|            0|            0|  0.00%|
  1093|         0|            0|            0|  0.00%|    """
  1094|         0|            0|            0|  0.00%|
  1095|         0|            0|            0|  0.00%|    output_size: _size_1_t
  1096|         0|            0|            0|  0.00%|
  1097|         0|            0|            0|  0.00%|    def forward(self, input: Tensor) -> Tensor:
  1098|         0|            0|            0|  0.00%|        return F.adaptive_avg_pool1d(input, self.output_size)
  1099|         0|            0|            0|  0.00%|
  1100|         0|            0|            0|  0.00%|
  1101|         0|            0|            0|  0.00%|class AdaptiveAvgPool2d(_AdaptiveAvgPoolNd):
  1102|         0|            0|            0|  0.00%|    r"""Applies a 2D adaptive average pooling over an input signal composed of several input planes.
  1103|         0|            0|            0|  0.00%|
  1104|         0|            0|            0|  0.00%|    The output is of size H x W, for any input size.
  1105|         0|            0|            0|  0.00%|    The number of output features is equal to the number of input planes.
  1106|         0|            0|            0|  0.00%|
  1107|         0|            0|            0|  0.00%|    Args:
  1108|         0|            0|            0|  0.00%|        output_size: the target output size of the image of the form H x W.
  1109|         0|            0|            0|  0.00%|                     Can be a tuple (H, W) or a single H for a square image H x H.
  1110|         0|            0|            0|  0.00%|                     H and W can be either a ``int``, or ``None`` which means the size will
  1111|         0|            0|            0|  0.00%|                     be the same as that of the input.
  1112|         0|            0|            0|  0.00%|
  1113|         0|            0|            0|  0.00%|    Examples:
  1114|         0|            0|            0|  0.00%|        >>> # target output size of 5x7
  1115|         0|            0|            0|  0.00%|        >>> m = nn.AdaptiveAvgPool2d((5,7))
  1116|         0|            0|            0|  0.00%|        >>> input = torch.randn(1, 64, 8, 9)
  1117|         0|            0|            0|  0.00%|        >>> output = m(input)
  1118|         0|            0|            0|  0.00%|        >>> # target output size of 7x7 (square)
  1119|         0|            0|            0|  0.00%|        >>> m = nn.AdaptiveAvgPool2d(7)
  1120|         0|            0|            0|  0.00%|        >>> input = torch.randn(1, 64, 10, 9)
  1121|         0|            0|            0|  0.00%|        >>> output = m(input)
  1122|         0|            0|            0|  0.00%|        >>> # target output size of 10x7
  1123|         0|            0|            0|  0.00%|        >>> m = nn.AdaptiveAvgPool2d((None, 7))
  1124|         0|            0|            0|  0.00%|        >>> input = torch.randn(1, 64, 10, 9)
  1125|         0|            0|            0|  0.00%|        >>> output = m(input)
  1126|         0|            0|            0|  0.00%|
  1127|         0|            0|            0|  0.00%|    """
  1128|         0|            0|            0|  0.00%|
  1129|         0|            0|            0|  0.00%|    output_size: _size_2_opt_t
  1130|         0|            0|            0|  0.00%|
  1131|       100|  0.000628233|  6.28233e-06|  0.00%|    def forward(self, input: Tensor) -> Tensor:
  1132|       100|     0.002388|    2.388e-05|  0.00%|        return F.adaptive_avg_pool2d(input, self.output_size)
(call)|       100|    0.0206275|  0.000206275|  0.04%|# /opt/conda/lib/python3.8/site-packages/torch/nn/functional.py:1116 adaptive_avg_pool2d
  1133|         0|            0|            0|  0.00%|
  1134|         0|            0|            0|  0.00%|
  1135|         0|            0|            0|  0.00%|class AdaptiveAvgPool3d(_AdaptiveAvgPoolNd):
  1136|         0|            0|            0|  0.00%|    r"""Applies a 3D adaptive average pooling over an input signal composed of several input planes.
  1137|         0|            0|            0|  0.00%|
  1138|         0|            0|            0|  0.00%|    The output is of size D x H x W, for any input size.
  1139|         0|            0|            0|  0.00%|    The number of output features is equal to the number of input planes.
  1140|         0|            0|            0|  0.00%|
  1141|         0|            0|            0|  0.00%|    Args:
  1142|         0|            0|            0|  0.00%|        output_size: the target output size of the form D x H x W.
  1143|         0|            0|            0|  0.00%|                     Can be a tuple (D, H, W) or a single number D for a cube D x D x D.
  1144|         0|            0|            0|  0.00%|                     D, H and W can be either a ``int``, or ``None`` which means the size will
  1145|         0|            0|            0|  0.00%|                     be the same as that of the input.
  1146|         0|            0|            0|  0.00%|
  1147|         0|            0|            0|  0.00%|    Examples:
  1148|         0|            0|            0|  0.00%|        >>> # target output size of 5x7x9
  1149|         0|            0|            0|  0.00%|        >>> m = nn.AdaptiveAvgPool3d((5,7,9))
  1150|         0|            0|            0|  0.00%|        >>> input = torch.randn(1, 64, 8, 9, 10)
  1151|         0|            0|            0|  0.00%|        >>> output = m(input)
  1152|         0|            0|            0|  0.00%|        >>> # target output size of 7x7x7 (cube)
  1153|         0|            0|            0|  0.00%|        >>> m = nn.AdaptiveAvgPool3d(7)
  1154|         0|            0|            0|  0.00%|        >>> input = torch.randn(1, 64, 10, 9, 8)
  1155|         0|            0|            0|  0.00%|        >>> output = m(input)
  1156|         0|            0|            0|  0.00%|        >>> # target output size of 7x9x8
  1157|         0|            0|            0|  0.00%|        >>> m = nn.AdaptiveAvgPool3d((7, None, None))
  1158|         0|            0|            0|  0.00%|        >>> input = torch.randn(1, 64, 10, 9, 8)
  1159|         0|            0|            0|  0.00%|        >>> output = m(input)
  1160|         0|            0|            0|  0.00%|
  1161|         0|            0|            0|  0.00%|    """
  1162|         0|            0|            0|  0.00%|
  1163|         0|            0|            0|  0.00%|    output_size: _size_3_opt_t
  1164|         0|            0|            0|  0.00%|
  1165|         0|            0|            0|  0.00%|    def forward(self, input: Tensor) -> Tensor:
  1166|         0|            0|            0|  0.00%|        return F.adaptive_avg_pool3d(input, self.output_size)
File: /opt/conda/lib/python3.8/multiprocessing/util.py
File duration: 0.00726247s (0.01%)
Line #|      Hits|         Time| Time per hit|      %|Source code
------+----------+-------------+-------------+-------+-----------
     1|         0|            0|            0|  0.00%|#
     2|         0|            0|            0|  0.00%|# Module providing various facilities to other parts of the package
     3|         0|            0|            0|  0.00%|#
     4|         0|            0|            0|  0.00%|# multiprocessing/util.py
     5|         0|            0|            0|  0.00%|#
     6|         0|            0|            0|  0.00%|# Copyright (c) 2006-2008, R Oudkerk
     7|         0|            0|            0|  0.00%|# Licensed to PSF under a Contributor Agreement.
     8|         0|            0|            0|  0.00%|#
     9|         0|            0|            0|  0.00%|
    10|         0|            0|            0|  0.00%|import os
    11|         0|            0|            0|  0.00%|import itertools
    12|         0|            0|            0|  0.00%|import sys
    13|         0|            0|            0|  0.00%|import weakref
    14|         0|            0|            0|  0.00%|import atexit
    15|         0|            0|            0|  0.00%|import threading        # we want threading to install it's
    16|         0|            0|            0|  0.00%|                        # cleanup function before multiprocessing does
    17|         0|            0|            0|  0.00%|from subprocess import _args_from_interpreter_flags
    18|         0|            0|            0|  0.00%|
    19|         0|            0|            0|  0.00%|from . import process
    20|         0|            0|            0|  0.00%|
    21|         0|            0|            0|  0.00%|__all__ = [
    22|         0|            0|            0|  0.00%|    'sub_debug', 'debug', 'info', 'sub_warning', 'get_logger',
    23|         0|            0|            0|  0.00%|    'log_to_stderr', 'get_temp_dir', 'register_after_fork',
    24|         0|            0|            0|  0.00%|    'is_exiting', 'Finalize', 'ForkAwareThreadLock', 'ForkAwareLocal',
    25|         0|            0|            0|  0.00%|    'close_all_fds_except', 'SUBDEBUG', 'SUBWARNING',
    26|         0|            0|            0|  0.00%|    ]
    27|         0|            0|            0|  0.00%|
    28|         0|            0|            0|  0.00%|#
    29|         0|            0|            0|  0.00%|# Logging
    30|         0|            0|            0|  0.00%|#
    31|         0|            0|            0|  0.00%|
    32|         0|            0|            0|  0.00%|NOTSET = 0
    33|         0|            0|            0|  0.00%|SUBDEBUG = 5
    34|         0|            0|            0|  0.00%|DEBUG = 10
    35|         0|            0|            0|  0.00%|INFO = 20
    36|         0|            0|            0|  0.00%|SUBWARNING = 25
    37|         0|            0|            0|  0.00%|
    38|         0|            0|            0|  0.00%|LOGGER_NAME = 'multiprocessing'
    39|         0|            0|            0|  0.00%|DEFAULT_LOGGING_FORMAT = '[%(levelname)s/%(processName)s] %(message)s'
    40|         0|            0|            0|  0.00%|
    41|         0|            0|            0|  0.00%|_logger = None
    42|         0|            0|            0|  0.00%|_log_to_stderr = False
    43|         0|            0|            0|  0.00%|
    44|        16|  7.24792e-05|  4.52995e-06|  0.00%|def sub_debug(msg, *args):
    45|        16|  6.74725e-05|  4.21703e-06|  0.00%|    if _logger:
    46|         0|            0|            0|  0.00%|        _logger.log(SUBDEBUG, msg, *args)
    47|         0|            0|            0|  0.00%|
    48|        98|   0.00060153|  6.13806e-06|  0.00%|def debug(msg, *args):
    49|        98|  0.000460148|  4.69539e-06|  0.00%|    if _logger:
    50|         0|            0|            0|  0.00%|        _logger.log(DEBUG, msg, *args)
    51|         0|            0|            0|  0.00%|
    52|         0|            0|            0|  0.00%|def info(msg, *args):
    53|         0|            0|            0|  0.00%|    if _logger:
    54|         0|            0|            0|  0.00%|        _logger.log(INFO, msg, *args)
    55|         0|            0|            0|  0.00%|
    56|         0|            0|            0|  0.00%|def sub_warning(msg, *args):
    57|         0|            0|            0|  0.00%|    if _logger:
    58|         0|            0|            0|  0.00%|        _logger.log(SUBWARNING, msg, *args)
    59|         0|            0|            0|  0.00%|
    60|         0|            0|            0|  0.00%|def get_logger():
    61|         0|            0|            0|  0.00%|    '''
    62|         0|            0|            0|  0.00%|    Returns logger used by multiprocessing
    63|         0|            0|            0|  0.00%|    '''
    64|         0|            0|            0|  0.00%|    global _logger
    65|         0|            0|            0|  0.00%|    import logging
    66|         0|            0|            0|  0.00%|
    67|         0|            0|            0|  0.00%|    logging._acquireLock()
    68|         0|            0|            0|  0.00%|    try:
    69|         0|            0|            0|  0.00%|        if not _logger:
    70|         0|            0|            0|  0.00%|
    71|         0|            0|            0|  0.00%|            _logger = logging.getLogger(LOGGER_NAME)
    72|         0|            0|            0|  0.00%|            _logger.propagate = 0
    73|         0|            0|            0|  0.00%|
    74|         0|            0|            0|  0.00%|            # XXX multiprocessing should cleanup before logging
    75|         0|            0|            0|  0.00%|            if hasattr(atexit, 'unregister'):
    76|         0|            0|            0|  0.00%|                atexit.unregister(_exit_function)
    77|         0|            0|            0|  0.00%|                atexit.register(_exit_function)
    78|         0|            0|            0|  0.00%|            else:
    79|         0|            0|            0|  0.00%|                atexit._exithandlers.remove((_exit_function, (), {}))
    80|         0|            0|            0|  0.00%|                atexit._exithandlers.append((_exit_function, (), {}))
    81|         0|            0|            0|  0.00%|
    82|         0|            0|            0|  0.00%|    finally:
    83|         0|            0|            0|  0.00%|        logging._releaseLock()
    84|         0|            0|            0|  0.00%|
    85|         0|            0|            0|  0.00%|    return _logger
    86|         0|            0|            0|  0.00%|
    87|         0|            0|            0|  0.00%|def log_to_stderr(level=None):
    88|         0|            0|            0|  0.00%|    '''
    89|         0|            0|            0|  0.00%|    Turn on logging and add a handler which prints to stderr
    90|         0|            0|            0|  0.00%|    '''
    91|         0|            0|            0|  0.00%|    global _log_to_stderr
    92|         0|            0|            0|  0.00%|    import logging
    93|         0|            0|            0|  0.00%|
    94|         0|            0|            0|  0.00%|    logger = get_logger()
    95|         0|            0|            0|  0.00%|    formatter = logging.Formatter(DEFAULT_LOGGING_FORMAT)
    96|         0|            0|            0|  0.00%|    handler = logging.StreamHandler()
    97|         0|            0|            0|  0.00%|    handler.setFormatter(formatter)
    98|         0|            0|            0|  0.00%|    logger.addHandler(handler)
    99|         0|            0|            0|  0.00%|
   100|         0|            0|            0|  0.00%|    if level:
   101|         0|            0|            0|  0.00%|        logger.setLevel(level)
   102|         0|            0|            0|  0.00%|    _log_to_stderr = True
   103|         0|            0|            0|  0.00%|    return _logger
   104|         0|            0|            0|  0.00%|
   105|         0|            0|            0|  0.00%|
   106|         0|            0|            0|  0.00%|# Abstract socket support
   107|         0|            0|            0|  0.00%|
   108|         0|            0|            0|  0.00%|def _platform_supports_abstract_sockets():
   109|         0|            0|            0|  0.00%|    if sys.platform == "linux":
   110|         0|            0|            0|  0.00%|        return True
   111|         0|            0|            0|  0.00%|    if hasattr(sys, 'getandroidapilevel'):
   112|         0|            0|            0|  0.00%|        return True
   113|         0|            0|            0|  0.00%|    return False
   114|         0|            0|            0|  0.00%|
   115|         0|            0|            0|  0.00%|
   116|         0|            0|            0|  0.00%|def is_abstract_socket_namespace(address):
   117|         0|            0|            0|  0.00%|    if not address:
   118|         0|            0|            0|  0.00%|        return False
   119|         0|            0|            0|  0.00%|    if isinstance(address, bytes):
   120|         0|            0|            0|  0.00%|        return address[0] == 0
   121|         0|            0|            0|  0.00%|    elif isinstance(address, str):
   122|         0|            0|            0|  0.00%|        return address[0] == "\0"
   123|         0|            0|            0|  0.00%|    raise TypeError('address type of {address!r} unrecognized')
   124|         0|            0|            0|  0.00%|
   125|         0|            0|            0|  0.00%|
   126|         0|            0|            0|  0.00%|abstract_sockets_supported = _platform_supports_abstract_sockets()
   127|         0|            0|            0|  0.00%|
   128|         0|            0|            0|  0.00%|#
   129|         0|            0|            0|  0.00%|# Function returning a temp directory which will be removed on exit
   130|         0|            0|            0|  0.00%|#
   131|         0|            0|            0|  0.00%|
   132|         0|            0|            0|  0.00%|def _remove_temp_dir(rmtree, tempdir):
   133|         0|            0|            0|  0.00%|    rmtree(tempdir)
   134|         0|            0|            0|  0.00%|
   135|         0|            0|            0|  0.00%|    current_process = process.current_process()
   136|         0|            0|            0|  0.00%|    # current_process() can be None if the finalizer is called
   137|         0|            0|            0|  0.00%|    # late during Python finalization
   138|         0|            0|            0|  0.00%|    if current_process is not None:
   139|         0|            0|            0|  0.00%|        current_process._config['tempdir'] = None
   140|         0|            0|            0|  0.00%|
   141|         0|            0|            0|  0.00%|def get_temp_dir():
   142|         0|            0|            0|  0.00%|    # get name of a temp directory which will be automatically cleaned up
   143|         0|            0|            0|  0.00%|    tempdir = process.current_process()._config.get('tempdir')
   144|         0|            0|            0|  0.00%|    if tempdir is None:
   145|         0|            0|            0|  0.00%|        import shutil, tempfile
   146|         0|            0|            0|  0.00%|        tempdir = tempfile.mkdtemp(prefix='pymp-')
   147|         0|            0|            0|  0.00%|        info('created temp directory %s', tempdir)
   148|         0|            0|            0|  0.00%|        # keep a strong reference to shutil.rmtree(), since the finalizer
   149|         0|            0|            0|  0.00%|        # can be called late during Python shutdown
   150|         0|            0|            0|  0.00%|        Finalize(None, _remove_temp_dir, args=(shutil.rmtree, tempdir),
   151|         0|            0|            0|  0.00%|                 exitpriority=-100)
   152|         0|            0|            0|  0.00%|        process.current_process()._config['tempdir'] = tempdir
   153|         0|            0|            0|  0.00%|    return tempdir
   154|         0|            0|            0|  0.00%|
   155|         0|            0|            0|  0.00%|#
   156|         0|            0|            0|  0.00%|# Support for reinitialization of objects when bootstrapping a child process
   157|         0|            0|            0|  0.00%|#
   158|         0|            0|            0|  0.00%|
   159|         0|            0|            0|  0.00%|_afterfork_registry = weakref.WeakValueDictionary()
   160|         0|            0|            0|  0.00%|_afterfork_counter = itertools.count()
   161|         0|            0|            0|  0.00%|
   162|         0|            0|            0|  0.00%|def _run_after_forkers():
   163|         0|            0|            0|  0.00%|    items = list(_afterfork_registry.items())
   164|         0|            0|            0|  0.00%|    items.sort()
   165|         0|            0|            0|  0.00%|    for (index, ident, func), obj in items:
   166|         0|            0|            0|  0.00%|        try:
   167|         0|            0|            0|  0.00%|            func(obj)
   168|         0|            0|            0|  0.00%|        except Exception as e:
   169|         0|            0|            0|  0.00%|            info('after forker raised exception %s', e)
   170|         0|            0|            0|  0.00%|
   171|        50|  0.000329256|  6.58512e-06|  0.00%|def register_after_fork(obj, func):
   172|        50|   0.00108123|  2.16246e-05|  0.00%|    _afterfork_registry[(next(_afterfork_counter), id(obj), func)] = obj
(call)|        50|   0.00406098|  8.12197e-05|  0.01%|# /opt/conda/lib/python3.8/weakref.py:159 __setitem__
   173|         0|            0|            0|  0.00%|
   174|         0|            0|            0|  0.00%|#
   175|         0|            0|            0|  0.00%|# Finalization using weakrefs
   176|         0|            0|            0|  0.00%|#
   177|         0|            0|            0|  0.00%|
   178|         0|            0|            0|  0.00%|_finalizer_registry = {}
   179|         0|            0|            0|  0.00%|_finalizer_counter = itertools.count()
   180|         0|            0|            0|  0.00%|
   181|         0|            0|            0|  0.00%|
   182|         0|            0|            0|  0.00%|class Finalize(object):
   183|         0|            0|            0|  0.00%|    '''
   184|         0|            0|            0|  0.00%|    Class which supports object finalization using weakrefs
   185|         0|            0|            0|  0.00%|    '''
   186|        16|  0.000140667|  8.79169e-06|  0.00%|    def __init__(self, obj, callback, args=(), kwargs=None, exitpriority=None):
   187|        16|  0.000167131|  1.04457e-05|  0.00%|        if (exitpriority is not None) and not isinstance(exitpriority,int):
   188|         0|            0|            0|  0.00%|            raise TypeError(
   189|         0|            0|            0|  0.00%|                "Exitpriority ({0!r}) must be None or int, not {1!s}".format(
   190|         0|            0|            0|  0.00%|                    exitpriority, type(exitpriority)))
   191|         0|            0|            0|  0.00%|
   192|        16|  0.000142336|  8.89599e-06|  0.00%|        if obj is not None:
   193|        16|  0.000695467|  4.34667e-05|  0.00%|            self._weakref = weakref.ref(obj, self)
   194|         0|            0|            0|  0.00%|        elif exitpriority is None:
   195|         0|            0|            0|  0.00%|            raise ValueError("Without object, exitpriority cannot be None")
   196|         0|            0|            0|  0.00%|
   197|        16|  0.000181437|  1.13398e-05|  0.00%|        self._callback = callback
   198|        16|  0.000153303|  9.58145e-06|  0.00%|        self._args = args
   199|        16|  0.000114918|  7.18236e-06|  0.00%|        self._kwargs = kwargs or {}
   200|        16|  0.000328302|  2.05189e-05|  0.00%|        self._key = (exitpriority, next(_finalizer_counter))
   201|        16|   0.00036931|  2.30819e-05|  0.00%|        self._pid = os.getpid()
   202|         0|            0|            0|  0.00%|
   203|        16|  0.000142574|  8.91089e-06|  0.00%|        _finalizer_registry[self._key] = self
   204|         0|            0|            0|  0.00%|
   205|        16|  9.46522e-05|  5.91576e-06|  0.00%|    def __call__(self, wr=None,
   206|         0|            0|            0|  0.00%|                 # Need to bind these locally because the globals can have
   207|         0|            0|            0|  0.00%|                 # been cleared at shutdown
   208|         0|            0|            0|  0.00%|                 _finalizer_registry=_finalizer_registry,
   209|         0|            0|            0|  0.00%|                 sub_debug=sub_debug, getpid=os.getpid):
   210|         0|            0|            0|  0.00%|        '''
   211|         0|            0|            0|  0.00%|        Run the callback unless it has already been called or cancelled
   212|         0|            0|            0|  0.00%|        '''
   213|        16|  8.46386e-05|  5.28991e-06|  0.00%|        try:
   214|        16|  7.79629e-05|  4.87268e-06|  0.00%|            del _finalizer_registry[self._key]
   215|         0|            0|            0|  0.00%|        except KeyError:
   216|         0|            0|            0|  0.00%|            sub_debug('finalizer no longer registered')
   217|         0|            0|            0|  0.00%|        else:
   218|        16|   0.00011754|  7.34627e-06|  0.00%|            if self._pid != getpid():
   219|         0|            0|            0|  0.00%|                sub_debug('finalizer ignored because different process')
   220|         0|            0|            0|  0.00%|                res = None
   221|         0|            0|            0|  0.00%|            else:
   222|        32|  0.000288486|   9.0152e-06|  0.00%|                sub_debug('finalizer calling %s with args %s and kwargs %s',
(call)|        16|  0.000139952|  8.74698e-06|  0.00%|# /opt/conda/lib/python3.8/multiprocessing/util.py:44 sub_debug
   223|        16|  7.22408e-05|  4.51505e-06|  0.00%|                          self._callback, self._args, self._kwargs)
   224|        16|  0.000246763|  1.54227e-05|  0.00%|                res = self._callback(*self._args, **self._kwargs)
(call)|         8|   0.00147462|  0.000184327|  0.00%|# /opt/conda/lib/python3.8/multiprocessing/queues.py:200 _finalize_close
(call)|         8|  0.000491142|  6.13928e-05|  0.00%|# /opt/conda/lib/python3.8/multiprocessing/util.py:461 close_fds
   225|        16|  8.03471e-05|  5.02169e-06|  0.00%|            self._weakref = self._callback = self._args = \
   226|        32|  0.000166416|  5.20051e-06|  0.00%|                            self._kwargs = self._key = None
   227|        16|  6.22272e-05|   3.8892e-06|  0.00%|            return res
   228|         0|            0|            0|  0.00%|
   229|         0|            0|            0|  0.00%|    def cancel(self):
   230|         0|            0|            0|  0.00%|        '''
   231|         0|            0|            0|  0.00%|        Cancel finalization of the object
   232|         0|            0|            0|  0.00%|        '''
   233|         0|            0|            0|  0.00%|        try:
   234|         0|            0|            0|  0.00%|            del _finalizer_registry[self._key]
   235|         0|            0|            0|  0.00%|        except KeyError:
   236|         0|            0|            0|  0.00%|            pass
   237|         0|            0|            0|  0.00%|        else:
   238|         0|            0|            0|  0.00%|            self._weakref = self._callback = self._args = \
   239|         0|            0|            0|  0.00%|                            self._kwargs = self._key = None
   240|         0|            0|            0|  0.00%|
   241|         0|            0|            0|  0.00%|    def still_active(self):
   242|         0|            0|            0|  0.00%|        '''
   243|         0|            0|            0|  0.00%|        Return whether this finalizer is still waiting to invoke callback
   244|         0|            0|            0|  0.00%|        '''
   245|         0|            0|            0|  0.00%|        return self._key in _finalizer_registry
   246|         0|            0|            0|  0.00%|
   247|         0|            0|            0|  0.00%|    def __repr__(self):
   248|         0|            0|            0|  0.00%|        try:
   249|         0|            0|            0|  0.00%|            obj = self._weakref()
   250|         0|            0|            0|  0.00%|        except (AttributeError, TypeError):
   251|         0|            0|            0|  0.00%|            obj = None
   252|         0|            0|            0|  0.00%|
   253|         0|            0|            0|  0.00%|        if obj is None:
   254|         0|            0|            0|  0.00%|            return '<%s object, dead>' % self.__class__.__name__
   255|         0|            0|            0|  0.00%|
   256|         0|            0|            0|  0.00%|        x = '<%s object, callback=%s' % (
   257|         0|            0|            0|  0.00%|                self.__class__.__name__,
   258|         0|            0|            0|  0.00%|                getattr(self._callback, '__name__', self._callback))
   259|         0|            0|            0|  0.00%|        if self._args:
   260|         0|            0|            0|  0.00%|            x += ', args=' + str(self._args)
   261|         0|            0|            0|  0.00%|        if self._kwargs:
   262|         0|            0|            0|  0.00%|            x += ', kwargs=' + str(self._kwargs)
   263|         0|            0|            0|  0.00%|        if self._key[0] is not None:
   264|         0|            0|            0|  0.00%|            x += ', exitpriority=' + str(self._key[0])
   265|         0|            0|            0|  0.00%|        return x + '>'
   266|         0|            0|            0|  0.00%|
   267|         0|            0|            0|  0.00%|
   268|         0|            0|            0|  0.00%|def _run_finalizers(minpriority=None):
   269|         0|            0|            0|  0.00%|    '''
   270|         0|            0|            0|  0.00%|    Run all finalizers whose exit priority is not None and at least minpriority
   271|         0|            0|            0|  0.00%|
   272|         0|            0|            0|  0.00%|    Finalizers with highest priority are called first; finalizers with
   273|         0|            0|            0|  0.00%|    the same priority will be called in reverse order of creation.
   274|         0|            0|            0|  0.00%|    '''
   275|         0|            0|            0|  0.00%|    if _finalizer_registry is None:
   276|         0|            0|            0|  0.00%|        # This function may be called after this module's globals are
   277|         0|            0|            0|  0.00%|        # destroyed.  See the _exit_function function in this module for more
   278|         0|            0|            0|  0.00%|        # notes.
   279|         0|            0|            0|  0.00%|        return
   280|         0|            0|            0|  0.00%|
   281|         0|            0|            0|  0.00%|    if minpriority is None:
   282|         0|            0|            0|  0.00%|        f = lambda p : p[0] is not None
   283|         0|            0|            0|  0.00%|    else:
   284|         0|            0|            0|  0.00%|        f = lambda p : p[0] is not None and p[0] >= minpriority
   285|         0|            0|            0|  0.00%|
   286|         0|            0|            0|  0.00%|    # Careful: _finalizer_registry may be mutated while this function
   287|         0|            0|            0|  0.00%|    # is running (either by a GC run or by another thread).
   288|         0|            0|            0|  0.00%|
   289|         0|            0|            0|  0.00%|    # list(_finalizer_registry) should be atomic, while
   290|         0|            0|            0|  0.00%|    # list(_finalizer_registry.items()) is not.
   291|         0|            0|            0|  0.00%|    keys = [key for key in list(_finalizer_registry) if f(key)]
   292|         0|            0|            0|  0.00%|    keys.sort(reverse=True)
   293|         0|            0|            0|  0.00%|
   294|         0|            0|            0|  0.00%|    for key in keys:
   295|         0|            0|            0|  0.00%|        finalizer = _finalizer_registry.get(key)
   296|         0|            0|            0|  0.00%|        # key may have been removed from the registry
   297|         0|            0|            0|  0.00%|        if finalizer is not None:
   298|         0|            0|            0|  0.00%|            sub_debug('calling %s', finalizer)
   299|         0|            0|            0|  0.00%|            try:
   300|         0|            0|            0|  0.00%|                finalizer()
   301|         0|            0|            0|  0.00%|            except Exception:
   302|         0|            0|            0|  0.00%|                import traceback
   303|         0|            0|            0|  0.00%|                traceback.print_exc()
   304|         0|            0|            0|  0.00%|
   305|         0|            0|            0|  0.00%|    if minpriority is None:
   306|         0|            0|            0|  0.00%|        _finalizer_registry.clear()
   307|         0|            0|            0|  0.00%|
   308|         0|            0|            0|  0.00%|#
   309|         0|            0|            0|  0.00%|# Clean up on exit
   310|         0|            0|            0|  0.00%|#
   311|         0|            0|            0|  0.00%|
   312|         0|            0|            0|  0.00%|def is_exiting():
   313|         0|            0|            0|  0.00%|    '''
   314|         0|            0|            0|  0.00%|    Returns true if the process is shutting down
   315|         0|            0|            0|  0.00%|    '''
   316|         0|            0|            0|  0.00%|    return _exiting or _exiting is None
   317|         0|            0|            0|  0.00%|
   318|         0|            0|            0|  0.00%|_exiting = False
   319|         0|            0|            0|  0.00%|
   320|         0|            0|            0|  0.00%|def _exit_function(info=info, debug=debug, _run_finalizers=_run_finalizers,
   321|         0|            0|            0|  0.00%|                   active_children=process.active_children,
   322|         0|            0|            0|  0.00%|                   current_process=process.current_process):
   323|         0|            0|            0|  0.00%|    # We hold on to references to functions in the arglist due to the
   324|         0|            0|            0|  0.00%|    # situation described below, where this function is called after this
   325|         0|            0|            0|  0.00%|    # module's globals are destroyed.
   326|         0|            0|            0|  0.00%|
   327|         0|            0|            0|  0.00%|    global _exiting
   328|         0|            0|            0|  0.00%|
   329|         0|            0|            0|  0.00%|    if not _exiting:
   330|         0|            0|            0|  0.00%|        _exiting = True
   331|         0|            0|            0|  0.00%|
   332|         0|            0|            0|  0.00%|        info('process shutting down')
   333|         0|            0|            0|  0.00%|        debug('running all "atexit" finalizers with priority >= 0')
   334|         0|            0|            0|  0.00%|        _run_finalizers(0)
   335|         0|            0|            0|  0.00%|
   336|         0|            0|            0|  0.00%|        if current_process() is not None:
   337|         0|            0|            0|  0.00%|            # We check if the current process is None here because if
   338|         0|            0|            0|  0.00%|            # it's None, any call to ``active_children()`` will raise
   339|         0|            0|            0|  0.00%|            # an AttributeError (active_children winds up trying to
   340|         0|            0|            0|  0.00%|            # get attributes from util._current_process).  One
   341|         0|            0|            0|  0.00%|            # situation where this can happen is if someone has
   342|         0|            0|            0|  0.00%|            # manipulated sys.modules, causing this module to be
   343|         0|            0|            0|  0.00%|            # garbage collected.  The destructor for the module type
   344|         0|            0|            0|  0.00%|            # then replaces all values in the module dict with None.
   345|         0|            0|            0|  0.00%|            # For instance, after setuptools runs a test it replaces
   346|         0|            0|            0|  0.00%|            # sys.modules with a copy created earlier.  See issues
   347|         0|            0|            0|  0.00%|            # #9775 and #15881.  Also related: #4106, #9205, and
   348|         0|            0|            0|  0.00%|            # #9207.
   349|         0|            0|            0|  0.00%|
   350|         0|            0|            0|  0.00%|            for p in active_children():
   351|         0|            0|            0|  0.00%|                if p.daemon:
   352|         0|            0|            0|  0.00%|                    info('calling terminate() for daemon %s', p.name)
   353|         0|            0|            0|  0.00%|                    p._popen.terminate()
   354|         0|            0|            0|  0.00%|
   355|         0|            0|            0|  0.00%|            for p in active_children():
   356|         0|            0|            0|  0.00%|                info('calling join() for process %s', p.name)
   357|         0|            0|            0|  0.00%|                p.join()
   358|         0|            0|            0|  0.00%|
   359|         0|            0|            0|  0.00%|        debug('running the remaining "atexit" finalizers')
   360|         0|            0|            0|  0.00%|        _run_finalizers()
   361|         0|            0|            0|  0.00%|
   362|         0|            0|            0|  0.00%|atexit.register(_exit_function)
   363|         0|            0|            0|  0.00%|
   364|         0|            0|            0|  0.00%|#
   365|         0|            0|            0|  0.00%|# Some fork aware types
   366|         0|            0|            0|  0.00%|#
   367|         0|            0|            0|  0.00%|
   368|         0|            0|            0|  0.00%|class ForkAwareThreadLock(object):
   369|         0|            0|            0|  0.00%|    def __init__(self):
   370|         0|            0|            0|  0.00%|        self._reset()
   371|         0|            0|            0|  0.00%|        register_after_fork(self, ForkAwareThreadLock._reset)
   372|         0|            0|            0|  0.00%|
   373|         0|            0|            0|  0.00%|    def _reset(self):
   374|         0|            0|            0|  0.00%|        self._lock = threading.Lock()
   375|         0|            0|            0|  0.00%|        self.acquire = self._lock.acquire
   376|         0|            0|            0|  0.00%|        self.release = self._lock.release
   377|         0|            0|            0|  0.00%|
   378|         0|            0|            0|  0.00%|    def __enter__(self):
   379|         0|            0|            0|  0.00%|        return self._lock.__enter__()
   380|         0|            0|            0|  0.00%|
   381|         0|            0|            0|  0.00%|    def __exit__(self, *args):
   382|         0|            0|            0|  0.00%|        return self._lock.__exit__(*args)
   383|         0|            0|            0|  0.00%|
   384|         0|            0|            0|  0.00%|
   385|         0|            0|            0|  0.00%|class ForkAwareLocal(threading.local):
   386|         0|            0|            0|  0.00%|    def __init__(self):
   387|         0|            0|            0|  0.00%|        register_after_fork(self, lambda obj : obj.__dict__.clear())
   388|         0|            0|            0|  0.00%|    def __reduce__(self):
   389|         0|            0|            0|  0.00%|        return type(self), ()
   390|         0|            0|            0|  0.00%|
   391|         0|            0|            0|  0.00%|#
   392|         0|            0|            0|  0.00%|# Close fds except those specified
   393|         0|            0|            0|  0.00%|#
   394|         0|            0|            0|  0.00%|
   395|         0|            0|            0|  0.00%|try:
   396|         0|            0|            0|  0.00%|    MAXFD = os.sysconf("SC_OPEN_MAX")
   397|         0|            0|            0|  0.00%|except Exception:
   398|         0|            0|            0|  0.00%|    MAXFD = 256
   399|         0|            0|            0|  0.00%|
   400|         0|            0|            0|  0.00%|def close_all_fds_except(fds):
   401|         0|            0|            0|  0.00%|    fds = list(fds) + [-1, MAXFD]
   402|         0|            0|            0|  0.00%|    fds.sort()
   403|         0|            0|            0|  0.00%|    assert fds[-1] == MAXFD, 'fd too large'
   404|         0|            0|            0|  0.00%|    for i in range(len(fds) - 1):
   405|         0|            0|            0|  0.00%|        os.closerange(fds[i]+1, fds[i+1])
   406|         0|            0|            0|  0.00%|#
   407|         0|            0|            0|  0.00%|# Close sys.stdin and replace stdin with os.devnull
   408|         0|            0|            0|  0.00%|#
   409|         0|            0|            0|  0.00%|
   410|         0|            0|            0|  0.00%|def _close_stdin():
   411|         0|            0|            0|  0.00%|    if sys.stdin is None:
   412|         0|            0|            0|  0.00%|        return
   413|         0|            0|            0|  0.00%|
   414|         0|            0|            0|  0.00%|    try:
   415|         0|            0|            0|  0.00%|        sys.stdin.close()
   416|         0|            0|            0|  0.00%|    except (OSError, ValueError):
   417|         0|            0|            0|  0.00%|        pass
   418|         0|            0|            0|  0.00%|
   419|         0|            0|            0|  0.00%|    try:
   420|         0|            0|            0|  0.00%|        fd = os.open(os.devnull, os.O_RDONLY)
   421|         0|            0|            0|  0.00%|        try:
   422|         0|            0|            0|  0.00%|            sys.stdin = open(fd, closefd=False)
   423|         0|            0|            0|  0.00%|        except:
   424|         0|            0|            0|  0.00%|            os.close(fd)
   425|         0|            0|            0|  0.00%|            raise
   426|         0|            0|            0|  0.00%|    except (OSError, ValueError):
   427|         0|            0|            0|  0.00%|        pass
   428|         0|            0|            0|  0.00%|
   429|         0|            0|            0|  0.00%|#
   430|         0|            0|            0|  0.00%|# Flush standard streams, if any
   431|         0|            0|            0|  0.00%|#
   432|         0|            0|            0|  0.00%|
   433|         8|  5.43594e-05|  6.79493e-06|  0.00%|def _flush_std_streams():
   434|         8|  3.55244e-05|  4.44055e-06|  0.00%|    try:
   435|         8|  0.000174522|  2.18153e-05|  0.00%|        sys.stdout.flush()
   436|         0|            0|            0|  0.00%|    except (AttributeError, ValueError):
   437|         0|            0|            0|  0.00%|        pass
   438|         8|  3.60012e-05|  4.50015e-06|  0.00%|    try:
   439|         8|  0.000132084|  1.65105e-05|  0.00%|        sys.stderr.flush()
   440|         0|            0|            0|  0.00%|    except (AttributeError, ValueError):
   441|         0|            0|            0|  0.00%|        pass
   442|         0|            0|            0|  0.00%|
   443|         0|            0|            0|  0.00%|#
   444|         0|            0|            0|  0.00%|# Start a program with only specified fds kept open
   445|         0|            0|            0|  0.00%|#
   446|         0|            0|            0|  0.00%|
   447|         0|            0|            0|  0.00%|def spawnv_passfds(path, args, passfds):
   448|         0|            0|            0|  0.00%|    import _posixsubprocess
   449|         0|            0|            0|  0.00%|    passfds = tuple(sorted(map(int, passfds)))
   450|         0|            0|            0|  0.00%|    errpipe_read, errpipe_write = os.pipe()
   451|         0|            0|            0|  0.00%|    try:
   452|         0|            0|            0|  0.00%|        return _posixsubprocess.fork_exec(
   453|         0|            0|            0|  0.00%|            args, [os.fsencode(path)], True, passfds, None, None,
   454|         0|            0|            0|  0.00%|            -1, -1, -1, -1, -1, -1, errpipe_read, errpipe_write,
   455|         0|            0|            0|  0.00%|            False, False, None)
   456|         0|            0|            0|  0.00%|    finally:
   457|         0|            0|            0|  0.00%|        os.close(errpipe_read)
   458|         0|            0|            0|  0.00%|        os.close(errpipe_write)
   459|         0|            0|            0|  0.00%|
   460|         0|            0|            0|  0.00%|
   461|         8|  3.76701e-05|  4.70877e-06|  0.00%|def close_fds(*fds):
   462|         0|            0|            0|  0.00%|    """Close each file descriptor given as an argument"""
   463|        24|  0.000126839|  5.28495e-06|  0.00%|    for fd in fds:
   464|        16|  0.000326633|  2.04146e-05|  0.00%|        os.close(fd)
   465|         0|            0|            0|  0.00%|
   466|         0|            0|            0|  0.00%|
   467|         0|            0|            0|  0.00%|def _cleanup_tests():
   468|         0|            0|            0|  0.00%|    """Cleanup multiprocessing resources when multiprocessing tests
   469|         0|            0|            0|  0.00%|    completed."""
   470|         0|            0|            0|  0.00%|
   471|         0|            0|            0|  0.00%|    from test import support
   472|         0|            0|            0|  0.00%|
   473|         0|            0|            0|  0.00%|    # cleanup multiprocessing
   474|         0|            0|            0|  0.00%|    process._cleanup()
   475|         0|            0|            0|  0.00%|
   476|         0|            0|            0|  0.00%|    # Stop the ForkServer process if it's running
   477|         0|            0|            0|  0.00%|    from multiprocessing import forkserver
   478|         0|            0|            0|  0.00%|    forkserver._forkserver._stop()
   479|         0|            0|            0|  0.00%|
   480|         0|            0|            0|  0.00%|    # Stop the ResourceTracker process if it's running
   481|         0|            0|            0|  0.00%|    from multiprocessing import resource_tracker
   482|         0|            0|            0|  0.00%|    resource_tracker._resource_tracker._stop()
   483|         0|            0|            0|  0.00%|
   484|         0|            0|            0|  0.00%|    # bpo-37421: Explicitly call _run_finalizers() to remove immediately
   485|         0|            0|            0|  0.00%|    # temporary directories created by multiprocessing.util.get_temp_dir().
   486|         0|            0|            0|  0.00%|    _run_finalizers()
   487|         0|            0|            0|  0.00%|    support.gc_collect()
   488|         0|            0|            0|  0.00%|
   489|         0|            0|            0|  0.00%|    support.reap_children()
File: /opt/conda/lib/python3.8/site-packages/torch/nn/modules/utils.py
File duration: 0.0071919s (0.01%)
Line #|      Hits|         Time| Time per hit|      %|Source code
------+----------+-------------+-------------+-------+-----------
     1|         0|            0|            0|  0.00%|
     2|         0|            0|            0|  0.00%|import collections
     3|         0|            0|            0|  0.00%|from itertools import repeat
     4|         0|            0|            0|  0.00%|from typing import List, Dict, Any
     5|         0|            0|            0|  0.00%|
     6|         0|            0|            0|  0.00%|
     7|         0|            0|            0|  0.00%|def _ntuple(n):
     8|         0|            0|            0|  0.00%|    def parse(x):
     9|         0|            0|            0|  0.00%|        if isinstance(x, collections.abc.Iterable):
    10|         0|            0|            0|  0.00%|            return tuple(x)
    11|         0|            0|            0|  0.00%|        return tuple(repeat(x, n))
    12|         0|            0|            0|  0.00%|    return parse
    13|         0|            0|            0|  0.00%|
    14|         0|            0|            0|  0.00%|_single = _ntuple(1)
    15|         0|            0|            0|  0.00%|_pair = _ntuple(2)
    16|         0|            0|            0|  0.00%|_triple = _ntuple(3)
    17|         0|            0|            0|  0.00%|_quadruple = _ntuple(4)
    18|         0|            0|            0|  0.00%|
    19|         0|            0|            0|  0.00%|
    20|         0|            0|            0|  0.00%|def _reverse_repeat_tuple(t, n):
    21|         0|            0|            0|  0.00%|    r"""Reverse the order of `t` and repeat each element for `n` times.
    22|         0|            0|            0|  0.00%|
    23|         0|            0|            0|  0.00%|    This can be used to translate padding arg used by Conv and Pooling modules
    24|         0|            0|            0|  0.00%|    to the ones used by `F.pad`.
    25|         0|            0|            0|  0.00%|    """
    26|         0|            0|            0|  0.00%|    return tuple(x for x in reversed(t) for _ in range(n))
    27|         0|            0|            0|  0.00%|
    28|         0|            0|            0|  0.00%|
    29|       100|   0.00073719|   7.3719e-06|  0.00%|def _list_with_default(out_size: List[int], defaults: List[int]) -> List[int]:
    30|       100|  0.000952721|  9.52721e-06|  0.00%|    if isinstance(out_size, int):
    31|         0|            0|            0|  0.00%|        return out_size
    32|       100|  0.000887871|  8.87871e-06|  0.00%|    if len(defaults) <= len(out_size):
    33|         0|            0|            0|  0.00%|        raise ValueError('Input dimension should be at least {}'.format(len(out_size) + 1))
    34|       500|   0.00461411|  9.22823e-06|  0.01%|    return [v if v is not None else d for v, d in zip(out_size, defaults[-len(out_size):])]
(call)|       100|   0.00204182|  2.04182e-05|  0.00%|# /opt/conda/lib/python3.8/site-packages/torch/nn/modules/utils.py:34 <listcomp>
    35|         0|            0|            0|  0.00%|
    36|         0|            0|            0|  0.00%|
    37|         0|            0|            0|  0.00%|def consume_prefix_in_state_dict_if_present(state_dict: Dict[str, Any], prefix: str):
    38|         0|            0|            0|  0.00%|    r"""Strip the prefix in state_dict, if any.
    39|         0|            0|            0|  0.00%|
    40|         0|            0|            0|  0.00%|    ..note::
    41|         0|            0|            0|  0.00%|        Given a `state_dict` from a DP/DDP model, a local model can load it by applying
    42|         0|            0|            0|  0.00%|        `consume_prefix_in_state_dict_if_present(state_dict, "module.")` before calling
    43|         0|            0|            0|  0.00%|        :meth:`torch.nn.Module.load_state_dict`.
    44|         0|            0|            0|  0.00%|
    45|         0|            0|            0|  0.00%|    Args:
    46|         0|            0|            0|  0.00%|        state_dict (OrderedDict): a state-dict to be loaded to the model.
    47|         0|            0|            0|  0.00%|        prefix (str): prefix.
    48|         0|            0|            0|  0.00%|    """
    49|         0|            0|            0|  0.00%|    keys = sorted(state_dict.keys())
    50|         0|            0|            0|  0.00%|    for key in keys:
    51|         0|            0|            0|  0.00%|        if key.startswith(prefix):
    52|         0|            0|            0|  0.00%|            newkey = key[len(prefix) :]
    53|         0|            0|            0|  0.00%|            state_dict[newkey] = state_dict.pop(key)
    54|         0|            0|            0|  0.00%|
    55|         0|            0|            0|  0.00%|    # also strip the prefix in metadata if any.
    56|         0|            0|            0|  0.00%|    if "_metadata" in state_dict:
    57|         0|            0|            0|  0.00%|        metadata = state_dict["_metadata"]
    58|         0|            0|            0|  0.00%|        for key in list(metadata.keys()):
    59|         0|            0|            0|  0.00%|            # for the metadata dict, the key can be:
    60|         0|            0|            0|  0.00%|            # '': for the DDP module, which we want to remove.
    61|         0|            0|            0|  0.00%|            # 'module': for the actual model.
    62|         0|            0|            0|  0.00%|            # 'module.xx.xx': for the rest.
    63|         0|            0|            0|  0.00%|
    64|         0|            0|            0|  0.00%|            if len(key) == 0:
    65|         0|            0|            0|  0.00%|                continue
    66|         0|            0|            0|  0.00%|            newkey = key[len(prefix) :]
    67|         0|            0|            0|  0.00%|            metadata[newkey] = metadata.pop(key)
File: /opt/conda/lib/python3.8/site-packages/torch/nn/modules/loss.py
File duration: 0.00546932s (0.01%)
Line #|      Hits|         Time| Time per hit|      %|Source code
------+----------+-------------+-------------+-------+-----------
     1|         0|            0|            0|  0.00%|import warnings
     2|         0|            0|            0|  0.00%|
     3|         0|            0|            0|  0.00%|from .distance import PairwiseDistance
     4|         0|            0|            0|  0.00%|from .module import Module
     5|         0|            0|            0|  0.00%|from .. import functional as F
     6|         0|            0|            0|  0.00%|from .. import _reduction as _Reduction
     7|         0|            0|            0|  0.00%|
     8|         0|            0|            0|  0.00%|from torch import Tensor
     9|         0|            0|            0|  0.00%|from typing import Callable, Optional
    10|         0|            0|            0|  0.00%|
    11|         0|            0|            0|  0.00%|
    12|         0|            0|            0|  0.00%|class _Loss(Module):
    13|         0|            0|            0|  0.00%|    reduction: str
    14|         0|            0|            0|  0.00%|
    15|         0|            0|            0|  0.00%|    def __init__(self, size_average=None, reduce=None, reduction: str = 'mean') -> None:
    16|         0|            0|            0|  0.00%|        super(_Loss, self).__init__()
    17|         0|            0|            0|  0.00%|        if size_average is not None or reduce is not None:
    18|         0|            0|            0|  0.00%|            self.reduction: str = _Reduction.legacy_get_string(size_average, reduce)
    19|         0|            0|            0|  0.00%|        else:
    20|         0|            0|            0|  0.00%|            self.reduction = reduction
    21|         0|            0|            0|  0.00%|
    22|         0|            0|            0|  0.00%|
    23|         0|            0|            0|  0.00%|class _WeightedLoss(_Loss):
    24|         0|            0|            0|  0.00%|    def __init__(self, weight: Optional[Tensor] = None, size_average=None, reduce=None, reduction: str = 'mean') -> None:
    25|         0|            0|            0|  0.00%|        super(_WeightedLoss, self).__init__(size_average, reduce, reduction)
    26|         0|            0|            0|  0.00%|        self.register_buffer('weight', weight)
    27|         0|            0|            0|  0.00%|        self.weight: Optional[Tensor]
    28|         0|            0|            0|  0.00%|
    29|         0|            0|            0|  0.00%|
    30|         0|            0|            0|  0.00%|class L1Loss(_Loss):
    31|         0|            0|            0|  0.00%|    r"""Creates a criterion that measures the mean absolute error (MAE) between each element in
    32|         0|            0|            0|  0.00%|    the input :math:`x` and target :math:`y`.
    33|         0|            0|            0|  0.00%|
    34|         0|            0|            0|  0.00%|    The unreduced (i.e. with :attr:`reduction` set to ``'none'``) loss can be described as:
    35|         0|            0|            0|  0.00%|
    36|         0|            0|            0|  0.00%|    .. math::
    37|         0|            0|            0|  0.00%|        \ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad
    38|         0|            0|            0|  0.00%|        l_n = \left| x_n - y_n \right|,
    39|         0|            0|            0|  0.00%|
    40|         0|            0|            0|  0.00%|    where :math:`N` is the batch size. If :attr:`reduction` is not ``'none'``
    41|         0|            0|            0|  0.00%|    (default ``'mean'``), then:
    42|         0|            0|            0|  0.00%|
    43|         0|            0|            0|  0.00%|    .. math::
    44|         0|            0|            0|  0.00%|        \ell(x, y) =
    45|         0|            0|            0|  0.00%|        \begin{cases}
    46|         0|            0|            0|  0.00%|            \operatorname{mean}(L), & \text{if reduction} = \text{`mean';}\\
    47|         0|            0|            0|  0.00%|            \operatorname{sum}(L),  & \text{if reduction} = \text{`sum'.}
    48|         0|            0|            0|  0.00%|        \end{cases}
    49|         0|            0|            0|  0.00%|
    50|         0|            0|            0|  0.00%|    :math:`x` and :math:`y` are tensors of arbitrary shapes with a total
    51|         0|            0|            0|  0.00%|    of :math:`n` elements each.
    52|         0|            0|            0|  0.00%|
    53|         0|            0|            0|  0.00%|    The sum operation still operates over all the elements, and divides by :math:`n`.
    54|         0|            0|            0|  0.00%|
    55|         0|            0|            0|  0.00%|    The division by :math:`n` can be avoided if one sets ``reduction = 'sum'``.
    56|         0|            0|            0|  0.00%|
    57|         0|            0|            0|  0.00%|    Supports real-valued and complex-valued inputs.
    58|         0|            0|            0|  0.00%|
    59|         0|            0|            0|  0.00%|    Args:
    60|         0|            0|            0|  0.00%|        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,
    61|         0|            0|            0|  0.00%|            the losses are averaged over each loss element in the batch. Note that for
    62|         0|            0|            0|  0.00%|            some losses, there are multiple elements per sample. If the field :attr:`size_average`
    63|         0|            0|            0|  0.00%|            is set to ``False``, the losses are instead summed for each minibatch. Ignored
    64|         0|            0|            0|  0.00%|            when :attr:`reduce` is ``False``. Default: ``True``
    65|         0|            0|            0|  0.00%|        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the
    66|         0|            0|            0|  0.00%|            losses are averaged or summed over observations for each minibatch depending
    67|         0|            0|            0|  0.00%|            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per
    68|         0|            0|            0|  0.00%|            batch element instead and ignores :attr:`size_average`. Default: ``True``
    69|         0|            0|            0|  0.00%|        reduction (string, optional): Specifies the reduction to apply to the output:
    70|         0|            0|            0|  0.00%|            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,
    71|         0|            0|            0|  0.00%|            ``'mean'``: the sum of the output will be divided by the number of
    72|         0|            0|            0|  0.00%|            elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`
    73|         0|            0|            0|  0.00%|            and :attr:`reduce` are in the process of being deprecated, and in the meantime,
    74|         0|            0|            0|  0.00%|            specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``
    75|         0|            0|            0|  0.00%|
    76|         0|            0|            0|  0.00%|    Shape:
    77|         0|            0|            0|  0.00%|        - Input: :math:`(N, *)` where :math:`*` means, any number of additional
    78|         0|            0|            0|  0.00%|          dimensions
    79|         0|            0|            0|  0.00%|        - Target: :math:`(N, *)`, same shape as the input
    80|         0|            0|            0|  0.00%|        - Output: scalar. If :attr:`reduction` is ``'none'``, then
    81|         0|            0|            0|  0.00%|          :math:`(N, *)`, same shape as the input
    82|         0|            0|            0|  0.00%|
    83|         0|            0|            0|  0.00%|    Examples::
    84|         0|            0|            0|  0.00%|
    85|         0|            0|            0|  0.00%|        >>> loss = nn.L1Loss()
    86|         0|            0|            0|  0.00%|        >>> input = torch.randn(3, 5, requires_grad=True)
    87|         0|            0|            0|  0.00%|        >>> target = torch.randn(3, 5)
    88|         0|            0|            0|  0.00%|        >>> output = loss(input, target)
    89|         0|            0|            0|  0.00%|        >>> output.backward()
    90|         0|            0|            0|  0.00%|    """
    91|         0|            0|            0|  0.00%|    __constants__ = ['reduction']
    92|         0|            0|            0|  0.00%|
    93|         0|            0|            0|  0.00%|    def __init__(self, size_average=None, reduce=None, reduction: str = 'mean') -> None:
    94|         0|            0|            0|  0.00%|        super(L1Loss, self).__init__(size_average, reduce, reduction)
    95|         0|            0|            0|  0.00%|
    96|         0|            0|            0|  0.00%|    def forward(self, input: Tensor, target: Tensor) -> Tensor:
    97|         0|            0|            0|  0.00%|        return F.l1_loss(input, target, reduction=self.reduction)
    98|         0|            0|            0|  0.00%|
    99|         0|            0|            0|  0.00%|
   100|         0|            0|            0|  0.00%|class NLLLoss(_WeightedLoss):
   101|         0|            0|            0|  0.00%|    r"""The negative log likelihood loss. It is useful to train a classification
   102|         0|            0|            0|  0.00%|    problem with `C` classes.
   103|         0|            0|            0|  0.00%|
   104|         0|            0|            0|  0.00%|    If provided, the optional argument :attr:`weight` should be a 1D Tensor assigning
   105|         0|            0|            0|  0.00%|    weight to each of the classes. This is particularly useful when you have an
   106|         0|            0|            0|  0.00%|    unbalanced training set.
   107|         0|            0|            0|  0.00%|
   108|         0|            0|            0|  0.00%|    The `input` given through a forward call is expected to contain
   109|         0|            0|            0|  0.00%|    log-probabilities of each class. `input` has to be a Tensor of size either
   110|         0|            0|            0|  0.00%|    :math:`(minibatch, C)` or :math:`(minibatch, C, d_1, d_2, ..., d_K)`
   111|         0|            0|            0|  0.00%|    with :math:`K \geq 1` for the `K`-dimensional case (described later).
   112|         0|            0|            0|  0.00%|
   113|         0|            0|            0|  0.00%|    Obtaining log-probabilities in a neural network is easily achieved by
   114|         0|            0|            0|  0.00%|    adding a  `LogSoftmax`  layer in the last layer of your network.
   115|         0|            0|            0|  0.00%|    You may use `CrossEntropyLoss` instead, if you prefer not to add an extra
   116|         0|            0|            0|  0.00%|    layer.
   117|         0|            0|            0|  0.00%|
   118|         0|            0|            0|  0.00%|    The `target` that this loss expects should be a class index in the range :math:`[0, C-1]`
   119|         0|            0|            0|  0.00%|    where `C = number of classes`; if `ignore_index` is specified, this loss also accepts
   120|         0|            0|            0|  0.00%|    this class index (this index may not necessarily be in the class range).
   121|         0|            0|            0|  0.00%|
   122|         0|            0|            0|  0.00%|    The unreduced (i.e. with :attr:`reduction` set to ``'none'``) loss can be described as:
   123|         0|            0|            0|  0.00%|
   124|         0|            0|            0|  0.00%|    .. math::
   125|         0|            0|            0|  0.00%|        \ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad
   126|         0|            0|            0|  0.00%|        l_n = - w_{y_n} x_{n,y_n}, \quad
   127|         0|            0|            0|  0.00%|        w_{c} = \text{weight}[c] \cdot \mathbb{1}\{c \not= \text{ignore\_index}\},
   128|         0|            0|            0|  0.00%|
   129|         0|            0|            0|  0.00%|    where :math:`x` is the input, :math:`y` is the target, :math:`w` is the weight, and
   130|         0|            0|            0|  0.00%|    :math:`N` is the batch size. If :attr:`reduction` is not ``'none'``
   131|         0|            0|            0|  0.00%|    (default ``'mean'``), then
   132|         0|            0|            0|  0.00%|
   133|         0|            0|            0|  0.00%|    .. math::
   134|         0|            0|            0|  0.00%|        \ell(x, y) = \begin{cases}
   135|         0|            0|            0|  0.00%|            \sum_{n=1}^N \frac{1}{\sum_{n=1}^N w_{y_n}} l_n, &
   136|         0|            0|            0|  0.00%|            \text{if reduction} = \text{`mean';}\\
   137|         0|            0|            0|  0.00%|            \sum_{n=1}^N l_n,  &
   138|         0|            0|            0|  0.00%|            \text{if reduction} = \text{`sum'.}
   139|         0|            0|            0|  0.00%|        \end{cases}
   140|         0|            0|            0|  0.00%|
   141|         0|            0|            0|  0.00%|    Can also be used for higher dimension inputs, such as 2D images, by providing
   142|         0|            0|            0|  0.00%|    an input of size :math:`(minibatch, C, d_1, d_2, ..., d_K)` with :math:`K \geq 1`,
   143|         0|            0|            0|  0.00%|    where :math:`K` is the number of dimensions, and a target of appropriate shape
   144|         0|            0|            0|  0.00%|    (see below). In the case of images, it computes NLL loss per-pixel.
   145|         0|            0|            0|  0.00%|
   146|         0|            0|            0|  0.00%|    Args:
   147|         0|            0|            0|  0.00%|        weight (Tensor, optional): a manual rescaling weight given to each
   148|         0|            0|            0|  0.00%|            class. If given, it has to be a Tensor of size `C`. Otherwise, it is
   149|         0|            0|            0|  0.00%|            treated as if having all ones.
   150|         0|            0|            0|  0.00%|        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,
   151|         0|            0|            0|  0.00%|            the losses are averaged over each loss element in the batch. Note that for
   152|         0|            0|            0|  0.00%|            some losses, there are multiple elements per sample. If the field :attr:`size_average`
   153|         0|            0|            0|  0.00%|            is set to ``False``, the losses are instead summed for each minibatch. Ignored
   154|         0|            0|            0|  0.00%|            when :attr:`reduce` is ``False``. Default: ``True``
   155|         0|            0|            0|  0.00%|        ignore_index (int, optional): Specifies a target value that is ignored
   156|         0|            0|            0|  0.00%|            and does not contribute to the input gradient. When
   157|         0|            0|            0|  0.00%|            :attr:`size_average` is ``True``, the loss is averaged over
   158|         0|            0|            0|  0.00%|            non-ignored targets.
   159|         0|            0|            0|  0.00%|        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the
   160|         0|            0|            0|  0.00%|            losses are averaged or summed over observations for each minibatch depending
   161|         0|            0|            0|  0.00%|            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per
   162|         0|            0|            0|  0.00%|            batch element instead and ignores :attr:`size_average`. Default: ``True``
   163|         0|            0|            0|  0.00%|        reduction (string, optional): Specifies the reduction to apply to the output:
   164|         0|            0|            0|  0.00%|            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will
   165|         0|            0|            0|  0.00%|            be applied, ``'mean'``: the weighted mean of the output is taken,
   166|         0|            0|            0|  0.00%|            ``'sum'``: the output will be summed. Note: :attr:`size_average`
   167|         0|            0|            0|  0.00%|            and :attr:`reduce` are in the process of being deprecated, and in
   168|         0|            0|            0|  0.00%|            the meantime, specifying either of those two args will override
   169|         0|            0|            0|  0.00%|            :attr:`reduction`. Default: ``'mean'``
   170|         0|            0|            0|  0.00%|
   171|         0|            0|            0|  0.00%|    Shape:
   172|         0|            0|            0|  0.00%|        - Input: :math:`(N, C)` where `C = number of classes`, or
   173|         0|            0|            0|  0.00%|          :math:`(N, C, d_1, d_2, ..., d_K)` with :math:`K \geq 1`
   174|         0|            0|            0|  0.00%|          in the case of `K`-dimensional loss.
   175|         0|            0|            0|  0.00%|        - Target: :math:`(N)` where each value is :math:`0 \leq \text{targets}[i] \leq C-1`, or
   176|         0|            0|            0|  0.00%|          :math:`(N, d_1, d_2, ..., d_K)` with :math:`K \geq 1` in the case of
   177|         0|            0|            0|  0.00%|          K-dimensional loss.
   178|         0|            0|            0|  0.00%|        - Output: scalar.
   179|         0|            0|            0|  0.00%|          If :attr:`reduction` is ``'none'``, then the same size as the target: :math:`(N)`, or
   180|         0|            0|            0|  0.00%|          :math:`(N, d_1, d_2, ..., d_K)` with :math:`K \geq 1` in the case
   181|         0|            0|            0|  0.00%|          of K-dimensional loss.
   182|         0|            0|            0|  0.00%|
   183|         0|            0|            0|  0.00%|    Examples::
   184|         0|            0|            0|  0.00%|
   185|         0|            0|            0|  0.00%|        >>> m = nn.LogSoftmax(dim=1)
   186|         0|            0|            0|  0.00%|        >>> loss = nn.NLLLoss()
   187|         0|            0|            0|  0.00%|        >>> # input is of size N x C = 3 x 5
   188|         0|            0|            0|  0.00%|        >>> input = torch.randn(3, 5, requires_grad=True)
   189|         0|            0|            0|  0.00%|        >>> # each element in target has to have 0 <= value < C
   190|         0|            0|            0|  0.00%|        >>> target = torch.tensor([1, 0, 4])
   191|         0|            0|            0|  0.00%|        >>> output = loss(m(input), target)
   192|         0|            0|            0|  0.00%|        >>> output.backward()
   193|         0|            0|            0|  0.00%|        >>>
   194|         0|            0|            0|  0.00%|        >>>
   195|         0|            0|            0|  0.00%|        >>> # 2D loss example (used, for example, with image inputs)
   196|         0|            0|            0|  0.00%|        >>> N, C = 5, 4
   197|         0|            0|            0|  0.00%|        >>> loss = nn.NLLLoss()
   198|         0|            0|            0|  0.00%|        >>> # input is of size N x C x height x width
   199|         0|            0|            0|  0.00%|        >>> data = torch.randn(N, 16, 10, 10)
   200|         0|            0|            0|  0.00%|        >>> conv = nn.Conv2d(16, C, (3, 3))
   201|         0|            0|            0|  0.00%|        >>> m = nn.LogSoftmax(dim=1)
   202|         0|            0|            0|  0.00%|        >>> # each element in target has to have 0 <= value < C
   203|         0|            0|            0|  0.00%|        >>> target = torch.empty(N, 8, 8, dtype=torch.long).random_(0, C)
   204|         0|            0|            0|  0.00%|        >>> output = loss(m(conv(data)), target)
   205|         0|            0|            0|  0.00%|        >>> output.backward()
   206|         0|            0|            0|  0.00%|    """
   207|         0|            0|            0|  0.00%|    __constants__ = ['ignore_index', 'reduction']
   208|         0|            0|            0|  0.00%|    ignore_index: int
   209|         0|            0|            0|  0.00%|
   210|         0|            0|            0|  0.00%|    def __init__(self, weight: Optional[Tensor] = None, size_average=None, ignore_index: int = -100,
   211|         0|            0|            0|  0.00%|                 reduce=None, reduction: str = 'mean') -> None:
   212|         0|            0|            0|  0.00%|        super(NLLLoss, self).__init__(weight, size_average, reduce, reduction)
   213|         0|            0|            0|  0.00%|        self.ignore_index = ignore_index
   214|         0|            0|            0|  0.00%|
   215|         0|            0|            0|  0.00%|    def forward(self, input: Tensor, target: Tensor) -> Tensor:
   216|         0|            0|            0|  0.00%|        return F.nll_loss(input, target, weight=self.weight, ignore_index=self.ignore_index, reduction=self.reduction)
   217|         0|            0|            0|  0.00%|
   218|         0|            0|            0|  0.00%|
   219|         0|            0|            0|  0.00%|class NLLLoss2d(NLLLoss):
   220|         0|            0|            0|  0.00%|    def __init__(self, weight: Optional[Tensor] = None, size_average=None, ignore_index: int = -100,
   221|         0|            0|            0|  0.00%|                 reduce=None, reduction: str = 'mean') -> None:
   222|         0|            0|            0|  0.00%|        warnings.warn("NLLLoss2d has been deprecated. "
   223|         0|            0|            0|  0.00%|                      "Please use NLLLoss instead as a drop-in replacement and see "
   224|         0|            0|            0|  0.00%|                      "https://pytorch.org/docs/master/nn.html#torch.nn.NLLLoss for more details.")
   225|         0|            0|            0|  0.00%|        super(NLLLoss2d, self).__init__(weight, size_average, ignore_index, reduce, reduction)
   226|         0|            0|            0|  0.00%|
   227|         0|            0|            0|  0.00%|
   228|         0|            0|            0|  0.00%|class PoissonNLLLoss(_Loss):
   229|         0|            0|            0|  0.00%|    r"""Negative log likelihood loss with Poisson distribution of target.
   230|         0|            0|            0|  0.00%|
   231|         0|            0|            0|  0.00%|    The loss can be described as:
   232|         0|            0|            0|  0.00%|
   233|         0|            0|            0|  0.00%|    .. math::
   234|         0|            0|            0|  0.00%|        \text{target} \sim \mathrm{Poisson}(\text{input})
   235|         0|            0|            0|  0.00%|
   236|         0|            0|            0|  0.00%|        \text{loss}(\text{input}, \text{target}) = \text{input} - \text{target} * \log(\text{input})
   237|         0|            0|            0|  0.00%|                                    + \log(\text{target!})
   238|         0|            0|            0|  0.00%|
   239|         0|            0|            0|  0.00%|    The last term can be omitted or approximated with Stirling formula. The
   240|         0|            0|            0|  0.00%|    approximation is used for target values more than 1. For targets less or
   241|         0|            0|            0|  0.00%|    equal to 1 zeros are added to the loss.
   242|         0|            0|            0|  0.00%|
   243|         0|            0|            0|  0.00%|    Args:
   244|         0|            0|            0|  0.00%|        log_input (bool, optional): if ``True`` the loss is computed as
   245|         0|            0|            0|  0.00%|            :math:`\exp(\text{input}) - \text{target}*\text{input}`, if ``False`` the loss is
   246|         0|            0|            0|  0.00%|            :math:`\text{input} - \text{target}*\log(\text{input}+\text{eps})`.
   247|         0|            0|            0|  0.00%|        full (bool, optional): whether to compute full loss, i. e. to add the
   248|         0|            0|            0|  0.00%|            Stirling approximation term
   249|         0|            0|            0|  0.00%|
   250|         0|            0|            0|  0.00%|            .. math::
   251|         0|            0|            0|  0.00%|                \text{target}*\log(\text{target}) - \text{target} + 0.5 * \log(2\pi\text{target}).
   252|         0|            0|            0|  0.00%|        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,
   253|         0|            0|            0|  0.00%|            the losses are averaged over each loss element in the batch. Note that for
   254|         0|            0|            0|  0.00%|            some losses, there are multiple elements per sample. If the field :attr:`size_average`
   255|         0|            0|            0|  0.00%|            is set to ``False``, the losses are instead summed for each minibatch. Ignored
   256|         0|            0|            0|  0.00%|            when :attr:`reduce` is ``False``. Default: ``True``
   257|         0|            0|            0|  0.00%|        eps (float, optional): Small value to avoid evaluation of :math:`\log(0)` when
   258|         0|            0|            0|  0.00%|            :attr:`log_input = False`. Default: 1e-8
   259|         0|            0|            0|  0.00%|        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the
   260|         0|            0|            0|  0.00%|            losses are averaged or summed over observations for each minibatch depending
   261|         0|            0|            0|  0.00%|            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per
   262|         0|            0|            0|  0.00%|            batch element instead and ignores :attr:`size_average`. Default: ``True``
   263|         0|            0|            0|  0.00%|        reduction (string, optional): Specifies the reduction to apply to the output:
   264|         0|            0|            0|  0.00%|            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,
   265|         0|            0|            0|  0.00%|            ``'mean'``: the sum of the output will be divided by the number of
   266|         0|            0|            0|  0.00%|            elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`
   267|         0|            0|            0|  0.00%|            and :attr:`reduce` are in the process of being deprecated, and in the meantime,
   268|         0|            0|            0|  0.00%|            specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``
   269|         0|            0|            0|  0.00%|
   270|         0|            0|            0|  0.00%|    Examples::
   271|         0|            0|            0|  0.00%|
   272|         0|            0|            0|  0.00%|        >>> loss = nn.PoissonNLLLoss()
   273|         0|            0|            0|  0.00%|        >>> log_input = torch.randn(5, 2, requires_grad=True)
   274|         0|            0|            0|  0.00%|        >>> target = torch.randn(5, 2)
   275|         0|            0|            0|  0.00%|        >>> output = loss(log_input, target)
   276|         0|            0|            0|  0.00%|        >>> output.backward()
   277|         0|            0|            0|  0.00%|
   278|         0|            0|            0|  0.00%|    Shape:
   279|         0|            0|            0|  0.00%|        - Input: :math:`(N, *)` where :math:`*` means, any number of additional
   280|         0|            0|            0|  0.00%|          dimensions
   281|         0|            0|            0|  0.00%|        - Target: :math:`(N, *)`, same shape as the input
   282|         0|            0|            0|  0.00%|        - Output: scalar by default. If :attr:`reduction` is ``'none'``, then :math:`(N, *)`,
   283|         0|            0|            0|  0.00%|          the same shape as the input
   284|         0|            0|            0|  0.00%|    """
   285|         0|            0|            0|  0.00%|    __constants__ = ['log_input', 'full', 'eps', 'reduction']
   286|         0|            0|            0|  0.00%|    log_input: bool
   287|         0|            0|            0|  0.00%|    full: bool
   288|         0|            0|            0|  0.00%|    eps: float
   289|         0|            0|            0|  0.00%|
   290|         0|            0|            0|  0.00%|    def __init__(self, log_input: bool = True, full: bool = False, size_average=None,
   291|         0|            0|            0|  0.00%|                 eps: float = 1e-8, reduce=None, reduction: str = 'mean') -> None:
   292|         0|            0|            0|  0.00%|        super(PoissonNLLLoss, self).__init__(size_average, reduce, reduction)
   293|         0|            0|            0|  0.00%|        self.log_input = log_input
   294|         0|            0|            0|  0.00%|        self.full = full
   295|         0|            0|            0|  0.00%|        self.eps = eps
   296|         0|            0|            0|  0.00%|
   297|         0|            0|            0|  0.00%|    def forward(self, log_input: Tensor, target: Tensor) -> Tensor:
   298|         0|            0|            0|  0.00%|        return F.poisson_nll_loss(log_input, target, log_input=self.log_input, full=self.full,
   299|         0|            0|            0|  0.00%|                                  eps=self.eps, reduction=self.reduction)
   300|         0|            0|            0|  0.00%|
   301|         0|            0|            0|  0.00%|
   302|         0|            0|            0|  0.00%|class GaussianNLLLoss(_Loss):
   303|         0|            0|            0|  0.00%|    r"""Gaussian negative log likelihood loss.
   304|         0|            0|            0|  0.00%|
   305|         0|            0|            0|  0.00%|    The targets are treated as samples from Gaussian distributions with
   306|         0|            0|            0|  0.00%|    expectations and variances predicted by the neural network. For a
   307|         0|            0|            0|  0.00%|    ``target`` tensor modelled as having Gaussian distribution with a tensor
   308|         0|            0|            0|  0.00%|    of expectations ``input`` and a tensor of positive variances ``var`` the loss is:
   309|         0|            0|            0|  0.00%|
   310|         0|            0|            0|  0.00%|    .. math::
   311|         0|            0|            0|  0.00%|        \text{loss} = \frac{1}{2}\left(\log\left(\text{max}\left(\text{var},
   312|         0|            0|            0|  0.00%|        \ \text{eps}\right)\right) + \frac{\left(\text{input} - \text{target}\right)^2}
   313|         0|            0|            0|  0.00%|        {\text{max}\left(\text{var}, \ \text{eps}\right)}\right) + \text{const.}
   314|         0|            0|            0|  0.00%|
   315|         0|            0|            0|  0.00%|    where :attr:`eps` is used for stability. By default, the constant term of
   316|         0|            0|            0|  0.00%|    the loss function is omitted unless :attr:`full` is ``True``. If ``var`` is not the same
   317|         0|            0|            0|  0.00%|    size as ``input`` (due to a homoscedastic assumption), it must either have a final dimension
   318|         0|            0|            0|  0.00%|    of 1 or have one fewer dimension (with all other sizes being the same) for correct broadcasting.
   319|         0|            0|            0|  0.00%|
   320|         0|            0|            0|  0.00%|    Args:
   321|         0|            0|            0|  0.00%|        full (bool, optional): include the constant term in the loss
   322|         0|            0|            0|  0.00%|            calculation. Default: ``False``.
   323|         0|            0|            0|  0.00%|        eps (float, optional): value used to clamp ``var`` (see note below), for
   324|         0|            0|            0|  0.00%|            stability. Default: 1e-6.
   325|         0|            0|            0|  0.00%|        reduction (string, optional): specifies the reduction to apply to the
   326|         0|            0|            0|  0.00%|            output:``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction
   327|         0|            0|            0|  0.00%|            will be applied, ``'mean'``: the output is the average of all batch
   328|         0|            0|            0|  0.00%|            member losses, ``'sum'``: the output is the sum of all batch member
   329|         0|            0|            0|  0.00%|            losses. Default: ``'mean'``.
   330|         0|            0|            0|  0.00%|
   331|         0|            0|            0|  0.00%|    Shape:
   332|         0|            0|            0|  0.00%|        - Input: :math:`(N, *)` where :math:`*` means any number of additional
   333|         0|            0|            0|  0.00%|          dimensions
   334|         0|            0|            0|  0.00%|        - Target: :math:`(N, *)`, same shape as the input, or same shape as the input
   335|         0|            0|            0|  0.00%|          but with one dimension equal to 1 (to allow for broadcasting)
   336|         0|            0|            0|  0.00%|        - Var: :math:`(N, *)`, same shape as the input, or same shape as the input but
   337|         0|            0|            0|  0.00%|          with one dimension equal to 1, or same shape as the input but with one fewer
   338|         0|            0|            0|  0.00%|          dimension (to allow for broadcasting)
   339|         0|            0|            0|  0.00%|        - Output: scalar if :attr:`reduction` is ``'mean'`` (default) or
   340|         0|            0|            0|  0.00%|          ``'sum'``. If :attr:`reduction` is ``'none'``, then :math:`(N, *)`, same
   341|         0|            0|            0|  0.00%|          shape as the input
   342|         0|            0|            0|  0.00%|
   343|         0|            0|            0|  0.00%|    Examples::
   344|         0|            0|            0|  0.00%|        >>> loss = nn.GaussianNLLLoss()
   345|         0|            0|            0|  0.00%|        >>> input = torch.randn(5, 2, requires_grad=True)
   346|         0|            0|            0|  0.00%|        >>> target = torch.randn(5, 2)
   347|         0|            0|            0|  0.00%|        >>> var = torch.ones(5, 2, requires_grad=True) #heteroscedastic
   348|         0|            0|            0|  0.00%|        >>> output = loss(input, target, var)
   349|         0|            0|            0|  0.00%|        >>> output.backward()
   350|         0|            0|            0|  0.00%|
   351|         0|            0|            0|  0.00%|        >>> loss = nn.GaussianNLLLoss()
   352|         0|            0|            0|  0.00%|        >>> input = torch.randn(5, 2, requires_grad=True)
   353|         0|            0|            0|  0.00%|        >>> target = torch.randn(5, 2)
   354|         0|            0|            0|  0.00%|        >>> var = torch.ones(5, 1, requires_grad=True) #homoscedastic
   355|         0|            0|            0|  0.00%|        >>> output = loss(input, target, var)
   356|         0|            0|            0|  0.00%|        >>> output.backward()
   357|         0|            0|            0|  0.00%|
   358|         0|            0|            0|  0.00%|    Note:
   359|         0|            0|            0|  0.00%|        The clamping of ``var`` is ignored with respect to autograd, and so the
   360|         0|            0|            0|  0.00%|        gradients are unaffected by it.
   361|         0|            0|            0|  0.00%|
   362|         0|            0|            0|  0.00%|    Reference:
   363|         0|            0|            0|  0.00%|        Nix, D. A. and Weigend, A. S., "Estimating the mean and variance of the
   364|         0|            0|            0|  0.00%|        target probability distribution", Proceedings of 1994 IEEE International
   365|         0|            0|            0|  0.00%|        Conference on Neural Networks (ICNN'94), Orlando, FL, USA, 1994, pp. 55-60
   366|         0|            0|            0|  0.00%|        vol.1, doi: 10.1109/ICNN.1994.374138.
   367|         0|            0|            0|  0.00%|    """
   368|         0|            0|            0|  0.00%|    __constants__ = ['full', 'eps', 'reduction']
   369|         0|            0|            0|  0.00%|    full: bool
   370|         0|            0|            0|  0.00%|    eps: float
   371|         0|            0|            0|  0.00%|
   372|         0|            0|            0|  0.00%|    def __init__(self, *, full: bool = False, eps: float = 1e-6, reduction: str = 'mean') -> None:
   373|         0|            0|            0|  0.00%|        super(GaussianNLLLoss, self).__init__(None, None, reduction)
   374|         0|            0|            0|  0.00%|        self.full = full
   375|         0|            0|            0|  0.00%|        self.eps = eps
   376|         0|            0|            0|  0.00%|
   377|         0|            0|            0|  0.00%|    def forward(self, input: Tensor, target: Tensor, var: Tensor) -> Tensor:
   378|         0|            0|            0|  0.00%|        return F.gaussian_nll_loss(input, target, var, full=self.full, eps=self.eps, reduction=self.reduction)
   379|         0|            0|            0|  0.00%|
   380|         0|            0|            0|  0.00%|
   381|         0|            0|            0|  0.00%|class KLDivLoss(_Loss):
   382|         0|            0|            0|  0.00%|    r"""The Kullback-Leibler divergence loss measure
   383|         0|            0|            0|  0.00%|
   384|         0|            0|            0|  0.00%|    `Kullback-Leibler divergence`_ is a useful distance measure for continuous
   385|         0|            0|            0|  0.00%|    distributions and is often useful when performing direct regression over
   386|         0|            0|            0|  0.00%|    the space of (discretely sampled) continuous output distributions.
   387|         0|            0|            0|  0.00%|
   388|         0|            0|            0|  0.00%|    As with :class:`~torch.nn.NLLLoss`, the `input` given is expected to contain
   389|         0|            0|            0|  0.00%|    *log-probabilities* and is not restricted to a 2D Tensor.
   390|         0|            0|            0|  0.00%|    The targets are interpreted as *probabilities* by default, but could be considered
   391|         0|            0|            0|  0.00%|    as *log-probabilities* with :attr:`log_target` set to ``True``.
   392|         0|            0|            0|  0.00%|
   393|         0|            0|            0|  0.00%|    This criterion expects a `target` `Tensor` of the same size as the
   394|         0|            0|            0|  0.00%|    `input` `Tensor`.
   395|         0|            0|            0|  0.00%|
   396|         0|            0|            0|  0.00%|    The unreduced (i.e. with :attr:`reduction` set to ``'none'``) loss can be described as:
   397|         0|            0|            0|  0.00%|
   398|         0|            0|            0|  0.00%|    .. math::
   399|         0|            0|            0|  0.00%|        l(x,y) = L = \{ l_1,\dots,l_N \}, \quad
   400|         0|            0|            0|  0.00%|        l_n = y_n \cdot \left( \log y_n - x_n \right)
   401|         0|            0|            0|  0.00%|
   402|         0|            0|            0|  0.00%|    where the index :math:`N` spans all dimensions of ``input`` and :math:`L` has the same
   403|         0|            0|            0|  0.00%|    shape as ``input``. If :attr:`reduction` is not ``'none'`` (default ``'mean'``), then:
   404|         0|            0|            0|  0.00%|
   405|         0|            0|            0|  0.00%|    .. math::
   406|         0|            0|            0|  0.00%|        \ell(x, y) = \begin{cases}
   407|         0|            0|            0|  0.00%|            \operatorname{mean}(L), & \text{if reduction} = \text{`mean';} \\
   408|         0|            0|            0|  0.00%|            \operatorname{sum}(L),  & \text{if reduction} = \text{`sum'.}
   409|         0|            0|            0|  0.00%|        \end{cases}
   410|         0|            0|            0|  0.00%|
   411|         0|            0|            0|  0.00%|    In default :attr:`reduction` mode ``'mean'``, the losses are averaged for each minibatch over observations
   412|         0|            0|            0|  0.00%|    **as well as** over dimensions. ``'batchmean'`` mode gives the correct KL divergence where losses
   413|         0|            0|            0|  0.00%|    are averaged over batch dimension only. ``'mean'`` mode's behavior will be changed to the same as
   414|         0|            0|            0|  0.00%|    ``'batchmean'`` in the next major release.
   415|         0|            0|            0|  0.00%|
   416|         0|            0|            0|  0.00%|    .. _`kullback-leibler divergence`: https://en.wikipedia.org/wiki/Kullback-Leibler_divergence
   417|         0|            0|            0|  0.00%|
   418|         0|            0|            0|  0.00%|    Args:
   419|         0|            0|            0|  0.00%|        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,
   420|         0|            0|            0|  0.00%|            the losses are averaged over each loss element in the batch. Note that for
   421|         0|            0|            0|  0.00%|            some losses, there are multiple elements per sample. If the field :attr:`size_average`
   422|         0|            0|            0|  0.00%|            is set to ``False``, the losses are instead summed for each minibatch. Ignored
   423|         0|            0|            0|  0.00%|            when :attr:`reduce` is ``False``. Default: ``True``
   424|         0|            0|            0|  0.00%|        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the
   425|         0|            0|            0|  0.00%|            losses are averaged or summed over observations for each minibatch depending
   426|         0|            0|            0|  0.00%|            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per
   427|         0|            0|            0|  0.00%|            batch element instead and ignores :attr:`size_average`. Default: ``True``
   428|         0|            0|            0|  0.00%|        reduction (string, optional): Specifies the reduction to apply to the output:
   429|         0|            0|            0|  0.00%|            ``'none'`` | ``'batchmean'`` | ``'sum'`` | ``'mean'``.
   430|         0|            0|            0|  0.00%|            ``'none'``: no reduction will be applied.
   431|         0|            0|            0|  0.00%|            ``'batchmean'``: the sum of the output will be divided by batchsize.
   432|         0|            0|            0|  0.00%|            ``'sum'``: the output will be summed.
   433|         0|            0|            0|  0.00%|            ``'mean'``: the output will be divided by the number of elements in the output.
   434|         0|            0|            0|  0.00%|            Default: ``'mean'``
   435|         0|            0|            0|  0.00%|        log_target (bool, optional): Specifies whether `target` is passed in the log space.
   436|         0|            0|            0|  0.00%|            Default: ``False``
   437|         0|            0|            0|  0.00%|
   438|         0|            0|            0|  0.00%|    .. note::
   439|         0|            0|            0|  0.00%|        :attr:`size_average` and :attr:`reduce` are in the process of being deprecated,
   440|         0|            0|            0|  0.00%|        and in the meantime, specifying either of those two args will override :attr:`reduction`.
   441|         0|            0|            0|  0.00%|
   442|         0|            0|            0|  0.00%|    .. note::
   443|         0|            0|            0|  0.00%|        :attr:`reduction` = ``'mean'`` doesn't return the true kl divergence value, please use
   444|         0|            0|            0|  0.00%|        :attr:`reduction` = ``'batchmean'`` which aligns with KL math definition.
   445|         0|            0|            0|  0.00%|        In the next major release, ``'mean'`` will be changed to be the same as ``'batchmean'``.
   446|         0|            0|            0|  0.00%|
   447|         0|            0|            0|  0.00%|    Shape:
   448|         0|            0|            0|  0.00%|        - Input: :math:`(N, *)` where :math:`*` means, any number of additional
   449|         0|            0|            0|  0.00%|          dimensions
   450|         0|            0|            0|  0.00%|        - Target: :math:`(N, *)`, same shape as the input
   451|         0|            0|            0|  0.00%|        - Output: scalar by default. If :attr:``reduction`` is ``'none'``, then :math:`(N, *)`,
   452|         0|            0|            0|  0.00%|          the same shape as the input
   453|         0|            0|            0|  0.00%|
   454|         0|            0|            0|  0.00%|    """
   455|         0|            0|            0|  0.00%|    __constants__ = ['reduction']
   456|         0|            0|            0|  0.00%|
   457|         0|            0|            0|  0.00%|    def __init__(self, size_average=None, reduce=None, reduction: str = 'mean', log_target: bool = False) -> None:
   458|         0|            0|            0|  0.00%|        super(KLDivLoss, self).__init__(size_average, reduce, reduction)
   459|         0|            0|            0|  0.00%|        self.log_target = log_target
   460|         0|            0|            0|  0.00%|
   461|         0|            0|            0|  0.00%|    def forward(self, input: Tensor, target: Tensor) -> Tensor:
   462|         0|            0|            0|  0.00%|        return F.kl_div(input, target, reduction=self.reduction, log_target=self.log_target)
   463|         0|            0|            0|  0.00%|
   464|         0|            0|            0|  0.00%|
   465|         0|            0|            0|  0.00%|class MSELoss(_Loss):
   466|         0|            0|            0|  0.00%|    r"""Creates a criterion that measures the mean squared error (squared L2 norm) between
   467|         0|            0|            0|  0.00%|    each element in the input :math:`x` and target :math:`y`.
   468|         0|            0|            0|  0.00%|
   469|         0|            0|            0|  0.00%|    The unreduced (i.e. with :attr:`reduction` set to ``'none'``) loss can be described as:
   470|         0|            0|            0|  0.00%|
   471|         0|            0|            0|  0.00%|    .. math::
   472|         0|            0|            0|  0.00%|        \ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad
   473|         0|            0|            0|  0.00%|        l_n = \left( x_n - y_n \right)^2,
   474|         0|            0|            0|  0.00%|
   475|         0|            0|            0|  0.00%|    where :math:`N` is the batch size. If :attr:`reduction` is not ``'none'``
   476|         0|            0|            0|  0.00%|    (default ``'mean'``), then:
   477|         0|            0|            0|  0.00%|
   478|         0|            0|            0|  0.00%|    .. math::
   479|         0|            0|            0|  0.00%|        \ell(x, y) =
   480|         0|            0|            0|  0.00%|        \begin{cases}
   481|         0|            0|            0|  0.00%|            \operatorname{mean}(L), &  \text{if reduction} = \text{`mean';}\\
   482|         0|            0|            0|  0.00%|            \operatorname{sum}(L),  &  \text{if reduction} = \text{`sum'.}
   483|         0|            0|            0|  0.00%|        \end{cases}
   484|         0|            0|            0|  0.00%|
   485|         0|            0|            0|  0.00%|    :math:`x` and :math:`y` are tensors of arbitrary shapes with a total
   486|         0|            0|            0|  0.00%|    of :math:`n` elements each.
   487|         0|            0|            0|  0.00%|
   488|         0|            0|            0|  0.00%|    The mean operation still operates over all the elements, and divides by :math:`n`.
   489|         0|            0|            0|  0.00%|
   490|         0|            0|            0|  0.00%|    The division by :math:`n` can be avoided if one sets ``reduction = 'sum'``.
   491|         0|            0|            0|  0.00%|
   492|         0|            0|            0|  0.00%|    Args:
   493|         0|            0|            0|  0.00%|        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,
   494|         0|            0|            0|  0.00%|            the losses are averaged over each loss element in the batch. Note that for
   495|         0|            0|            0|  0.00%|            some losses, there are multiple elements per sample. If the field :attr:`size_average`
   496|         0|            0|            0|  0.00%|            is set to ``False``, the losses are instead summed for each minibatch. Ignored
   497|         0|            0|            0|  0.00%|            when :attr:`reduce` is ``False``. Default: ``True``
   498|         0|            0|            0|  0.00%|        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the
   499|         0|            0|            0|  0.00%|            losses are averaged or summed over observations for each minibatch depending
   500|         0|            0|            0|  0.00%|            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per
   501|         0|            0|            0|  0.00%|            batch element instead and ignores :attr:`size_average`. Default: ``True``
   502|         0|            0|            0|  0.00%|        reduction (string, optional): Specifies the reduction to apply to the output:
   503|         0|            0|            0|  0.00%|            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,
   504|         0|            0|            0|  0.00%|            ``'mean'``: the sum of the output will be divided by the number of
   505|         0|            0|            0|  0.00%|            elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`
   506|         0|            0|            0|  0.00%|            and :attr:`reduce` are in the process of being deprecated, and in the meantime,
   507|         0|            0|            0|  0.00%|            specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``
   508|         0|            0|            0|  0.00%|
   509|         0|            0|            0|  0.00%|    Shape:
   510|         0|            0|            0|  0.00%|        - Input: :math:`(N, *)` where :math:`*` means, any number of additional
   511|         0|            0|            0|  0.00%|          dimensions
   512|         0|            0|            0|  0.00%|        - Target: :math:`(N, *)`, same shape as the input
   513|         0|            0|            0|  0.00%|
   514|         0|            0|            0|  0.00%|    Examples::
   515|         0|            0|            0|  0.00%|
   516|         0|            0|            0|  0.00%|        >>> loss = nn.MSELoss()
   517|         0|            0|            0|  0.00%|        >>> input = torch.randn(3, 5, requires_grad=True)
   518|         0|            0|            0|  0.00%|        >>> target = torch.randn(3, 5)
   519|         0|            0|            0|  0.00%|        >>> output = loss(input, target)
   520|         0|            0|            0|  0.00%|        >>> output.backward()
   521|         0|            0|            0|  0.00%|    """
   522|         0|            0|            0|  0.00%|    __constants__ = ['reduction']
   523|         0|            0|            0|  0.00%|
   524|         0|            0|            0|  0.00%|    def __init__(self, size_average=None, reduce=None, reduction: str = 'mean') -> None:
   525|         0|            0|            0|  0.00%|        super(MSELoss, self).__init__(size_average, reduce, reduction)
   526|         0|            0|            0|  0.00%|
   527|         0|            0|            0|  0.00%|    def forward(self, input: Tensor, target: Tensor) -> Tensor:
   528|         0|            0|            0|  0.00%|        return F.mse_loss(input, target, reduction=self.reduction)
   529|         0|            0|            0|  0.00%|
   530|         0|            0|            0|  0.00%|
   531|         0|            0|            0|  0.00%|class BCELoss(_WeightedLoss):
   532|         0|            0|            0|  0.00%|    r"""Creates a criterion that measures the Binary Cross Entropy
   533|         0|            0|            0|  0.00%|    between the target and the output:
   534|         0|            0|            0|  0.00%|
   535|         0|            0|            0|  0.00%|    The unreduced (i.e. with :attr:`reduction` set to ``'none'``) loss can be described as:
   536|         0|            0|            0|  0.00%|
   537|         0|            0|            0|  0.00%|    .. math::
   538|         0|            0|            0|  0.00%|        \ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad
   539|         0|            0|            0|  0.00%|        l_n = - w_n \left[ y_n \cdot \log x_n + (1 - y_n) \cdot \log (1 - x_n) \right],
   540|         0|            0|            0|  0.00%|
   541|         0|            0|            0|  0.00%|    where :math:`N` is the batch size. If :attr:`reduction` is not ``'none'``
   542|         0|            0|            0|  0.00%|    (default ``'mean'``), then
   543|         0|            0|            0|  0.00%|
   544|         0|            0|            0|  0.00%|    .. math::
   545|         0|            0|            0|  0.00%|        \ell(x, y) = \begin{cases}
   546|         0|            0|            0|  0.00%|            \operatorname{mean}(L), & \text{if reduction} = \text{`mean';}\\
   547|         0|            0|            0|  0.00%|            \operatorname{sum}(L),  & \text{if reduction} = \text{`sum'.}
   548|         0|            0|            0|  0.00%|        \end{cases}
   549|         0|            0|            0|  0.00%|
   550|         0|            0|            0|  0.00%|    This is used for measuring the error of a reconstruction in for example
   551|         0|            0|            0|  0.00%|    an auto-encoder. Note that the targets :math:`y` should be numbers
   552|         0|            0|            0|  0.00%|    between 0 and 1.
   553|         0|            0|            0|  0.00%|
   554|         0|            0|            0|  0.00%|    Notice that if :math:`x_n` is either 0 or 1, one of the log terms would be
   555|         0|            0|            0|  0.00%|    mathematically undefined in the above loss equation. PyTorch chooses to set
   556|         0|            0|            0|  0.00%|    :math:`\log (0) = -\infty`, since :math:`\lim_{x\to 0} \log (x) = -\infty`.
   557|         0|            0|            0|  0.00%|    However, an infinite term in the loss equation is not desirable for several reasons.
   558|         0|            0|            0|  0.00%|
   559|         0|            0|            0|  0.00%|    For one, if either :math:`y_n = 0` or :math:`(1 - y_n) = 0`, then we would be
   560|         0|            0|            0|  0.00%|    multiplying 0 with infinity. Secondly, if we have an infinite loss value, then
   561|         0|            0|            0|  0.00%|    we would also have an infinite term in our gradient, since
   562|         0|            0|            0|  0.00%|    :math:`\lim_{x\to 0} \frac{d}{dx} \log (x) = \infty`.
   563|         0|            0|            0|  0.00%|    This would make BCELoss's backward method nonlinear with respect to :math:`x_n`,
   564|         0|            0|            0|  0.00%|    and using it for things like linear regression would not be straight-forward.
   565|         0|            0|            0|  0.00%|
   566|         0|            0|            0|  0.00%|    Our solution is that BCELoss clamps its log function outputs to be greater than
   567|         0|            0|            0|  0.00%|    or equal to -100. This way, we can always have a finite loss value and a linear
   568|         0|            0|            0|  0.00%|    backward method.
   569|         0|            0|            0|  0.00%|
   570|         0|            0|            0|  0.00%|
   571|         0|            0|            0|  0.00%|    Args:
   572|         0|            0|            0|  0.00%|        weight (Tensor, optional): a manual rescaling weight given to the loss
   573|         0|            0|            0|  0.00%|            of each batch element. If given, has to be a Tensor of size `nbatch`.
   574|         0|            0|            0|  0.00%|        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,
   575|         0|            0|            0|  0.00%|            the losses are averaged over each loss element in the batch. Note that for
   576|         0|            0|            0|  0.00%|            some losses, there are multiple elements per sample. If the field :attr:`size_average`
   577|         0|            0|            0|  0.00%|            is set to ``False``, the losses are instead summed for each minibatch. Ignored
   578|         0|            0|            0|  0.00%|            when :attr:`reduce` is ``False``. Default: ``True``
   579|         0|            0|            0|  0.00%|        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the
   580|         0|            0|            0|  0.00%|            losses are averaged or summed over observations for each minibatch depending
   581|         0|            0|            0|  0.00%|            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per
   582|         0|            0|            0|  0.00%|            batch element instead and ignores :attr:`size_average`. Default: ``True``
   583|         0|            0|            0|  0.00%|        reduction (string, optional): Specifies the reduction to apply to the output:
   584|         0|            0|            0|  0.00%|            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,
   585|         0|            0|            0|  0.00%|            ``'mean'``: the sum of the output will be divided by the number of
   586|         0|            0|            0|  0.00%|            elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`
   587|         0|            0|            0|  0.00%|            and :attr:`reduce` are in the process of being deprecated, and in the meantime,
   588|         0|            0|            0|  0.00%|            specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``
   589|         0|            0|            0|  0.00%|
   590|         0|            0|            0|  0.00%|    Shape:
   591|         0|            0|            0|  0.00%|        - Input: :math:`(N, *)` where :math:`*` means, any number of additional
   592|         0|            0|            0|  0.00%|          dimensions
   593|         0|            0|            0|  0.00%|        - Target: :math:`(N, *)`, same shape as the input
   594|         0|            0|            0|  0.00%|        - Output: scalar. If :attr:`reduction` is ``'none'``, then :math:`(N, *)`, same
   595|         0|            0|            0|  0.00%|          shape as input.
   596|         0|            0|            0|  0.00%|
   597|         0|            0|            0|  0.00%|    Examples::
   598|         0|            0|            0|  0.00%|
   599|         0|            0|            0|  0.00%|        >>> m = nn.Sigmoid()
   600|         0|            0|            0|  0.00%|        >>> loss = nn.BCELoss()
   601|         0|            0|            0|  0.00%|        >>> input = torch.randn(3, requires_grad=True)
   602|         0|            0|            0|  0.00%|        >>> target = torch.empty(3).random_(2)
   603|         0|            0|            0|  0.00%|        >>> output = loss(m(input), target)
   604|         0|            0|            0|  0.00%|        >>> output.backward()
   605|         0|            0|            0|  0.00%|    """
   606|         0|            0|            0|  0.00%|    __constants__ = ['reduction']
   607|         0|            0|            0|  0.00%|
   608|         0|            0|            0|  0.00%|    def __init__(self, weight: Optional[Tensor] = None, size_average=None, reduce=None, reduction: str = 'mean') -> None:
   609|         0|            0|            0|  0.00%|        super(BCELoss, self).__init__(weight, size_average, reduce, reduction)
   610|         0|            0|            0|  0.00%|
   611|         0|            0|            0|  0.00%|    def forward(self, input: Tensor, target: Tensor) -> Tensor:
   612|         0|            0|            0|  0.00%|        return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)
   613|         0|            0|            0|  0.00%|
   614|         0|            0|            0|  0.00%|
   615|         0|            0|            0|  0.00%|class BCEWithLogitsLoss(_Loss):
   616|         0|            0|            0|  0.00%|    r"""This loss combines a `Sigmoid` layer and the `BCELoss` in one single
   617|         0|            0|            0|  0.00%|    class. This version is more numerically stable than using a plain `Sigmoid`
   618|         0|            0|            0|  0.00%|    followed by a `BCELoss` as, by combining the operations into one layer,
   619|         0|            0|            0|  0.00%|    we take advantage of the log-sum-exp trick for numerical stability.
   620|         0|            0|            0|  0.00%|
   621|         0|            0|            0|  0.00%|    The unreduced (i.e. with :attr:`reduction` set to ``'none'``) loss can be described as:
   622|         0|            0|            0|  0.00%|
   623|         0|            0|            0|  0.00%|    .. math::
   624|         0|            0|            0|  0.00%|        \ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad
   625|         0|            0|            0|  0.00%|        l_n = - w_n \left[ y_n \cdot \log \sigma(x_n)
   626|         0|            0|            0|  0.00%|        + (1 - y_n) \cdot \log (1 - \sigma(x_n)) \right],
   627|         0|            0|            0|  0.00%|
   628|         0|            0|            0|  0.00%|    where :math:`N` is the batch size. If :attr:`reduction` is not ``'none'``
   629|         0|            0|            0|  0.00%|    (default ``'mean'``), then
   630|         0|            0|            0|  0.00%|
   631|         0|            0|            0|  0.00%|    .. math::
   632|         0|            0|            0|  0.00%|        \ell(x, y) = \begin{cases}
   633|         0|            0|            0|  0.00%|            \operatorname{mean}(L), & \text{if reduction} = \text{`mean';}\\
   634|         0|            0|            0|  0.00%|            \operatorname{sum}(L),  & \text{if reduction} = \text{`sum'.}
   635|         0|            0|            0|  0.00%|        \end{cases}
   636|         0|            0|            0|  0.00%|
   637|         0|            0|            0|  0.00%|    This is used for measuring the error of a reconstruction in for example
   638|         0|            0|            0|  0.00%|    an auto-encoder. Note that the targets `t[i]` should be numbers
   639|         0|            0|            0|  0.00%|    between 0 and 1.
   640|         0|            0|            0|  0.00%|
   641|         0|            0|            0|  0.00%|    It's possible to trade off recall and precision by adding weights to positive examples.
   642|         0|            0|            0|  0.00%|    In the case of multi-label classification the loss can be described as:
   643|         0|            0|            0|  0.00%|
   644|         0|            0|            0|  0.00%|    .. math::
   645|         0|            0|            0|  0.00%|        \ell_c(x, y) = L_c = \{l_{1,c},\dots,l_{N,c}\}^\top, \quad
   646|         0|            0|            0|  0.00%|        l_{n,c} = - w_{n,c} \left[ p_c y_{n,c} \cdot \log \sigma(x_{n,c})
   647|         0|            0|            0|  0.00%|        + (1 - y_{n,c}) \cdot \log (1 - \sigma(x_{n,c})) \right],
   648|         0|            0|            0|  0.00%|
   649|         0|            0|            0|  0.00%|    where :math:`c` is the class number (:math:`c > 1` for multi-label binary classification,
   650|         0|            0|            0|  0.00%|    :math:`c = 1` for single-label binary classification),
   651|         0|            0|            0|  0.00%|    :math:`n` is the number of the sample in the batch and
   652|         0|            0|            0|  0.00%|    :math:`p_c` is the weight of the positive answer for the class :math:`c`.
   653|         0|            0|            0|  0.00%|
   654|         0|            0|            0|  0.00%|    :math:`p_c > 1` increases the recall, :math:`p_c < 1` increases the precision.
   655|         0|            0|            0|  0.00%|
   656|         0|            0|            0|  0.00%|    For example, if a dataset contains 100 positive and 300 negative examples of a single class,
   657|         0|            0|            0|  0.00%|    then `pos_weight` for the class should be equal to :math:`\frac{300}{100}=3`.
   658|         0|            0|            0|  0.00%|    The loss would act as if the dataset contains :math:`3\times 100=300` positive examples.
   659|         0|            0|            0|  0.00%|
   660|         0|            0|            0|  0.00%|    Examples::
   661|         0|            0|            0|  0.00%|
   662|         0|            0|            0|  0.00%|        >>> target = torch.ones([10, 64], dtype=torch.float32)  # 64 classes, batch size = 10
   663|         0|            0|            0|  0.00%|        >>> output = torch.full([10, 64], 1.5)  # A prediction (logit)
   664|         0|            0|            0|  0.00%|        >>> pos_weight = torch.ones([64])  # All weights are equal to 1
   665|         0|            0|            0|  0.00%|        >>> criterion = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)
   666|         0|            0|            0|  0.00%|        >>> criterion(output, target)  # -log(sigmoid(1.5))
   667|         0|            0|            0|  0.00%|        tensor(0.2014)
   668|         0|            0|            0|  0.00%|
   669|         0|            0|            0|  0.00%|    Args:
   670|         0|            0|            0|  0.00%|        weight (Tensor, optional): a manual rescaling weight given to the loss
   671|         0|            0|            0|  0.00%|            of each batch element. If given, has to be a Tensor of size `nbatch`.
   672|         0|            0|            0|  0.00%|        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,
   673|         0|            0|            0|  0.00%|            the losses are averaged over each loss element in the batch. Note that for
   674|         0|            0|            0|  0.00%|            some losses, there are multiple elements per sample. If the field :attr:`size_average`
   675|         0|            0|            0|  0.00%|            is set to ``False``, the losses are instead summed for each minibatch. Ignored
   676|         0|            0|            0|  0.00%|            when :attr:`reduce` is ``False``. Default: ``True``
   677|         0|            0|            0|  0.00%|        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the
   678|         0|            0|            0|  0.00%|            losses are averaged or summed over observations for each minibatch depending
   679|         0|            0|            0|  0.00%|            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per
   680|         0|            0|            0|  0.00%|            batch element instead and ignores :attr:`size_average`. Default: ``True``
   681|         0|            0|            0|  0.00%|        reduction (string, optional): Specifies the reduction to apply to the output:
   682|         0|            0|            0|  0.00%|            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,
   683|         0|            0|            0|  0.00%|            ``'mean'``: the sum of the output will be divided by the number of
   684|         0|            0|            0|  0.00%|            elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`
   685|         0|            0|            0|  0.00%|            and :attr:`reduce` are in the process of being deprecated, and in the meantime,
   686|         0|            0|            0|  0.00%|            specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``
   687|         0|            0|            0|  0.00%|        pos_weight (Tensor, optional): a weight of positive examples.
   688|         0|            0|            0|  0.00%|                Must be a vector with length equal to the number of classes.
   689|         0|            0|            0|  0.00%|
   690|         0|            0|            0|  0.00%|    Shape:
   691|         0|            0|            0|  0.00%|        - Input: :math:`(N, *)` where :math:`*` means, any number of additional dimensions
   692|         0|            0|            0|  0.00%|        - Target: :math:`(N, *)`, same shape as the input
   693|         0|            0|            0|  0.00%|        - Output: scalar. If :attr:`reduction` is ``'none'``, then :math:`(N, *)`, same
   694|         0|            0|            0|  0.00%|          shape as input.
   695|         0|            0|            0|  0.00%|
   696|         0|            0|            0|  0.00%|     Examples::
   697|         0|            0|            0|  0.00%|
   698|         0|            0|            0|  0.00%|        >>> loss = nn.BCEWithLogitsLoss()
   699|         0|            0|            0|  0.00%|        >>> input = torch.randn(3, requires_grad=True)
   700|         0|            0|            0|  0.00%|        >>> target = torch.empty(3).random_(2)
   701|         0|            0|            0|  0.00%|        >>> output = loss(input, target)
   702|         0|            0|            0|  0.00%|        >>> output.backward()
   703|         0|            0|            0|  0.00%|    """
   704|         0|            0|            0|  0.00%|    def __init__(self, weight: Optional[Tensor] = None, size_average=None, reduce=None, reduction: str = 'mean',
   705|         0|            0|            0|  0.00%|                 pos_weight: Optional[Tensor] = None) -> None:
   706|         0|            0|            0|  0.00%|        super(BCEWithLogitsLoss, self).__init__(size_average, reduce, reduction)
   707|         0|            0|            0|  0.00%|        self.register_buffer('weight', weight)
   708|         0|            0|            0|  0.00%|        self.register_buffer('pos_weight', pos_weight)
   709|         0|            0|            0|  0.00%|        self.weight: Optional[Tensor]
   710|         0|            0|            0|  0.00%|        self.pos_weight: Optional[Tensor]
   711|         0|            0|            0|  0.00%|
   712|         0|            0|            0|  0.00%|    def forward(self, input: Tensor, target: Tensor) -> Tensor:
   713|         0|            0|            0|  0.00%|        return F.binary_cross_entropy_with_logits(input, target,
   714|         0|            0|            0|  0.00%|                                                  self.weight,
   715|         0|            0|            0|  0.00%|                                                  pos_weight=self.pos_weight,
   716|         0|            0|            0|  0.00%|                                                  reduction=self.reduction)
   717|         0|            0|            0|  0.00%|
   718|         0|            0|            0|  0.00%|
   719|         0|            0|            0|  0.00%|class HingeEmbeddingLoss(_Loss):
   720|         0|            0|            0|  0.00%|    r"""Measures the loss given an input tensor :math:`x` and a labels tensor :math:`y`
   721|         0|            0|            0|  0.00%|    (containing 1 or -1).
   722|         0|            0|            0|  0.00%|    This is usually used for measuring whether two inputs are similar or
   723|         0|            0|            0|  0.00%|    dissimilar, e.g. using the L1 pairwise distance as :math:`x`, and is typically
   724|         0|            0|            0|  0.00%|    used for learning nonlinear embeddings or semi-supervised learning.
   725|         0|            0|            0|  0.00%|
   726|         0|            0|            0|  0.00%|    The loss function for :math:`n`-th sample in the mini-batch is
   727|         0|            0|            0|  0.00%|
   728|         0|            0|            0|  0.00%|    .. math::
   729|         0|            0|            0|  0.00%|        l_n = \begin{cases}
   730|         0|            0|            0|  0.00%|            x_n, & \text{if}\; y_n = 1,\\
   731|         0|            0|            0|  0.00%|            \max \{0, \Delta - x_n\}, & \text{if}\; y_n = -1,
   732|         0|            0|            0|  0.00%|        \end{cases}
   733|         0|            0|            0|  0.00%|
   734|         0|            0|            0|  0.00%|    and the total loss functions is
   735|         0|            0|            0|  0.00%|
   736|         0|            0|            0|  0.00%|    .. math::
   737|         0|            0|            0|  0.00%|        \ell(x, y) = \begin{cases}
   738|         0|            0|            0|  0.00%|            \operatorname{mean}(L), & \text{if reduction} = \text{`mean';}\\
   739|         0|            0|            0|  0.00%|            \operatorname{sum}(L),  & \text{if reduction} = \text{`sum'.}
   740|         0|            0|            0|  0.00%|        \end{cases}
   741|         0|            0|            0|  0.00%|
   742|         0|            0|            0|  0.00%|    where :math:`L = \{l_1,\dots,l_N\}^\top`.
   743|         0|            0|            0|  0.00%|
   744|         0|            0|            0|  0.00%|    Args:
   745|         0|            0|            0|  0.00%|        margin (float, optional): Has a default value of `1`.
   746|         0|            0|            0|  0.00%|        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,
   747|         0|            0|            0|  0.00%|            the losses are averaged over each loss element in the batch. Note that for
   748|         0|            0|            0|  0.00%|            some losses, there are multiple elements per sample. If the field :attr:`size_average`
   749|         0|            0|            0|  0.00%|            is set to ``False``, the losses are instead summed for each minibatch. Ignored
   750|         0|            0|            0|  0.00%|            when :attr:`reduce` is ``False``. Default: ``True``
   751|         0|            0|            0|  0.00%|        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the
   752|         0|            0|            0|  0.00%|            losses are averaged or summed over observations for each minibatch depending
   753|         0|            0|            0|  0.00%|            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per
   754|         0|            0|            0|  0.00%|            batch element instead and ignores :attr:`size_average`. Default: ``True``
   755|         0|            0|            0|  0.00%|        reduction (string, optional): Specifies the reduction to apply to the output:
   756|         0|            0|            0|  0.00%|            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,
   757|         0|            0|            0|  0.00%|            ``'mean'``: the sum of the output will be divided by the number of
   758|         0|            0|            0|  0.00%|            elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`
   759|         0|            0|            0|  0.00%|            and :attr:`reduce` are in the process of being deprecated, and in the meantime,
   760|         0|            0|            0|  0.00%|            specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``
   761|         0|            0|            0|  0.00%|
   762|         0|            0|            0|  0.00%|    Shape:
   763|         0|            0|            0|  0.00%|        - Input: :math:`(*)` where :math:`*` means, any number of dimensions. The sum operation
   764|         0|            0|            0|  0.00%|          operates over all the elements.
   765|         0|            0|            0|  0.00%|        - Target: :math:`(*)`, same shape as the input
   766|         0|            0|            0|  0.00%|        - Output: scalar. If :attr:`reduction` is ``'none'``, then same shape as the input
   767|         0|            0|            0|  0.00%|    """
   768|         0|            0|            0|  0.00%|    __constants__ = ['margin', 'reduction']
   769|         0|            0|            0|  0.00%|    margin: float
   770|         0|            0|            0|  0.00%|
   771|         0|            0|            0|  0.00%|    def __init__(self, margin: float = 1.0, size_average=None, reduce=None, reduction: str = 'mean') -> None:
   772|         0|            0|            0|  0.00%|        super(HingeEmbeddingLoss, self).__init__(size_average, reduce, reduction)
   773|         0|            0|            0|  0.00%|        self.margin = margin
   774|         0|            0|            0|  0.00%|
   775|         0|            0|            0|  0.00%|    def forward(self, input: Tensor, target: Tensor) -> Tensor:
   776|         0|            0|            0|  0.00%|        return F.hinge_embedding_loss(input, target, margin=self.margin, reduction=self.reduction)
   777|         0|            0|            0|  0.00%|
   778|         0|            0|            0|  0.00%|
   779|         0|            0|            0|  0.00%|class MultiLabelMarginLoss(_Loss):
   780|         0|            0|            0|  0.00%|    r"""Creates a criterion that optimizes a multi-class multi-classification
   781|         0|            0|            0|  0.00%|    hinge loss (margin-based loss) between input :math:`x` (a 2D mini-batch `Tensor`)
   782|         0|            0|            0|  0.00%|    and output :math:`y` (which is a 2D `Tensor` of target class indices).
   783|         0|            0|            0|  0.00%|    For each sample in the mini-batch:
   784|         0|            0|            0|  0.00%|
   785|         0|            0|            0|  0.00%|    .. math::
   786|         0|            0|            0|  0.00%|        \text{loss}(x, y) = \sum_{ij}\frac{\max(0, 1 - (x[y[j]] - x[i]))}{\text{x.size}(0)}
   787|         0|            0|            0|  0.00%|
   788|         0|            0|            0|  0.00%|    where :math:`x \in \left\{0, \; \cdots , \; \text{x.size}(0) - 1\right\}`, \
   789|         0|            0|            0|  0.00%|    :math:`y \in \left\{0, \; \cdots , \; \text{y.size}(0) - 1\right\}`, \
   790|         0|            0|            0|  0.00%|    :math:`0 \leq y[j] \leq \text{x.size}(0)-1`, \
   791|         0|            0|            0|  0.00%|    and :math:`i \neq y[j]` for all :math:`i` and :math:`j`.
   792|         0|            0|            0|  0.00%|
   793|         0|            0|            0|  0.00%|    :math:`y` and :math:`x` must have the same size.
   794|         0|            0|            0|  0.00%|
   795|         0|            0|            0|  0.00%|    The criterion only considers a contiguous block of non-negative targets that
   796|         0|            0|            0|  0.00%|    starts at the front.
   797|         0|            0|            0|  0.00%|
   798|         0|            0|            0|  0.00%|    This allows for different samples to have variable amounts of target classes.
   799|         0|            0|            0|  0.00%|
   800|         0|            0|            0|  0.00%|    Args:
   801|         0|            0|            0|  0.00%|        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,
   802|         0|            0|            0|  0.00%|            the losses are averaged over each loss element in the batch. Note that for
   803|         0|            0|            0|  0.00%|            some losses, there are multiple elements per sample. If the field :attr:`size_average`
   804|         0|            0|            0|  0.00%|            is set to ``False``, the losses are instead summed for each minibatch. Ignored
   805|         0|            0|            0|  0.00%|            when :attr:`reduce` is ``False``. Default: ``True``
   806|         0|            0|            0|  0.00%|        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the
   807|         0|            0|            0|  0.00%|            losses are averaged or summed over observations for each minibatch depending
   808|         0|            0|            0|  0.00%|            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per
   809|         0|            0|            0|  0.00%|            batch element instead and ignores :attr:`size_average`. Default: ``True``
   810|         0|            0|            0|  0.00%|        reduction (string, optional): Specifies the reduction to apply to the output:
   811|         0|            0|            0|  0.00%|            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,
   812|         0|            0|            0|  0.00%|            ``'mean'``: the sum of the output will be divided by the number of
   813|         0|            0|            0|  0.00%|            elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`
   814|         0|            0|            0|  0.00%|            and :attr:`reduce` are in the process of being deprecated, and in the meantime,
   815|         0|            0|            0|  0.00%|            specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``
   816|         0|            0|            0|  0.00%|
   817|         0|            0|            0|  0.00%|    Shape:
   818|         0|            0|            0|  0.00%|        - Input: :math:`(C)` or :math:`(N, C)` where `N` is the batch size and `C`
   819|         0|            0|            0|  0.00%|          is the number of classes.
   820|         0|            0|            0|  0.00%|        - Target: :math:`(C)` or :math:`(N, C)`, label targets padded by -1 ensuring same shape as the input.
   821|         0|            0|            0|  0.00%|        - Output: scalar. If :attr:`reduction` is ``'none'``, then :math:`(N)`.
   822|         0|            0|            0|  0.00%|
   823|         0|            0|            0|  0.00%|    Examples::
   824|         0|            0|            0|  0.00%|
   825|         0|            0|            0|  0.00%|        >>> loss = nn.MultiLabelMarginLoss()
   826|         0|            0|            0|  0.00%|        >>> x = torch.FloatTensor([[0.1, 0.2, 0.4, 0.8]])
   827|         0|            0|            0|  0.00%|        >>> # for target y, only consider labels 3 and 0, not after label -1
   828|         0|            0|            0|  0.00%|        >>> y = torch.LongTensor([[3, 0, -1, 1]])
   829|         0|            0|            0|  0.00%|        >>> loss(x, y)
   830|         0|            0|            0|  0.00%|        >>> # 0.25 * ((1-(0.1-0.2)) + (1-(0.1-0.4)) + (1-(0.8-0.2)) + (1-(0.8-0.4)))
   831|         0|            0|            0|  0.00%|        tensor(0.8500)
   832|         0|            0|            0|  0.00%|
   833|         0|            0|            0|  0.00%|    """
   834|         0|            0|            0|  0.00%|    __constants__ = ['reduction']
   835|         0|            0|            0|  0.00%|
   836|         0|            0|            0|  0.00%|    def __init__(self, size_average=None, reduce=None, reduction: str = 'mean') -> None:
   837|         0|            0|            0|  0.00%|        super(MultiLabelMarginLoss, self).__init__(size_average, reduce, reduction)
   838|         0|            0|            0|  0.00%|
   839|         0|            0|            0|  0.00%|    def forward(self, input: Tensor, target: Tensor) -> Tensor:
   840|         0|            0|            0|  0.00%|        return F.multilabel_margin_loss(input, target, reduction=self.reduction)
   841|         0|            0|            0|  0.00%|
   842|         0|            0|            0|  0.00%|
   843|         0|            0|            0|  0.00%|class SmoothL1Loss(_Loss):
   844|         0|            0|            0|  0.00%|    r"""Creates a criterion that uses a squared term if the absolute
   845|         0|            0|            0|  0.00%|    element-wise error falls below beta and an L1 term otherwise.
   846|         0|            0|            0|  0.00%|    It is less sensitive to outliers than :class:`torch.nn.MSELoss` and in some cases
   847|         0|            0|            0|  0.00%|    prevents exploding gradients (e.g. see the paper `Fast R-CNN`_ by Ross Girshick).
   848|         0|            0|            0|  0.00%|
   849|         0|            0|            0|  0.00%|    For a batch of size :math:`N`, the unreduced loss can be described as:
   850|         0|            0|            0|  0.00%|
   851|         0|            0|            0|  0.00%|    .. math::
   852|         0|            0|            0|  0.00%|        \ell(x, y) = L = \{l_1, ..., l_N\}^T
   853|         0|            0|            0|  0.00%|
   854|         0|            0|            0|  0.00%|    with
   855|         0|            0|            0|  0.00%|
   856|         0|            0|            0|  0.00%|    .. math::
   857|         0|            0|            0|  0.00%|        l_n = \begin{cases}
   858|         0|            0|            0|  0.00%|        0.5 (x_n - y_n)^2 / beta, & \text{if } |x_n - y_n| < beta \\
   859|         0|            0|            0|  0.00%|        |x_n - y_n| - 0.5 * beta, & \text{otherwise }
   860|         0|            0|            0|  0.00%|        \end{cases}
   861|         0|            0|            0|  0.00%|
   862|         0|            0|            0|  0.00%|    If `reduction` is not `none`, then:
   863|         0|            0|            0|  0.00%|
   864|         0|            0|            0|  0.00%|    .. math::
   865|         0|            0|            0|  0.00%|        \ell(x, y) =
   866|         0|            0|            0|  0.00%|        \begin{cases}
   867|         0|            0|            0|  0.00%|            \operatorname{mean}(L), &  \text{if reduction} = \text{`mean';}\\
   868|         0|            0|            0|  0.00%|            \operatorname{sum}(L),  &  \text{if reduction} = \text{`sum'.}
   869|         0|            0|            0|  0.00%|        \end{cases}
   870|         0|            0|            0|  0.00%|
   871|         0|            0|            0|  0.00%|    .. note::
   872|         0|            0|            0|  0.00%|        Smooth L1 loss can be seen as exactly :class:`L1Loss`, but with the :math:`|x - y| < beta`
   873|         0|            0|            0|  0.00%|        portion replaced with a quadratic function such that its slope is 1 at :math:`|x - y| = beta`.
   874|         0|            0|            0|  0.00%|        The quadratic segment smooths the L1 loss near :math:`|x - y| = 0`.
   875|         0|            0|            0|  0.00%|
   876|         0|            0|            0|  0.00%|    .. note::
   877|         0|            0|            0|  0.00%|        Smooth L1 loss is closely related to :class:`HuberLoss`, being
   878|         0|            0|            0|  0.00%|        equivalent to :math:`huber(x, y) / beta` (note that Smooth L1's beta hyper-parameter is
   879|         0|            0|            0|  0.00%|        also known as delta for Huber). This leads to the following differences:
   880|         0|            0|            0|  0.00%|
   881|         0|            0|            0|  0.00%|        * As beta -> 0, Smooth L1 loss converges to :class:`L1Loss`, while :class:`HuberLoss`
   882|         0|            0|            0|  0.00%|          converges to a constant 0 loss.
   883|         0|            0|            0|  0.00%|        * As beta -> :math:`+\infty`, Smooth L1 loss converges to a constant 0 loss, while
   884|         0|            0|            0|  0.00%|          :class:`HuberLoss` converges to :class:`MSELoss`.
   885|         0|            0|            0|  0.00%|        * For Smooth L1 loss, as beta varies, the L1 segment of the loss has a constant slope of 1.
   886|         0|            0|            0|  0.00%|          For :class:`HuberLoss`, the slope of the L1 segment is beta.
   887|         0|            0|            0|  0.00%|
   888|         0|            0|            0|  0.00%|    .. _`Fast R-CNN`: https://arxiv.org/abs/1504.08083
   889|         0|            0|            0|  0.00%|
   890|         0|            0|            0|  0.00%|    Args:
   891|         0|            0|            0|  0.00%|        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,
   892|         0|            0|            0|  0.00%|            the losses are averaged over each loss element in the batch. Note that for
   893|         0|            0|            0|  0.00%|            some losses, there are multiple elements per sample. If the field :attr:`size_average`
   894|         0|            0|            0|  0.00%|            is set to ``False``, the losses are instead summed for each minibatch. Ignored
   895|         0|            0|            0|  0.00%|            when :attr:`reduce` is ``False``. Default: ``True``
   896|         0|            0|            0|  0.00%|        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the
   897|         0|            0|            0|  0.00%|            losses are averaged or summed over observations for each minibatch depending
   898|         0|            0|            0|  0.00%|            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per
   899|         0|            0|            0|  0.00%|            batch element instead and ignores :attr:`size_average`. Default: ``True``
   900|         0|            0|            0|  0.00%|        reduction (string, optional): Specifies the reduction to apply to the output:
   901|         0|            0|            0|  0.00%|            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,
   902|         0|            0|            0|  0.00%|            ``'mean'``: the sum of the output will be divided by the number of
   903|         0|            0|            0|  0.00%|            elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`
   904|         0|            0|            0|  0.00%|            and :attr:`reduce` are in the process of being deprecated, and in the meantime,
   905|         0|            0|            0|  0.00%|            specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``
   906|         0|            0|            0|  0.00%|        beta (float, optional): Specifies the threshold at which to change between L1 and L2 loss.
   907|         0|            0|            0|  0.00%|            The value must be non-negative. Default: 1.0
   908|         0|            0|            0|  0.00%|
   909|         0|            0|            0|  0.00%|    Shape:
   910|         0|            0|            0|  0.00%|        - Input: :math:`(N, *)` where :math:`*` means any number of additional dimensions
   911|         0|            0|            0|  0.00%|        - Target: :math:`(N, *)`; same shape as the input
   912|         0|            0|            0|  0.00%|        - Output: scalar. If :attr:`reduction` is ``'none'``, then :math:`(N, *)`; same shape as the input
   913|         0|            0|            0|  0.00%|    """
   914|         0|            0|            0|  0.00%|    __constants__ = ['reduction']
   915|         0|            0|            0|  0.00%|
   916|         0|            0|            0|  0.00%|    def __init__(self, size_average=None, reduce=None, reduction: str = 'mean', beta: float = 1.0) -> None:
   917|         0|            0|            0|  0.00%|        super(SmoothL1Loss, self).__init__(size_average, reduce, reduction)
   918|         0|            0|            0|  0.00%|        self.beta = beta
   919|         0|            0|            0|  0.00%|
   920|         0|            0|            0|  0.00%|    def forward(self, input: Tensor, target: Tensor) -> Tensor:
   921|         0|            0|            0|  0.00%|        return F.smooth_l1_loss(input, target, reduction=self.reduction, beta=self.beta)
   922|         0|            0|            0|  0.00%|
   923|         0|            0|            0|  0.00%|
   924|         0|            0|            0|  0.00%|class HuberLoss(_Loss):
   925|         0|            0|            0|  0.00%|    r"""Creates a criterion that uses a squared term if the absolute
   926|         0|            0|            0|  0.00%|    element-wise error falls below delta and a delta-scaled L1 term otherwise.
   927|         0|            0|            0|  0.00%|    This loss combines advantages of both :class:`L1Loss` and :class:`MSELoss`; the
   928|         0|            0|            0|  0.00%|    delta-scaled L1 region makes the loss less sensitive to outliers than :class:`MSELoss`,
   929|         0|            0|            0|  0.00%|    while the L2 region provides smoothness over :class:`L1Loss` near 0. See
   930|         0|            0|            0|  0.00%|    `Huber loss <https://en.wikipedia.org/wiki/Huber_loss>`_ for more information.
   931|         0|            0|            0|  0.00%|
   932|         0|            0|            0|  0.00%|    For a batch of size :math:`N`, the unreduced loss can be described as:
   933|         0|            0|            0|  0.00%|
   934|         0|            0|            0|  0.00%|    .. math::
   935|         0|            0|            0|  0.00%|        \ell(x, y) = L = \{l_1, ..., l_N\}^T
   936|         0|            0|            0|  0.00%|
   937|         0|            0|            0|  0.00%|    with
   938|         0|            0|            0|  0.00%|
   939|         0|            0|            0|  0.00%|    .. math::
   940|         0|            0|            0|  0.00%|        l_n = \begin{cases}
   941|         0|            0|            0|  0.00%|        0.5 (x_n - y_n)^2, & \text{if } |x_n - y_n| < delta \\
   942|         0|            0|            0|  0.00%|        delta * (|x_n - y_n| - 0.5 * delta), & \text{otherwise }
   943|         0|            0|            0|  0.00%|        \end{cases}
   944|         0|            0|            0|  0.00%|
   945|         0|            0|            0|  0.00%|    If `reduction` is not `none`, then:
   946|         0|            0|            0|  0.00%|
   947|         0|            0|            0|  0.00%|    .. math::
   948|         0|            0|            0|  0.00%|        \ell(x, y) =
   949|         0|            0|            0|  0.00%|        \begin{cases}
   950|         0|            0|            0|  0.00%|            \operatorname{mean}(L), &  \text{if reduction} = \text{`mean';}\\
   951|         0|            0|            0|  0.00%|            \operatorname{sum}(L),  &  \text{if reduction} = \text{`sum'.}
   952|         0|            0|            0|  0.00%|        \end{cases}
   953|         0|            0|            0|  0.00%|
   954|         0|            0|            0|  0.00%|    .. note::
   955|         0|            0|            0|  0.00%|        When delta is set to 1, this loss is equivalent to :class:`SmoothL1Loss`.
   956|         0|            0|            0|  0.00%|        In general, this loss differs from :class:`SmoothL1Loss` by a factor of delta (AKA beta
   957|         0|            0|            0|  0.00%|        in Smooth L1).
   958|         0|            0|            0|  0.00%|        See :class:`SmoothL1Loss` for additional discussion on the differences in behavior
   959|         0|            0|            0|  0.00%|        between the two losses.
   960|         0|            0|            0|  0.00%|
   961|         0|            0|            0|  0.00%|    Args:
   962|         0|            0|            0|  0.00%|        reduction (string, optional): Specifies the reduction to apply to the output:
   963|         0|            0|            0|  0.00%|            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,
   964|         0|            0|            0|  0.00%|            ``'mean'``: the sum of the output will be divided by the number of
   965|         0|            0|            0|  0.00%|            elements in the output, ``'sum'``: the output will be summed. Default: ``'mean'``
   966|         0|            0|            0|  0.00%|        delta (float, optional): Specifies the threshold at which to change between delta-scaled L1 and L2 loss.
   967|         0|            0|            0|  0.00%|            The value must be positive.  Default: 1.0
   968|         0|            0|            0|  0.00%|
   969|         0|            0|            0|  0.00%|    Shape:
   970|         0|            0|            0|  0.00%|        - Input: :math:`(N, *)` where :math:`*` means any number of additional dimensions
   971|         0|            0|            0|  0.00%|        - Target: :math:`(N, *)`; same shape as the input
   972|         0|            0|            0|  0.00%|        - Output: scalar. If :attr:`reduction` is ``'none'``, then :math:`(N, *)`; same shape as the input
   973|         0|            0|            0|  0.00%|    """
   974|         0|            0|            0|  0.00%|    __constants__ = ['reduction', 'delta']
   975|         0|            0|            0|  0.00%|
   976|         0|            0|            0|  0.00%|    def __init__(self, reduction: str = 'mean', delta: float = 1.0) -> None:
   977|         0|            0|            0|  0.00%|        super().__init__(reduction=reduction)
   978|         0|            0|            0|  0.00%|        self.delta = delta
   979|         0|            0|            0|  0.00%|
   980|         0|            0|            0|  0.00%|    def forward(self, input: Tensor, target: Tensor) -> Tensor:
   981|         0|            0|            0|  0.00%|        return F.huber_loss(input, target, reduction=self.reduction, delta=self.delta)
   982|         0|            0|            0|  0.00%|
   983|         0|            0|            0|  0.00%|
   984|         0|            0|            0|  0.00%|class SoftMarginLoss(_Loss):
   985|         0|            0|            0|  0.00%|    r"""Creates a criterion that optimizes a two-class classification
   986|         0|            0|            0|  0.00%|    logistic loss between input tensor :math:`x` and target tensor :math:`y`
   987|         0|            0|            0|  0.00%|    (containing 1 or -1).
   988|         0|            0|            0|  0.00%|
   989|         0|            0|            0|  0.00%|    .. math::
   990|         0|            0|            0|  0.00%|        \text{loss}(x, y) = \sum_i \frac{\log(1 + \exp(-y[i]*x[i]))}{\text{x.nelement}()}
   991|         0|            0|            0|  0.00%|
   992|         0|            0|            0|  0.00%|    Args:
   993|         0|            0|            0|  0.00%|        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,
   994|         0|            0|            0|  0.00%|            the losses are averaged over each loss element in the batch. Note that for
   995|         0|            0|            0|  0.00%|            some losses, there are multiple elements per sample. If the field :attr:`size_average`
   996|         0|            0|            0|  0.00%|            is set to ``False``, the losses are instead summed for each minibatch. Ignored
   997|         0|            0|            0|  0.00%|            when :attr:`reduce` is ``False``. Default: ``True``
   998|         0|            0|            0|  0.00%|        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the
   999|         0|            0|            0|  0.00%|            losses are averaged or summed over observations for each minibatch depending
  1000|         0|            0|            0|  0.00%|            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per
  1001|         0|            0|            0|  0.00%|            batch element instead and ignores :attr:`size_average`. Default: ``True``
  1002|         0|            0|            0|  0.00%|        reduction (string, optional): Specifies the reduction to apply to the output:
  1003|         0|            0|            0|  0.00%|            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,
  1004|         0|            0|            0|  0.00%|            ``'mean'``: the sum of the output will be divided by the number of
  1005|         0|            0|            0|  0.00%|            elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`
  1006|         0|            0|            0|  0.00%|            and :attr:`reduce` are in the process of being deprecated, and in the meantime,
  1007|         0|            0|            0|  0.00%|            specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``
  1008|         0|            0|            0|  0.00%|
  1009|         0|            0|            0|  0.00%|    Shape:
  1010|         0|            0|            0|  0.00%|        - Input: :math:`(*)` where :math:`*` means, any number of additional
  1011|         0|            0|            0|  0.00%|          dimensions
  1012|         0|            0|            0|  0.00%|        - Target: :math:`(*)`, same shape as the input
  1013|         0|            0|            0|  0.00%|        - Output: scalar. If :attr:`reduction` is ``'none'``, then same shape as the input
  1014|         0|            0|            0|  0.00%|
  1015|         0|            0|            0|  0.00%|    """
  1016|         0|            0|            0|  0.00%|    __constants__ = ['reduction']
  1017|         0|            0|            0|  0.00%|
  1018|         0|            0|            0|  0.00%|    def __init__(self, size_average=None, reduce=None, reduction: str = 'mean') -> None:
  1019|         0|            0|            0|  0.00%|        super(SoftMarginLoss, self).__init__(size_average, reduce, reduction)
  1020|         0|            0|            0|  0.00%|
  1021|         0|            0|            0|  0.00%|    def forward(self, input: Tensor, target: Tensor) -> Tensor:
  1022|         0|            0|            0|  0.00%|        return F.soft_margin_loss(input, target, reduction=self.reduction)
  1023|         0|            0|            0|  0.00%|
  1024|         0|            0|            0|  0.00%|
  1025|         0|            0|            0|  0.00%|class CrossEntropyLoss(_WeightedLoss):
  1026|         0|            0|            0|  0.00%|    r"""This criterion combines :class:`~torch.nn.LogSoftmax` and :class:`~torch.nn.NLLLoss` in one single class.
  1027|         0|            0|            0|  0.00%|
  1028|         0|            0|            0|  0.00%|    It is useful when training a classification problem with `C` classes.
  1029|         0|            0|            0|  0.00%|    If provided, the optional argument :attr:`weight` should be a 1D `Tensor`
  1030|         0|            0|            0|  0.00%|    assigning weight to each of the classes.
  1031|         0|            0|            0|  0.00%|    This is particularly useful when you have an unbalanced training set.
  1032|         0|            0|            0|  0.00%|
  1033|         0|            0|            0|  0.00%|    The `input` is expected to contain raw, unnormalized scores for each class.
  1034|         0|            0|            0|  0.00%|
  1035|         0|            0|            0|  0.00%|    `input` has to be a Tensor of size either :math:`(minibatch, C)` or
  1036|         0|            0|            0|  0.00%|    :math:`(minibatch, C, d_1, d_2, ..., d_K)`
  1037|         0|            0|            0|  0.00%|    with :math:`K \geq 1` for the `K`-dimensional case (described later).
  1038|         0|            0|            0|  0.00%|
  1039|         0|            0|            0|  0.00%|    This criterion expects a class index in the range :math:`[0, C-1]` as the
  1040|         0|            0|            0|  0.00%|    `target` for each value of a 1D tensor of size `minibatch`; if `ignore_index`
  1041|         0|            0|            0|  0.00%|    is specified, this criterion also accepts this class index (this index may not
  1042|         0|            0|            0|  0.00%|    necessarily be in the class range).
  1043|         0|            0|            0|  0.00%|
  1044|         0|            0|            0|  0.00%|    The loss can be described as:
  1045|         0|            0|            0|  0.00%|
  1046|         0|            0|            0|  0.00%|    .. math::
  1047|         0|            0|            0|  0.00%|        \text{loss}(x, class) = -\log\left(\frac{\exp(x[class])}{\sum_j \exp(x[j])}\right)
  1048|         0|            0|            0|  0.00%|                       = -x[class] + \log\left(\sum_j \exp(x[j])\right)
  1049|         0|            0|            0|  0.00%|
  1050|         0|            0|            0|  0.00%|    or in the case of the :attr:`weight` argument being specified:
  1051|         0|            0|            0|  0.00%|
  1052|         0|            0|            0|  0.00%|    .. math::
  1053|         0|            0|            0|  0.00%|        \text{loss}(x, class) = weight[class] \left(-x[class] + \log\left(\sum_j \exp(x[j])\right)\right)
  1054|         0|            0|            0|  0.00%|
  1055|         0|            0|            0|  0.00%|    The losses are averaged across observations for each minibatch. If the
  1056|         0|            0|            0|  0.00%|    :attr:`weight` argument is specified then this is a weighted average:
  1057|         0|            0|            0|  0.00%|
  1058|         0|            0|            0|  0.00%|    .. math::
  1059|         0|            0|            0|  0.00%|        \text{loss} = \frac{\sum^{N}_{i=1} loss(i, class[i])}{\sum^{N}_{i=1} weight[class[i]]}
  1060|         0|            0|            0|  0.00%|
  1061|         0|            0|            0|  0.00%|    Can also be used for higher dimension inputs, such as 2D images, by providing
  1062|         0|            0|            0|  0.00%|    an input of size :math:`(minibatch, C, d_1, d_2, ..., d_K)` with :math:`K \geq 1`,
  1063|         0|            0|            0|  0.00%|    where :math:`K` is the number of dimensions, and a target of appropriate shape
  1064|         0|            0|            0|  0.00%|    (see below).
  1065|         0|            0|            0|  0.00%|
  1066|         0|            0|            0|  0.00%|
  1067|         0|            0|            0|  0.00%|    Args:
  1068|         0|            0|            0|  0.00%|        weight (Tensor, optional): a manual rescaling weight given to each class.
  1069|         0|            0|            0|  0.00%|            If given, has to be a Tensor of size `C`
  1070|         0|            0|            0|  0.00%|        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,
  1071|         0|            0|            0|  0.00%|            the losses are averaged over each loss element in the batch. Note that for
  1072|         0|            0|            0|  0.00%|            some losses, there are multiple elements per sample. If the field :attr:`size_average`
  1073|         0|            0|            0|  0.00%|            is set to ``False``, the losses are instead summed for each minibatch. Ignored
  1074|         0|            0|            0|  0.00%|            when :attr:`reduce` is ``False``. Default: ``True``
  1075|         0|            0|            0|  0.00%|        ignore_index (int, optional): Specifies a target value that is ignored
  1076|         0|            0|            0|  0.00%|            and does not contribute to the input gradient. When :attr:`size_average` is
  1077|         0|            0|            0|  0.00%|            ``True``, the loss is averaged over non-ignored targets.
  1078|         0|            0|            0|  0.00%|        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the
  1079|         0|            0|            0|  0.00%|            losses are averaged or summed over observations for each minibatch depending
  1080|         0|            0|            0|  0.00%|            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per
  1081|         0|            0|            0|  0.00%|            batch element instead and ignores :attr:`size_average`. Default: ``True``
  1082|         0|            0|            0|  0.00%|        reduction (string, optional): Specifies the reduction to apply to the output:
  1083|         0|            0|            0|  0.00%|            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will
  1084|         0|            0|            0|  0.00%|            be applied, ``'mean'``: the weighted mean of the output is taken,
  1085|         0|            0|            0|  0.00%|            ``'sum'``: the output will be summed. Note: :attr:`size_average`
  1086|         0|            0|            0|  0.00%|            and :attr:`reduce` are in the process of being deprecated, and in
  1087|         0|            0|            0|  0.00%|            the meantime, specifying either of those two args will override
  1088|         0|            0|            0|  0.00%|            :attr:`reduction`. Default: ``'mean'``
  1089|         0|            0|            0|  0.00%|
  1090|         0|            0|            0|  0.00%|    Shape:
  1091|         0|            0|            0|  0.00%|        - Input: :math:`(N, C)` where `C = number of classes`, or
  1092|         0|            0|            0|  0.00%|          :math:`(N, C, d_1, d_2, ..., d_K)` with :math:`K \geq 1`
  1093|         0|            0|            0|  0.00%|          in the case of `K`-dimensional loss.
  1094|         0|            0|            0|  0.00%|        - Target: :math:`(N)` where each value is :math:`0 \leq \text{targets}[i] \leq C-1`, or
  1095|         0|            0|            0|  0.00%|          :math:`(N, d_1, d_2, ..., d_K)` with :math:`K \geq 1` in the case of
  1096|         0|            0|            0|  0.00%|          K-dimensional loss.
  1097|         0|            0|            0|  0.00%|        - Output: scalar.
  1098|         0|            0|            0|  0.00%|          If :attr:`reduction` is ``'none'``, then the same size as the target:
  1099|         0|            0|            0|  0.00%|          :math:`(N)`, or
  1100|         0|            0|            0|  0.00%|          :math:`(N, d_1, d_2, ..., d_K)` with :math:`K \geq 1` in the case
  1101|         0|            0|            0|  0.00%|          of K-dimensional loss.
  1102|         0|            0|            0|  0.00%|
  1103|         0|            0|            0|  0.00%|    Examples::
  1104|         0|            0|            0|  0.00%|
  1105|         0|            0|            0|  0.00%|        >>> loss = nn.CrossEntropyLoss()
  1106|         0|            0|            0|  0.00%|        >>> input = torch.randn(3, 5, requires_grad=True)
  1107|         0|            0|            0|  0.00%|        >>> target = torch.empty(3, dtype=torch.long).random_(5)
  1108|         0|            0|            0|  0.00%|        >>> output = loss(input, target)
  1109|         0|            0|            0|  0.00%|        >>> output.backward()
  1110|         0|            0|            0|  0.00%|    """
  1111|         0|            0|            0|  0.00%|    __constants__ = ['ignore_index', 'reduction']
  1112|         0|            0|            0|  0.00%|    ignore_index: int
  1113|         0|            0|            0|  0.00%|
  1114|         0|            0|            0|  0.00%|    def __init__(self, weight: Optional[Tensor] = None, size_average=None, ignore_index: int = -100,
  1115|         0|            0|            0|  0.00%|                 reduce=None, reduction: str = 'mean') -> None:
  1116|         0|            0|            0|  0.00%|        super(CrossEntropyLoss, self).__init__(weight, size_average, reduce, reduction)
  1117|         0|            0|            0|  0.00%|        self.ignore_index = ignore_index
  1118|         0|            0|            0|  0.00%|
  1119|       100|   0.00066638|   6.6638e-06|  0.00%|    def forward(self, input: Tensor, target: Tensor) -> Tensor:
  1120|       200|   0.00423455|  2.11728e-05|  0.01%|        return F.cross_entropy(input, target, weight=self.weight,
(call)|       100|   0.00385189|  3.85189e-05|  0.01%|# /opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1117 __getattr__
(call)|       100|    0.0158386|  0.000158386|  0.03%|# /opt/conda/lib/python3.8/site-packages/torch/nn/functional.py:2761 cross_entropy
  1121|       100|   0.00056839|   5.6839e-06|  0.00%|                               ignore_index=self.ignore_index, reduction=self.reduction)
  1122|         0|            0|            0|  0.00%|
  1123|         0|            0|            0|  0.00%|
  1124|         0|            0|            0|  0.00%|class MultiLabelSoftMarginLoss(_WeightedLoss):
  1125|         0|            0|            0|  0.00%|    r"""Creates a criterion that optimizes a multi-label one-versus-all
  1126|         0|            0|            0|  0.00%|    loss based on max-entropy, between input :math:`x` and target :math:`y` of size
  1127|         0|            0|            0|  0.00%|    :math:`(N, C)`.
  1128|         0|            0|            0|  0.00%|    For each sample in the minibatch:
  1129|         0|            0|            0|  0.00%|
  1130|         0|            0|            0|  0.00%|    .. math::
  1131|         0|            0|            0|  0.00%|        loss(x, y) = - \frac{1}{C} * \sum_i y[i] * \log((1 + \exp(-x[i]))^{-1})
  1132|         0|            0|            0|  0.00%|                         + (1-y[i]) * \log\left(\frac{\exp(-x[i])}{(1 + \exp(-x[i]))}\right)
  1133|         0|            0|            0|  0.00%|
  1134|         0|            0|            0|  0.00%|    where :math:`i \in \left\{0, \; \cdots , \; \text{x.nElement}() - 1\right\}`,
  1135|         0|            0|            0|  0.00%|    :math:`y[i] \in \left\{0, \; 1\right\}`.
  1136|         0|            0|            0|  0.00%|
  1137|         0|            0|            0|  0.00%|    Args:
  1138|         0|            0|            0|  0.00%|        weight (Tensor, optional): a manual rescaling weight given to each
  1139|         0|            0|            0|  0.00%|            class. If given, it has to be a Tensor of size `C`. Otherwise, it is
  1140|         0|            0|            0|  0.00%|            treated as if having all ones.
  1141|         0|            0|            0|  0.00%|        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,
  1142|         0|            0|            0|  0.00%|            the losses are averaged over each loss element in the batch. Note that for
  1143|         0|            0|            0|  0.00%|            some losses, there are multiple elements per sample. If the field :attr:`size_average`
  1144|         0|            0|            0|  0.00%|            is set to ``False``, the losses are instead summed for each minibatch. Ignored
  1145|         0|            0|            0|  0.00%|            when :attr:`reduce` is ``False``. Default: ``True``
  1146|         0|            0|            0|  0.00%|        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the
  1147|         0|            0|            0|  0.00%|            losses are averaged or summed over observations for each minibatch depending
  1148|         0|            0|            0|  0.00%|            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per
  1149|         0|            0|            0|  0.00%|            batch element instead and ignores :attr:`size_average`. Default: ``True``
  1150|         0|            0|            0|  0.00%|        reduction (string, optional): Specifies the reduction to apply to the output:
  1151|         0|            0|            0|  0.00%|            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,
  1152|         0|            0|            0|  0.00%|            ``'mean'``: the sum of the output will be divided by the number of
  1153|         0|            0|            0|  0.00%|            elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`
  1154|         0|            0|            0|  0.00%|            and :attr:`reduce` are in the process of being deprecated, and in the meantime,
  1155|         0|            0|            0|  0.00%|            specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``
  1156|         0|            0|            0|  0.00%|
  1157|         0|            0|            0|  0.00%|    Shape:
  1158|         0|            0|            0|  0.00%|        - Input: :math:`(N, C)` where `N` is the batch size and `C` is the number of classes.
  1159|         0|            0|            0|  0.00%|        - Target: :math:`(N, C)`, label targets padded by -1 ensuring same shape as the input.
  1160|         0|            0|            0|  0.00%|        - Output: scalar. If :attr:`reduction` is ``'none'``, then :math:`(N)`.
  1161|         0|            0|            0|  0.00%|    """
  1162|         0|            0|            0|  0.00%|    __constants__ = ['reduction']
  1163|         0|            0|            0|  0.00%|
  1164|         0|            0|            0|  0.00%|    def __init__(self, weight: Optional[Tensor] = None, size_average=None, reduce=None, reduction: str = 'mean') -> None:
  1165|         0|            0|            0|  0.00%|        super(MultiLabelSoftMarginLoss, self).__init__(weight, size_average, reduce, reduction)
  1166|         0|            0|            0|  0.00%|
  1167|         0|            0|            0|  0.00%|    def forward(self, input: Tensor, target: Tensor) -> Tensor:
  1168|         0|            0|            0|  0.00%|        return F.multilabel_soft_margin_loss(input, target, weight=self.weight, reduction=self.reduction)
  1169|         0|            0|            0|  0.00%|
  1170|         0|            0|            0|  0.00%|
  1171|         0|            0|            0|  0.00%|class CosineEmbeddingLoss(_Loss):
  1172|         0|            0|            0|  0.00%|    r"""Creates a criterion that measures the loss given input tensors
  1173|         0|            0|            0|  0.00%|    :math:`x_1`, :math:`x_2` and a `Tensor` label :math:`y` with values 1 or -1.
  1174|         0|            0|            0|  0.00%|    This is used for measuring whether two inputs are similar or dissimilar,
  1175|         0|            0|            0|  0.00%|    using the cosine distance, and is typically used for learning nonlinear
  1176|         0|            0|            0|  0.00%|    embeddings or semi-supervised learning.
  1177|         0|            0|            0|  0.00%|
  1178|         0|            0|            0|  0.00%|    The loss function for each sample is:
  1179|         0|            0|            0|  0.00%|
  1180|         0|            0|            0|  0.00%|    .. math::
  1181|         0|            0|            0|  0.00%|        \text{loss}(x, y) =
  1182|         0|            0|            0|  0.00%|        \begin{cases}
  1183|         0|            0|            0|  0.00%|        1 - \cos(x_1, x_2), & \text{if } y = 1 \\
  1184|         0|            0|            0|  0.00%|        \max(0, \cos(x_1, x_2) - \text{margin}), & \text{if } y = -1
  1185|         0|            0|            0|  0.00%|        \end{cases}
  1186|         0|            0|            0|  0.00%|
  1187|         0|            0|            0|  0.00%|    Args:
  1188|         0|            0|            0|  0.00%|        margin (float, optional): Should be a number from :math:`-1` to :math:`1`,
  1189|         0|            0|            0|  0.00%|            :math:`0` to :math:`0.5` is suggested. If :attr:`margin` is missing, the
  1190|         0|            0|            0|  0.00%|            default value is :math:`0`.
  1191|         0|            0|            0|  0.00%|        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,
  1192|         0|            0|            0|  0.00%|            the losses are averaged over each loss element in the batch. Note that for
  1193|         0|            0|            0|  0.00%|            some losses, there are multiple elements per sample. If the field :attr:`size_average`
  1194|         0|            0|            0|  0.00%|            is set to ``False``, the losses are instead summed for each minibatch. Ignored
  1195|         0|            0|            0|  0.00%|            when :attr:`reduce` is ``False``. Default: ``True``
  1196|         0|            0|            0|  0.00%|        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the
  1197|         0|            0|            0|  0.00%|            losses are averaged or summed over observations for each minibatch depending
  1198|         0|            0|            0|  0.00%|            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per
  1199|         0|            0|            0|  0.00%|            batch element instead and ignores :attr:`size_average`. Default: ``True``
  1200|         0|            0|            0|  0.00%|        reduction (string, optional): Specifies the reduction to apply to the output:
  1201|         0|            0|            0|  0.00%|            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,
  1202|         0|            0|            0|  0.00%|            ``'mean'``: the sum of the output will be divided by the number of
  1203|         0|            0|            0|  0.00%|            elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`
  1204|         0|            0|            0|  0.00%|            and :attr:`reduce` are in the process of being deprecated, and in the meantime,
  1205|         0|            0|            0|  0.00%|            specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``
  1206|         0|            0|            0|  0.00%|
  1207|         0|            0|            0|  0.00%|    Shape:
  1208|         0|            0|            0|  0.00%|        - Input1: :math:`(N, D)`, where `N` is the batch size and `D` is the embedding dimension.
  1209|         0|            0|            0|  0.00%|        - Input2: :math:`(N, D)`, same shape as Input1.
  1210|         0|            0|            0|  0.00%|        - Target: :math:`(N)`.
  1211|         0|            0|            0|  0.00%|        - Output: If :attr:`reduction` is ``'none'``, then :math:`(N)`, otherwise scalar.
  1212|         0|            0|            0|  0.00%|    """
  1213|         0|            0|            0|  0.00%|    __constants__ = ['margin', 'reduction']
  1214|         0|            0|            0|  0.00%|    margin: float
  1215|         0|            0|            0|  0.00%|
  1216|         0|            0|            0|  0.00%|    def __init__(self, margin: float = 0., size_average=None, reduce=None, reduction: str = 'mean') -> None:
  1217|         0|            0|            0|  0.00%|        super(CosineEmbeddingLoss, self).__init__(size_average, reduce, reduction)
  1218|         0|            0|            0|  0.00%|        self.margin = margin
  1219|         0|            0|            0|  0.00%|
  1220|         0|            0|            0|  0.00%|    def forward(self, input1: Tensor, input2: Tensor, target: Tensor) -> Tensor:
  1221|         0|            0|            0|  0.00%|        return F.cosine_embedding_loss(input1, input2, target, margin=self.margin, reduction=self.reduction)
  1222|         0|            0|            0|  0.00%|
  1223|         0|            0|            0|  0.00%|
  1224|         0|            0|            0|  0.00%|class MarginRankingLoss(_Loss):
  1225|         0|            0|            0|  0.00%|    r"""Creates a criterion that measures the loss given
  1226|         0|            0|            0|  0.00%|    inputs :math:`x1`, :math:`x2`, two 1D mini-batch `Tensors`,
  1227|         0|            0|            0|  0.00%|    and a label 1D mini-batch tensor :math:`y` (containing 1 or -1).
  1228|         0|            0|            0|  0.00%|
  1229|         0|            0|            0|  0.00%|    If :math:`y = 1` then it assumed the first input should be ranked higher
  1230|         0|            0|            0|  0.00%|    (have a larger value) than the second input, and vice-versa for :math:`y = -1`.
  1231|         0|            0|            0|  0.00%|
  1232|         0|            0|            0|  0.00%|    The loss function for each pair of samples in the mini-batch is:
  1233|         0|            0|            0|  0.00%|
  1234|         0|            0|            0|  0.00%|    .. math::
  1235|         0|            0|            0|  0.00%|        \text{loss}(x1, x2, y) = \max(0, -y * (x1 - x2) + \text{margin})
  1236|         0|            0|            0|  0.00%|
  1237|         0|            0|            0|  0.00%|    Args:
  1238|         0|            0|            0|  0.00%|        margin (float, optional): Has a default value of :math:`0`.
  1239|         0|            0|            0|  0.00%|        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,
  1240|         0|            0|            0|  0.00%|            the losses are averaged over each loss element in the batch. Note that for
  1241|         0|            0|            0|  0.00%|            some losses, there are multiple elements per sample. If the field :attr:`size_average`
  1242|         0|            0|            0|  0.00%|            is set to ``False``, the losses are instead summed for each minibatch. Ignored
  1243|         0|            0|            0|  0.00%|            when :attr:`reduce` is ``False``. Default: ``True``
  1244|         0|            0|            0|  0.00%|        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the
  1245|         0|            0|            0|  0.00%|            losses are averaged or summed over observations for each minibatch depending
  1246|         0|            0|            0|  0.00%|            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per
  1247|         0|            0|            0|  0.00%|            batch element instead and ignores :attr:`size_average`. Default: ``True``
  1248|         0|            0|            0|  0.00%|        reduction (string, optional): Specifies the reduction to apply to the output:
  1249|         0|            0|            0|  0.00%|            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,
  1250|         0|            0|            0|  0.00%|            ``'mean'``: the sum of the output will be divided by the number of
  1251|         0|            0|            0|  0.00%|            elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`
  1252|         0|            0|            0|  0.00%|            and :attr:`reduce` are in the process of being deprecated, and in the meantime,
  1253|         0|            0|            0|  0.00%|            specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``
  1254|         0|            0|            0|  0.00%|
  1255|         0|            0|            0|  0.00%|    Shape:
  1256|         0|            0|            0|  0.00%|        - Input1: :math:`(N)` where `N` is the batch size.
  1257|         0|            0|            0|  0.00%|        - Input2: :math:`(N)`, same shape as the Input1.
  1258|         0|            0|            0|  0.00%|        - Target: :math:`(N)`, same shape as the inputs.
  1259|         0|            0|            0|  0.00%|        - Output: scalar. If :attr:`reduction` is ``'none'``, then :math:`(N)`.
  1260|         0|            0|            0|  0.00%|
  1261|         0|            0|            0|  0.00%|    Examples::
  1262|         0|            0|            0|  0.00%|
  1263|         0|            0|            0|  0.00%|        >>> loss = nn.MarginRankingLoss()
  1264|         0|            0|            0|  0.00%|        >>> input1 = torch.randn(3, requires_grad=True)
  1265|         0|            0|            0|  0.00%|        >>> input2 = torch.randn(3, requires_grad=True)
  1266|         0|            0|            0|  0.00%|        >>> target = torch.randn(3).sign()
  1267|         0|            0|            0|  0.00%|        >>> output = loss(input1, input2, target)
  1268|         0|            0|            0|  0.00%|        >>> output.backward()
  1269|         0|            0|            0|  0.00%|    """
  1270|         0|            0|            0|  0.00%|    __constants__ = ['margin', 'reduction']
  1271|         0|            0|            0|  0.00%|    margin: float
  1272|         0|            0|            0|  0.00%|
  1273|         0|            0|            0|  0.00%|    def __init__(self, margin: float = 0., size_average=None, reduce=None, reduction: str = 'mean') -> None:
  1274|         0|            0|            0|  0.00%|        super(MarginRankingLoss, self).__init__(size_average, reduce, reduction)
  1275|         0|            0|            0|  0.00%|        self.margin = margin
  1276|         0|            0|            0|  0.00%|
  1277|         0|            0|            0|  0.00%|    def forward(self, input1: Tensor, input2: Tensor, target: Tensor) -> Tensor:
  1278|         0|            0|            0|  0.00%|        return F.margin_ranking_loss(input1, input2, target, margin=self.margin, reduction=self.reduction)
  1279|         0|            0|            0|  0.00%|
  1280|         0|            0|            0|  0.00%|
  1281|         0|            0|            0|  0.00%|class MultiMarginLoss(_WeightedLoss):
  1282|         0|            0|            0|  0.00%|    r"""Creates a criterion that optimizes a multi-class classification hinge
  1283|         0|            0|            0|  0.00%|    loss (margin-based loss) between input :math:`x` (a 2D mini-batch `Tensor`) and
  1284|         0|            0|            0|  0.00%|    output :math:`y` (which is a 1D tensor of target class indices,
  1285|         0|            0|            0|  0.00%|    :math:`0 \leq y \leq \text{x.size}(1)-1`):
  1286|         0|            0|            0|  0.00%|
  1287|         0|            0|            0|  0.00%|    For each mini-batch sample, the loss in terms of the 1D input :math:`x` and scalar
  1288|         0|            0|            0|  0.00%|    output :math:`y` is:
  1289|         0|            0|            0|  0.00%|
  1290|         0|            0|            0|  0.00%|    .. math::
  1291|         0|            0|            0|  0.00%|        \text{loss}(x, y) = \frac{\sum_i \max(0, \text{margin} - x[y] + x[i]))^p}{\text{x.size}(0)}
  1292|         0|            0|            0|  0.00%|
  1293|         0|            0|            0|  0.00%|    where :math:`x \in \left\{0, \; \cdots , \; \text{x.size}(0) - 1\right\}`
  1294|         0|            0|            0|  0.00%|    and :math:`i \neq y`.
  1295|         0|            0|            0|  0.00%|
  1296|         0|            0|            0|  0.00%|    Optionally, you can give non-equal weighting on the classes by passing
  1297|         0|            0|            0|  0.00%|    a 1D :attr:`weight` tensor into the constructor.
  1298|         0|            0|            0|  0.00%|
  1299|         0|            0|            0|  0.00%|    The loss function then becomes:
  1300|         0|            0|            0|  0.00%|
  1301|         0|            0|            0|  0.00%|    .. math::
  1302|         0|            0|            0|  0.00%|        \text{loss}(x, y) = \frac{\sum_i \max(0, w[y] * (\text{margin} - x[y] + x[i]))^p)}{\text{x.size}(0)}
  1303|         0|            0|            0|  0.00%|
  1304|         0|            0|            0|  0.00%|    Args:
  1305|         0|            0|            0|  0.00%|        p (int, optional): Has a default value of :math:`1`. :math:`1` and :math:`2`
  1306|         0|            0|            0|  0.00%|            are the only supported values.
  1307|         0|            0|            0|  0.00%|        margin (float, optional): Has a default value of :math:`1`.
  1308|         0|            0|            0|  0.00%|        weight (Tensor, optional): a manual rescaling weight given to each
  1309|         0|            0|            0|  0.00%|            class. If given, it has to be a Tensor of size `C`. Otherwise, it is
  1310|         0|            0|            0|  0.00%|            treated as if having all ones.
  1311|         0|            0|            0|  0.00%|        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,
  1312|         0|            0|            0|  0.00%|            the losses are averaged over each loss element in the batch. Note that for
  1313|         0|            0|            0|  0.00%|            some losses, there are multiple elements per sample. If the field :attr:`size_average`
  1314|         0|            0|            0|  0.00%|            is set to ``False``, the losses are instead summed for each minibatch. Ignored
  1315|         0|            0|            0|  0.00%|            when :attr:`reduce` is ``False``. Default: ``True``
  1316|         0|            0|            0|  0.00%|        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the
  1317|         0|            0|            0|  0.00%|            losses are averaged or summed over observations for each minibatch depending
  1318|         0|            0|            0|  0.00%|            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per
  1319|         0|            0|            0|  0.00%|            batch element instead and ignores :attr:`size_average`. Default: ``True``
  1320|         0|            0|            0|  0.00%|        reduction (string, optional): Specifies the reduction to apply to the output:
  1321|         0|            0|            0|  0.00%|            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,
  1322|         0|            0|            0|  0.00%|            ``'mean'``: the sum of the output will be divided by the number of
  1323|         0|            0|            0|  0.00%|            elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`
  1324|         0|            0|            0|  0.00%|            and :attr:`reduce` are in the process of being deprecated, and in the meantime,
  1325|         0|            0|            0|  0.00%|            specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``
  1326|         0|            0|            0|  0.00%|    """
  1327|         0|            0|            0|  0.00%|    __constants__ = ['p', 'margin', 'reduction']
  1328|         0|            0|            0|  0.00%|    margin: float
  1329|         0|            0|            0|  0.00%|    p: int
  1330|         0|            0|            0|  0.00%|
  1331|         0|            0|            0|  0.00%|    def __init__(self, p: int = 1, margin: float = 1., weight: Optional[Tensor] = None, size_average=None,
  1332|         0|            0|            0|  0.00%|                 reduce=None, reduction: str = 'mean') -> None:
  1333|         0|            0|            0|  0.00%|        super(MultiMarginLoss, self).__init__(weight, size_average, reduce, reduction)
  1334|         0|            0|            0|  0.00%|        if p != 1 and p != 2:
  1335|         0|            0|            0|  0.00%|            raise ValueError("only p == 1 and p == 2 supported")
  1336|         0|            0|            0|  0.00%|        assert weight is None or weight.dim() == 1
  1337|         0|            0|            0|  0.00%|        self.p = p
  1338|         0|            0|            0|  0.00%|        self.margin = margin
  1339|         0|            0|            0|  0.00%|
  1340|         0|            0|            0|  0.00%|    def forward(self, input: Tensor, target: Tensor) -> Tensor:
  1341|         0|            0|            0|  0.00%|        return F.multi_margin_loss(input, target, p=self.p, margin=self.margin,
  1342|         0|            0|            0|  0.00%|                                   weight=self.weight, reduction=self.reduction)
  1343|         0|            0|            0|  0.00%|
  1344|         0|            0|            0|  0.00%|
  1345|         0|            0|            0|  0.00%|class TripletMarginLoss(_Loss):
  1346|         0|            0|            0|  0.00%|    r"""Creates a criterion that measures the triplet loss given an input
  1347|         0|            0|            0|  0.00%|    tensors :math:`x1`, :math:`x2`, :math:`x3` and a margin with a value greater than :math:`0`.
  1348|         0|            0|            0|  0.00%|    This is used for measuring a relative similarity between samples. A triplet
  1349|         0|            0|            0|  0.00%|    is composed by `a`, `p` and `n` (i.e., `anchor`, `positive examples` and `negative
  1350|         0|            0|            0|  0.00%|    examples` respectively). The shapes of all input tensors should be
  1351|         0|            0|            0|  0.00%|    :math:`(N, D)`.
  1352|         0|            0|            0|  0.00%|
  1353|         0|            0|            0|  0.00%|    The distance swap is described in detail in the paper `Learning shallow
  1354|         0|            0|            0|  0.00%|    convolutional feature descriptors with triplet losses`_ by
  1355|         0|            0|            0|  0.00%|    V. Balntas, E. Riba et al.
  1356|         0|            0|            0|  0.00%|
  1357|         0|            0|            0|  0.00%|    The loss function for each sample in the mini-batch is:
  1358|         0|            0|            0|  0.00%|
  1359|         0|            0|            0|  0.00%|    .. math::
  1360|         0|            0|            0|  0.00%|        L(a, p, n) = \max \{d(a_i, p_i) - d(a_i, n_i) + {\rm margin}, 0\}
  1361|         0|            0|            0|  0.00%|
  1362|         0|            0|            0|  0.00%|
  1363|         0|            0|            0|  0.00%|    where
  1364|         0|            0|            0|  0.00%|
  1365|         0|            0|            0|  0.00%|    .. math::
  1366|         0|            0|            0|  0.00%|        d(x_i, y_i) = \left\lVert {\bf x}_i - {\bf y}_i \right\rVert_p
  1367|         0|            0|            0|  0.00%|
  1368|         0|            0|            0|  0.00%|    See also :class:`~torch.nn.TripletMarginWithDistanceLoss`, which computes the
  1369|         0|            0|            0|  0.00%|    triplet margin loss for input tensors using a custom distance function.
  1370|         0|            0|            0|  0.00%|
  1371|         0|            0|            0|  0.00%|    Args:
  1372|         0|            0|            0|  0.00%|        margin (float, optional): Default: :math:`1`.
  1373|         0|            0|            0|  0.00%|        p (int, optional): The norm degree for pairwise distance. Default: :math:`2`.
  1374|         0|            0|            0|  0.00%|        swap (bool, optional): The distance swap is described in detail in the paper
  1375|         0|            0|            0|  0.00%|            `Learning shallow convolutional feature descriptors with triplet losses` by
  1376|         0|            0|            0|  0.00%|            V. Balntas, E. Riba et al. Default: ``False``.
  1377|         0|            0|            0|  0.00%|        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,
  1378|         0|            0|            0|  0.00%|            the losses are averaged over each loss element in the batch. Note that for
  1379|         0|            0|            0|  0.00%|            some losses, there are multiple elements per sample. If the field :attr:`size_average`
  1380|         0|            0|            0|  0.00%|            is set to ``False``, the losses are instead summed for each minibatch. Ignored
  1381|         0|            0|            0|  0.00%|            when :attr:`reduce` is ``False``. Default: ``True``
  1382|         0|            0|            0|  0.00%|        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the
  1383|         0|            0|            0|  0.00%|            losses are averaged or summed over observations for each minibatch depending
  1384|         0|            0|            0|  0.00%|            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per
  1385|         0|            0|            0|  0.00%|            batch element instead and ignores :attr:`size_average`. Default: ``True``
  1386|         0|            0|            0|  0.00%|        reduction (string, optional): Specifies the reduction to apply to the output:
  1387|         0|            0|            0|  0.00%|            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,
  1388|         0|            0|            0|  0.00%|            ``'mean'``: the sum of the output will be divided by the number of
  1389|         0|            0|            0|  0.00%|            elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`
  1390|         0|            0|            0|  0.00%|            and :attr:`reduce` are in the process of being deprecated, and in the meantime,
  1391|         0|            0|            0|  0.00%|            specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``
  1392|         0|            0|            0|  0.00%|
  1393|         0|            0|            0|  0.00%|    Shape:
  1394|         0|            0|            0|  0.00%|        - Input: :math:`(N, D)` where :math:`D` is the vector dimension.
  1395|         0|            0|            0|  0.00%|        - Output: A Tensor of shape :math:`(N)` if :attr:`reduction` is ``'none'``, or a scalar
  1396|         0|            0|            0|  0.00%|          otherwise.
  1397|         0|            0|            0|  0.00%|
  1398|         0|            0|            0|  0.00%|    Examples::
  1399|         0|            0|            0|  0.00%|
  1400|         0|            0|            0|  0.00%|    >>> triplet_loss = nn.TripletMarginLoss(margin=1.0, p=2)
  1401|         0|            0|            0|  0.00%|    >>> anchor = torch.randn(100, 128, requires_grad=True)
  1402|         0|            0|            0|  0.00%|    >>> positive = torch.randn(100, 128, requires_grad=True)
  1403|         0|            0|            0|  0.00%|    >>> negative = torch.randn(100, 128, requires_grad=True)
  1404|         0|            0|            0|  0.00%|    >>> output = triplet_loss(anchor, positive, negative)
  1405|         0|            0|            0|  0.00%|    >>> output.backward()
  1406|         0|            0|            0|  0.00%|
  1407|         0|            0|            0|  0.00%|    .. _Learning shallow convolutional feature descriptors with triplet losses:
  1408|         0|            0|            0|  0.00%|        http://www.bmva.org/bmvc/2016/papers/paper119/index.html
  1409|         0|            0|            0|  0.00%|    """
  1410|         0|            0|            0|  0.00%|    __constants__ = ['margin', 'p', 'eps', 'swap', 'reduction']
  1411|         0|            0|            0|  0.00%|    margin: float
  1412|         0|            0|            0|  0.00%|    p: float
  1413|         0|            0|            0|  0.00%|    eps: float
  1414|         0|            0|            0|  0.00%|    swap: bool
  1415|         0|            0|            0|  0.00%|
  1416|         0|            0|            0|  0.00%|    def __init__(self, margin: float = 1.0, p: float = 2., eps: float = 1e-6, swap: bool = False, size_average=None,
  1417|         0|            0|            0|  0.00%|                 reduce=None, reduction: str = 'mean'):
  1418|         0|            0|            0|  0.00%|        super(TripletMarginLoss, self).__init__(size_average, reduce, reduction)
  1419|         0|            0|            0|  0.00%|        self.margin = margin
  1420|         0|            0|            0|  0.00%|        self.p = p
  1421|         0|            0|            0|  0.00%|        self.eps = eps
  1422|         0|            0|            0|  0.00%|        self.swap = swap
  1423|         0|            0|            0|  0.00%|
  1424|         0|            0|            0|  0.00%|    def forward(self, anchor: Tensor, positive: Tensor, negative: Tensor) -> Tensor:
  1425|         0|            0|            0|  0.00%|        return F.triplet_margin_loss(anchor, positive, negative, margin=self.margin, p=self.p,
  1426|         0|            0|            0|  0.00%|                                     eps=self.eps, swap=self.swap, reduction=self.reduction)
  1427|         0|            0|            0|  0.00%|
  1428|         0|            0|            0|  0.00%|
  1429|         0|            0|            0|  0.00%|class TripletMarginWithDistanceLoss(_Loss):
  1430|         0|            0|            0|  0.00%|    r"""Creates a criterion that measures the triplet loss given input
  1431|         0|            0|            0|  0.00%|    tensors :math:`a`, :math:`p`, and :math:`n` (representing anchor,
  1432|         0|            0|            0|  0.00%|    positive, and negative examples, respectively), and a nonnegative,
  1433|         0|            0|            0|  0.00%|    real-valued function ("distance function") used to compute the relationship
  1434|         0|            0|            0|  0.00%|    between the anchor and positive example ("positive distance") and the
  1435|         0|            0|            0|  0.00%|    anchor and negative example ("negative distance").
  1436|         0|            0|            0|  0.00%|
  1437|         0|            0|            0|  0.00%|    The unreduced loss (i.e., with :attr:`reduction` set to ``'none'``)
  1438|         0|            0|            0|  0.00%|    can be described as:
  1439|         0|            0|            0|  0.00%|
  1440|         0|            0|            0|  0.00%|    .. math::
  1441|         0|            0|            0|  0.00%|        \ell(a, p, n) = L = \{l_1,\dots,l_N\}^\top, \quad
  1442|         0|            0|            0|  0.00%|        l_i = \max \{d(a_i, p_i) - d(a_i, n_i) + {\rm margin}, 0\}
  1443|         0|            0|            0|  0.00%|
  1444|         0|            0|            0|  0.00%|    where :math:`N` is the batch size; :math:`d` is a nonnegative, real-valued function
  1445|         0|            0|            0|  0.00%|    quantifying the closeness of two tensors, referred to as the :attr:`distance_function`;
  1446|         0|            0|            0|  0.00%|    and :math:`margin` is a nonnegative margin representing the minimum difference
  1447|         0|            0|            0|  0.00%|    between the positive and negative distances that is required for the loss to
  1448|         0|            0|            0|  0.00%|    be 0.  The input tensors have :math:`N` elements each and can be of any shape
  1449|         0|            0|            0|  0.00%|    that the distance function can handle.
  1450|         0|            0|            0|  0.00%|
  1451|         0|            0|            0|  0.00%|    If :attr:`reduction` is not ``'none'``
  1452|         0|            0|            0|  0.00%|    (default ``'mean'``), then:
  1453|         0|            0|            0|  0.00%|
  1454|         0|            0|            0|  0.00%|    .. math::
  1455|         0|            0|            0|  0.00%|        \ell(x, y) =
  1456|         0|            0|            0|  0.00%|        \begin{cases}
  1457|         0|            0|            0|  0.00%|            \operatorname{mean}(L), &  \text{if reduction} = \text{`mean';}\\
  1458|         0|            0|            0|  0.00%|            \operatorname{sum}(L),  &  \text{if reduction} = \text{`sum'.}
  1459|         0|            0|            0|  0.00%|        \end{cases}
  1460|         0|            0|            0|  0.00%|
  1461|         0|            0|            0|  0.00%|    See also :class:`~torch.nn.TripletMarginLoss`, which computes the triplet
  1462|         0|            0|            0|  0.00%|    loss for input tensors using the :math:`l_p` distance as the distance function.
  1463|         0|            0|            0|  0.00%|
  1464|         0|            0|            0|  0.00%|    Args:
  1465|         0|            0|            0|  0.00%|        distance_function (callable, optional): A nonnegative, real-valued function that
  1466|         0|            0|            0|  0.00%|            quantifies the closeness of two tensors. If not specified,
  1467|         0|            0|            0|  0.00%|            `nn.PairwiseDistance` will be used.  Default: ``None``
  1468|         0|            0|            0|  0.00%|        margin (float, optional): A nonnegative margin representing the minimum difference
  1469|         0|            0|            0|  0.00%|            between the positive and negative distances required for the loss to be 0. Larger
  1470|         0|            0|            0|  0.00%|            margins penalize cases where the negative examples are not distant enough from the
  1471|         0|            0|            0|  0.00%|            anchors, relative to the positives. Default: :math:`1`.
  1472|         0|            0|            0|  0.00%|        swap (bool, optional): Whether to use the distance swap described in the paper
  1473|         0|            0|            0|  0.00%|            `Learning shallow convolutional feature descriptors with triplet losses` by
  1474|         0|            0|            0|  0.00%|            V. Balntas, E. Riba et al. If True, and if the positive example is closer to the
  1475|         0|            0|            0|  0.00%|            negative example than the anchor is, swaps the positive example and the anchor in
  1476|         0|            0|            0|  0.00%|            the loss computation. Default: ``False``.
  1477|         0|            0|            0|  0.00%|        reduction (string, optional): Specifies the (optional) reduction to apply to the output:
  1478|         0|            0|            0|  0.00%|            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,
  1479|         0|            0|            0|  0.00%|            ``'mean'``: the sum of the output will be divided by the number of
  1480|         0|            0|            0|  0.00%|            elements in the output, ``'sum'``: the output will be summed. Default: ``'mean'``
  1481|         0|            0|            0|  0.00%|
  1482|         0|            0|            0|  0.00%|
  1483|         0|            0|            0|  0.00%|    Shape:
  1484|         0|            0|            0|  0.00%|        - Input: :math:`(N, *)` where :math:`*` represents any number of additional dimensions
  1485|         0|            0|            0|  0.00%|          as supported by the distance function.
  1486|         0|            0|            0|  0.00%|        - Output: A Tensor of shape :math:`(N)` if :attr:`reduction` is ``'none'``, or a scalar
  1487|         0|            0|            0|  0.00%|          otherwise.
  1488|         0|            0|            0|  0.00%|
  1489|         0|            0|            0|  0.00%|    Examples::
  1490|         0|            0|            0|  0.00%|
  1491|         0|            0|            0|  0.00%|    >>> # Initialize embeddings
  1492|         0|            0|            0|  0.00%|    >>> embedding = nn.Embedding(1000, 128)
  1493|         0|            0|            0|  0.00%|    >>> anchor_ids = torch.randint(0, 1000, (1,))
  1494|         0|            0|            0|  0.00%|    >>> positive_ids = torch.randint(0, 1000, (1,))
  1495|         0|            0|            0|  0.00%|    >>> negative_ids = torch.randint(0, 1000, (1,))
  1496|         0|            0|            0|  0.00%|    >>> anchor = embedding(anchor_ids)
  1497|         0|            0|            0|  0.00%|    >>> positive = embedding(positive_ids)
  1498|         0|            0|            0|  0.00%|    >>> negative = embedding(negative_ids)
  1499|         0|            0|            0|  0.00%|    >>>
  1500|         0|            0|            0|  0.00%|    >>> # Built-in Distance Function
  1501|         0|            0|            0|  0.00%|    >>> triplet_loss = \
  1502|         0|            0|            0|  0.00%|    >>>     nn.TripletMarginWithDistanceLoss(distance_function=nn.PairwiseDistance())
  1503|         0|            0|            0|  0.00%|    >>> output = triplet_loss(anchor, positive, negative)
  1504|         0|            0|            0|  0.00%|    >>> output.backward()
  1505|         0|            0|            0|  0.00%|    >>>
  1506|         0|            0|            0|  0.00%|    >>> # Custom Distance Function
  1507|         0|            0|            0|  0.00%|    >>> def l_infinity(x1, x2):
  1508|         0|            0|            0|  0.00%|    >>>     return torch.max(torch.abs(x1 - x2), dim=1).values
  1509|         0|            0|            0|  0.00%|    >>>
  1510|         0|            0|            0|  0.00%|    >>> triplet_loss = \
  1511|         0|            0|            0|  0.00%|    >>>     nn.TripletMarginWithDistanceLoss(distance_function=l_infinity, margin=1.5)
  1512|         0|            0|            0|  0.00%|    >>> output = triplet_loss(anchor, positive, negative)
  1513|         0|            0|            0|  0.00%|    >>> output.backward()
  1514|         0|            0|            0|  0.00%|    >>>
  1515|         0|            0|            0|  0.00%|    >>> # Custom Distance Function (Lambda)
  1516|         0|            0|            0|  0.00%|    >>> triplet_loss = \
  1517|         0|            0|            0|  0.00%|    >>>     nn.TripletMarginWithDistanceLoss(
  1518|         0|            0|            0|  0.00%|    >>>         distance_function=lambda x, y: 1.0 - F.cosine_similarity(x, y))
  1519|         0|            0|            0|  0.00%|    >>> output = triplet_loss(anchor, positive, negative)
  1520|         0|            0|            0|  0.00%|    >>> output.backward()
  1521|         0|            0|            0|  0.00%|
  1522|         0|            0|            0|  0.00%|    Reference:
  1523|         0|            0|            0|  0.00%|        V. Balntas, et al.: Learning shallow convolutional feature descriptors with triplet losses:
  1524|         0|            0|            0|  0.00%|        http://www.bmva.org/bmvc/2016/papers/paper119/index.html
  1525|         0|            0|            0|  0.00%|    """
  1526|         0|            0|            0|  0.00%|    __constants__ = ['margin', 'swap', 'reduction']
  1527|         0|            0|            0|  0.00%|    margin: float
  1528|         0|            0|            0|  0.00%|    swap: bool
  1529|         0|            0|            0|  0.00%|
  1530|         0|            0|            0|  0.00%|    def __init__(self, *, distance_function: Optional[Callable[[Tensor, Tensor], Tensor]] = None,
  1531|         0|            0|            0|  0.00%|                 margin: float = 1.0, swap: bool = False, reduction: str = 'mean'):
  1532|         0|            0|            0|  0.00%|        super(TripletMarginWithDistanceLoss, self).__init__(size_average=None, reduce=None, reduction=reduction)
  1533|         0|            0|            0|  0.00%|        self.distance_function: Optional[Callable[[Tensor, Tensor], Tensor]] = \
  1534|         0|            0|            0|  0.00%|            distance_function if distance_function is not None else PairwiseDistance()
  1535|         0|            0|            0|  0.00%|        self.margin = margin
  1536|         0|            0|            0|  0.00%|        self.swap = swap
  1537|         0|            0|            0|  0.00%|
  1538|         0|            0|            0|  0.00%|    def forward(self, anchor: Tensor, positive: Tensor, negative: Tensor) -> Tensor:
  1539|         0|            0|            0|  0.00%|        return F.triplet_margin_with_distance_loss(anchor, positive, negative,
  1540|         0|            0|            0|  0.00%|                                                   distance_function=self.distance_function,
  1541|         0|            0|            0|  0.00%|                                                   margin=self.margin, swap=self.swap, reduction=self.reduction)
  1542|         0|            0|            0|  0.00%|
  1543|         0|            0|            0|  0.00%|
  1544|         0|            0|            0|  0.00%|class CTCLoss(_Loss):
  1545|         0|            0|            0|  0.00%|    r"""The Connectionist Temporal Classification loss.
  1546|         0|            0|            0|  0.00%|
  1547|         0|            0|            0|  0.00%|    Calculates loss between a continuous (unsegmented) time series and a target sequence. CTCLoss sums over the
  1548|         0|            0|            0|  0.00%|    probability of possible alignments of input to target, producing a loss value which is differentiable
  1549|         0|            0|            0|  0.00%|    with respect to each input node. The alignment of input to target is assumed to be "many-to-one", which
  1550|         0|            0|            0|  0.00%|    limits the length of the target sequence such that it must be :math:`\leq` the input length.
  1551|         0|            0|            0|  0.00%|
  1552|         0|            0|            0|  0.00%|    Args:
  1553|         0|            0|            0|  0.00%|        blank (int, optional): blank label. Default :math:`0`.
  1554|         0|            0|            0|  0.00%|        reduction (string, optional): Specifies the reduction to apply to the output:
  1555|         0|            0|            0|  0.00%|            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,
  1556|         0|            0|            0|  0.00%|            ``'mean'``: the output losses will be divided by the target lengths and
  1557|         0|            0|            0|  0.00%|            then the mean over the batch is taken. Default: ``'mean'``
  1558|         0|            0|            0|  0.00%|        zero_infinity (bool, optional):
  1559|         0|            0|            0|  0.00%|            Whether to zero infinite losses and the associated gradients.
  1560|         0|            0|            0|  0.00%|            Default: ``False``
  1561|         0|            0|            0|  0.00%|            Infinite losses mainly occur when the inputs are too short
  1562|         0|            0|            0|  0.00%|            to be aligned to the targets.
  1563|         0|            0|            0|  0.00%|
  1564|         0|            0|            0|  0.00%|    Shape:
  1565|         0|            0|            0|  0.00%|        - Log_probs: Tensor of size :math:`(T, N, C)`,
  1566|         0|            0|            0|  0.00%|          where :math:`T = \text{input length}`,
  1567|         0|            0|            0|  0.00%|          :math:`N = \text{batch size}`, and
  1568|         0|            0|            0|  0.00%|          :math:`C = \text{number of classes (including blank)}`.
  1569|         0|            0|            0|  0.00%|          The logarithmized probabilities of the outputs (e.g. obtained with
  1570|         0|            0|            0|  0.00%|          :func:`torch.nn.functional.log_softmax`).
  1571|         0|            0|            0|  0.00%|        - Targets: Tensor of size :math:`(N, S)` or
  1572|         0|            0|            0|  0.00%|          :math:`(\operatorname{sum}(\text{target\_lengths}))`,
  1573|         0|            0|            0|  0.00%|          where :math:`N = \text{batch size}` and
  1574|         0|            0|            0|  0.00%|          :math:`S = \text{max target length, if shape is } (N, S)`.
  1575|         0|            0|            0|  0.00%|          It represent the target sequences. Each element in the target
  1576|         0|            0|            0|  0.00%|          sequence is a class index. And the target index cannot be blank (default=0).
  1577|         0|            0|            0|  0.00%|          In the :math:`(N, S)` form, targets are padded to the
  1578|         0|            0|            0|  0.00%|          length of the longest sequence, and stacked.
  1579|         0|            0|            0|  0.00%|          In the :math:`(\operatorname{sum}(\text{target\_lengths}))` form,
  1580|         0|            0|            0|  0.00%|          the targets are assumed to be un-padded and
  1581|         0|            0|            0|  0.00%|          concatenated within 1 dimension.
  1582|         0|            0|            0|  0.00%|        - Input_lengths: Tuple or tensor of size :math:`(N)`,
  1583|         0|            0|            0|  0.00%|          where :math:`N = \text{batch size}`. It represent the lengths of the
  1584|         0|            0|            0|  0.00%|          inputs (must each be :math:`\leq T`). And the lengths are specified
  1585|         0|            0|            0|  0.00%|          for each sequence to achieve masking under the assumption that sequences
  1586|         0|            0|            0|  0.00%|          are padded to equal lengths.
  1587|         0|            0|            0|  0.00%|        - Target_lengths: Tuple or tensor of size :math:`(N)`,
  1588|         0|            0|            0|  0.00%|          where :math:`N = \text{batch size}`. It represent lengths of the targets.
  1589|         0|            0|            0|  0.00%|          Lengths are specified for each sequence to achieve masking under the
  1590|         0|            0|            0|  0.00%|          assumption that sequences are padded to equal lengths. If target shape is
  1591|         0|            0|            0|  0.00%|          :math:`(N,S)`, target_lengths are effectively the stop index
  1592|         0|            0|            0|  0.00%|          :math:`s_n` for each target sequence, such that ``target_n = targets[n,0:s_n]`` for
  1593|         0|            0|            0|  0.00%|          each target in a batch. Lengths must each be :math:`\leq S`
  1594|         0|            0|            0|  0.00%|          If the targets are given as a 1d tensor that is the concatenation of individual
  1595|         0|            0|            0|  0.00%|          targets, the target_lengths must add up to the total length of the tensor.
  1596|         0|            0|            0|  0.00%|        - Output: scalar. If :attr:`reduction` is ``'none'``, then
  1597|         0|            0|            0|  0.00%|          :math:`(N)`, where :math:`N = \text{batch size}`.
  1598|         0|            0|            0|  0.00%|
  1599|         0|            0|            0|  0.00%|    Examples::
  1600|         0|            0|            0|  0.00%|
  1601|         0|            0|            0|  0.00%|        >>> # Target are to be padded
  1602|         0|            0|            0|  0.00%|        >>> T = 50      # Input sequence length
  1603|         0|            0|            0|  0.00%|        >>> C = 20      # Number of classes (including blank)
  1604|         0|            0|            0|  0.00%|        >>> N = 16      # Batch size
  1605|         0|            0|            0|  0.00%|        >>> S = 30      # Target sequence length of longest target in batch (padding length)
  1606|         0|            0|            0|  0.00%|        >>> S_min = 10  # Minimum target length, for demonstration purposes
  1607|         0|            0|            0|  0.00%|        >>>
  1608|         0|            0|            0|  0.00%|        >>> # Initialize random batch of input vectors, for *size = (T,N,C)
  1609|         0|            0|            0|  0.00%|        >>> input = torch.randn(T, N, C).log_softmax(2).detach().requires_grad_()
  1610|         0|            0|            0|  0.00%|        >>>
  1611|         0|            0|            0|  0.00%|        >>> # Initialize random batch of targets (0 = blank, 1:C = classes)
  1612|         0|            0|            0|  0.00%|        >>> target = torch.randint(low=1, high=C, size=(N, S), dtype=torch.long)
  1613|         0|            0|            0|  0.00%|        >>>
  1614|         0|            0|            0|  0.00%|        >>> input_lengths = torch.full(size=(N,), fill_value=T, dtype=torch.long)
  1615|         0|            0|            0|  0.00%|        >>> target_lengths = torch.randint(low=S_min, high=S, size=(N,), dtype=torch.long)
  1616|         0|            0|            0|  0.00%|        >>> ctc_loss = nn.CTCLoss()
  1617|         0|            0|            0|  0.00%|        >>> loss = ctc_loss(input, target, input_lengths, target_lengths)
  1618|         0|            0|            0|  0.00%|        >>> loss.backward()
  1619|         0|            0|            0|  0.00%|        >>>
  1620|         0|            0|            0|  0.00%|        >>>
  1621|         0|            0|            0|  0.00%|        >>> # Target are to be un-padded
  1622|         0|            0|            0|  0.00%|        >>> T = 50      # Input sequence length
  1623|         0|            0|            0|  0.00%|        >>> C = 20      # Number of classes (including blank)
  1624|         0|            0|            0|  0.00%|        >>> N = 16      # Batch size
  1625|         0|            0|            0|  0.00%|        >>>
  1626|         0|            0|            0|  0.00%|        >>> # Initialize random batch of input vectors, for *size = (T,N,C)
  1627|         0|            0|            0|  0.00%|        >>> input = torch.randn(T, N, C).log_softmax(2).detach().requires_grad_()
  1628|         0|            0|            0|  0.00%|        >>> input_lengths = torch.full(size=(N,), fill_value=T, dtype=torch.long)
  1629|         0|            0|            0|  0.00%|        >>>
  1630|         0|            0|            0|  0.00%|        >>> # Initialize random batch of targets (0 = blank, 1:C = classes)
  1631|         0|            0|            0|  0.00%|        >>> target_lengths = torch.randint(low=1, high=T, size=(N,), dtype=torch.long)
  1632|         0|            0|            0|  0.00%|        >>> target = torch.randint(low=1, high=C, size=(sum(target_lengths),), dtype=torch.long)
  1633|         0|            0|            0|  0.00%|        >>> ctc_loss = nn.CTCLoss()
  1634|         0|            0|            0|  0.00%|        >>> loss = ctc_loss(input, target, input_lengths, target_lengths)
  1635|         0|            0|            0|  0.00%|        >>> loss.backward()
  1636|         0|            0|            0|  0.00%|
  1637|         0|            0|            0|  0.00%|    Reference:
  1638|         0|            0|            0|  0.00%|        A. Graves et al.: Connectionist Temporal Classification:
  1639|         0|            0|            0|  0.00%|        Labelling Unsegmented Sequence Data with Recurrent Neural Networks:
  1640|         0|            0|            0|  0.00%|        https://www.cs.toronto.edu/~graves/icml_2006.pdf
  1641|         0|            0|            0|  0.00%|
  1642|         0|            0|            0|  0.00%|    Note:
  1643|         0|            0|            0|  0.00%|        In order to use CuDNN, the following must be satisfied: :attr:`targets` must be
  1644|         0|            0|            0|  0.00%|        in concatenated format, all :attr:`input_lengths` must be `T`.  :math:`blank=0`,
  1645|         0|            0|            0|  0.00%|        :attr:`target_lengths` :math:`\leq 256`, the integer arguments must be of
  1646|         0|            0|            0|  0.00%|        dtype :attr:`torch.int32`.
  1647|         0|            0|            0|  0.00%|
  1648|         0|            0|            0|  0.00%|        The regular implementation uses the (more common in PyTorch) `torch.long` dtype.
  1649|         0|            0|            0|  0.00%|
  1650|         0|            0|            0|  0.00%|
  1651|         0|            0|            0|  0.00%|    Note:
  1652|         0|            0|            0|  0.00%|        In some circumstances when using the CUDA backend with CuDNN, this operator
  1653|         0|            0|            0|  0.00%|        may select a nondeterministic algorithm to increase performance. If this is
  1654|         0|            0|            0|  0.00%|        undesirable, you can try to make the operation deterministic (potentially at
  1655|         0|            0|            0|  0.00%|        a performance cost) by setting ``torch.backends.cudnn.deterministic =
  1656|         0|            0|            0|  0.00%|        True``.
  1657|         0|            0|            0|  0.00%|        Please see the notes on :doc:`/notes/randomness` for background.
  1658|         0|            0|            0|  0.00%|    """
  1659|         0|            0|            0|  0.00%|    __constants__ = ['blank', 'reduction']
  1660|         0|            0|            0|  0.00%|    blank: int
  1661|         0|            0|            0|  0.00%|    zero_infinity: bool
  1662|         0|            0|            0|  0.00%|
  1663|         0|            0|            0|  0.00%|    def __init__(self, blank: int = 0, reduction: str = 'mean', zero_infinity: bool = False):
  1664|         0|            0|            0|  0.00%|        super(CTCLoss, self).__init__(reduction=reduction)
  1665|         0|            0|            0|  0.00%|        self.blank = blank
  1666|         0|            0|            0|  0.00%|        self.zero_infinity = zero_infinity
  1667|         0|            0|            0|  0.00%|
  1668|         0|            0|            0|  0.00%|    def forward(self, log_probs: Tensor, targets: Tensor, input_lengths: Tensor, target_lengths: Tensor) -> Tensor:
  1669|         0|            0|            0|  0.00%|        return F.ctc_loss(log_probs, targets, input_lengths, target_lengths, self.blank, self.reduction,
  1670|         0|            0|            0|  0.00%|                          self.zero_infinity)
  1671|         0|            0|            0|  0.00%|
  1672|         0|            0|            0|  0.00%|# TODO: L1HingeEmbeddingCriterion
  1673|         0|            0|            0|  0.00%|# TODO: MSECriterion weight
  1674|         0|            0|            0|  0.00%|# TODO: ClassSimplexCriterion
File: /opt/conda/lib/python3.8/weakref.py
File duration: 0.00514007s (0.01%)
Line #|      Hits|         Time| Time per hit|      %|Source code
------+----------+-------------+-------------+-------+-----------
     1|         0|            0|            0|  0.00%|"""Weak reference support for Python.
     2|         0|            0|            0|  0.00%|
     3|         0|            0|            0|  0.00%|This module is an implementation of PEP 205:
     4|         0|            0|            0|  0.00%|
     5|         0|            0|            0|  0.00%|http://www.python.org/dev/peps/pep-0205/
     6|         0|            0|            0|  0.00%|"""
     7|         0|            0|            0|  0.00%|
     8|         0|            0|            0|  0.00%|# Naming convention: Variables named "wr" are weak reference objects;
     9|         0|            0|            0|  0.00%|# they are called this instead of "ref" to avoid name collisions with
    10|         0|            0|            0|  0.00%|# the module-global ref() function imported from _weakref.
    11|         0|            0|            0|  0.00%|
    12|         0|            0|            0|  0.00%|from _weakref import (
    13|         0|            0|            0|  0.00%|     getweakrefcount,
    14|         0|            0|            0|  0.00%|     getweakrefs,
    15|         0|            0|            0|  0.00%|     ref,
    16|         0|            0|            0|  0.00%|     proxy,
    17|         0|            0|            0|  0.00%|     CallableProxyType,
    18|         0|            0|            0|  0.00%|     ProxyType,
    19|         0|            0|            0|  0.00%|     ReferenceType,
    20|         0|            0|            0|  0.00%|     _remove_dead_weakref)
    21|         0|            0|            0|  0.00%|
    22|         0|            0|            0|  0.00%|from _weakrefset import WeakSet, _IterationGuard
    23|         0|            0|            0|  0.00%|
    24|         0|            0|            0|  0.00%|import _collections_abc  # Import after _weakref to avoid circular import.
    25|         0|            0|            0|  0.00%|import sys
    26|         0|            0|            0|  0.00%|import itertools
    27|         0|            0|            0|  0.00%|
    28|         0|            0|            0|  0.00%|ProxyTypes = (ProxyType, CallableProxyType)
    29|         0|            0|            0|  0.00%|
    30|         0|            0|            0|  0.00%|__all__ = ["ref", "proxy", "getweakrefcount", "getweakrefs",
    31|         0|            0|            0|  0.00%|           "WeakKeyDictionary", "ReferenceType", "ProxyType",
    32|         0|            0|            0|  0.00%|           "CallableProxyType", "ProxyTypes", "WeakValueDictionary",
    33|         0|            0|            0|  0.00%|           "WeakSet", "WeakMethod", "finalize"]
    34|         0|            0|            0|  0.00%|
    35|         0|            0|            0|  0.00%|
    36|         0|            0|            0|  0.00%|class WeakMethod(ref):
    37|         0|            0|            0|  0.00%|    """
    38|         0|            0|            0|  0.00%|    A custom `weakref.ref` subclass which simulates a weak reference to
    39|         0|            0|            0|  0.00%|    a bound method, working around the lifetime problem of bound methods.
    40|         0|            0|            0|  0.00%|    """
    41|         0|            0|            0|  0.00%|
    42|         0|            0|            0|  0.00%|    __slots__ = "_func_ref", "_meth_type", "_alive", "__weakref__"
    43|         0|            0|            0|  0.00%|
    44|         0|            0|            0|  0.00%|    def __new__(cls, meth, callback=None):
    45|         0|            0|            0|  0.00%|        try:
    46|         0|            0|            0|  0.00%|            obj = meth.__self__
    47|         0|            0|            0|  0.00%|            func = meth.__func__
    48|         0|            0|            0|  0.00%|        except AttributeError:
    49|         0|            0|            0|  0.00%|            raise TypeError("argument should be a bound method, not {}"
    50|         0|            0|            0|  0.00%|                            .format(type(meth))) from None
    51|         0|            0|            0|  0.00%|        def _cb(arg):
    52|         0|            0|            0|  0.00%|            # The self-weakref trick is needed to avoid creating a reference
    53|         0|            0|            0|  0.00%|            # cycle.
    54|         0|            0|            0|  0.00%|            self = self_wr()
    55|         0|            0|            0|  0.00%|            if self._alive:
    56|         0|            0|            0|  0.00%|                self._alive = False
    57|         0|            0|            0|  0.00%|                if callback is not None:
    58|         0|            0|            0|  0.00%|                    callback(self)
    59|         0|            0|            0|  0.00%|        self = ref.__new__(cls, obj, _cb)
    60|         0|            0|            0|  0.00%|        self._func_ref = ref(func, _cb)
    61|         0|            0|            0|  0.00%|        self._meth_type = type(meth)
    62|         0|            0|            0|  0.00%|        self._alive = True
    63|         0|            0|            0|  0.00%|        self_wr = ref(self)
    64|         0|            0|            0|  0.00%|        return self
    65|         0|            0|            0|  0.00%|
    66|         0|            0|            0|  0.00%|    def __call__(self):
    67|         0|            0|            0|  0.00%|        obj = super().__call__()
    68|         0|            0|            0|  0.00%|        func = self._func_ref()
    69|         0|            0|            0|  0.00%|        if obj is None or func is None:
    70|         0|            0|            0|  0.00%|            return None
    71|         0|            0|            0|  0.00%|        return self._meth_type(func, obj)
    72|         0|            0|            0|  0.00%|
    73|         0|            0|            0|  0.00%|    def __eq__(self, other):
    74|         0|            0|            0|  0.00%|        if isinstance(other, WeakMethod):
    75|         0|            0|            0|  0.00%|            if not self._alive or not other._alive:
    76|         0|            0|            0|  0.00%|                return self is other
    77|         0|            0|            0|  0.00%|            return ref.__eq__(self, other) and self._func_ref == other._func_ref
    78|         0|            0|            0|  0.00%|        return False
    79|         0|            0|            0|  0.00%|
    80|         0|            0|            0|  0.00%|    def __ne__(self, other):
    81|         0|            0|            0|  0.00%|        if isinstance(other, WeakMethod):
    82|         0|            0|            0|  0.00%|            if not self._alive or not other._alive:
    83|         0|            0|            0|  0.00%|                return self is not other
    84|         0|            0|            0|  0.00%|            return ref.__ne__(self, other) or self._func_ref != other._func_ref
    85|         0|            0|            0|  0.00%|        return True
    86|         0|            0|            0|  0.00%|
    87|         0|            0|            0|  0.00%|    __hash__ = ref.__hash__
    88|         0|            0|            0|  0.00%|
    89|         0|            0|            0|  0.00%|
    90|         0|            0|            0|  0.00%|class WeakValueDictionary(_collections_abc.MutableMapping):
    91|         0|            0|            0|  0.00%|    """Mapping class that references values weakly.
    92|         0|            0|            0|  0.00%|
    93|         0|            0|            0|  0.00%|    Entries in the dictionary will be discarded when no strong
    94|         0|            0|            0|  0.00%|    reference to the value exists anymore
    95|         0|            0|            0|  0.00%|    """
    96|         0|            0|            0|  0.00%|    # We inherit the constructor without worrying about the input
    97|         0|            0|            0|  0.00%|    # dictionary; since it uses our .update() method, we get the right
    98|         0|            0|            0|  0.00%|    # checks (if the other dictionary is a WeakValueDictionary,
    99|         0|            0|            0|  0.00%|    # objects are unwrapped on the way out, and we always wrap on the
   100|         0|            0|            0|  0.00%|    # way in).
   101|         0|            0|            0|  0.00%|
   102|         0|            0|            0|  0.00%|    def __init__(self, other=(), /, **kw):
   103|        40|  0.000260115|  6.50287e-06|  0.00%|        def remove(wr, selfref=ref(self), _atomic_removal=_remove_dead_weakref):
   104|        40|  0.000211239|  5.28097e-06|  0.00%|            self = selfref()
   105|        40|  0.000154495|  3.86238e-06|  0.00%|            if self is not None:
   106|        40|  0.000164032|   4.1008e-06|  0.00%|                if self._iterating:
   107|         0|            0|            0|  0.00%|                    self._pending_removals.append(wr.key)
   108|         0|            0|            0|  0.00%|                else:
   109|         0|            0|            0|  0.00%|                    # Atomic removal is necessary since this function
   110|         0|            0|            0|  0.00%|                    # can be called asynchronously by the GC
   111|        40|  0.000289202|  7.23004e-06|  0.00%|                    _atomic_removal(self.data, wr.key)
   112|         0|            0|            0|  0.00%|        self._remove = remove
   113|         0|            0|            0|  0.00%|        # A list of keys to be removed
   114|         0|            0|            0|  0.00%|        self._pending_removals = []
   115|         0|            0|            0|  0.00%|        self._iterating = set()
   116|         0|            0|            0|  0.00%|        self.data = {}
   117|         0|            0|            0|  0.00%|        self.update(other, **kw)
   118|         0|            0|            0|  0.00%|
   119|         0|            0|            0|  0.00%|    def _commit_removals(self):
   120|         0|            0|            0|  0.00%|        l = self._pending_removals
   121|         0|            0|            0|  0.00%|        d = self.data
   122|         0|            0|            0|  0.00%|        # We shouldn't encounter any KeyError, because this method should
   123|         0|            0|            0|  0.00%|        # always be called *before* mutating the dict.
   124|         0|            0|            0|  0.00%|        while l:
   125|         0|            0|            0|  0.00%|            key = l.pop()
   126|         0|            0|            0|  0.00%|            _remove_dead_weakref(d, key)
   127|         0|            0|            0|  0.00%|
   128|         0|            0|            0|  0.00%|    def __getitem__(self, key):
   129|         0|            0|            0|  0.00%|        if self._pending_removals:
   130|         0|            0|            0|  0.00%|            self._commit_removals()
   131|         0|            0|            0|  0.00%|        o = self.data[key]()
   132|         0|            0|            0|  0.00%|        if o is None:
   133|         0|            0|            0|  0.00%|            raise KeyError(key)
   134|         0|            0|            0|  0.00%|        else:
   135|         0|            0|            0|  0.00%|            return o
   136|         0|            0|            0|  0.00%|
   137|         0|            0|            0|  0.00%|    def __delitem__(self, key):
   138|         0|            0|            0|  0.00%|        if self._pending_removals:
   139|         0|            0|            0|  0.00%|            self._commit_removals()
   140|         0|            0|            0|  0.00%|        del self.data[key]
   141|         0|            0|            0|  0.00%|
   142|         0|            0|            0|  0.00%|    def __len__(self):
   143|         0|            0|            0|  0.00%|        if self._pending_removals:
   144|         0|            0|            0|  0.00%|            self._commit_removals()
   145|         0|            0|            0|  0.00%|        return len(self.data)
   146|         0|            0|            0|  0.00%|
   147|         0|            0|            0|  0.00%|    def __contains__(self, key):
   148|         0|            0|            0|  0.00%|        if self._pending_removals:
   149|         0|            0|            0|  0.00%|            self._commit_removals()
   150|         0|            0|            0|  0.00%|        try:
   151|         0|            0|            0|  0.00%|            o = self.data[key]()
   152|         0|            0|            0|  0.00%|        except KeyError:
   153|         0|            0|            0|  0.00%|            return False
   154|         0|            0|            0|  0.00%|        return o is not None
   155|         0|            0|            0|  0.00%|
   156|         0|            0|            0|  0.00%|    def __repr__(self):
   157|         0|            0|            0|  0.00%|        return "<%s at %#x>" % (self.__class__.__name__, id(self))
   158|         0|            0|            0|  0.00%|
   159|        50|  0.000231266|  4.62532e-06|  0.00%|    def __setitem__(self, key, value):
   160|        50|  0.000287771|  5.75542e-06|  0.00%|        if self._pending_removals:
   161|         0|            0|            0|  0.00%|            self._commit_removals()
   162|        50|    0.0016439|  3.28779e-05|  0.00%|        self.data[key] = KeyedRef(value, self._remove, key)
(call)|        50|   0.00120735|   2.4147e-05|  0.00%|# /opt/conda/lib/python3.8/weakref.py:323 __new__
(call)|        50|  0.000690699|   1.3814e-05|  0.00%|# /opt/conda/lib/python3.8/weakref.py:328 __init__
   163|         0|            0|            0|  0.00%|
   164|         0|            0|            0|  0.00%|    def copy(self):
   165|         0|            0|            0|  0.00%|        if self._pending_removals:
   166|         0|            0|            0|  0.00%|            self._commit_removals()
   167|         0|            0|            0|  0.00%|        new = WeakValueDictionary()
   168|         0|            0|            0|  0.00%|        with _IterationGuard(self):
   169|         0|            0|            0|  0.00%|            for key, wr in self.data.items():
   170|         0|            0|            0|  0.00%|                o = wr()
   171|         0|            0|            0|  0.00%|                if o is not None:
   172|         0|            0|            0|  0.00%|                    new[key] = o
   173|         0|            0|            0|  0.00%|        return new
   174|         0|            0|            0|  0.00%|
   175|         0|            0|            0|  0.00%|    __copy__ = copy
   176|         0|            0|            0|  0.00%|
   177|         0|            0|            0|  0.00%|    def __deepcopy__(self, memo):
   178|         0|            0|            0|  0.00%|        from copy import deepcopy
   179|         0|            0|            0|  0.00%|        if self._pending_removals:
   180|         0|            0|            0|  0.00%|            self._commit_removals()
   181|         0|            0|            0|  0.00%|        new = self.__class__()
   182|         0|            0|            0|  0.00%|        with _IterationGuard(self):
   183|         0|            0|            0|  0.00%|            for key, wr in self.data.items():
   184|         0|            0|            0|  0.00%|                o = wr()
   185|         0|            0|            0|  0.00%|                if o is not None:
   186|         0|            0|            0|  0.00%|                    new[deepcopy(key, memo)] = o
   187|         0|            0|            0|  0.00%|        return new
   188|         0|            0|            0|  0.00%|
   189|         0|            0|            0|  0.00%|    def get(self, key, default=None):
   190|         0|            0|            0|  0.00%|        if self._pending_removals:
   191|         0|            0|            0|  0.00%|            self._commit_removals()
   192|         0|            0|            0|  0.00%|        try:
   193|         0|            0|            0|  0.00%|            wr = self.data[key]
   194|         0|            0|            0|  0.00%|        except KeyError:
   195|         0|            0|            0|  0.00%|            return default
   196|         0|            0|            0|  0.00%|        else:
   197|         0|            0|            0|  0.00%|            o = wr()
   198|         0|            0|            0|  0.00%|            if o is None:
   199|         0|            0|            0|  0.00%|                # This should only happen
   200|         0|            0|            0|  0.00%|                return default
   201|         0|            0|            0|  0.00%|            else:
   202|         0|            0|            0|  0.00%|                return o
   203|         0|            0|            0|  0.00%|
   204|         0|            0|            0|  0.00%|    def items(self):
   205|         0|            0|            0|  0.00%|        if self._pending_removals:
   206|         0|            0|            0|  0.00%|            self._commit_removals()
   207|         0|            0|            0|  0.00%|        with _IterationGuard(self):
   208|         0|            0|            0|  0.00%|            for k, wr in self.data.items():
   209|         0|            0|            0|  0.00%|                v = wr()
   210|         0|            0|            0|  0.00%|                if v is not None:
   211|         0|            0|            0|  0.00%|                    yield k, v
   212|         0|            0|            0|  0.00%|
   213|         0|            0|            0|  0.00%|    def keys(self):
   214|         0|            0|            0|  0.00%|        if self._pending_removals:
   215|         0|            0|            0|  0.00%|            self._commit_removals()
   216|         0|            0|            0|  0.00%|        with _IterationGuard(self):
   217|         0|            0|            0|  0.00%|            for k, wr in self.data.items():
   218|         0|            0|            0|  0.00%|                if wr() is not None:
   219|         0|            0|            0|  0.00%|                    yield k
   220|         0|            0|            0|  0.00%|
   221|         0|            0|            0|  0.00%|    __iter__ = keys
   222|         0|            0|            0|  0.00%|
   223|         0|            0|            0|  0.00%|    def itervaluerefs(self):
   224|         0|            0|            0|  0.00%|        """Return an iterator that yields the weak references to the values.
   225|         0|            0|            0|  0.00%|
   226|         0|            0|            0|  0.00%|        The references are not guaranteed to be 'live' at the time
   227|         0|            0|            0|  0.00%|        they are used, so the result of calling the references needs
   228|         0|            0|            0|  0.00%|        to be checked before being used.  This can be used to avoid
   229|         0|            0|            0|  0.00%|        creating references that will cause the garbage collector to
   230|         0|            0|            0|  0.00%|        keep the values around longer than needed.
   231|         0|            0|            0|  0.00%|
   232|         0|            0|            0|  0.00%|        """
   233|         0|            0|            0|  0.00%|        if self._pending_removals:
   234|         0|            0|            0|  0.00%|            self._commit_removals()
   235|         0|            0|            0|  0.00%|        with _IterationGuard(self):
   236|         0|            0|            0|  0.00%|            yield from self.data.values()
   237|         0|            0|            0|  0.00%|
   238|         0|            0|            0|  0.00%|    def values(self):
   239|         0|            0|            0|  0.00%|        if self._pending_removals:
   240|         0|            0|            0|  0.00%|            self._commit_removals()
   241|         0|            0|            0|  0.00%|        with _IterationGuard(self):
   242|         0|            0|            0|  0.00%|            for wr in self.data.values():
   243|         0|            0|            0|  0.00%|                obj = wr()
   244|         0|            0|            0|  0.00%|                if obj is not None:
   245|         0|            0|            0|  0.00%|                    yield obj
   246|         0|            0|            0|  0.00%|
   247|         0|            0|            0|  0.00%|    def popitem(self):
   248|         0|            0|            0|  0.00%|        if self._pending_removals:
   249|         0|            0|            0|  0.00%|            self._commit_removals()
   250|         0|            0|            0|  0.00%|        while True:
   251|         0|            0|            0|  0.00%|            key, wr = self.data.popitem()
   252|         0|            0|            0|  0.00%|            o = wr()
   253|         0|            0|            0|  0.00%|            if o is not None:
   254|         0|            0|            0|  0.00%|                return key, o
   255|         0|            0|            0|  0.00%|
   256|         0|            0|            0|  0.00%|    def pop(self, key, *args):
   257|         0|            0|            0|  0.00%|        if self._pending_removals:
   258|         0|            0|            0|  0.00%|            self._commit_removals()
   259|         0|            0|            0|  0.00%|        try:
   260|         0|            0|            0|  0.00%|            o = self.data.pop(key)()
   261|         0|            0|            0|  0.00%|        except KeyError:
   262|         0|            0|            0|  0.00%|            o = None
   263|         0|            0|            0|  0.00%|        if o is None:
   264|         0|            0|            0|  0.00%|            if args:
   265|         0|            0|            0|  0.00%|                return args[0]
   266|         0|            0|            0|  0.00%|            else:
   267|         0|            0|            0|  0.00%|                raise KeyError(key)
   268|         0|            0|            0|  0.00%|        else:
   269|         0|            0|            0|  0.00%|            return o
   270|         0|            0|            0|  0.00%|
   271|         0|            0|            0|  0.00%|    def setdefault(self, key, default=None):
   272|         0|            0|            0|  0.00%|        try:
   273|         0|            0|            0|  0.00%|            o = self.data[key]()
   274|         0|            0|            0|  0.00%|        except KeyError:
   275|         0|            0|            0|  0.00%|            o = None
   276|         0|            0|            0|  0.00%|        if o is None:
   277|         0|            0|            0|  0.00%|            if self._pending_removals:
   278|         0|            0|            0|  0.00%|                self._commit_removals()
   279|         0|            0|            0|  0.00%|            self.data[key] = KeyedRef(default, self._remove, key)
   280|         0|            0|            0|  0.00%|            return default
   281|         0|            0|            0|  0.00%|        else:
   282|         0|            0|            0|  0.00%|            return o
   283|         0|            0|            0|  0.00%|
   284|         0|            0|            0|  0.00%|    def update(self, other=None, /, **kwargs):
   285|         0|            0|            0|  0.00%|        if self._pending_removals:
   286|         0|            0|            0|  0.00%|            self._commit_removals()
   287|         0|            0|            0|  0.00%|        d = self.data
   288|         0|            0|            0|  0.00%|        if other is not None:
   289|         0|            0|            0|  0.00%|            if not hasattr(other, "items"):
   290|         0|            0|            0|  0.00%|                other = dict(other)
   291|         0|            0|            0|  0.00%|            for key, o in other.items():
   292|         0|            0|            0|  0.00%|                d[key] = KeyedRef(o, self._remove, key)
   293|         0|            0|            0|  0.00%|        for key, o in kwargs.items():
   294|         0|            0|            0|  0.00%|            d[key] = KeyedRef(o, self._remove, key)
   295|         0|            0|            0|  0.00%|
   296|         0|            0|            0|  0.00%|    def valuerefs(self):
   297|         0|            0|            0|  0.00%|        """Return a list of weak references to the values.
   298|         0|            0|            0|  0.00%|
   299|         0|            0|            0|  0.00%|        The references are not guaranteed to be 'live' at the time
   300|         0|            0|            0|  0.00%|        they are used, so the result of calling the references needs
   301|         0|            0|            0|  0.00%|        to be checked before being used.  This can be used to avoid
   302|         0|            0|            0|  0.00%|        creating references that will cause the garbage collector to
   303|         0|            0|            0|  0.00%|        keep the values around longer than needed.
   304|         0|            0|            0|  0.00%|
   305|         0|            0|            0|  0.00%|        """
   306|         0|            0|            0|  0.00%|        if self._pending_removals:
   307|         0|            0|            0|  0.00%|            self._commit_removals()
   308|         0|            0|            0|  0.00%|        return list(self.data.values())
   309|         0|            0|            0|  0.00%|
   310|         0|            0|            0|  0.00%|
   311|         0|            0|            0|  0.00%|class KeyedRef(ref):
   312|         0|            0|            0|  0.00%|    """Specialized reference that includes a key corresponding to the value.
   313|         0|            0|            0|  0.00%|
   314|         0|            0|            0|  0.00%|    This is used in the WeakValueDictionary to avoid having to create
   315|         0|            0|            0|  0.00%|    a function object for each key stored in the mapping.  A shared
   316|         0|            0|            0|  0.00%|    callback object can use the 'key' attribute of a KeyedRef instead
   317|         0|            0|            0|  0.00%|    of getting a reference to the key from an enclosing scope.
   318|         0|            0|            0|  0.00%|
   319|         0|            0|            0|  0.00%|    """
   320|         0|            0|            0|  0.00%|
   321|         0|            0|            0|  0.00%|    __slots__ = "key",
   322|         0|            0|            0|  0.00%|
   323|        50|  0.000261545|   5.2309e-06|  0.00%|    def __new__(type, ob, callback, key):
   324|        50|  0.000479937|  9.59873e-06|  0.00%|        self = ref.__new__(type, ob, callback)
   325|        50|  0.000262499|  5.24998e-06|  0.00%|        self.key = key
   326|        50|  0.000203371|  4.06742e-06|  0.00%|        return self
   327|         0|            0|            0|  0.00%|
   328|        50|  0.000251532|  5.03063e-06|  0.00%|    def __init__(self, ob, callback, key):
   329|        50|  0.000439167|  8.78334e-06|  0.00%|        super().__init__(ob, callback)
   330|         0|            0|            0|  0.00%|
   331|         0|            0|            0|  0.00%|
   332|         0|            0|            0|  0.00%|class WeakKeyDictionary(_collections_abc.MutableMapping):
   333|         0|            0|            0|  0.00%|    """ Mapping class that references keys weakly.
   334|         0|            0|            0|  0.00%|
   335|         0|            0|            0|  0.00%|    Entries in the dictionary will be discarded when there is no
   336|         0|            0|            0|  0.00%|    longer a strong reference to the key. This can be used to
   337|         0|            0|            0|  0.00%|    associate additional data with an object owned by other parts of
   338|         0|            0|            0|  0.00%|    an application without adding attributes to those objects. This
   339|         0|            0|            0|  0.00%|    can be especially useful with objects that override attribute
   340|         0|            0|            0|  0.00%|    accesses.
   341|         0|            0|            0|  0.00%|    """
   342|         0|            0|            0|  0.00%|
   343|         0|            0|            0|  0.00%|    def __init__(self, dict=None):
   344|         0|            0|            0|  0.00%|        self.data = {}
   345|         0|            0|            0|  0.00%|        def remove(k, selfref=ref(self)):
   346|         0|            0|            0|  0.00%|            self = selfref()
   347|         0|            0|            0|  0.00%|            if self is not None:
   348|         0|            0|            0|  0.00%|                if self._iterating:
   349|         0|            0|            0|  0.00%|                    self._pending_removals.append(k)
   350|         0|            0|            0|  0.00%|                else:
   351|         0|            0|            0|  0.00%|                    del self.data[k]
   352|         0|            0|            0|  0.00%|        self._remove = remove
   353|         0|            0|            0|  0.00%|        # A list of dead weakrefs (keys to be removed)
   354|         0|            0|            0|  0.00%|        self._pending_removals = []
   355|         0|            0|            0|  0.00%|        self._iterating = set()
   356|         0|            0|            0|  0.00%|        self._dirty_len = False
   357|         0|            0|            0|  0.00%|        if dict is not None:
   358|         0|            0|            0|  0.00%|            self.update(dict)
   359|         0|            0|            0|  0.00%|
   360|         0|            0|            0|  0.00%|    def _commit_removals(self):
   361|         0|            0|            0|  0.00%|        # NOTE: We don't need to call this method before mutating the dict,
   362|         0|            0|            0|  0.00%|        # because a dead weakref never compares equal to a live weakref,
   363|         0|            0|            0|  0.00%|        # even if they happened to refer to equal objects.
   364|         0|            0|            0|  0.00%|        # However, it means keys may already have been removed.
   365|         0|            0|            0|  0.00%|        l = self._pending_removals
   366|         0|            0|            0|  0.00%|        d = self.data
   367|         0|            0|            0|  0.00%|        while l:
   368|         0|            0|            0|  0.00%|            try:
   369|         0|            0|            0|  0.00%|                del d[l.pop()]
   370|         0|            0|            0|  0.00%|            except KeyError:
   371|         0|            0|            0|  0.00%|                pass
   372|         0|            0|            0|  0.00%|
   373|         0|            0|            0|  0.00%|    def _scrub_removals(self):
   374|         0|            0|            0|  0.00%|        d = self.data
   375|         0|            0|            0|  0.00%|        self._pending_removals = [k for k in self._pending_removals if k in d]
   376|         0|            0|            0|  0.00%|        self._dirty_len = False
   377|         0|            0|            0|  0.00%|
   378|         0|            0|            0|  0.00%|    def __delitem__(self, key):
   379|         0|            0|            0|  0.00%|        self._dirty_len = True
   380|         0|            0|            0|  0.00%|        del self.data[ref(key)]
   381|         0|            0|            0|  0.00%|
   382|         0|            0|            0|  0.00%|    def __getitem__(self, key):
   383|         0|            0|            0|  0.00%|        return self.data[ref(key)]
   384|         0|            0|            0|  0.00%|
   385|         0|            0|            0|  0.00%|    def __len__(self):
   386|         0|            0|            0|  0.00%|        if self._dirty_len and self._pending_removals:
   387|         0|            0|            0|  0.00%|            # self._pending_removals may still contain keys which were
   388|         0|            0|            0|  0.00%|            # explicitly removed, we have to scrub them (see issue #21173).
   389|         0|            0|            0|  0.00%|            self._scrub_removals()
   390|         0|            0|            0|  0.00%|        return len(self.data) - len(self._pending_removals)
   391|         0|            0|            0|  0.00%|
   392|         0|            0|            0|  0.00%|    def __repr__(self):
   393|         0|            0|            0|  0.00%|        return "<%s at %#x>" % (self.__class__.__name__, id(self))
   394|         0|            0|            0|  0.00%|
   395|         0|            0|            0|  0.00%|    def __setitem__(self, key, value):
   396|         0|            0|            0|  0.00%|        self.data[ref(key, self._remove)] = value
   397|         0|            0|            0|  0.00%|
   398|         0|            0|            0|  0.00%|    def copy(self):
   399|         0|            0|            0|  0.00%|        new = WeakKeyDictionary()
   400|         0|            0|            0|  0.00%|        with _IterationGuard(self):
   401|         0|            0|            0|  0.00%|            for key, value in self.data.items():
   402|         0|            0|            0|  0.00%|                o = key()
   403|         0|            0|            0|  0.00%|                if o is not None:
   404|         0|            0|            0|  0.00%|                    new[o] = value
   405|         0|            0|            0|  0.00%|        return new
   406|         0|            0|            0|  0.00%|
   407|         0|            0|            0|  0.00%|    __copy__ = copy
   408|         0|            0|            0|  0.00%|
   409|         0|            0|            0|  0.00%|    def __deepcopy__(self, memo):
   410|         0|            0|            0|  0.00%|        from copy import deepcopy
   411|         0|            0|            0|  0.00%|        new = self.__class__()
   412|         0|            0|            0|  0.00%|        with _IterationGuard(self):
   413|         0|            0|            0|  0.00%|            for key, value in self.data.items():
   414|         0|            0|            0|  0.00%|                o = key()
   415|         0|            0|            0|  0.00%|                if o is not None:
   416|         0|            0|            0|  0.00%|                    new[o] = deepcopy(value, memo)
   417|         0|            0|            0|  0.00%|        return new
   418|         0|            0|            0|  0.00%|
   419|         0|            0|            0|  0.00%|    def get(self, key, default=None):
   420|         0|            0|            0|  0.00%|        return self.data.get(ref(key),default)
   421|         0|            0|            0|  0.00%|
   422|         0|            0|            0|  0.00%|    def __contains__(self, key):
   423|         0|            0|            0|  0.00%|        try:
   424|         0|            0|            0|  0.00%|            wr = ref(key)
   425|         0|            0|            0|  0.00%|        except TypeError:
   426|         0|            0|            0|  0.00%|            return False
   427|         0|            0|            0|  0.00%|        return wr in self.data
   428|         0|            0|            0|  0.00%|
   429|         0|            0|            0|  0.00%|    def items(self):
   430|         0|            0|            0|  0.00%|        with _IterationGuard(self):
   431|         0|            0|            0|  0.00%|            for wr, value in self.data.items():
   432|         0|            0|            0|  0.00%|                key = wr()
   433|         0|            0|            0|  0.00%|                if key is not None:
   434|         0|            0|            0|  0.00%|                    yield key, value
   435|         0|            0|            0|  0.00%|
   436|         0|            0|            0|  0.00%|    def keys(self):
   437|         0|            0|            0|  0.00%|        with _IterationGuard(self):
   438|         0|            0|            0|  0.00%|            for wr in self.data:
   439|         0|            0|            0|  0.00%|                obj = wr()
   440|         0|            0|            0|  0.00%|                if obj is not None:
   441|         0|            0|            0|  0.00%|                    yield obj
   442|         0|            0|            0|  0.00%|
   443|         0|            0|            0|  0.00%|    __iter__ = keys
   444|         0|            0|            0|  0.00%|
   445|         0|            0|            0|  0.00%|    def values(self):
   446|         0|            0|            0|  0.00%|        with _IterationGuard(self):
   447|         0|            0|            0|  0.00%|            for wr, value in self.data.items():
   448|         0|            0|            0|  0.00%|                if wr() is not None:
   449|         0|            0|            0|  0.00%|                    yield value
   450|         0|            0|            0|  0.00%|
   451|         0|            0|            0|  0.00%|    def keyrefs(self):
   452|         0|            0|            0|  0.00%|        """Return a list of weak references to the keys.
   453|         0|            0|            0|  0.00%|
   454|         0|            0|            0|  0.00%|        The references are not guaranteed to be 'live' at the time
   455|         0|            0|            0|  0.00%|        they are used, so the result of calling the references needs
   456|         0|            0|            0|  0.00%|        to be checked before being used.  This can be used to avoid
   457|         0|            0|            0|  0.00%|        creating references that will cause the garbage collector to
   458|         0|            0|            0|  0.00%|        keep the keys around longer than needed.
   459|         0|            0|            0|  0.00%|
   460|         0|            0|            0|  0.00%|        """
   461|         0|            0|            0|  0.00%|        return list(self.data)
   462|         0|            0|            0|  0.00%|
   463|         0|            0|            0|  0.00%|    def popitem(self):
   464|         0|            0|            0|  0.00%|        self._dirty_len = True
   465|         0|            0|            0|  0.00%|        while True:
   466|         0|            0|            0|  0.00%|            key, value = self.data.popitem()
   467|         0|            0|            0|  0.00%|            o = key()
   468|         0|            0|            0|  0.00%|            if o is not None:
   469|         0|            0|            0|  0.00%|                return o, value
   470|         0|            0|            0|  0.00%|
   471|         0|            0|            0|  0.00%|    def pop(self, key, *args):
   472|         0|            0|            0|  0.00%|        self._dirty_len = True
   473|         0|            0|            0|  0.00%|        return self.data.pop(ref(key), *args)
   474|         0|            0|            0|  0.00%|
   475|         0|            0|            0|  0.00%|    def setdefault(self, key, default=None):
   476|         0|            0|            0|  0.00%|        return self.data.setdefault(ref(key, self._remove),default)
   477|         0|            0|            0|  0.00%|
   478|         0|            0|            0|  0.00%|    def update(self, dict=None, /, **kwargs):
   479|         0|            0|            0|  0.00%|        d = self.data
   480|         0|            0|            0|  0.00%|        if dict is not None:
   481|         0|            0|            0|  0.00%|            if not hasattr(dict, "items"):
   482|         0|            0|            0|  0.00%|                dict = type({})(dict)
   483|         0|            0|            0|  0.00%|            for key, value in dict.items():
   484|         0|            0|            0|  0.00%|                d[ref(key, self._remove)] = value
   485|         0|            0|            0|  0.00%|        if len(kwargs):
   486|         0|            0|            0|  0.00%|            self.update(kwargs)
   487|         0|            0|            0|  0.00%|
   488|         0|            0|            0|  0.00%|
   489|         0|            0|            0|  0.00%|class finalize:
   490|         0|            0|            0|  0.00%|    """Class for finalization of weakrefable objects
   491|         0|            0|            0|  0.00%|
   492|         0|            0|            0|  0.00%|    finalize(obj, func, *args, **kwargs) returns a callable finalizer
   493|         0|            0|            0|  0.00%|    object which will be called when obj is garbage collected. The
   494|         0|            0|            0|  0.00%|    first time the finalizer is called it evaluates func(*arg, **kwargs)
   495|         0|            0|            0|  0.00%|    and returns the result. After this the finalizer is dead, and
   496|         0|            0|            0|  0.00%|    calling it just returns None.
   497|         0|            0|            0|  0.00%|
   498|         0|            0|            0|  0.00%|    When the program exits any remaining finalizers for which the
   499|         0|            0|            0|  0.00%|    atexit attribute is true will be run in reverse order of creation.
   500|         0|            0|            0|  0.00%|    By default atexit is true.
   501|         0|            0|            0|  0.00%|    """
   502|         0|            0|            0|  0.00%|
   503|         0|            0|            0|  0.00%|    # Finalizer objects don't have any state of their own.  They are
   504|         0|            0|            0|  0.00%|    # just used as keys to lookup _Info objects in the registry.  This
   505|         0|            0|            0|  0.00%|    # ensures that they cannot be part of a ref-cycle.
   506|         0|            0|            0|  0.00%|
   507|         0|            0|            0|  0.00%|    __slots__ = ()
   508|         0|            0|            0|  0.00%|    _registry = {}
   509|         0|            0|            0|  0.00%|    _shutdown = False
   510|         0|            0|            0|  0.00%|    _index_iter = itertools.count()
   511|         0|            0|            0|  0.00%|    _dirty = False
   512|         0|            0|            0|  0.00%|    _registered_with_atexit = False
   513|         0|            0|            0|  0.00%|
   514|         0|            0|            0|  0.00%|    class _Info:
   515|         0|            0|            0|  0.00%|        __slots__ = ("weakref", "func", "args", "kwargs", "atexit", "index")
   516|         0|            0|            0|  0.00%|
   517|         0|            0|            0|  0.00%|    def __init__(*args, **kwargs):
   518|         0|            0|            0|  0.00%|        if len(args) >= 3:
   519|         0|            0|            0|  0.00%|            self, obj, func, *args = args
   520|         0|            0|            0|  0.00%|        elif not args:
   521|         0|            0|            0|  0.00%|            raise TypeError("descriptor '__init__' of 'finalize' object "
   522|         0|            0|            0|  0.00%|                            "needs an argument")
   523|         0|            0|            0|  0.00%|        else:
   524|         0|            0|            0|  0.00%|            if 'func' not in kwargs:
   525|         0|            0|            0|  0.00%|                raise TypeError('finalize expected at least 2 positional '
   526|         0|            0|            0|  0.00%|                                'arguments, got %d' % (len(args)-1))
   527|         0|            0|            0|  0.00%|            func = kwargs.pop('func')
   528|         0|            0|            0|  0.00%|            if len(args) >= 2:
   529|         0|            0|            0|  0.00%|                self, obj, *args = args
   530|         0|            0|            0|  0.00%|                import warnings
   531|         0|            0|            0|  0.00%|                warnings.warn("Passing 'func' as keyword argument is deprecated",
   532|         0|            0|            0|  0.00%|                              DeprecationWarning, stacklevel=2)
   533|         0|            0|            0|  0.00%|            else:
   534|         0|            0|            0|  0.00%|                if 'obj' not in kwargs:
   535|         0|            0|            0|  0.00%|                    raise TypeError('finalize expected at least 2 positional '
   536|         0|            0|            0|  0.00%|                                    'arguments, got %d' % (len(args)-1))
   537|         0|            0|            0|  0.00%|                obj = kwargs.pop('obj')
   538|         0|            0|            0|  0.00%|                self, *args = args
   539|         0|            0|            0|  0.00%|                import warnings
   540|         0|            0|            0|  0.00%|                warnings.warn("Passing 'obj' as keyword argument is deprecated",
   541|         0|            0|            0|  0.00%|                              DeprecationWarning, stacklevel=2)
   542|         0|            0|            0|  0.00%|        args = tuple(args)
   543|         0|            0|            0|  0.00%|
   544|         0|            0|            0|  0.00%|        if not self._registered_with_atexit:
   545|         0|            0|            0|  0.00%|            # We may register the exit function more than once because
   546|         0|            0|            0|  0.00%|            # of a thread race, but that is harmless
   547|         0|            0|            0|  0.00%|            import atexit
   548|         0|            0|            0|  0.00%|            atexit.register(self._exitfunc)
   549|         0|            0|            0|  0.00%|            finalize._registered_with_atexit = True
   550|         0|            0|            0|  0.00%|        info = self._Info()
   551|         0|            0|            0|  0.00%|        info.weakref = ref(obj, self)
   552|         0|            0|            0|  0.00%|        info.func = func
   553|         0|            0|            0|  0.00%|        info.args = args
   554|         0|            0|            0|  0.00%|        info.kwargs = kwargs or None
   555|         0|            0|            0|  0.00%|        info.atexit = True
   556|         0|            0|            0|  0.00%|        info.index = next(self._index_iter)
   557|         0|            0|            0|  0.00%|        self._registry[self] = info
   558|         0|            0|            0|  0.00%|        finalize._dirty = True
   559|         0|            0|            0|  0.00%|    __init__.__text_signature__ = '($self, obj, func, /, *args, **kwargs)'
   560|         0|            0|            0|  0.00%|
   561|         0|            0|            0|  0.00%|    def __call__(self, _=None):
   562|         0|            0|            0|  0.00%|        """If alive then mark as dead and return func(*args, **kwargs);
   563|         0|            0|            0|  0.00%|        otherwise return None"""
   564|         0|            0|            0|  0.00%|        info = self._registry.pop(self, None)
   565|         0|            0|            0|  0.00%|        if info and not self._shutdown:
   566|         0|            0|            0|  0.00%|            return info.func(*info.args, **(info.kwargs or {}))
   567|         0|            0|            0|  0.00%|
   568|         0|            0|            0|  0.00%|    def detach(self):
   569|         0|            0|            0|  0.00%|        """If alive then mark as dead and return (obj, func, args, kwargs);
   570|         0|            0|            0|  0.00%|        otherwise return None"""
   571|         0|            0|            0|  0.00%|        info = self._registry.get(self)
   572|         0|            0|            0|  0.00%|        obj = info and info.weakref()
   573|         0|            0|            0|  0.00%|        if obj is not None and self._registry.pop(self, None):
   574|         0|            0|            0|  0.00%|            return (obj, info.func, info.args, info.kwargs or {})
   575|         0|            0|            0|  0.00%|
   576|         0|            0|            0|  0.00%|    def peek(self):
   577|         0|            0|            0|  0.00%|        """If alive then return (obj, func, args, kwargs);
   578|         0|            0|            0|  0.00%|        otherwise return None"""
   579|         0|            0|            0|  0.00%|        info = self._registry.get(self)
   580|         0|            0|            0|  0.00%|        obj = info and info.weakref()
   581|         0|            0|            0|  0.00%|        if obj is not None:
   582|         0|            0|            0|  0.00%|            return (obj, info.func, info.args, info.kwargs or {})
   583|         0|            0|            0|  0.00%|
   584|         0|            0|            0|  0.00%|    @property
   585|         0|            0|            0|  0.00%|    def alive(self):
   586|         0|            0|            0|  0.00%|        """Whether finalizer is alive"""
   587|         0|            0|            0|  0.00%|        return self in self._registry
   588|         0|            0|            0|  0.00%|
   589|         0|            0|            0|  0.00%|    @property
   590|         0|            0|            0|  0.00%|    def atexit(self):
   591|         0|            0|            0|  0.00%|        """Whether finalizer should be called at exit"""
   592|         0|            0|            0|  0.00%|        info = self._registry.get(self)
   593|         0|            0|            0|  0.00%|        return bool(info) and info.atexit
   594|         0|            0|            0|  0.00%|
   595|         0|            0|            0|  0.00%|    @atexit.setter
   596|         0|            0|            0|  0.00%|    def atexit(self, value):
   597|         0|            0|            0|  0.00%|        info = self._registry.get(self)
   598|         0|            0|            0|  0.00%|        if info:
   599|         0|            0|            0|  0.00%|            info.atexit = bool(value)
   600|         0|            0|            0|  0.00%|
   601|         0|            0|            0|  0.00%|    def __repr__(self):
   602|         0|            0|            0|  0.00%|        info = self._registry.get(self)
   603|         0|            0|            0|  0.00%|        obj = info and info.weakref()
   604|         0|            0|            0|  0.00%|        if obj is None:
   605|         0|            0|            0|  0.00%|            return '<%s object at %#x; dead>' % (type(self).__name__, id(self))
   606|         0|            0|            0|  0.00%|        else:
   607|         0|            0|            0|  0.00%|            return '<%s object at %#x; for %r at %#x>' % \
   608|         0|            0|            0|  0.00%|                (type(self).__name__, id(self), type(obj).__name__, id(obj))
   609|         0|            0|            0|  0.00%|
   610|         0|            0|            0|  0.00%|    @classmethod
   611|         0|            0|            0|  0.00%|    def _select_for_exit(cls):
   612|         0|            0|            0|  0.00%|        # Return live finalizers marked for exit, oldest first
   613|         0|            0|            0|  0.00%|        L = [(f,i) for (f,i) in cls._registry.items() if i.atexit]
   614|         0|            0|            0|  0.00%|        L.sort(key=lambda item:item[1].index)
   615|         0|            0|            0|  0.00%|        return [f for (f,i) in L]
   616|         0|            0|            0|  0.00%|
   617|         0|            0|            0|  0.00%|    @classmethod
   618|         0|            0|            0|  0.00%|    def _exitfunc(cls):
   619|         0|            0|            0|  0.00%|        # At shutdown invoke finalizers for which atexit is true.
   620|         0|            0|            0|  0.00%|        # This is called once all other non-daemonic threads have been
   621|         0|            0|            0|  0.00%|        # joined.
   622|         0|            0|            0|  0.00%|        reenable_gc = False
   623|         0|            0|            0|  0.00%|        try:
   624|         0|            0|            0|  0.00%|            if cls._registry:
   625|         0|            0|            0|  0.00%|                import gc
   626|         0|            0|            0|  0.00%|                if gc.isenabled():
   627|         0|            0|            0|  0.00%|                    reenable_gc = True
   628|         0|            0|            0|  0.00%|                    gc.disable()
   629|         0|            0|            0|  0.00%|                pending = None
   630|         0|            0|            0|  0.00%|                while True:
   631|         0|            0|            0|  0.00%|                    if pending is None or finalize._dirty:
   632|         0|            0|            0|  0.00%|                        pending = cls._select_for_exit()
   633|         0|            0|            0|  0.00%|                        finalize._dirty = False
   634|         0|            0|            0|  0.00%|                    if not pending:
   635|         0|            0|            0|  0.00%|                        break
   636|         0|            0|            0|  0.00%|                    f = pending.pop()
   637|         0|            0|            0|  0.00%|                    try:
   638|         0|            0|            0|  0.00%|                        # gc is disabled, so (assuming no daemonic
   639|         0|            0|            0|  0.00%|                        # threads) the following is the only line in
   640|         0|            0|            0|  0.00%|                        # this function which might trigger creation
   641|         0|            0|            0|  0.00%|                        # of a new finalizer
   642|         0|            0|            0|  0.00%|                        f()
   643|         0|            0|            0|  0.00%|                    except Exception:
   644|         0|            0|            0|  0.00%|                        sys.excepthook(*sys.exc_info())
   645|         0|            0|            0|  0.00%|                    assert f not in cls._registry
   646|         0|            0|            0|  0.00%|        finally:
   647|         0|            0|            0|  0.00%|            # prevent any more finalizers from executing during shutdown
   648|         0|            0|            0|  0.00%|            finalize._shutdown = True
   649|         0|            0|            0|  0.00%|            if reenable_gc:
   650|         0|            0|            0|  0.00%|                gc.enable()
File: <frozen importlib._bootstrap>
File duration: 0.00510836s (0.01%)
Line #|      Hits|         Time| Time per hit|      %|Source code
------+----------+-------------+-------------+-------+-----------
     1|         0|            0|            0|  0.00%|
     2|         0|            0|            0|  0.00%|
     3|         0|            0|            0|  0.00%|
     4|         0|            0|            0|  0.00%|
     5|         0|            0|            0|  0.00%|
     6|         0|            0|            0|  0.00%|
     7|         0|            0|            0|  0.00%|
     8|         0|            0|            0|  0.00%|
     9|         0|            0|            0|  0.00%|
    10|         0|            0|            0|  0.00%|
    11|         0|            0|            0|  0.00%|
    12|         0|            0|            0|  0.00%|
    13|         0|            0|            0|  0.00%|
    14|         0|            0|            0|  0.00%|
    15|         0|            0|            0|  0.00%|
    16|         0|            0|            0|  0.00%|
    17|         0|            0|            0|  0.00%|
    18|         0|            0|            0|  0.00%|
    19|         0|            0|            0|  0.00%|
    20|         0|            0|            0|  0.00%|
    21|         0|            0|            0|  0.00%|
    22|         0|            0|            0|  0.00%|
    23|         0|            0|            0|  0.00%|
    24|         0|            0|            0|  0.00%|
    25|         0|            0|            0|  0.00%|
    26|         0|            0|            0|  0.00%|
    27|         0|            0|            0|  0.00%|
    28|         0|            0|            0|  0.00%|
    29|         0|            0|            0|  0.00%|
    30|         0|            0|            0|  0.00%|
    31|         0|            0|            0|  0.00%|
    32|         0|            0|            0|  0.00%|
    33|         0|            0|            0|  0.00%|
    34|         0|            0|            0|  0.00%|
    35|         0|            0|            0|  0.00%|
    36|         0|            0|            0|  0.00%|
    37|         0|            0|            0|  0.00%|
    38|         0|            0|            0|  0.00%|
    39|         0|            0|            0|  0.00%|
    40|         0|            0|            0|  0.00%|
    41|         0|            0|            0|  0.00%|
    42|         0|            0|            0|  0.00%|
    43|         0|            0|            0|  0.00%|
    44|         0|            0|            0|  0.00%|
    45|         0|            0|            0|  0.00%|
    46|         0|            0|            0|  0.00%|
    47|         0|            0|            0|  0.00%|
    48|         0|            0|            0|  0.00%|
    49|         0|            0|            0|  0.00%|
    50|         0|            0|            0|  0.00%|
    51|         0|            0|            0|  0.00%|
    52|         0|            0|            0|  0.00%|
    53|         0|            0|            0|  0.00%|
    54|         0|            0|            0|  0.00%|
    55|         0|            0|            0|  0.00%|
    56|         0|            0|            0|  0.00%|
    57|         0|            0|            0|  0.00%|
    58|         0|            0|            0|  0.00%|
    59|         0|            0|            0|  0.00%|
    60|         0|            0|            0|  0.00%|
    61|         0|            0|            0|  0.00%|
    62|         0|            0|            0|  0.00%|
    63|         0|            0|            0|  0.00%|
    64|         0|            0|            0|  0.00%|
    65|         0|            0|            0|  0.00%|
    66|         0|            0|            0|  0.00%|
    67|         0|            0|            0|  0.00%|
    68|         0|            0|            0|  0.00%|
    69|         0|            0|            0|  0.00%|
    70|         0|            0|            0|  0.00%|
    71|         0|            0|            0|  0.00%|
    72|         0|            0|            0|  0.00%|
    73|         0|            0|            0|  0.00%|
    74|         0|            0|            0|  0.00%|
    75|         0|            0|            0|  0.00%|
    76|         0|            0|            0|  0.00%|
    77|         0|            0|            0|  0.00%|
    78|         0|            0|            0|  0.00%|
    79|         0|            0|            0|  0.00%|
    80|         0|            0|            0|  0.00%|
    81|         0|            0|            0|  0.00%|
    82|         0|            0|            0|  0.00%|
    83|         0|            0|            0|  0.00%|
    84|         0|            0|            0|  0.00%|
    85|         0|            0|            0|  0.00%|
    86|         0|            0|            0|  0.00%|
    87|         0|            0|            0|  0.00%|
    88|         0|            0|            0|  0.00%|
    89|         0|            0|            0|  0.00%|
    90|         0|            0|            0|  0.00%|
    91|         0|            0|            0|  0.00%|
    92|         0|            0|            0|  0.00%|
    93|         0|            0|            0|  0.00%|
    94|         0|            0|            0|  0.00%|
    95|         0|            0|            0|  0.00%|
    96|         0|            0|            0|  0.00%|
    97|         0|            0|            0|  0.00%|
    98|         0|            0|            0|  0.00%|
    99|         0|            0|            0|  0.00%|
   100|         0|            0|            0|  0.00%|
   101|         0|            0|            0|  0.00%|
   102|         0|            0|            0|  0.00%|
   103|         0|            0|            0|  0.00%|
   104|         0|            0|            0|  0.00%|
   105|         0|            0|            0|  0.00%|
   106|         0|            0|            0|  0.00%|
   107|         0|            0|            0|  0.00%|
   108|         0|            0|            0|  0.00%|
   109|         0|            0|            0|  0.00%|
   110|         0|            0|            0|  0.00%|
   111|         0|            0|            0|  0.00%|
   112|         0|            0|            0|  0.00%|
   113|         0|            0|            0|  0.00%|
   114|         0|            0|            0|  0.00%|
   115|         0|            0|            0|  0.00%|
   116|         0|            0|            0|  0.00%|
   117|         0|            0|            0|  0.00%|
   118|         0|            0|            0|  0.00%|
   119|         0|            0|            0|  0.00%|
   120|         0|            0|            0|  0.00%|
   121|         0|            0|            0|  0.00%|
   122|         0|            0|            0|  0.00%|
   123|         0|            0|            0|  0.00%|
   124|         0|            0|            0|  0.00%|
   125|         0|            0|            0|  0.00%|
   126|         0|            0|            0|  0.00%|
   127|         0|            0|            0|  0.00%|
   128|         0|            0|            0|  0.00%|
   129|         0|            0|            0|  0.00%|
   130|         0|            0|            0|  0.00%|
   131|         0|            0|            0|  0.00%|
   132|         0|            0|            0|  0.00%|
   133|         0|            0|            0|  0.00%|
   134|         0|            0|            0|  0.00%|
   135|         0|            0|            0|  0.00%|
   136|         0|            0|            0|  0.00%|
   137|         0|            0|            0|  0.00%|
   138|         0|            0|            0|  0.00%|
   139|         0|            0|            0|  0.00%|
   140|         0|            0|            0|  0.00%|
   141|         0|            0|            0|  0.00%|
   142|         0|            0|            0|  0.00%|
   143|         0|            0|            0|  0.00%|
   144|         0|            0|            0|  0.00%|
   145|         0|            0|            0|  0.00%|
   146|         0|            0|            0|  0.00%|
   147|         0|            0|            0|  0.00%|
   148|         0|            0|            0|  0.00%|
   149|         0|            0|            0|  0.00%|
   150|         0|            0|            0|  0.00%|
   151|         0|            0|            0|  0.00%|
   152|         0|            0|            0|  0.00%|
   153|         0|            0|            0|  0.00%|
   154|         0|            0|            0|  0.00%|
   155|         0|            0|            0|  0.00%|
   156|         0|            0|            0|  0.00%|
   157|         0|            0|            0|  0.00%|
   158|         0|            0|            0|  0.00%|
   159|         0|            0|            0|  0.00%|
   160|         0|            0|            0|  0.00%|
   161|         0|            0|            0|  0.00%|
   162|         0|            0|            0|  0.00%|
   163|         0|            0|            0|  0.00%|
   164|         0|            0|            0|  0.00%|
   165|         0|            0|            0|  0.00%|
   166|         0|            0|            0|  0.00%|
   167|         0|            0|            0|  0.00%|
   168|         0|            0|            0|  0.00%|
   169|         0|            0|            0|  0.00%|
   170|         0|            0|            0|  0.00%|
   171|         0|            0|            0|  0.00%|
   172|         0|            0|            0|  0.00%|
   173|         0|            0|            0|  0.00%|
   174|         0|            0|            0|  0.00%|
   175|         0|            0|            0|  0.00%|
   176|         0|            0|            0|  0.00%|
   177|         0|            0|            0|  0.00%|
   178|         0|            0|            0|  0.00%|
   179|         0|            0|            0|  0.00%|
   180|         0|            0|            0|  0.00%|
   181|         0|            0|            0|  0.00%|
   182|         0|            0|            0|  0.00%|
   183|         0|            0|            0|  0.00%|
   184|         0|            0|            0|  0.00%|
   185|         0|            0|            0|  0.00%|
   186|         0|            0|            0|  0.00%|
   187|         0|            0|            0|  0.00%|
   188|         0|            0|            0|  0.00%|
   189|         0|            0|            0|  0.00%|
   190|         0|            0|            0|  0.00%|
   191|         0|            0|            0|  0.00%|
   192|         0|            0|            0|  0.00%|
   193|         0|            0|            0|  0.00%|
   194|         0|            0|            0|  0.00%|
   195|         0|            0|            0|  0.00%|
   196|         0|            0|            0|  0.00%|
   197|         0|            0|            0|  0.00%|
   198|         0|            0|            0|  0.00%|
   199|         0|            0|            0|  0.00%|
   200|         0|            0|            0|  0.00%|
   201|         0|            0|            0|  0.00%|
   202|         0|            0|            0|  0.00%|
   203|         0|            0|            0|  0.00%|
   204|         0|            0|            0|  0.00%|
   205|         0|            0|            0|  0.00%|
   206|         0|            0|            0|  0.00%|
   207|         0|            0|            0|  0.00%|
   208|         0|            0|            0|  0.00%|
   209|         0|            0|            0|  0.00%|
   210|         0|            0|            0|  0.00%|
   211|         0|            0|            0|  0.00%|
   212|         0|            0|            0|  0.00%|
   213|         0|            0|            0|  0.00%|
   214|         0|            0|            0|  0.00%|
   215|         0|            0|            0|  0.00%|
   216|         0|            0|            0|  0.00%|
   217|         0|            0|            0|  0.00%|
   218|         0|            0|            0|  0.00%|
   219|         0|            0|            0|  0.00%|
   220|         0|            0|            0|  0.00%|
   221|         0|            0|            0|  0.00%|
   222|         0|            0|            0|  0.00%|
   223|         0|            0|            0|  0.00%|
   224|         0|            0|            0|  0.00%|
   225|         0|            0|            0|  0.00%|
   226|         0|            0|            0|  0.00%|
   227|         0|            0|            0|  0.00%|
   228|         0|            0|            0|  0.00%|
   229|         0|            0|            0|  0.00%|
   230|         0|            0|            0|  0.00%|
   231|         0|            0|            0|  0.00%|
   232|         0|            0|            0|  0.00%|
   233|         0|            0|            0|  0.00%|
   234|         0|            0|            0|  0.00%|
   235|         0|            0|            0|  0.00%|
   236|         0|            0|            0|  0.00%|
   237|         0|            0|            0|  0.00%|
   238|         0|            0|            0|  0.00%|
   239|         0|            0|            0|  0.00%|
   240|         0|            0|            0|  0.00%|
   241|         0|            0|            0|  0.00%|
   242|         0|            0|            0|  0.00%|
   243|         0|            0|            0|  0.00%|
   244|         0|            0|            0|  0.00%|
   245|         0|            0|            0|  0.00%|
   246|         0|            0|            0|  0.00%|
   247|         0|            0|            0|  0.00%|
   248|         0|            0|            0|  0.00%|
   249|         0|            0|            0|  0.00%|
   250|         0|            0|            0|  0.00%|
   251|         0|            0|            0|  0.00%|
   252|         0|            0|            0|  0.00%|
   253|         0|            0|            0|  0.00%|
   254|         0|            0|            0|  0.00%|
   255|         0|            0|            0|  0.00%|
   256|         0|            0|            0|  0.00%|
   257|         0|            0|            0|  0.00%|
   258|         0|            0|            0|  0.00%|
   259|         0|            0|            0|  0.00%|
   260|         0|            0|            0|  0.00%|
   261|         0|            0|            0|  0.00%|
   262|         0|            0|            0|  0.00%|
   263|         0|            0|            0|  0.00%|
   264|         0|            0|            0|  0.00%|
   265|         0|            0|            0|  0.00%|
   266|         0|            0|            0|  0.00%|
   267|         0|            0|            0|  0.00%|
   268|         0|            0|            0|  0.00%|
   269|         0|            0|            0|  0.00%|
   270|         0|            0|            0|  0.00%|
   271|         0|            0|            0|  0.00%|
   272|         0|            0|            0|  0.00%|
   273|         0|            0|            0|  0.00%|
   274|         0|            0|            0|  0.00%|
   275|         0|            0|            0|  0.00%|
   276|         0|            0|            0|  0.00%|
   277|         0|            0|            0|  0.00%|
   278|         0|            0|            0|  0.00%|
   279|         0|            0|            0|  0.00%|
   280|         0|            0|            0|  0.00%|
   281|         0|            0|            0|  0.00%|
   282|         0|            0|            0|  0.00%|
   283|         0|            0|            0|  0.00%|
   284|         0|            0|            0|  0.00%|
   285|         0|            0|            0|  0.00%|
   286|         0|            0|            0|  0.00%|
   287|         0|            0|            0|  0.00%|
   288|         0|            0|            0|  0.00%|
   289|         0|            0|            0|  0.00%|
   290|         0|            0|            0|  0.00%|
   291|         0|            0|            0|  0.00%|
   292|         0|            0|            0|  0.00%|
   293|         0|            0|            0|  0.00%|
   294|         0|            0|            0|  0.00%|
   295|         0|            0|            0|  0.00%|
   296|         0|            0|            0|  0.00%|
   297|         0|            0|            0|  0.00%|
   298|         0|            0|            0|  0.00%|
   299|         0|            0|            0|  0.00%|
   300|         0|            0|            0|  0.00%|
   301|         0|            0|            0|  0.00%|
   302|         0|            0|            0|  0.00%|
   303|         0|            0|            0|  0.00%|
   304|         0|            0|            0|  0.00%|
   305|         0|            0|            0|  0.00%|
   306|         0|            0|            0|  0.00%|
   307|         0|            0|            0|  0.00%|
   308|         0|            0|            0|  0.00%|
   309|         0|            0|            0|  0.00%|
   310|         0|            0|            0|  0.00%|
   311|         0|            0|            0|  0.00%|
   312|         0|            0|            0|  0.00%|
   313|         0|            0|            0|  0.00%|
   314|         0|            0|            0|  0.00%|
   315|         0|            0|            0|  0.00%|
   316|         0|            0|            0|  0.00%|
   317|         0|            0|            0|  0.00%|
   318|         0|            0|            0|  0.00%|
   319|         0|            0|            0|  0.00%|
   320|         0|            0|            0|  0.00%|
   321|         0|            0|            0|  0.00%|
   322|         0|            0|            0|  0.00%|
   323|         0|            0|            0|  0.00%|
   324|         0|            0|            0|  0.00%|
   325|         0|            0|            0|  0.00%|
   326|         0|            0|            0|  0.00%|
   327|         0|            0|            0|  0.00%|
   328|         0|            0|            0|  0.00%|
   329|         0|            0|            0|  0.00%|
   330|         0|            0|            0|  0.00%|
   331|         0|            0|            0|  0.00%|
   332|         0|            0|            0|  0.00%|
   333|         0|            0|            0|  0.00%|
   334|         0|            0|            0|  0.00%|
   335|         0|            0|            0|  0.00%|
   336|         0|            0|            0|  0.00%|
   337|         0|            0|            0|  0.00%|
   338|         0|            0|            0|  0.00%|
   339|         0|            0|            0|  0.00%|
   340|         0|            0|            0|  0.00%|
   341|         0|            0|            0|  0.00%|
   342|         0|            0|            0|  0.00%|
   343|         0|            0|            0|  0.00%|
   344|         0|            0|            0|  0.00%|
   345|         0|            0|            0|  0.00%|
   346|         0|            0|            0|  0.00%|
   347|         0|            0|            0|  0.00%|
   348|         0|            0|            0|  0.00%|
   349|         0|            0|            0|  0.00%|
   350|         0|            0|            0|  0.00%|
   351|         0|            0|            0|  0.00%|
   352|         0|            0|            0|  0.00%|
   353|         0|            0|            0|  0.00%|
   354|         0|            0|            0|  0.00%|
   355|         0|            0|            0|  0.00%|
   356|         0|            0|            0|  0.00%|
   357|         0|            0|            0|  0.00%|
   358|         0|            0|            0|  0.00%|
   359|         0|            0|            0|  0.00%|
   360|         0|            0|            0|  0.00%|
   361|         0|            0|            0|  0.00%|
   362|         0|            0|            0|  0.00%|
   363|         0|            0|            0|  0.00%|
   364|         0|            0|            0|  0.00%|
   365|         0|            0|            0|  0.00%|
   366|         0|            0|            0|  0.00%|
   367|         0|            0|            0|  0.00%|
   368|         0|            0|            0|  0.00%|
   369|         0|            0|            0|  0.00%|
   370|         0|            0|            0|  0.00%|
   371|         0|            0|            0|  0.00%|
   372|         0|            0|            0|  0.00%|
   373|         0|            0|            0|  0.00%|
   374|         0|            0|            0|  0.00%|
   375|         0|            0|            0|  0.00%|
   376|         0|            0|            0|  0.00%|
   377|         0|            0|            0|  0.00%|
   378|         0|            0|            0|  0.00%|
   379|         0|            0|            0|  0.00%|
   380|         0|            0|            0|  0.00%|
   381|         0|            0|            0|  0.00%|
   382|         0|            0|            0|  0.00%|
   383|         0|            0|            0|  0.00%|
   384|         0|            0|            0|  0.00%|
   385|         0|            0|            0|  0.00%|
   386|         0|            0|            0|  0.00%|
   387|         0|            0|            0|  0.00%|
   388|         0|            0|            0|  0.00%|
   389|       272|   0.00153732|  5.65192e-06|  0.00%|
   390|         0|            0|            0|  0.00%|
   391|         0|            0|            0|  0.00%|
   392|       272|   0.00140715|  5.17333e-06|  0.00%|
   393|       272|   0.00216389|  7.95547e-06|  0.00%|
File: /opt/conda/lib/python3.8/site-packages/torch/nn/modules/linear.py
File duration: 0.00499415s (0.01%)
Line #|      Hits|         Time| Time per hit|      %|Source code
------+----------+-------------+-------------+-------+-----------
     1|         0|            0|            0|  0.00%|import math
     2|         0|            0|            0|  0.00%|
     3|         0|            0|            0|  0.00%|import torch
     4|         0|            0|            0|  0.00%|from torch import Tensor
     5|         0|            0|            0|  0.00%|from torch.nn.parameter import Parameter, UninitializedParameter
     6|         0|            0|            0|  0.00%|from .. import functional as F
     7|         0|            0|            0|  0.00%|from .. import init
     8|         0|            0|            0|  0.00%|from .module import Module
     9|         0|            0|            0|  0.00%|from .lazy import LazyModuleMixin
    10|         0|            0|            0|  0.00%|
    11|         0|            0|            0|  0.00%|
    12|         0|            0|            0|  0.00%|class Identity(Module):
    13|         0|            0|            0|  0.00%|    r"""A placeholder identity operator that is argument-insensitive.
    14|         0|            0|            0|  0.00%|
    15|         0|            0|            0|  0.00%|    Args:
    16|         0|            0|            0|  0.00%|        args: any argument (unused)
    17|         0|            0|            0|  0.00%|        kwargs: any keyword argument (unused)
    18|         0|            0|            0|  0.00%|
    19|         0|            0|            0|  0.00%|    Examples::
    20|         0|            0|            0|  0.00%|
    21|         0|            0|            0|  0.00%|        >>> m = nn.Identity(54, unused_argument1=0.1, unused_argument2=False)
    22|         0|            0|            0|  0.00%|        >>> input = torch.randn(128, 20)
    23|         0|            0|            0|  0.00%|        >>> output = m(input)
    24|         0|            0|            0|  0.00%|        >>> print(output.size())
    25|         0|            0|            0|  0.00%|        torch.Size([128, 20])
    26|         0|            0|            0|  0.00%|
    27|         0|            0|            0|  0.00%|    """
    28|         0|            0|            0|  0.00%|    def __init__(self, *args, **kwargs):
    29|         0|            0|            0|  0.00%|        super(Identity, self).__init__()
    30|         0|            0|            0|  0.00%|
    31|         0|            0|            0|  0.00%|    def forward(self, input: Tensor) -> Tensor:
    32|         0|            0|            0|  0.00%|        return input
    33|         0|            0|            0|  0.00%|
    34|         0|            0|            0|  0.00%|
    35|         0|            0|            0|  0.00%|class Linear(Module):
    36|         0|            0|            0|  0.00%|    r"""Applies a linear transformation to the incoming data: :math:`y = xA^T + b`
    37|         0|            0|            0|  0.00%|
    38|         0|            0|            0|  0.00%|    This module supports :ref:`TensorFloat32<tf32_on_ampere>`.
    39|         0|            0|            0|  0.00%|
    40|         0|            0|            0|  0.00%|    Args:
    41|         0|            0|            0|  0.00%|        in_features: size of each input sample
    42|         0|            0|            0|  0.00%|        out_features: size of each output sample
    43|         0|            0|            0|  0.00%|        bias: If set to ``False``, the layer will not learn an additive bias.
    44|         0|            0|            0|  0.00%|            Default: ``True``
    45|         0|            0|            0|  0.00%|
    46|         0|            0|            0|  0.00%|    Shape:
    47|         0|            0|            0|  0.00%|        - Input: :math:`(N, *, H_{in})` where :math:`*` means any number of
    48|         0|            0|            0|  0.00%|          additional dimensions and :math:`H_{in} = \text{in\_features}`
    49|         0|            0|            0|  0.00%|        - Output: :math:`(N, *, H_{out})` where all but the last dimension
    50|         0|            0|            0|  0.00%|          are the same shape as the input and :math:`H_{out} = \text{out\_features}`.
    51|         0|            0|            0|  0.00%|
    52|         0|            0|            0|  0.00%|    Attributes:
    53|         0|            0|            0|  0.00%|        weight: the learnable weights of the module of shape
    54|         0|            0|            0|  0.00%|            :math:`(\text{out\_features}, \text{in\_features})`. The values are
    55|         0|            0|            0|  0.00%|            initialized from :math:`\mathcal{U}(-\sqrt{k}, \sqrt{k})`, where
    56|         0|            0|            0|  0.00%|            :math:`k = \frac{1}{\text{in\_features}}`
    57|         0|            0|            0|  0.00%|        bias:   the learnable bias of the module of shape :math:`(\text{out\_features})`.
    58|         0|            0|            0|  0.00%|                If :attr:`bias` is ``True``, the values are initialized from
    59|         0|            0|            0|  0.00%|                :math:`\mathcal{U}(-\sqrt{k}, \sqrt{k})` where
    60|         0|            0|            0|  0.00%|                :math:`k = \frac{1}{\text{in\_features}}`
    61|         0|            0|            0|  0.00%|
    62|         0|            0|            0|  0.00%|    Examples::
    63|         0|            0|            0|  0.00%|
    64|         0|            0|            0|  0.00%|        >>> m = nn.Linear(20, 30)
    65|         0|            0|            0|  0.00%|        >>> input = torch.randn(128, 20)
    66|         0|            0|            0|  0.00%|        >>> output = m(input)
    67|         0|            0|            0|  0.00%|        >>> print(output.size())
    68|         0|            0|            0|  0.00%|        torch.Size([128, 30])
    69|         0|            0|            0|  0.00%|    """
    70|         0|            0|            0|  0.00%|    __constants__ = ['in_features', 'out_features']
    71|         0|            0|            0|  0.00%|    in_features: int
    72|         0|            0|            0|  0.00%|    out_features: int
    73|         0|            0|            0|  0.00%|    weight: Tensor
    74|         0|            0|            0|  0.00%|
    75|         0|            0|            0|  0.00%|    def __init__(self, in_features: int, out_features: int, bias: bool = True,
    76|         0|            0|            0|  0.00%|                 device=None, dtype=None) -> None:
    77|         0|            0|            0|  0.00%|        factory_kwargs = {'device': device, 'dtype': dtype}
    78|         0|            0|            0|  0.00%|        super(Linear, self).__init__()
    79|         0|            0|            0|  0.00%|        self.in_features = in_features
    80|         0|            0|            0|  0.00%|        self.out_features = out_features
    81|         0|            0|            0|  0.00%|        self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
    82|         0|            0|            0|  0.00%|        if bias:
    83|         0|            0|            0|  0.00%|            self.bias = Parameter(torch.empty(out_features, **factory_kwargs))
    84|         0|            0|            0|  0.00%|        else:
    85|         0|            0|            0|  0.00%|            self.register_parameter('bias', None)
    86|         0|            0|            0|  0.00%|        self.reset_parameters()
    87|         0|            0|            0|  0.00%|
    88|         0|            0|            0|  0.00%|    def reset_parameters(self) -> None:
    89|         0|            0|            0|  0.00%|        init.kaiming_uniform_(self.weight, a=math.sqrt(5))
    90|         0|            0|            0|  0.00%|        if self.bias is not None:
    91|         0|            0|            0|  0.00%|            fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)
    92|         0|            0|            0|  0.00%|            bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0
    93|         0|            0|            0|  0.00%|            init.uniform_(self.bias, -bound, bound)
    94|         0|            0|            0|  0.00%|
    95|       100|  0.000607491|  6.07491e-06|  0.00%|    def forward(self, input: Tensor) -> Tensor:
    96|       100|   0.00438666|  4.38666e-05|  0.01%|        return F.linear(input, self.weight, self.bias)
(call)|       200|    0.0047791|  2.38955e-05|  0.01%|# /opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1117 __getattr__
(call)|       100|    0.0107336|  0.000107336|  0.02%|# /opt/conda/lib/python3.8/site-packages/torch/nn/functional.py:1831 linear
    97|         0|            0|            0|  0.00%|
    98|         0|            0|            0|  0.00%|    def extra_repr(self) -> str:
    99|         0|            0|            0|  0.00%|        return 'in_features={}, out_features={}, bias={}'.format(
   100|         0|            0|            0|  0.00%|            self.in_features, self.out_features, self.bias is not None
   101|         0|            0|            0|  0.00%|        )
   102|         0|            0|            0|  0.00%|
   103|         0|            0|            0|  0.00%|
   104|         0|            0|            0|  0.00%|# This class exists solely to avoid triggering an obscure error when scripting
   105|         0|            0|            0|  0.00%|# an improperly quantized attention layer. See this issue for details:
   106|         0|            0|            0|  0.00%|# https://github.com/pytorch/pytorch/issues/58969
   107|         0|            0|            0|  0.00%|# TODO: fail fast on quantization API usage error, then remove this class
   108|         0|            0|            0|  0.00%|# and replace uses of it with plain Linear
   109|         0|            0|            0|  0.00%|class NonDynamicallyQuantizableLinear(Linear):
   110|         0|            0|            0|  0.00%|    def __init__(self, in_features: int, out_features: int, bias: bool = True,
   111|         0|            0|            0|  0.00%|                 device=None, dtype=None) -> None:
   112|         0|            0|            0|  0.00%|        super().__init__(in_features, out_features, bias=bias,
   113|         0|            0|            0|  0.00%|                         device=device, dtype=dtype)
   114|         0|            0|            0|  0.00%|
   115|         0|            0|            0|  0.00%|
   116|         0|            0|            0|  0.00%|class Bilinear(Module):
   117|         0|            0|            0|  0.00%|    r"""Applies a bilinear transformation to the incoming data:
   118|         0|            0|            0|  0.00%|    :math:`y = x_1^T A x_2 + b`
   119|         0|            0|            0|  0.00%|
   120|         0|            0|            0|  0.00%|    Args:
   121|         0|            0|            0|  0.00%|        in1_features: size of each first input sample
   122|         0|            0|            0|  0.00%|        in2_features: size of each second input sample
   123|         0|            0|            0|  0.00%|        out_features: size of each output sample
   124|         0|            0|            0|  0.00%|        bias: If set to False, the layer will not learn an additive bias.
   125|         0|            0|            0|  0.00%|            Default: ``True``
   126|         0|            0|            0|  0.00%|
   127|         0|            0|            0|  0.00%|    Shape:
   128|         0|            0|            0|  0.00%|        - Input1: :math:`(N, *, H_{in1})` where :math:`H_{in1}=\text{in1\_features}` and
   129|         0|            0|            0|  0.00%|          :math:`*` means any number of additional dimensions. All but the last dimension
   130|         0|            0|            0|  0.00%|          of the inputs should be the same.
   131|         0|            0|            0|  0.00%|        - Input2: :math:`(N, *, H_{in2})` where :math:`H_{in2}=\text{in2\_features}`.
   132|         0|            0|            0|  0.00%|        - Output: :math:`(N, *, H_{out})` where :math:`H_{out}=\text{out\_features}`
   133|         0|            0|            0|  0.00%|          and all but the last dimension are the same shape as the input.
   134|         0|            0|            0|  0.00%|
   135|         0|            0|            0|  0.00%|    Attributes:
   136|         0|            0|            0|  0.00%|        weight: the learnable weights of the module of shape
   137|         0|            0|            0|  0.00%|            :math:`(\text{out\_features}, \text{in1\_features}, \text{in2\_features})`.
   138|         0|            0|            0|  0.00%|            The values are initialized from :math:`\mathcal{U}(-\sqrt{k}, \sqrt{k})`, where
   139|         0|            0|            0|  0.00%|            :math:`k = \frac{1}{\text{in1\_features}}`
   140|         0|            0|            0|  0.00%|        bias:   the learnable bias of the module of shape :math:`(\text{out\_features})`.
   141|         0|            0|            0|  0.00%|                If :attr:`bias` is ``True``, the values are initialized from
   142|         0|            0|            0|  0.00%|                :math:`\mathcal{U}(-\sqrt{k}, \sqrt{k})`, where
   143|         0|            0|            0|  0.00%|                :math:`k = \frac{1}{\text{in1\_features}}`
   144|         0|            0|            0|  0.00%|
   145|         0|            0|            0|  0.00%|    Examples::
   146|         0|            0|            0|  0.00%|
   147|         0|            0|            0|  0.00%|        >>> m = nn.Bilinear(20, 30, 40)
   148|         0|            0|            0|  0.00%|        >>> input1 = torch.randn(128, 20)
   149|         0|            0|            0|  0.00%|        >>> input2 = torch.randn(128, 30)
   150|         0|            0|            0|  0.00%|        >>> output = m(input1, input2)
   151|         0|            0|            0|  0.00%|        >>> print(output.size())
   152|         0|            0|            0|  0.00%|        torch.Size([128, 40])
   153|         0|            0|            0|  0.00%|    """
   154|         0|            0|            0|  0.00%|    __constants__ = ['in1_features', 'in2_features', 'out_features']
   155|         0|            0|            0|  0.00%|    in1_features: int
   156|         0|            0|            0|  0.00%|    in2_features: int
   157|         0|            0|            0|  0.00%|    out_features: int
   158|         0|            0|            0|  0.00%|    weight: Tensor
   159|         0|            0|            0|  0.00%|
   160|         0|            0|            0|  0.00%|    def __init__(self, in1_features: int, in2_features: int, out_features: int, bias: bool = True,
   161|         0|            0|            0|  0.00%|                 device=None, dtype=None) -> None:
   162|         0|            0|            0|  0.00%|        factory_kwargs = {'device': device, 'dtype': dtype}
   163|         0|            0|            0|  0.00%|        super(Bilinear, self).__init__()
   164|         0|            0|            0|  0.00%|        self.in1_features = in1_features
   165|         0|            0|            0|  0.00%|        self.in2_features = in2_features
   166|         0|            0|            0|  0.00%|        self.out_features = out_features
   167|         0|            0|            0|  0.00%|        self.weight = Parameter(torch.empty((out_features, in1_features, in2_features), **factory_kwargs))
   168|         0|            0|            0|  0.00%|
   169|         0|            0|            0|  0.00%|        if bias:
   170|         0|            0|            0|  0.00%|            self.bias = Parameter(torch.empty(out_features, **factory_kwargs))
   171|         0|            0|            0|  0.00%|        else:
   172|         0|            0|            0|  0.00%|            self.register_parameter('bias', None)
   173|         0|            0|            0|  0.00%|        self.reset_parameters()
   174|         0|            0|            0|  0.00%|
   175|         0|            0|            0|  0.00%|    def reset_parameters(self) -> None:
   176|         0|            0|            0|  0.00%|        bound = 1 / math.sqrt(self.weight.size(1))
   177|         0|            0|            0|  0.00%|        init.uniform_(self.weight, -bound, bound)
   178|         0|            0|            0|  0.00%|        if self.bias is not None:
   179|         0|            0|            0|  0.00%|            init.uniform_(self.bias, -bound, bound)
   180|         0|            0|            0|  0.00%|
   181|         0|            0|            0|  0.00%|    def forward(self, input1: Tensor, input2: Tensor) -> Tensor:
   182|         0|            0|            0|  0.00%|        return F.bilinear(input1, input2, self.weight, self.bias)
   183|         0|            0|            0|  0.00%|
   184|         0|            0|            0|  0.00%|    def extra_repr(self) -> str:
   185|         0|            0|            0|  0.00%|        return 'in1_features={}, in2_features={}, out_features={}, bias={}'.format(
   186|         0|            0|            0|  0.00%|            self.in1_features, self.in2_features, self.out_features, self.bias is not None
   187|         0|            0|            0|  0.00%|        )
   188|         0|            0|            0|  0.00%|
   189|         0|            0|            0|  0.00%|
   190|         0|            0|            0|  0.00%|class LazyLinear(LazyModuleMixin, Linear):
   191|         0|            0|            0|  0.00%|    r"""A :class:`torch.nn.Linear` module where `in_features` is inferred.
   192|         0|            0|            0|  0.00%|
   193|         0|            0|            0|  0.00%|    In this module, the `weight` and `bias` are of :class:`torch.nn.UninitializedParameter`
   194|         0|            0|            0|  0.00%|    class. They will be initialized after the first call to ``forward`` is done and the
   195|         0|            0|            0|  0.00%|    module will become a regular :class:`torch.nn.Linear` module. The ``in_features`` argument
   196|         0|            0|            0|  0.00%|    of the :class:`Linear` is inferred from the ``input.shape[-1]``.
   197|         0|            0|            0|  0.00%|
   198|         0|            0|            0|  0.00%|    Check the :class:`torch.nn.modules.lazy.LazyModuleMixin` for further documentation
   199|         0|            0|            0|  0.00%|    on lazy modules and their limitations.
   200|         0|            0|            0|  0.00%|
   201|         0|            0|            0|  0.00%|    Args:
   202|         0|            0|            0|  0.00%|        out_features: size of each output sample
   203|         0|            0|            0|  0.00%|        bias: If set to ``False``, the layer will not learn an additive bias.
   204|         0|            0|            0|  0.00%|            Default: ``True``
   205|         0|            0|            0|  0.00%|
   206|         0|            0|            0|  0.00%|    Attributes:
   207|         0|            0|            0|  0.00%|        weight: the learnable weights of the module of shape
   208|         0|            0|            0|  0.00%|            :math:`(\text{out\_features}, \text{in\_features})`. The values are
   209|         0|            0|            0|  0.00%|            initialized from :math:`\mathcal{U}(-\sqrt{k}, \sqrt{k})`, where
   210|         0|            0|            0|  0.00%|            :math:`k = \frac{1}{\text{in\_features}}`
   211|         0|            0|            0|  0.00%|        bias:   the learnable bias of the module of shape :math:`(\text{out\_features})`.
   212|         0|            0|            0|  0.00%|                If :attr:`bias` is ``True``, the values are initialized from
   213|         0|            0|            0|  0.00%|                :math:`\mathcal{U}(-\sqrt{k}, \sqrt{k})` where
   214|         0|            0|            0|  0.00%|                :math:`k = \frac{1}{\text{in\_features}}`
   215|         0|            0|            0|  0.00%|
   216|         0|            0|            0|  0.00%|
   217|         0|            0|            0|  0.00%|    """
   218|         0|            0|            0|  0.00%|
   219|         0|            0|            0|  0.00%|    cls_to_become = Linear  # type: ignore[assignment]
   220|         0|            0|            0|  0.00%|    weight: UninitializedParameter
   221|         0|            0|            0|  0.00%|    bias: UninitializedParameter  # type: ignore[assignment]
   222|         0|            0|            0|  0.00%|
   223|         0|            0|            0|  0.00%|    def __init__(self, out_features: int, bias: bool = True,
   224|         0|            0|            0|  0.00%|                 device=None, dtype=None) -> None:
   225|         0|            0|            0|  0.00%|        factory_kwargs = {'device': device, 'dtype': dtype}
   226|         0|            0|            0|  0.00%|        # bias is hardcoded to False to avoid creating tensor
   227|         0|            0|            0|  0.00%|        # that will soon be overwritten.
   228|         0|            0|            0|  0.00%|        super().__init__(0, 0, False)
   229|         0|            0|            0|  0.00%|        self.weight = UninitializedParameter(**factory_kwargs)
   230|         0|            0|            0|  0.00%|        self.out_features = out_features
   231|         0|            0|            0|  0.00%|        if bias:
   232|         0|            0|            0|  0.00%|            self.bias = UninitializedParameter(**factory_kwargs)
   233|         0|            0|            0|  0.00%|
   234|         0|            0|            0|  0.00%|    def reset_parameters(self) -> None:
   235|         0|            0|            0|  0.00%|        if not self.has_uninitialized_params() and self.in_features != 0:
   236|         0|            0|            0|  0.00%|            super().reset_parameters()
   237|         0|            0|            0|  0.00%|
   238|         0|            0|            0|  0.00%|    def initialize_parameters(self, input) -> None:  # type: ignore[override]
   239|         0|            0|            0|  0.00%|        if self.has_uninitialized_params():
   240|         0|            0|            0|  0.00%|            with torch.no_grad():
   241|         0|            0|            0|  0.00%|                self.in_features = input.shape[-1]
   242|         0|            0|            0|  0.00%|                self.weight.materialize((self.out_features, self.in_features))
   243|         0|            0|            0|  0.00%|                if self.bias is not None:
   244|         0|            0|            0|  0.00%|                    self.bias.materialize((self.out_features,))
   245|         0|            0|            0|  0.00%|                self.reset_parameters()
   246|         0|            0|            0|  0.00%|# TODO: PartialLinear - maybe in sparse?
File: /opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py
File duration: 0.0044713s (0.01%)
Line #|      Hits|         Time| Time per hit|      %|Source code
------+----------+-------------+-------------+-------+-----------
     1|         0|            0|            0|  0.00%|import types
     2|         0|            0|            0|  0.00%|import math
     3|         0|            0|            0|  0.00%|from torch._six import inf
     4|         0|            0|            0|  0.00%|from functools import wraps
     5|         0|            0|            0|  0.00%|import warnings
     6|         0|            0|            0|  0.00%|import weakref
     7|         0|            0|            0|  0.00%|from collections import Counter
     8|         0|            0|            0|  0.00%|from bisect import bisect_right
     9|         0|            0|            0|  0.00%|
    10|         0|            0|            0|  0.00%|from .optimizer import Optimizer
    11|         0|            0|            0|  0.00%|
    12|         0|            0|            0|  0.00%|
    13|         0|            0|            0|  0.00%|EPOCH_DEPRECATION_WARNING = (
    14|         0|            0|            0|  0.00%|    "The epoch parameter in `scheduler.step()` was not necessary and is being "
    15|         0|            0|            0|  0.00%|    "deprecated where possible. Please use `scheduler.step()` to step the "
    16|         0|            0|            0|  0.00%|    "scheduler. During the deprecation, if epoch is different from None, the "
    17|         0|            0|            0|  0.00%|    "closed form is used instead of the new chainable form, where available. "
    18|         0|            0|            0|  0.00%|    "Please open an issue if you are unable to replicate your use case: "
    19|         0|            0|            0|  0.00%|    "https://github.com/pytorch/pytorch/issues/new/choose."
    20|         0|            0|            0|  0.00%|)
    21|         0|            0|            0|  0.00%|
    22|         0|            0|            0|  0.00%|class _LRScheduler(object):
    23|         0|            0|            0|  0.00%|
    24|         0|            0|            0|  0.00%|    def __init__(self, optimizer, last_epoch=-1, verbose=False):
    25|         0|            0|            0|  0.00%|
    26|         0|            0|            0|  0.00%|        # Attach optimizer
    27|         0|            0|            0|  0.00%|        if not isinstance(optimizer, Optimizer):
    28|         0|            0|            0|  0.00%|            raise TypeError('{} is not an Optimizer'.format(
    29|         0|            0|            0|  0.00%|                type(optimizer).__name__))
    30|         0|            0|            0|  0.00%|        self.optimizer = optimizer
    31|         0|            0|            0|  0.00%|
    32|         0|            0|            0|  0.00%|        # Initialize epoch and base learning rates
    33|         0|            0|            0|  0.00%|        if last_epoch == -1:
    34|         0|            0|            0|  0.00%|            for group in optimizer.param_groups:
    35|         0|            0|            0|  0.00%|                group.setdefault('initial_lr', group['lr'])
    36|         0|            0|            0|  0.00%|        else:
    37|         0|            0|            0|  0.00%|            for i, group in enumerate(optimizer.param_groups):
    38|         0|            0|            0|  0.00%|                if 'initial_lr' not in group:
    39|         0|            0|            0|  0.00%|                    raise KeyError("param 'initial_lr' is not specified "
    40|         0|            0|            0|  0.00%|                                   "in param_groups[{}] when resuming an optimizer".format(i))
    41|         0|            0|            0|  0.00%|        self.base_lrs = [group['initial_lr'] for group in optimizer.param_groups]
    42|         0|            0|            0|  0.00%|        self.last_epoch = last_epoch
    43|         0|            0|            0|  0.00%|
    44|         0|            0|            0|  0.00%|        # Following https://github.com/pytorch/pytorch/issues/20124
    45|         0|            0|            0|  0.00%|        # We would like to ensure that `lr_scheduler.step()` is called after
    46|         0|            0|            0|  0.00%|        # `optimizer.step()`
    47|         0|            0|            0|  0.00%|        def with_counter(method):
    48|         0|            0|            0|  0.00%|            if getattr(method, '_with_counter', False):
    49|         0|            0|            0|  0.00%|                # `optimizer.step()` has already been replaced, return.
    50|         0|            0|            0|  0.00%|                return method
    51|         0|            0|            0|  0.00%|
    52|         0|            0|            0|  0.00%|            # Keep a weak reference to the optimizer instance to prevent
    53|         0|            0|            0|  0.00%|            # cyclic references.
    54|         0|            0|            0|  0.00%|            instance_ref = weakref.ref(method.__self__)
    55|         0|            0|            0|  0.00%|            # Get the unbound method for the same purpose.
    56|         0|            0|            0|  0.00%|            func = method.__func__
    57|         0|            0|            0|  0.00%|            cls = instance_ref().__class__
    58|         0|            0|            0|  0.00%|            del method
    59|         0|            0|            0|  0.00%|
    60|        61|  0.000895262|  1.46764e-05|  0.00%|            @wraps(func)
    61|         0|            0|            0|  0.00%|            def wrapper(*args, **kwargs):
    62|        61|  0.000591278|  9.69308e-06|  0.00%|                instance = instance_ref()
    63|        61|  0.000402927|  6.60537e-06|  0.00%|                instance._step_count += 1
    64|        61|  0.000490904|   8.0476e-06|  0.00%|                wrapped = func.__get__(instance, cls)
    65|        61|   0.00151587|  2.48503e-05|  0.00%|                return wrapped(*args, **kwargs)
(call)|        61|      1.45609|    0.0238703|  2.72%|# /opt/conda/lib/python3.8/site-packages/torch/optim/optimizer.py:83 wrapper
    66|         0|            0|            0|  0.00%|
    67|         0|            0|            0|  0.00%|            # Note that the returned function here is no longer a bound method,
    68|         0|            0|            0|  0.00%|            # so attributes like `__func__` and `__self__` no longer exist.
    69|         0|            0|            0|  0.00%|            wrapper._with_counter = True
    70|         0|            0|            0|  0.00%|            return wrapper
    71|         0|            0|            0|  0.00%|
    72|         0|            0|            0|  0.00%|        self.optimizer.step = with_counter(self.optimizer.step)
    73|         0|            0|            0|  0.00%|        self.optimizer._step_count = 0
    74|         0|            0|            0|  0.00%|        self._step_count = 0
    75|         0|            0|            0|  0.00%|        self.verbose = verbose
    76|         0|            0|            0|  0.00%|
    77|         0|            0|            0|  0.00%|        self.step()
    78|         0|            0|            0|  0.00%|
    79|         0|            0|            0|  0.00%|    def state_dict(self):
    80|         0|            0|            0|  0.00%|        """Returns the state of the scheduler as a :class:`dict`.
    81|         0|            0|            0|  0.00%|
    82|         0|            0|            0|  0.00%|        It contains an entry for every variable in self.__dict__ which
    83|         0|            0|            0|  0.00%|        is not the optimizer.
    84|         0|            0|            0|  0.00%|        """
    85|         0|            0|            0|  0.00%|        return {key: value for key, value in self.__dict__.items() if key != 'optimizer'}
    86|         0|            0|            0|  0.00%|
    87|         0|            0|            0|  0.00%|    def load_state_dict(self, state_dict):
    88|         0|            0|            0|  0.00%|        """Loads the schedulers state.
    89|         0|            0|            0|  0.00%|
    90|         0|            0|            0|  0.00%|        Args:
    91|         0|            0|            0|  0.00%|            state_dict (dict): scheduler state. Should be an object returned
    92|         0|            0|            0|  0.00%|                from a call to :meth:`state_dict`.
    93|         0|            0|            0|  0.00%|        """
    94|         0|            0|            0|  0.00%|        self.__dict__.update(state_dict)
    95|         0|            0|            0|  0.00%|
    96|         0|            0|            0|  0.00%|    def get_last_lr(self):
    97|         0|            0|            0|  0.00%|        """ Return last computed learning rate by current scheduler.
    98|         0|            0|            0|  0.00%|        """
    99|         0|            0|            0|  0.00%|        return self._last_lr
   100|         0|            0|            0|  0.00%|
   101|         0|            0|            0|  0.00%|    def get_lr(self):
   102|         0|            0|            0|  0.00%|        # Compute learning rate using chainable form of the scheduler
   103|         0|            0|            0|  0.00%|        raise NotImplementedError
   104|         0|            0|            0|  0.00%|
   105|         1|  9.53674e-06|  9.53674e-06|  0.00%|    def print_lr(self, is_verbose, group, lr, epoch=None):
   106|         0|            0|            0|  0.00%|        """Display the current learning rate.
   107|         0|            0|            0|  0.00%|        """
   108|         1|   6.4373e-06|   6.4373e-06|  0.00%|        if is_verbose:
   109|         0|            0|            0|  0.00%|            if epoch is None:
   110|         0|            0|            0|  0.00%|                print('Adjusting learning rate'
   111|         0|            0|            0|  0.00%|                      ' of group {} to {:.4e}.'.format(group, lr))
   112|         0|            0|            0|  0.00%|            else:
   113|         0|            0|            0|  0.00%|                print('Epoch {:5d}: adjusting learning rate'
   114|         0|            0|            0|  0.00%|                      ' of group {} to {:.4e}.'.format(epoch, group, lr))
   115|         0|            0|            0|  0.00%|
   116|         0|            0|            0|  0.00%|
   117|         1|  3.31402e-05|  3.31402e-05|  0.00%|    def step(self, epoch=None):
   118|         0|            0|            0|  0.00%|        # Raise a warning if old pattern is detected
   119|         0|            0|            0|  0.00%|        # https://github.com/pytorch/pytorch/issues/20124
   120|         1|  1.04904e-05|  1.04904e-05|  0.00%|        if self._step_count == 1:
   121|         1|  1.23978e-05|  1.23978e-05|  0.00%|            if not hasattr(self.optimizer.step, "_with_counter"):
   122|         0|            0|            0|  0.00%|                warnings.warn("Seems like `optimizer.step()` has been overridden after learning rate scheduler "
   123|         0|            0|            0|  0.00%|                              "initialization. Please, make sure to call `optimizer.step()` before "
   124|         0|            0|            0|  0.00%|                              "`lr_scheduler.step()`. See more details at "
   125|         0|            0|            0|  0.00%|                              "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
   126|         0|            0|            0|  0.00%|
   127|         0|            0|            0|  0.00%|            # Just check if there were two first lr_scheduler.step() calls before optimizer.step()
   128|         1|  7.86781e-06|  7.86781e-06|  0.00%|            elif self.optimizer._step_count < 1:
   129|         0|            0|            0|  0.00%|                warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
   130|         0|            0|            0|  0.00%|                              "In PyTorch 1.1.0 and later, you should call them in the opposite order: "
   131|         0|            0|            0|  0.00%|                              "`optimizer.step()` before `lr_scheduler.step()`.  Failure to do this "
   132|         0|            0|            0|  0.00%|                              "will result in PyTorch skipping the first value of the learning rate schedule. "
   133|         0|            0|            0|  0.00%|                              "See more details at "
   134|         0|            0|            0|  0.00%|                              "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
   135|         1|  8.34465e-06|  8.34465e-06|  0.00%|        self._step_count += 1
   136|         0|            0|            0|  0.00%|
   137|         3|  9.27448e-05|  3.09149e-05|  0.00%|        class _enable_get_lr_call:
(call)|         1|  4.43459e-05|  4.43459e-05|  0.00%|# /opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:137 _enable_get_lr_call
   138|         0|            0|            0|  0.00%|
   139|         2|  1.33514e-05|  6.67572e-06|  0.00%|            def __init__(self, o):
   140|         1|  6.19888e-06|  6.19888e-06|  0.00%|                self.o = o
   141|         0|            0|            0|  0.00%|
   142|         2|  3.24249e-05|  1.62125e-05|  0.00%|            def __enter__(self):
   143|         1|   6.4373e-06|   6.4373e-06|  0.00%|                self.o._get_lr_called_within_step = True
   144|         1|  5.72205e-06|  5.72205e-06|  0.00%|                return self
   145|         0|            0|            0|  0.00%|
   146|         2|  1.43051e-05|  7.15256e-06|  0.00%|            def __exit__(self, type, value, traceback):
   147|         1|  5.72205e-06|  5.72205e-06|  0.00%|                self.o._get_lr_called_within_step = False
   148|         0|            0|            0|  0.00%|
   149|         1|  4.86374e-05|  4.86374e-05|  0.00%|        with _enable_get_lr_call(self):
(call)|         1|  1.52588e-05|  1.52588e-05|  0.00%|# /opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:139 __init__
(call)|         1|   2.7895e-05|   2.7895e-05|  0.00%|# /opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:142 __enter__
   150|         1|  8.10623e-06|  8.10623e-06|  0.00%|            if epoch is None:
   151|         1|  8.34465e-06|  8.34465e-06|  0.00%|                self.last_epoch += 1
   152|         1|  4.62532e-05|  4.62532e-05|  0.00%|                values = self.get_lr()
(call)|         1|  9.27448e-05|  9.27448e-05|  0.00%|# /opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:367 get_lr
(call)|         1|  1.35899e-05|  1.35899e-05|  0.00%|# /opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:146 __exit__
   153|         0|            0|            0|  0.00%|            else:
   154|         0|            0|            0|  0.00%|                warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
   155|         0|            0|            0|  0.00%|                self.last_epoch = epoch
   156|         0|            0|            0|  0.00%|                if hasattr(self, "_get_closed_form_lr"):
   157|         0|            0|            0|  0.00%|                    values = self._get_closed_form_lr()
   158|         0|            0|            0|  0.00%|                else:
   159|         0|            0|            0|  0.00%|                    values = self.get_lr()
   160|         0|            0|            0|  0.00%|
   161|         2|  2.07424e-05|  1.03712e-05|  0.00%|        for i, data in enumerate(zip(self.optimizer.param_groups, values)):
   162|         1|  7.39098e-06|  7.39098e-06|  0.00%|            param_group, lr = data
   163|         1|  7.15256e-06|  7.15256e-06|  0.00%|            param_group['lr'] = lr
   164|         1|  3.40939e-05|  3.40939e-05|  0.00%|            self.print_lr(self.verbose, i, lr, epoch)
(call)|         1|   1.5974e-05|   1.5974e-05|  0.00%|# /opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:105 print_lr
   165|         0|            0|            0|  0.00%|
   166|         4|   3.6478e-05|  9.11951e-06|  0.00%|        self._last_lr = [group['lr'] for group in self.optimizer.param_groups]
(call)|         1|  1.81198e-05|  1.81198e-05|  0.00%|# /opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:166 <listcomp>
   167|         0|            0|            0|  0.00%|
   168|         0|            0|            0|  0.00%|
   169|         0|            0|            0|  0.00%|class LambdaLR(_LRScheduler):
   170|         0|            0|            0|  0.00%|    """Sets the learning rate of each parameter group to the initial lr
   171|         0|            0|            0|  0.00%|    times a given function. When last_epoch=-1, sets initial lr as lr.
   172|         0|            0|            0|  0.00%|
   173|         0|            0|            0|  0.00%|    Args:
   174|         0|            0|            0|  0.00%|        optimizer (Optimizer): Wrapped optimizer.
   175|         0|            0|            0|  0.00%|        lr_lambda (function or list): A function which computes a multiplicative
   176|         0|            0|            0|  0.00%|            factor given an integer parameter epoch, or a list of such
   177|         0|            0|            0|  0.00%|            functions, one for each group in optimizer.param_groups.
   178|         0|            0|            0|  0.00%|        last_epoch (int): The index of last epoch. Default: -1.
   179|         0|            0|            0|  0.00%|        verbose (bool): If ``True``, prints a message to stdout for
   180|         0|            0|            0|  0.00%|            each update. Default: ``False``.
   181|         0|            0|            0|  0.00%|
   182|         0|            0|            0|  0.00%|    Example:
   183|         0|            0|            0|  0.00%|        >>> # Assuming optimizer has two groups.
   184|         0|            0|            0|  0.00%|        >>> lambda1 = lambda epoch: epoch // 30
   185|         0|            0|            0|  0.00%|        >>> lambda2 = lambda epoch: 0.95 ** epoch
   186|         0|            0|            0|  0.00%|        >>> scheduler = LambdaLR(optimizer, lr_lambda=[lambda1, lambda2])
   187|         0|            0|            0|  0.00%|        >>> for epoch in range(100):
   188|         0|            0|            0|  0.00%|        >>>     train(...)
   189|         0|            0|            0|  0.00%|        >>>     validate(...)
   190|         0|            0|            0|  0.00%|        >>>     scheduler.step()
   191|         0|            0|            0|  0.00%|    """
   192|         0|            0|            0|  0.00%|
   193|         0|            0|            0|  0.00%|    def __init__(self, optimizer, lr_lambda, last_epoch=-1, verbose=False):
   194|         0|            0|            0|  0.00%|        self.optimizer = optimizer
   195|         0|            0|            0|  0.00%|
   196|         0|            0|            0|  0.00%|        if not isinstance(lr_lambda, list) and not isinstance(lr_lambda, tuple):
   197|         0|            0|            0|  0.00%|            self.lr_lambdas = [lr_lambda] * len(optimizer.param_groups)
   198|         0|            0|            0|  0.00%|        else:
   199|         0|            0|            0|  0.00%|            if len(lr_lambda) != len(optimizer.param_groups):
   200|         0|            0|            0|  0.00%|                raise ValueError("Expected {} lr_lambdas, but got {}".format(
   201|         0|            0|            0|  0.00%|                    len(optimizer.param_groups), len(lr_lambda)))
   202|         0|            0|            0|  0.00%|            self.lr_lambdas = list(lr_lambda)
   203|         0|            0|            0|  0.00%|        super(LambdaLR, self).__init__(optimizer, last_epoch, verbose)
   204|         0|            0|            0|  0.00%|
   205|         0|            0|            0|  0.00%|    def state_dict(self):
   206|         0|            0|            0|  0.00%|        """Returns the state of the scheduler as a :class:`dict`.
   207|         0|            0|            0|  0.00%|
   208|         0|            0|            0|  0.00%|        It contains an entry for every variable in self.__dict__ which
   209|         0|            0|            0|  0.00%|        is not the optimizer.
   210|         0|            0|            0|  0.00%|        The learning rate lambda functions will only be saved if they are callable objects
   211|         0|            0|            0|  0.00%|        and not if they are functions or lambdas.
   212|         0|            0|            0|  0.00%|
   213|         0|            0|            0|  0.00%|        When saving or loading the scheduler, please make sure to also save or load the state of the optimizer.
   214|         0|            0|            0|  0.00%|        """
   215|         0|            0|            0|  0.00%|
   216|         0|            0|            0|  0.00%|        state_dict = {key: value for key, value in self.__dict__.items() if key not in ('optimizer', 'lr_lambdas')}
   217|         0|            0|            0|  0.00%|        state_dict['lr_lambdas'] = [None] * len(self.lr_lambdas)
   218|         0|            0|            0|  0.00%|
   219|         0|            0|            0|  0.00%|        for idx, fn in enumerate(self.lr_lambdas):
   220|         0|            0|            0|  0.00%|            if not isinstance(fn, types.FunctionType):
   221|         0|            0|            0|  0.00%|                state_dict['lr_lambdas'][idx] = fn.__dict__.copy()
   222|         0|            0|            0|  0.00%|
   223|         0|            0|            0|  0.00%|        return state_dict
   224|         0|            0|            0|  0.00%|
   225|         0|            0|            0|  0.00%|    def load_state_dict(self, state_dict):
   226|         0|            0|            0|  0.00%|        """Loads the schedulers state.
   227|         0|            0|            0|  0.00%|
   228|         0|            0|            0|  0.00%|        When saving or loading the scheduler, please make sure to also save or load the state of the optimizer.
   229|         0|            0|            0|  0.00%|
   230|         0|            0|            0|  0.00%|        Args:
   231|         0|            0|            0|  0.00%|            state_dict (dict): scheduler state. Should be an object returned
   232|         0|            0|            0|  0.00%|                from a call to :meth:`state_dict`.
   233|         0|            0|            0|  0.00%|        """
   234|         0|            0|            0|  0.00%|
   235|         0|            0|            0|  0.00%|        lr_lambdas = state_dict.pop('lr_lambdas')
   236|         0|            0|            0|  0.00%|        self.__dict__.update(state_dict)
   237|         0|            0|            0|  0.00%|        # Restore state_dict keys in order to prevent side effects
   238|         0|            0|            0|  0.00%|        # https://github.com/pytorch/pytorch/issues/32756
   239|         0|            0|            0|  0.00%|        state_dict['lr_lambdas'] = lr_lambdas
   240|         0|            0|            0|  0.00%|
   241|         0|            0|            0|  0.00%|        for idx, fn in enumerate(lr_lambdas):
   242|         0|            0|            0|  0.00%|            if fn is not None:
   243|         0|            0|            0|  0.00%|                self.lr_lambdas[idx].__dict__.update(fn)
   244|         0|            0|            0|  0.00%|
   245|         0|            0|            0|  0.00%|    def get_lr(self):
   246|         0|            0|            0|  0.00%|        if not self._get_lr_called_within_step:
   247|         0|            0|            0|  0.00%|            warnings.warn("To get the last learning rate computed by the scheduler, "
   248|         0|            0|            0|  0.00%|                          "please use `get_last_lr()`.")
   249|         0|            0|            0|  0.00%|
   250|         0|            0|            0|  0.00%|        return [base_lr * lmbda(self.last_epoch)
   251|         0|            0|            0|  0.00%|                for lmbda, base_lr in zip(self.lr_lambdas, self.base_lrs)]
   252|         0|            0|            0|  0.00%|
   253|         0|            0|            0|  0.00%|
   254|         0|            0|            0|  0.00%|class MultiplicativeLR(_LRScheduler):
   255|         0|            0|            0|  0.00%|    """Multiply the learning rate of each parameter group by the factor given
   256|         0|            0|            0|  0.00%|    in the specified function. When last_epoch=-1, sets initial lr as lr.
   257|         0|            0|            0|  0.00%|
   258|         0|            0|            0|  0.00%|    Args:
   259|         0|            0|            0|  0.00%|        optimizer (Optimizer): Wrapped optimizer.
   260|         0|            0|            0|  0.00%|        lr_lambda (function or list): A function which computes a multiplicative
   261|         0|            0|            0|  0.00%|            factor given an integer parameter epoch, or a list of such
   262|         0|            0|            0|  0.00%|            functions, one for each group in optimizer.param_groups.
   263|         0|            0|            0|  0.00%|        last_epoch (int): The index of last epoch. Default: -1.
   264|         0|            0|            0|  0.00%|        verbose (bool): If ``True``, prints a message to stdout for
   265|         0|            0|            0|  0.00%|            each update. Default: ``False``.
   266|         0|            0|            0|  0.00%|
   267|         0|            0|            0|  0.00%|    Example:
   268|         0|            0|            0|  0.00%|        >>> lmbda = lambda epoch: 0.95
   269|         0|            0|            0|  0.00%|        >>> scheduler = MultiplicativeLR(optimizer, lr_lambda=lmbda)
   270|         0|            0|            0|  0.00%|        >>> for epoch in range(100):
   271|         0|            0|            0|  0.00%|        >>>     train(...)
   272|         0|            0|            0|  0.00%|        >>>     validate(...)
   273|         0|            0|            0|  0.00%|        >>>     scheduler.step()
   274|         0|            0|            0|  0.00%|    """
   275|         0|            0|            0|  0.00%|
   276|         0|            0|            0|  0.00%|    def __init__(self, optimizer, lr_lambda, last_epoch=-1, verbose=False):
   277|         0|            0|            0|  0.00%|        self.optimizer = optimizer
   278|         0|            0|            0|  0.00%|
   279|         0|            0|            0|  0.00%|        if not isinstance(lr_lambda, list) and not isinstance(lr_lambda, tuple):
   280|         0|            0|            0|  0.00%|            self.lr_lambdas = [lr_lambda] * len(optimizer.param_groups)
   281|         0|            0|            0|  0.00%|        else:
   282|         0|            0|            0|  0.00%|            if len(lr_lambda) != len(optimizer.param_groups):
   283|         0|            0|            0|  0.00%|                raise ValueError("Expected {} lr_lambdas, but got {}".format(
   284|         0|            0|            0|  0.00%|                    len(optimizer.param_groups), len(lr_lambda)))
   285|         0|            0|            0|  0.00%|            self.lr_lambdas = list(lr_lambda)
   286|         0|            0|            0|  0.00%|        super(MultiplicativeLR, self).__init__(optimizer, last_epoch, verbose)
   287|         0|            0|            0|  0.00%|
   288|         0|            0|            0|  0.00%|    def state_dict(self):
   289|         0|            0|            0|  0.00%|        """Returns the state of the scheduler as a :class:`dict`.
   290|         0|            0|            0|  0.00%|
   291|         0|            0|            0|  0.00%|        It contains an entry for every variable in self.__dict__ which
   292|         0|            0|            0|  0.00%|        is not the optimizer.
   293|         0|            0|            0|  0.00%|        The learning rate lambda functions will only be saved if they are callable objects
   294|         0|            0|            0|  0.00%|        and not if they are functions or lambdas.
   295|         0|            0|            0|  0.00%|        """
   296|         0|            0|            0|  0.00%|        state_dict = {key: value for key, value in self.__dict__.items() if key not in ('optimizer', 'lr_lambdas')}
   297|         0|            0|            0|  0.00%|        state_dict['lr_lambdas'] = [None] * len(self.lr_lambdas)
   298|         0|            0|            0|  0.00%|
   299|         0|            0|            0|  0.00%|        for idx, fn in enumerate(self.lr_lambdas):
   300|         0|            0|            0|  0.00%|            if not isinstance(fn, types.FunctionType):
   301|         0|            0|            0|  0.00%|                state_dict['lr_lambdas'][idx] = fn.__dict__.copy()
   302|         0|            0|            0|  0.00%|
   303|         0|            0|            0|  0.00%|        return state_dict
   304|         0|            0|            0|  0.00%|
   305|         0|            0|            0|  0.00%|    def load_state_dict(self, state_dict):
   306|         0|            0|            0|  0.00%|        """Loads the schedulers state.
   307|         0|            0|            0|  0.00%|
   308|         0|            0|            0|  0.00%|        Args:
   309|         0|            0|            0|  0.00%|            state_dict (dict): scheduler state. Should be an object returned
   310|         0|            0|            0|  0.00%|                from a call to :meth:`state_dict`.
   311|         0|            0|            0|  0.00%|        """
   312|         0|            0|            0|  0.00%|        lr_lambdas = state_dict.pop('lr_lambdas')
   313|         0|            0|            0|  0.00%|        self.__dict__.update(state_dict)
   314|         0|            0|            0|  0.00%|        # Restore state_dict keys in order to prevent side effects
   315|         0|            0|            0|  0.00%|        # https://github.com/pytorch/pytorch/issues/32756
   316|         0|            0|            0|  0.00%|        state_dict['lr_lambdas'] = lr_lambdas
   317|         0|            0|            0|  0.00%|
   318|         0|            0|            0|  0.00%|        for idx, fn in enumerate(lr_lambdas):
   319|         0|            0|            0|  0.00%|            if fn is not None:
   320|         0|            0|            0|  0.00%|                self.lr_lambdas[idx].__dict__.update(fn)
   321|         0|            0|            0|  0.00%|
   322|         0|            0|            0|  0.00%|    def get_lr(self):
   323|         0|            0|            0|  0.00%|        if not self._get_lr_called_within_step:
   324|         0|            0|            0|  0.00%|            warnings.warn("To get the last learning rate computed by the scheduler, "
   325|         0|            0|            0|  0.00%|                          "please use `get_last_lr()`.", UserWarning)
   326|         0|            0|            0|  0.00%|
   327|         0|            0|            0|  0.00%|        if self.last_epoch > 0:
   328|         0|            0|            0|  0.00%|            return [group['lr'] * lmbda(self.last_epoch)
   329|         0|            0|            0|  0.00%|                    for lmbda, group in zip(self.lr_lambdas, self.optimizer.param_groups)]
   330|         0|            0|            0|  0.00%|        else:
   331|         0|            0|            0|  0.00%|            return list(self.base_lrs)
   332|         0|            0|            0|  0.00%|
   333|         0|            0|            0|  0.00%|
   334|         0|            0|            0|  0.00%|class StepLR(_LRScheduler):
   335|         0|            0|            0|  0.00%|    """Decays the learning rate of each parameter group by gamma every
   336|         0|            0|            0|  0.00%|    step_size epochs. Notice that such decay can happen simultaneously with
   337|         0|            0|            0|  0.00%|    other changes to the learning rate from outside this scheduler. When
   338|         0|            0|            0|  0.00%|    last_epoch=-1, sets initial lr as lr.
   339|         0|            0|            0|  0.00%|
   340|         0|            0|            0|  0.00%|    Args:
   341|         0|            0|            0|  0.00%|        optimizer (Optimizer): Wrapped optimizer.
   342|         0|            0|            0|  0.00%|        step_size (int): Period of learning rate decay.
   343|         0|            0|            0|  0.00%|        gamma (float): Multiplicative factor of learning rate decay.
   344|         0|            0|            0|  0.00%|            Default: 0.1.
   345|         0|            0|            0|  0.00%|        last_epoch (int): The index of last epoch. Default: -1.
   346|         0|            0|            0|  0.00%|        verbose (bool): If ``True``, prints a message to stdout for
   347|         0|            0|            0|  0.00%|            each update. Default: ``False``.
   348|         0|            0|            0|  0.00%|
   349|         0|            0|            0|  0.00%|    Example:
   350|         0|            0|            0|  0.00%|        >>> # Assuming optimizer uses lr = 0.05 for all groups
   351|         0|            0|            0|  0.00%|        >>> # lr = 0.05     if epoch < 30
   352|         0|            0|            0|  0.00%|        >>> # lr = 0.005    if 30 <= epoch < 60
   353|         0|            0|            0|  0.00%|        >>> # lr = 0.0005   if 60 <= epoch < 90
   354|         0|            0|            0|  0.00%|        >>> # ...
   355|         0|            0|            0|  0.00%|        >>> scheduler = StepLR(optimizer, step_size=30, gamma=0.1)
   356|         0|            0|            0|  0.00%|        >>> for epoch in range(100):
   357|         0|            0|            0|  0.00%|        >>>     train(...)
   358|         0|            0|            0|  0.00%|        >>>     validate(...)
   359|         0|            0|            0|  0.00%|        >>>     scheduler.step()
   360|         0|            0|            0|  0.00%|    """
   361|         0|            0|            0|  0.00%|
   362|         0|            0|            0|  0.00%|    def __init__(self, optimizer, step_size, gamma=0.1, last_epoch=-1, verbose=False):
   363|         0|            0|            0|  0.00%|        self.step_size = step_size
   364|         0|            0|            0|  0.00%|        self.gamma = gamma
   365|         0|            0|            0|  0.00%|        super(StepLR, self).__init__(optimizer, last_epoch, verbose)
   366|         0|            0|            0|  0.00%|
   367|         1|   1.4782e-05|   1.4782e-05|  0.00%|    def get_lr(self):
   368|         1|  7.39098e-06|  7.39098e-06|  0.00%|        if not self._get_lr_called_within_step:
   369|         0|            0|            0|  0.00%|            warnings.warn("To get the last learning rate computed by the scheduler, "
   370|         0|            0|            0|  0.00%|                          "please use `get_last_lr()`.", UserWarning)
   371|         0|            0|            0|  0.00%|
   372|         1|  2.57492e-05|  2.57492e-05|  0.00%|        if (self.last_epoch == 0) or (self.last_epoch % self.step_size != 0):
   373|         4|  4.48227e-05|  1.12057e-05|  0.00%|            return [group['lr'] for group in self.optimizer.param_groups]
(call)|         1|  2.52724e-05|  2.52724e-05|  0.00%|# /opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:373 <listcomp>
   374|         0|            0|            0|  0.00%|        return [group['lr'] * self.gamma
   375|         0|            0|            0|  0.00%|                for group in self.optimizer.param_groups]
   376|         0|            0|            0|  0.00%|
   377|         0|            0|            0|  0.00%|    def _get_closed_form_lr(self):
   378|         0|            0|            0|  0.00%|        return [base_lr * self.gamma ** (self.last_epoch // self.step_size)
   379|         0|            0|            0|  0.00%|                for base_lr in self.base_lrs]
   380|         0|            0|            0|  0.00%|
   381|         0|            0|            0|  0.00%|
   382|         0|            0|            0|  0.00%|class MultiStepLR(_LRScheduler):
   383|         0|            0|            0|  0.00%|    """Decays the learning rate of each parameter group by gamma once the
   384|         0|            0|            0|  0.00%|    number of epoch reaches one of the milestones. Notice that such decay can
   385|         0|            0|            0|  0.00%|    happen simultaneously with other changes to the learning rate from outside
   386|         0|            0|            0|  0.00%|    this scheduler. When last_epoch=-1, sets initial lr as lr.
   387|         0|            0|            0|  0.00%|
   388|         0|            0|            0|  0.00%|    Args:
   389|         0|            0|            0|  0.00%|        optimizer (Optimizer): Wrapped optimizer.
   390|         0|            0|            0|  0.00%|        milestones (list): List of epoch indices. Must be increasing.
   391|         0|            0|            0|  0.00%|        gamma (float): Multiplicative factor of learning rate decay.
   392|         0|            0|            0|  0.00%|            Default: 0.1.
   393|         0|            0|            0|  0.00%|        last_epoch (int): The index of last epoch. Default: -1.
   394|         0|            0|            0|  0.00%|        verbose (bool): If ``True``, prints a message to stdout for
   395|         0|            0|            0|  0.00%|            each update. Default: ``False``.
   396|         0|            0|            0|  0.00%|
   397|         0|            0|            0|  0.00%|    Example:
   398|         0|            0|            0|  0.00%|        >>> # Assuming optimizer uses lr = 0.05 for all groups
   399|         0|            0|            0|  0.00%|        >>> # lr = 0.05     if epoch < 30
   400|         0|            0|            0|  0.00%|        >>> # lr = 0.005    if 30 <= epoch < 80
   401|         0|            0|            0|  0.00%|        >>> # lr = 0.0005   if epoch >= 80
   402|         0|            0|            0|  0.00%|        >>> scheduler = MultiStepLR(optimizer, milestones=[30,80], gamma=0.1)
   403|         0|            0|            0|  0.00%|        >>> for epoch in range(100):
   404|         0|            0|            0|  0.00%|        >>>     train(...)
   405|         0|            0|            0|  0.00%|        >>>     validate(...)
   406|         0|            0|            0|  0.00%|        >>>     scheduler.step()
   407|         0|            0|            0|  0.00%|    """
   408|         0|            0|            0|  0.00%|
   409|         0|            0|            0|  0.00%|    def __init__(self, optimizer, milestones, gamma=0.1, last_epoch=-1, verbose=False):
   410|         0|            0|            0|  0.00%|        self.milestones = Counter(milestones)
   411|         0|            0|            0|  0.00%|        self.gamma = gamma
   412|         0|            0|            0|  0.00%|        super(MultiStepLR, self).__init__(optimizer, last_epoch, verbose)
   413|         0|            0|            0|  0.00%|
   414|         0|            0|            0|  0.00%|    def get_lr(self):
   415|         0|            0|            0|  0.00%|        if not self._get_lr_called_within_step:
   416|         0|            0|            0|  0.00%|            warnings.warn("To get the last learning rate computed by the scheduler, "
   417|         0|            0|            0|  0.00%|                          "please use `get_last_lr()`.", UserWarning)
   418|         0|            0|            0|  0.00%|
   419|         0|            0|            0|  0.00%|        if self.last_epoch not in self.milestones:
   420|         0|            0|            0|  0.00%|            return [group['lr'] for group in self.optimizer.param_groups]
   421|         0|            0|            0|  0.00%|        return [group['lr'] * self.gamma ** self.milestones[self.last_epoch]
   422|         0|            0|            0|  0.00%|                for group in self.optimizer.param_groups]
   423|         0|            0|            0|  0.00%|
   424|         0|            0|            0|  0.00%|    def _get_closed_form_lr(self):
   425|         0|            0|            0|  0.00%|        milestones = list(sorted(self.milestones.elements()))
   426|         0|            0|            0|  0.00%|        return [base_lr * self.gamma ** bisect_right(milestones, self.last_epoch)
   427|         0|            0|            0|  0.00%|                for base_lr in self.base_lrs]
   428|         0|            0|            0|  0.00%|
   429|         0|            0|            0|  0.00%|
   430|         0|            0|            0|  0.00%|class ExponentialLR(_LRScheduler):
   431|         0|            0|            0|  0.00%|    """Decays the learning rate of each parameter group by gamma every epoch.
   432|         0|            0|            0|  0.00%|    When last_epoch=-1, sets initial lr as lr.
   433|         0|            0|            0|  0.00%|
   434|         0|            0|            0|  0.00%|    Args:
   435|         0|            0|            0|  0.00%|        optimizer (Optimizer): Wrapped optimizer.
   436|         0|            0|            0|  0.00%|        gamma (float): Multiplicative factor of learning rate decay.
   437|         0|            0|            0|  0.00%|        last_epoch (int): The index of last epoch. Default: -1.
   438|         0|            0|            0|  0.00%|        verbose (bool): If ``True``, prints a message to stdout for
   439|         0|            0|            0|  0.00%|            each update. Default: ``False``.
   440|         0|            0|            0|  0.00%|    """
   441|         0|            0|            0|  0.00%|
   442|         0|            0|            0|  0.00%|    def __init__(self, optimizer, gamma, last_epoch=-1, verbose=False):
   443|         0|            0|            0|  0.00%|        self.gamma = gamma
   444|         0|            0|            0|  0.00%|        super(ExponentialLR, self).__init__(optimizer, last_epoch, verbose)
   445|         0|            0|            0|  0.00%|
   446|         0|            0|            0|  0.00%|    def get_lr(self):
   447|         0|            0|            0|  0.00%|        if not self._get_lr_called_within_step:
   448|         0|            0|            0|  0.00%|            warnings.warn("To get the last learning rate computed by the scheduler, "
   449|         0|            0|            0|  0.00%|                          "please use `get_last_lr()`.", UserWarning)
   450|         0|            0|            0|  0.00%|
   451|         0|            0|            0|  0.00%|        if self.last_epoch == 0:
   452|         0|            0|            0|  0.00%|            return self.base_lrs
   453|         0|            0|            0|  0.00%|        return [group['lr'] * self.gamma
   454|         0|            0|            0|  0.00%|                for group in self.optimizer.param_groups]
   455|         0|            0|            0|  0.00%|
   456|         0|            0|            0|  0.00%|    def _get_closed_form_lr(self):
   457|         0|            0|            0|  0.00%|        return [base_lr * self.gamma ** self.last_epoch
   458|         0|            0|            0|  0.00%|                for base_lr in self.base_lrs]
   459|         0|            0|            0|  0.00%|
   460|         0|            0|            0|  0.00%|
   461|         0|            0|            0|  0.00%|class CosineAnnealingLR(_LRScheduler):
   462|         0|            0|            0|  0.00%|    r"""Set the learning rate of each parameter group using a cosine annealing
   463|         0|            0|            0|  0.00%|    schedule, where :math:`\eta_{max}` is set to the initial lr and
   464|         0|            0|            0|  0.00%|    :math:`T_{cur}` is the number of epochs since the last restart in SGDR:
   465|         0|            0|            0|  0.00%|
   466|         0|            0|            0|  0.00%|    .. math::
   467|         0|            0|            0|  0.00%|        \begin{aligned}
   468|         0|            0|            0|  0.00%|            \eta_t & = \eta_{min} + \frac{1}{2}(\eta_{max} - \eta_{min})\left(1
   469|         0|            0|            0|  0.00%|            + \cos\left(\frac{T_{cur}}{T_{max}}\pi\right)\right),
   470|         0|            0|            0|  0.00%|            & T_{cur} \neq (2k+1)T_{max}; \\
   471|         0|            0|            0|  0.00%|            \eta_{t+1} & = \eta_{t} + \frac{1}{2}(\eta_{max} - \eta_{min})
   472|         0|            0|            0|  0.00%|            \left(1 - \cos\left(\frac{1}{T_{max}}\pi\right)\right),
   473|         0|            0|            0|  0.00%|            & T_{cur} = (2k+1)T_{max}.
   474|         0|            0|            0|  0.00%|        \end{aligned}
   475|         0|            0|            0|  0.00%|
   476|         0|            0|            0|  0.00%|    When last_epoch=-1, sets initial lr as lr. Notice that because the schedule
   477|         0|            0|            0|  0.00%|    is defined recursively, the learning rate can be simultaneously modified
   478|         0|            0|            0|  0.00%|    outside this scheduler by other operators. If the learning rate is set
   479|         0|            0|            0|  0.00%|    solely by this scheduler, the learning rate at each step becomes:
   480|         0|            0|            0|  0.00%|
   481|         0|            0|            0|  0.00%|    .. math::
   482|         0|            0|            0|  0.00%|        \eta_t = \eta_{min} + \frac{1}{2}(\eta_{max} - \eta_{min})\left(1 +
   483|         0|            0|            0|  0.00%|        \cos\left(\frac{T_{cur}}{T_{max}}\pi\right)\right)
   484|         0|            0|            0|  0.00%|
   485|         0|            0|            0|  0.00%|    It has been proposed in
   486|         0|            0|            0|  0.00%|    `SGDR: Stochastic Gradient Descent with Warm Restarts`_. Note that this only
   487|         0|            0|            0|  0.00%|    implements the cosine annealing part of SGDR, and not the restarts.
   488|         0|            0|            0|  0.00%|
   489|         0|            0|            0|  0.00%|    Args:
   490|         0|            0|            0|  0.00%|        optimizer (Optimizer): Wrapped optimizer.
   491|         0|            0|            0|  0.00%|        T_max (int): Maximum number of iterations.
   492|         0|            0|            0|  0.00%|        eta_min (float): Minimum learning rate. Default: 0.
   493|         0|            0|            0|  0.00%|        last_epoch (int): The index of last epoch. Default: -1.
   494|         0|            0|            0|  0.00%|        verbose (bool): If ``True``, prints a message to stdout for
   495|         0|            0|            0|  0.00%|            each update. Default: ``False``.
   496|         0|            0|            0|  0.00%|
   497|         0|            0|            0|  0.00%|    .. _SGDR\: Stochastic Gradient Descent with Warm Restarts:
   498|         0|            0|            0|  0.00%|        https://arxiv.org/abs/1608.03983
   499|         0|            0|            0|  0.00%|    """
   500|         0|            0|            0|  0.00%|
   501|         0|            0|            0|  0.00%|    def __init__(self, optimizer, T_max, eta_min=0, last_epoch=-1, verbose=False):
   502|         0|            0|            0|  0.00%|        self.T_max = T_max
   503|         0|            0|            0|  0.00%|        self.eta_min = eta_min
   504|         0|            0|            0|  0.00%|        super(CosineAnnealingLR, self).__init__(optimizer, last_epoch, verbose)
   505|         0|            0|            0|  0.00%|
   506|         0|            0|            0|  0.00%|    def get_lr(self):
   507|         0|            0|            0|  0.00%|        if not self._get_lr_called_within_step:
   508|         0|            0|            0|  0.00%|            warnings.warn("To get the last learning rate computed by the scheduler, "
   509|         0|            0|            0|  0.00%|                          "please use `get_last_lr()`.", UserWarning)
   510|         0|            0|            0|  0.00%|
   511|         0|            0|            0|  0.00%|        if self.last_epoch == 0:
   512|         0|            0|            0|  0.00%|            return self.base_lrs
   513|         0|            0|            0|  0.00%|        elif (self.last_epoch - 1 - self.T_max) % (2 * self.T_max) == 0:
   514|         0|            0|            0|  0.00%|            return [group['lr'] + (base_lr - self.eta_min) *
   515|         0|            0|            0|  0.00%|                    (1 - math.cos(math.pi / self.T_max)) / 2
   516|         0|            0|            0|  0.00%|                    for base_lr, group in
   517|         0|            0|            0|  0.00%|                    zip(self.base_lrs, self.optimizer.param_groups)]
   518|         0|            0|            0|  0.00%|        return [(1 + math.cos(math.pi * self.last_epoch / self.T_max)) /
   519|         0|            0|            0|  0.00%|                (1 + math.cos(math.pi * (self.last_epoch - 1) / self.T_max)) *
   520|         0|            0|            0|  0.00%|                (group['lr'] - self.eta_min) + self.eta_min
   521|         0|            0|            0|  0.00%|                for group in self.optimizer.param_groups]
   522|         0|            0|            0|  0.00%|
   523|         0|            0|            0|  0.00%|    def _get_closed_form_lr(self):
   524|         0|            0|            0|  0.00%|        return [self.eta_min + (base_lr - self.eta_min) *
   525|         0|            0|            0|  0.00%|                (1 + math.cos(math.pi * self.last_epoch / self.T_max)) / 2
   526|         0|            0|            0|  0.00%|                for base_lr in self.base_lrs]
   527|         0|            0|            0|  0.00%|
   528|         0|            0|            0|  0.00%|
   529|         0|            0|            0|  0.00%|class ReduceLROnPlateau(object):
   530|         0|            0|            0|  0.00%|    """Reduce learning rate when a metric has stopped improving.
   531|         0|            0|            0|  0.00%|    Models often benefit from reducing the learning rate by a factor
   532|         0|            0|            0|  0.00%|    of 2-10 once learning stagnates. This scheduler reads a metrics
   533|         0|            0|            0|  0.00%|    quantity and if no improvement is seen for a 'patience' number
   534|         0|            0|            0|  0.00%|    of epochs, the learning rate is reduced.
   535|         0|            0|            0|  0.00%|
   536|         0|            0|            0|  0.00%|    Args:
   537|         0|            0|            0|  0.00%|        optimizer (Optimizer): Wrapped optimizer.
   538|         0|            0|            0|  0.00%|        mode (str): One of `min`, `max`. In `min` mode, lr will
   539|         0|            0|            0|  0.00%|            be reduced when the quantity monitored has stopped
   540|         0|            0|            0|  0.00%|            decreasing; in `max` mode it will be reduced when the
   541|         0|            0|            0|  0.00%|            quantity monitored has stopped increasing. Default: 'min'.
   542|         0|            0|            0|  0.00%|        factor (float): Factor by which the learning rate will be
   543|         0|            0|            0|  0.00%|            reduced. new_lr = lr * factor. Default: 0.1.
   544|         0|            0|            0|  0.00%|        patience (int): Number of epochs with no improvement after
   545|         0|            0|            0|  0.00%|            which learning rate will be reduced. For example, if
   546|         0|            0|            0|  0.00%|            `patience = 2`, then we will ignore the first 2 epochs
   547|         0|            0|            0|  0.00%|            with no improvement, and will only decrease the LR after the
   548|         0|            0|            0|  0.00%|            3rd epoch if the loss still hasn't improved then.
   549|         0|            0|            0|  0.00%|            Default: 10.
   550|         0|            0|            0|  0.00%|        threshold (float): Threshold for measuring the new optimum,
   551|         0|            0|            0|  0.00%|            to only focus on significant changes. Default: 1e-4.
   552|         0|            0|            0|  0.00%|        threshold_mode (str): One of `rel`, `abs`. In `rel` mode,
   553|         0|            0|            0|  0.00%|            dynamic_threshold = best * ( 1 + threshold ) in 'max'
   554|         0|            0|            0|  0.00%|            mode or best * ( 1 - threshold ) in `min` mode.
   555|         0|            0|            0|  0.00%|            In `abs` mode, dynamic_threshold = best + threshold in
   556|         0|            0|            0|  0.00%|            `max` mode or best - threshold in `min` mode. Default: 'rel'.
   557|         0|            0|            0|  0.00%|        cooldown (int): Number of epochs to wait before resuming
   558|         0|            0|            0|  0.00%|            normal operation after lr has been reduced. Default: 0.
   559|         0|            0|            0|  0.00%|        min_lr (float or list): A scalar or a list of scalars. A
   560|         0|            0|            0|  0.00%|            lower bound on the learning rate of all param groups
   561|         0|            0|            0|  0.00%|            or each group respectively. Default: 0.
   562|         0|            0|            0|  0.00%|        eps (float): Minimal decay applied to lr. If the difference
   563|         0|            0|            0|  0.00%|            between new and old lr is smaller than eps, the update is
   564|         0|            0|            0|  0.00%|            ignored. Default: 1e-8.
   565|         0|            0|            0|  0.00%|        verbose (bool): If ``True``, prints a message to stdout for
   566|         0|            0|            0|  0.00%|            each update. Default: ``False``.
   567|         0|            0|            0|  0.00%|
   568|         0|            0|            0|  0.00%|    Example:
   569|         0|            0|            0|  0.00%|        >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)
   570|         0|            0|            0|  0.00%|        >>> scheduler = ReduceLROnPlateau(optimizer, 'min')
   571|         0|            0|            0|  0.00%|        >>> for epoch in range(10):
   572|         0|            0|            0|  0.00%|        >>>     train(...)
   573|         0|            0|            0|  0.00%|        >>>     val_loss = validate(...)
   574|         0|            0|            0|  0.00%|        >>>     # Note that step should be called after validate()
   575|         0|            0|            0|  0.00%|        >>>     scheduler.step(val_loss)
   576|         0|            0|            0|  0.00%|    """
   577|         0|            0|            0|  0.00%|
   578|         0|            0|            0|  0.00%|    def __init__(self, optimizer, mode='min', factor=0.1, patience=10,
   579|         0|            0|            0|  0.00%|                 threshold=1e-4, threshold_mode='rel', cooldown=0,
   580|         0|            0|            0|  0.00%|                 min_lr=0, eps=1e-8, verbose=False):
   581|         0|            0|            0|  0.00%|
   582|         0|            0|            0|  0.00%|        if factor >= 1.0:
   583|         0|            0|            0|  0.00%|            raise ValueError('Factor should be < 1.0.')
   584|         0|            0|            0|  0.00%|        self.factor = factor
   585|         0|            0|            0|  0.00%|
   586|         0|            0|            0|  0.00%|        # Attach optimizer
   587|         0|            0|            0|  0.00%|        if not isinstance(optimizer, Optimizer):
   588|         0|            0|            0|  0.00%|            raise TypeError('{} is not an Optimizer'.format(
   589|         0|            0|            0|  0.00%|                type(optimizer).__name__))
   590|         0|            0|            0|  0.00%|        self.optimizer = optimizer
   591|         0|            0|            0|  0.00%|
   592|         0|            0|            0|  0.00%|        if isinstance(min_lr, list) or isinstance(min_lr, tuple):
   593|         0|            0|            0|  0.00%|            if len(min_lr) != len(optimizer.param_groups):
   594|         0|            0|            0|  0.00%|                raise ValueError("expected {} min_lrs, got {}".format(
   595|         0|            0|            0|  0.00%|                    len(optimizer.param_groups), len(min_lr)))
   596|         0|            0|            0|  0.00%|            self.min_lrs = list(min_lr)
   597|         0|            0|            0|  0.00%|        else:
   598|         0|            0|            0|  0.00%|            self.min_lrs = [min_lr] * len(optimizer.param_groups)
   599|         0|            0|            0|  0.00%|
   600|         0|            0|            0|  0.00%|        self.patience = patience
   601|         0|            0|            0|  0.00%|        self.verbose = verbose
   602|         0|            0|            0|  0.00%|        self.cooldown = cooldown
   603|         0|            0|            0|  0.00%|        self.cooldown_counter = 0
   604|         0|            0|            0|  0.00%|        self.mode = mode
   605|         0|            0|            0|  0.00%|        self.threshold = threshold
   606|         0|            0|            0|  0.00%|        self.threshold_mode = threshold_mode
   607|         0|            0|            0|  0.00%|        self.best = None
   608|         0|            0|            0|  0.00%|        self.num_bad_epochs = None
   609|         0|            0|            0|  0.00%|        self.mode_worse = None  # the worse value for the chosen mode
   610|         0|            0|            0|  0.00%|        self.eps = eps
   611|         0|            0|            0|  0.00%|        self.last_epoch = 0
   612|         0|            0|            0|  0.00%|        self._init_is_better(mode=mode, threshold=threshold,
   613|         0|            0|            0|  0.00%|                             threshold_mode=threshold_mode)
   614|         0|            0|            0|  0.00%|        self._reset()
   615|         0|            0|            0|  0.00%|
   616|         0|            0|            0|  0.00%|    def _reset(self):
   617|         0|            0|            0|  0.00%|        """Resets num_bad_epochs counter and cooldown counter."""
   618|         0|            0|            0|  0.00%|        self.best = self.mode_worse
   619|         0|            0|            0|  0.00%|        self.cooldown_counter = 0
   620|         0|            0|            0|  0.00%|        self.num_bad_epochs = 0
   621|         0|            0|            0|  0.00%|
   622|         0|            0|            0|  0.00%|    def step(self, metrics, epoch=None):
   623|         0|            0|            0|  0.00%|        # convert `metrics` to float, in case it's a zero-dim Tensor
   624|         0|            0|            0|  0.00%|        current = float(metrics)
   625|         0|            0|            0|  0.00%|        if epoch is None:
   626|         0|            0|            0|  0.00%|            epoch = self.last_epoch + 1
   627|         0|            0|            0|  0.00%|        else:
   628|         0|            0|            0|  0.00%|            warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
   629|         0|            0|            0|  0.00%|        self.last_epoch = epoch
   630|         0|            0|            0|  0.00%|
   631|         0|            0|            0|  0.00%|        if self.is_better(current, self.best):
   632|         0|            0|            0|  0.00%|            self.best = current
   633|         0|            0|            0|  0.00%|            self.num_bad_epochs = 0
   634|         0|            0|            0|  0.00%|        else:
   635|         0|            0|            0|  0.00%|            self.num_bad_epochs += 1
   636|         0|            0|            0|  0.00%|
   637|         0|            0|            0|  0.00%|        if self.in_cooldown:
   638|         0|            0|            0|  0.00%|            self.cooldown_counter -= 1
   639|         0|            0|            0|  0.00%|            self.num_bad_epochs = 0  # ignore any bad epochs in cooldown
   640|         0|            0|            0|  0.00%|
   641|         0|            0|            0|  0.00%|        if self.num_bad_epochs > self.patience:
   642|         0|            0|            0|  0.00%|            self._reduce_lr(epoch)
   643|         0|            0|            0|  0.00%|            self.cooldown_counter = self.cooldown
   644|         0|            0|            0|  0.00%|            self.num_bad_epochs = 0
   645|         0|            0|            0|  0.00%|
   646|         0|            0|            0|  0.00%|        self._last_lr = [group['lr'] for group in self.optimizer.param_groups]
   647|         0|            0|            0|  0.00%|
   648|         0|            0|            0|  0.00%|    def _reduce_lr(self, epoch):
   649|         0|            0|            0|  0.00%|        for i, param_group in enumerate(self.optimizer.param_groups):
   650|         0|            0|            0|  0.00%|            old_lr = float(param_group['lr'])
   651|         0|            0|            0|  0.00%|            new_lr = max(old_lr * self.factor, self.min_lrs[i])
   652|         0|            0|            0|  0.00%|            if old_lr - new_lr > self.eps:
   653|         0|            0|            0|  0.00%|                param_group['lr'] = new_lr
   654|         0|            0|            0|  0.00%|                if self.verbose:
   655|         0|            0|            0|  0.00%|                    print('Epoch {:5d}: reducing learning rate'
   656|         0|            0|            0|  0.00%|                          ' of group {} to {:.4e}.'.format(epoch, i, new_lr))
   657|         0|            0|            0|  0.00%|
   658|         0|            0|            0|  0.00%|    @property
   659|         0|            0|            0|  0.00%|    def in_cooldown(self):
   660|         0|            0|            0|  0.00%|        return self.cooldown_counter > 0
   661|         0|            0|            0|  0.00%|
   662|         0|            0|            0|  0.00%|    def is_better(self, a, best):
   663|         0|            0|            0|  0.00%|        if self.mode == 'min' and self.threshold_mode == 'rel':
   664|         0|            0|            0|  0.00%|            rel_epsilon = 1. - self.threshold
   665|         0|            0|            0|  0.00%|            return a < best * rel_epsilon
   666|         0|            0|            0|  0.00%|
   667|         0|            0|            0|  0.00%|        elif self.mode == 'min' and self.threshold_mode == 'abs':
   668|         0|            0|            0|  0.00%|            return a < best - self.threshold
   669|         0|            0|            0|  0.00%|
   670|         0|            0|            0|  0.00%|        elif self.mode == 'max' and self.threshold_mode == 'rel':
   671|         0|            0|            0|  0.00%|            rel_epsilon = self.threshold + 1.
   672|         0|            0|            0|  0.00%|            return a > best * rel_epsilon
   673|         0|            0|            0|  0.00%|
   674|         0|            0|            0|  0.00%|        else:  # mode == 'max' and epsilon_mode == 'abs':
   675|         0|            0|            0|  0.00%|            return a > best + self.threshold
   676|         0|            0|            0|  0.00%|
   677|         0|            0|            0|  0.00%|    def _init_is_better(self, mode, threshold, threshold_mode):
   678|         0|            0|            0|  0.00%|        if mode not in {'min', 'max'}:
   679|         0|            0|            0|  0.00%|            raise ValueError('mode ' + mode + ' is unknown!')
   680|         0|            0|            0|  0.00%|        if threshold_mode not in {'rel', 'abs'}:
   681|         0|            0|            0|  0.00%|            raise ValueError('threshold mode ' + threshold_mode + ' is unknown!')
   682|         0|            0|            0|  0.00%|
   683|         0|            0|            0|  0.00%|        if mode == 'min':
   684|         0|            0|            0|  0.00%|            self.mode_worse = inf
   685|         0|            0|            0|  0.00%|        else:  # mode == 'max':
   686|         0|            0|            0|  0.00%|            self.mode_worse = -inf
   687|         0|            0|            0|  0.00%|
   688|         0|            0|            0|  0.00%|        self.mode = mode
   689|         0|            0|            0|  0.00%|        self.threshold = threshold
   690|         0|            0|            0|  0.00%|        self.threshold_mode = threshold_mode
   691|         0|            0|            0|  0.00%|
   692|         0|            0|            0|  0.00%|    def state_dict(self):
   693|         0|            0|            0|  0.00%|        return {key: value for key, value in self.__dict__.items() if key != 'optimizer'}
   694|         0|            0|            0|  0.00%|
   695|         0|            0|            0|  0.00%|    def load_state_dict(self, state_dict):
   696|         0|            0|            0|  0.00%|        self.__dict__.update(state_dict)
   697|         0|            0|            0|  0.00%|        self._init_is_better(mode=self.mode, threshold=self.threshold, threshold_mode=self.threshold_mode)
   698|         0|            0|            0|  0.00%|
   699|         0|            0|            0|  0.00%|
   700|         0|            0|            0|  0.00%|class CyclicLR(_LRScheduler):
   701|         0|            0|            0|  0.00%|    r"""Sets the learning rate of each parameter group according to
   702|         0|            0|            0|  0.00%|    cyclical learning rate policy (CLR). The policy cycles the learning
   703|         0|            0|            0|  0.00%|    rate between two boundaries with a constant frequency, as detailed in
   704|         0|            0|            0|  0.00%|    the paper `Cyclical Learning Rates for Training Neural Networks`_.
   705|         0|            0|            0|  0.00%|    The distance between the two boundaries can be scaled on a per-iteration
   706|         0|            0|            0|  0.00%|    or per-cycle basis.
   707|         0|            0|            0|  0.00%|
   708|         0|            0|            0|  0.00%|    Cyclical learning rate policy changes the learning rate after every batch.
   709|         0|            0|            0|  0.00%|    `step` should be called after a batch has been used for training.
   710|         0|            0|            0|  0.00%|
   711|         0|            0|            0|  0.00%|    This class has three built-in policies, as put forth in the paper:
   712|         0|            0|            0|  0.00%|
   713|         0|            0|            0|  0.00%|    * "triangular": A basic triangular cycle without amplitude scaling.
   714|         0|            0|            0|  0.00%|    * "triangular2": A basic triangular cycle that scales initial amplitude by half each cycle.
   715|         0|            0|            0|  0.00%|    * "exp_range": A cycle that scales initial amplitude by :math:`\text{gamma}^{\text{cycle iterations}}`
   716|         0|            0|            0|  0.00%|      at each cycle iteration.
   717|         0|            0|            0|  0.00%|
   718|         0|            0|            0|  0.00%|    This implementation was adapted from the github repo: `bckenstler/CLR`_
   719|         0|            0|            0|  0.00%|
   720|         0|            0|            0|  0.00%|    Args:
   721|         0|            0|            0|  0.00%|        optimizer (Optimizer): Wrapped optimizer.
   722|         0|            0|            0|  0.00%|        base_lr (float or list): Initial learning rate which is the
   723|         0|            0|            0|  0.00%|            lower boundary in the cycle for each parameter group.
   724|         0|            0|            0|  0.00%|        max_lr (float or list): Upper learning rate boundaries in the cycle
   725|         0|            0|            0|  0.00%|            for each parameter group. Functionally,
   726|         0|            0|            0|  0.00%|            it defines the cycle amplitude (max_lr - base_lr).
   727|         0|            0|            0|  0.00%|            The lr at any cycle is the sum of base_lr
   728|         0|            0|            0|  0.00%|            and some scaling of the amplitude; therefore
   729|         0|            0|            0|  0.00%|            max_lr may not actually be reached depending on
   730|         0|            0|            0|  0.00%|            scaling function.
   731|         0|            0|            0|  0.00%|        step_size_up (int): Number of training iterations in the
   732|         0|            0|            0|  0.00%|            increasing half of a cycle. Default: 2000
   733|         0|            0|            0|  0.00%|        step_size_down (int): Number of training iterations in the
   734|         0|            0|            0|  0.00%|            decreasing half of a cycle. If step_size_down is None,
   735|         0|            0|            0|  0.00%|            it is set to step_size_up. Default: None
   736|         0|            0|            0|  0.00%|        mode (str): One of {triangular, triangular2, exp_range}.
   737|         0|            0|            0|  0.00%|            Values correspond to policies detailed above.
   738|         0|            0|            0|  0.00%|            If scale_fn is not None, this argument is ignored.
   739|         0|            0|            0|  0.00%|            Default: 'triangular'
   740|         0|            0|            0|  0.00%|        gamma (float): Constant in 'exp_range' scaling function:
   741|         0|            0|            0|  0.00%|            gamma**(cycle iterations)
   742|         0|            0|            0|  0.00%|            Default: 1.0
   743|         0|            0|            0|  0.00%|        scale_fn (function): Custom scaling policy defined by a single
   744|         0|            0|            0|  0.00%|            argument lambda function, where
   745|         0|            0|            0|  0.00%|            0 <= scale_fn(x) <= 1 for all x >= 0.
   746|         0|            0|            0|  0.00%|            If specified, then 'mode' is ignored.
   747|         0|            0|            0|  0.00%|            Default: None
   748|         0|            0|            0|  0.00%|        scale_mode (str): {'cycle', 'iterations'}.
   749|         0|            0|            0|  0.00%|            Defines whether scale_fn is evaluated on
   750|         0|            0|            0|  0.00%|            cycle number or cycle iterations (training
   751|         0|            0|            0|  0.00%|            iterations since start of cycle).
   752|         0|            0|            0|  0.00%|            Default: 'cycle'
   753|         0|            0|            0|  0.00%|        cycle_momentum (bool): If ``True``, momentum is cycled inversely
   754|         0|            0|            0|  0.00%|            to learning rate between 'base_momentum' and 'max_momentum'.
   755|         0|            0|            0|  0.00%|            Default: True
   756|         0|            0|            0|  0.00%|        base_momentum (float or list): Lower momentum boundaries in the cycle
   757|         0|            0|            0|  0.00%|            for each parameter group. Note that momentum is cycled inversely
   758|         0|            0|            0|  0.00%|            to learning rate; at the peak of a cycle, momentum is
   759|         0|            0|            0|  0.00%|            'base_momentum' and learning rate is 'max_lr'.
   760|         0|            0|            0|  0.00%|            Default: 0.8
   761|         0|            0|            0|  0.00%|        max_momentum (float or list): Upper momentum boundaries in the cycle
   762|         0|            0|            0|  0.00%|            for each parameter group. Functionally,
   763|         0|            0|            0|  0.00%|            it defines the cycle amplitude (max_momentum - base_momentum).
   764|         0|            0|            0|  0.00%|            The momentum at any cycle is the difference of max_momentum
   765|         0|            0|            0|  0.00%|            and some scaling of the amplitude; therefore
   766|         0|            0|            0|  0.00%|            base_momentum may not actually be reached depending on
   767|         0|            0|            0|  0.00%|            scaling function. Note that momentum is cycled inversely
   768|         0|            0|            0|  0.00%|            to learning rate; at the start of a cycle, momentum is 'max_momentum'
   769|         0|            0|            0|  0.00%|            and learning rate is 'base_lr'
   770|         0|            0|            0|  0.00%|            Default: 0.9
   771|         0|            0|            0|  0.00%|        last_epoch (int): The index of the last batch. This parameter is used when
   772|         0|            0|            0|  0.00%|            resuming a training job. Since `step()` should be invoked after each
   773|         0|            0|            0|  0.00%|            batch instead of after each epoch, this number represents the total
   774|         0|            0|            0|  0.00%|            number of *batches* computed, not the total number of epochs computed.
   775|         0|            0|            0|  0.00%|            When last_epoch=-1, the schedule is started from the beginning.
   776|         0|            0|            0|  0.00%|            Default: -1
   777|         0|            0|            0|  0.00%|        verbose (bool): If ``True``, prints a message to stdout for
   778|         0|            0|            0|  0.00%|            each update. Default: ``False``.
   779|         0|            0|            0|  0.00%|
   780|         0|            0|            0|  0.00%|    Example:
   781|         0|            0|            0|  0.00%|        >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)
   782|         0|            0|            0|  0.00%|        >>> scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.01, max_lr=0.1)
   783|         0|            0|            0|  0.00%|        >>> data_loader = torch.utils.data.DataLoader(...)
   784|         0|            0|            0|  0.00%|        >>> for epoch in range(10):
   785|         0|            0|            0|  0.00%|        >>>     for batch in data_loader:
   786|         0|            0|            0|  0.00%|        >>>         train_batch(...)
   787|         0|            0|            0|  0.00%|        >>>         scheduler.step()
   788|         0|            0|            0|  0.00%|
   789|         0|            0|            0|  0.00%|
   790|         0|            0|            0|  0.00%|    .. _Cyclical Learning Rates for Training Neural Networks: https://arxiv.org/abs/1506.01186
   791|         0|            0|            0|  0.00%|    .. _bckenstler/CLR: https://github.com/bckenstler/CLR
   792|         0|            0|            0|  0.00%|    """
   793|         0|            0|            0|  0.00%|
   794|         0|            0|            0|  0.00%|    def __init__(self,
   795|         0|            0|            0|  0.00%|                 optimizer,
   796|         0|            0|            0|  0.00%|                 base_lr,
   797|         0|            0|            0|  0.00%|                 max_lr,
   798|         0|            0|            0|  0.00%|                 step_size_up=2000,
   799|         0|            0|            0|  0.00%|                 step_size_down=None,
   800|         0|            0|            0|  0.00%|                 mode='triangular',
   801|         0|            0|            0|  0.00%|                 gamma=1.,
   802|         0|            0|            0|  0.00%|                 scale_fn=None,
   803|         0|            0|            0|  0.00%|                 scale_mode='cycle',
   804|         0|            0|            0|  0.00%|                 cycle_momentum=True,
   805|         0|            0|            0|  0.00%|                 base_momentum=0.8,
   806|         0|            0|            0|  0.00%|                 max_momentum=0.9,
   807|         0|            0|            0|  0.00%|                 last_epoch=-1,
   808|         0|            0|            0|  0.00%|                 verbose=False):
   809|         0|            0|            0|  0.00%|
   810|         0|            0|            0|  0.00%|        # Attach optimizer
   811|         0|            0|            0|  0.00%|        if not isinstance(optimizer, Optimizer):
   812|         0|            0|            0|  0.00%|            raise TypeError('{} is not an Optimizer'.format(
   813|         0|            0|            0|  0.00%|                type(optimizer).__name__))
   814|         0|            0|            0|  0.00%|        self.optimizer = optimizer
   815|         0|            0|            0|  0.00%|
   816|         0|            0|            0|  0.00%|        base_lrs = self._format_param('base_lr', optimizer, base_lr)
   817|         0|            0|            0|  0.00%|        if last_epoch == -1:
   818|         0|            0|            0|  0.00%|            for lr, group in zip(base_lrs, optimizer.param_groups):
   819|         0|            0|            0|  0.00%|                group['lr'] = lr
   820|         0|            0|            0|  0.00%|
   821|         0|            0|            0|  0.00%|        self.max_lrs = self._format_param('max_lr', optimizer, max_lr)
   822|         0|            0|            0|  0.00%|
   823|         0|            0|            0|  0.00%|        step_size_up = float(step_size_up)
   824|         0|            0|            0|  0.00%|        step_size_down = float(step_size_down) if step_size_down is not None else step_size_up
   825|         0|            0|            0|  0.00%|        self.total_size = step_size_up + step_size_down
   826|         0|            0|            0|  0.00%|        self.step_ratio = step_size_up / self.total_size
   827|         0|            0|            0|  0.00%|
   828|         0|            0|            0|  0.00%|        if mode not in ['triangular', 'triangular2', 'exp_range'] \
   829|         0|            0|            0|  0.00%|                and scale_fn is None:
   830|         0|            0|            0|  0.00%|            raise ValueError('mode is invalid and scale_fn is None')
   831|         0|            0|            0|  0.00%|
   832|         0|            0|            0|  0.00%|        self.mode = mode
   833|         0|            0|            0|  0.00%|        self.gamma = gamma
   834|         0|            0|            0|  0.00%|
   835|         0|            0|            0|  0.00%|        if scale_fn is None:
   836|         0|            0|            0|  0.00%|            if self.mode == 'triangular':
   837|         0|            0|            0|  0.00%|                self.scale_fn = self._triangular_scale_fn
   838|         0|            0|            0|  0.00%|                self.scale_mode = 'cycle'
   839|         0|            0|            0|  0.00%|            elif self.mode == 'triangular2':
   840|         0|            0|            0|  0.00%|                self.scale_fn = self._triangular2_scale_fn
   841|         0|            0|            0|  0.00%|                self.scale_mode = 'cycle'
   842|         0|            0|            0|  0.00%|            elif self.mode == 'exp_range':
   843|         0|            0|            0|  0.00%|                self.scale_fn = self._exp_range_scale_fn
   844|         0|            0|            0|  0.00%|                self.scale_mode = 'iterations'
   845|         0|            0|            0|  0.00%|        else:
   846|         0|            0|            0|  0.00%|            self.scale_fn = scale_fn
   847|         0|            0|            0|  0.00%|            self.scale_mode = scale_mode
   848|         0|            0|            0|  0.00%|
   849|         0|            0|            0|  0.00%|        self.cycle_momentum = cycle_momentum
   850|         0|            0|            0|  0.00%|        if cycle_momentum:
   851|         0|            0|            0|  0.00%|            if 'momentum' not in optimizer.defaults:
   852|         0|            0|            0|  0.00%|                raise ValueError('optimizer must support momentum with `cycle_momentum` option enabled')
   853|         0|            0|            0|  0.00%|
   854|         0|            0|            0|  0.00%|            base_momentums = self._format_param('base_momentum', optimizer, base_momentum)
   855|         0|            0|            0|  0.00%|            if last_epoch == -1:
   856|         0|            0|            0|  0.00%|                for momentum, group in zip(base_momentums, optimizer.param_groups):
   857|         0|            0|            0|  0.00%|                    group['momentum'] = momentum
   858|         0|            0|            0|  0.00%|            self.base_momentums = [group['momentum'] for group in optimizer.param_groups]
   859|         0|            0|            0|  0.00%|            self.max_momentums = self._format_param('max_momentum', optimizer, max_momentum)
   860|         0|            0|            0|  0.00%|
   861|         0|            0|            0|  0.00%|        super(CyclicLR, self).__init__(optimizer, last_epoch, verbose)
   862|         0|            0|            0|  0.00%|        self.base_lrs = base_lrs
   863|         0|            0|            0|  0.00%|
   864|         0|            0|            0|  0.00%|    def _format_param(self, name, optimizer, param):
   865|         0|            0|            0|  0.00%|        """Return correctly formatted lr/momentum for each param group."""
   866|         0|            0|            0|  0.00%|        if isinstance(param, (list, tuple)):
   867|         0|            0|            0|  0.00%|            if len(param) != len(optimizer.param_groups):
   868|         0|            0|            0|  0.00%|                raise ValueError("expected {} values for {}, got {}".format(
   869|         0|            0|            0|  0.00%|                    len(optimizer.param_groups), name, len(param)))
   870|         0|            0|            0|  0.00%|            return param
   871|         0|            0|            0|  0.00%|        else:
   872|         0|            0|            0|  0.00%|            return [param] * len(optimizer.param_groups)
   873|         0|            0|            0|  0.00%|
   874|         0|            0|            0|  0.00%|    def _triangular_scale_fn(self, x):
   875|         0|            0|            0|  0.00%|        return 1.
   876|         0|            0|            0|  0.00%|
   877|         0|            0|            0|  0.00%|    def _triangular2_scale_fn(self, x):
   878|         0|            0|            0|  0.00%|        return 1 / (2. ** (x - 1))
   879|         0|            0|            0|  0.00%|
   880|         0|            0|            0|  0.00%|    def _exp_range_scale_fn(self, x):
   881|         0|            0|            0|  0.00%|        return self.gamma**(x)
   882|         0|            0|            0|  0.00%|
   883|         0|            0|            0|  0.00%|    def get_lr(self):
   884|         0|            0|            0|  0.00%|        """Calculates the learning rate at batch index. This function treats
   885|         0|            0|            0|  0.00%|        `self.last_epoch` as the last batch index.
   886|         0|            0|            0|  0.00%|
   887|         0|            0|            0|  0.00%|        If `self.cycle_momentum` is ``True``, this function has a side effect of
   888|         0|            0|            0|  0.00%|        updating the optimizer's momentum.
   889|         0|            0|            0|  0.00%|        """
   890|         0|            0|            0|  0.00%|
   891|         0|            0|            0|  0.00%|        if not self._get_lr_called_within_step:
   892|         0|            0|            0|  0.00%|            warnings.warn("To get the last learning rate computed by the scheduler, "
   893|         0|            0|            0|  0.00%|                          "please use `get_last_lr()`.", UserWarning)
   894|         0|            0|            0|  0.00%|
   895|         0|            0|            0|  0.00%|        cycle = math.floor(1 + self.last_epoch / self.total_size)
   896|         0|            0|            0|  0.00%|        x = 1. + self.last_epoch / self.total_size - cycle
   897|         0|            0|            0|  0.00%|        if x <= self.step_ratio:
   898|         0|            0|            0|  0.00%|            scale_factor = x / self.step_ratio
   899|         0|            0|            0|  0.00%|        else:
   900|         0|            0|            0|  0.00%|            scale_factor = (x - 1) / (self.step_ratio - 1)
   901|         0|            0|            0|  0.00%|
   902|         0|            0|            0|  0.00%|        lrs = []
   903|         0|            0|            0|  0.00%|        for base_lr, max_lr in zip(self.base_lrs, self.max_lrs):
   904|         0|            0|            0|  0.00%|            base_height = (max_lr - base_lr) * scale_factor
   905|         0|            0|            0|  0.00%|            if self.scale_mode == 'cycle':
   906|         0|            0|            0|  0.00%|                lr = base_lr + base_height * self.scale_fn(cycle)
   907|         0|            0|            0|  0.00%|            else:
   908|         0|            0|            0|  0.00%|                lr = base_lr + base_height * self.scale_fn(self.last_epoch)
   909|         0|            0|            0|  0.00%|            lrs.append(lr)
   910|         0|            0|            0|  0.00%|
   911|         0|            0|            0|  0.00%|        if self.cycle_momentum:
   912|         0|            0|            0|  0.00%|            momentums = []
   913|         0|            0|            0|  0.00%|            for base_momentum, max_momentum in zip(self.base_momentums, self.max_momentums):
   914|         0|            0|            0|  0.00%|                base_height = (max_momentum - base_momentum) * scale_factor
   915|         0|            0|            0|  0.00%|                if self.scale_mode == 'cycle':
   916|         0|            0|            0|  0.00%|                    momentum = max_momentum - base_height * self.scale_fn(cycle)
   917|         0|            0|            0|  0.00%|                else:
   918|         0|            0|            0|  0.00%|                    momentum = max_momentum - base_height * self.scale_fn(self.last_epoch)
   919|         0|            0|            0|  0.00%|                momentums.append(momentum)
   920|         0|            0|            0|  0.00%|            for param_group, momentum in zip(self.optimizer.param_groups, momentums):
   921|         0|            0|            0|  0.00%|                param_group['momentum'] = momentum
   922|         0|            0|            0|  0.00%|
   923|         0|            0|            0|  0.00%|        return lrs
   924|         0|            0|            0|  0.00%|
   925|         0|            0|            0|  0.00%|
   926|         0|            0|            0|  0.00%|class CosineAnnealingWarmRestarts(_LRScheduler):
   927|         0|            0|            0|  0.00%|    r"""Set the learning rate of each parameter group using a cosine annealing
   928|         0|            0|            0|  0.00%|    schedule, where :math:`\eta_{max}` is set to the initial lr, :math:`T_{cur}`
   929|         0|            0|            0|  0.00%|    is the number of epochs since the last restart and :math:`T_{i}` is the number
   930|         0|            0|            0|  0.00%|    of epochs between two warm restarts in SGDR:
   931|         0|            0|            0|  0.00%|
   932|         0|            0|            0|  0.00%|    .. math::
   933|         0|            0|            0|  0.00%|        \eta_t = \eta_{min} + \frac{1}{2}(\eta_{max} - \eta_{min})\left(1 +
   934|         0|            0|            0|  0.00%|        \cos\left(\frac{T_{cur}}{T_{i}}\pi\right)\right)
   935|         0|            0|            0|  0.00%|
   936|         0|            0|            0|  0.00%|    When :math:`T_{cur}=T_{i}`, set :math:`\eta_t = \eta_{min}`.
   937|         0|            0|            0|  0.00%|    When :math:`T_{cur}=0` after restart, set :math:`\eta_t=\eta_{max}`.
   938|         0|            0|            0|  0.00%|
   939|         0|            0|            0|  0.00%|    It has been proposed in
   940|         0|            0|            0|  0.00%|    `SGDR: Stochastic Gradient Descent with Warm Restarts`_.
   941|         0|            0|            0|  0.00%|
   942|         0|            0|            0|  0.00%|    Args:
   943|         0|            0|            0|  0.00%|        optimizer (Optimizer): Wrapped optimizer.
   944|         0|            0|            0|  0.00%|        T_0 (int): Number of iterations for the first restart.
   945|         0|            0|            0|  0.00%|        T_mult (int, optional): A factor increases :math:`T_{i}` after a restart. Default: 1.
   946|         0|            0|            0|  0.00%|        eta_min (float, optional): Minimum learning rate. Default: 0.
   947|         0|            0|            0|  0.00%|        last_epoch (int, optional): The index of last epoch. Default: -1.
   948|         0|            0|            0|  0.00%|        verbose (bool): If ``True``, prints a message to stdout for
   949|         0|            0|            0|  0.00%|            each update. Default: ``False``.
   950|         0|            0|            0|  0.00%|
   951|         0|            0|            0|  0.00%|    .. _SGDR\: Stochastic Gradient Descent with Warm Restarts:
   952|         0|            0|            0|  0.00%|        https://arxiv.org/abs/1608.03983
   953|         0|            0|            0|  0.00%|    """
   954|         0|            0|            0|  0.00%|
   955|         0|            0|            0|  0.00%|    def __init__(self, optimizer, T_0, T_mult=1, eta_min=0, last_epoch=-1, verbose=False):
   956|         0|            0|            0|  0.00%|        if T_0 <= 0 or not isinstance(T_0, int):
   957|         0|            0|            0|  0.00%|            raise ValueError("Expected positive integer T_0, but got {}".format(T_0))
   958|         0|            0|            0|  0.00%|        if T_mult < 1 or not isinstance(T_mult, int):
   959|         0|            0|            0|  0.00%|            raise ValueError("Expected integer T_mult >= 1, but got {}".format(T_mult))
   960|         0|            0|            0|  0.00%|        self.T_0 = T_0
   961|         0|            0|            0|  0.00%|        self.T_i = T_0
   962|         0|            0|            0|  0.00%|        self.T_mult = T_mult
   963|         0|            0|            0|  0.00%|        self.eta_min = eta_min
   964|         0|            0|            0|  0.00%|
   965|         0|            0|            0|  0.00%|        super(CosineAnnealingWarmRestarts, self).__init__(optimizer, last_epoch, verbose)
   966|         0|            0|            0|  0.00%|
   967|         0|            0|            0|  0.00%|        self.T_cur = self.last_epoch
   968|         0|            0|            0|  0.00%|
   969|         0|            0|            0|  0.00%|    def get_lr(self):
   970|         0|            0|            0|  0.00%|        if not self._get_lr_called_within_step:
   971|         0|            0|            0|  0.00%|            warnings.warn("To get the last learning rate computed by the scheduler, "
   972|         0|            0|            0|  0.00%|                          "please use `get_last_lr()`.", UserWarning)
   973|         0|            0|            0|  0.00%|
   974|         0|            0|            0|  0.00%|        return [self.eta_min + (base_lr - self.eta_min) * (1 + math.cos(math.pi * self.T_cur / self.T_i)) / 2
   975|         0|            0|            0|  0.00%|                for base_lr in self.base_lrs]
   976|         0|            0|            0|  0.00%|
   977|         0|            0|            0|  0.00%|    def step(self, epoch=None):
   978|         0|            0|            0|  0.00%|        """Step could be called after every batch update
   979|         0|            0|            0|  0.00%|
   980|         0|            0|            0|  0.00%|        Example:
   981|         0|            0|            0|  0.00%|            >>> scheduler = CosineAnnealingWarmRestarts(optimizer, T_0, T_mult)
   982|         0|            0|            0|  0.00%|            >>> iters = len(dataloader)
   983|         0|            0|            0|  0.00%|            >>> for epoch in range(20):
   984|         0|            0|            0|  0.00%|            >>>     for i, sample in enumerate(dataloader):
   985|         0|            0|            0|  0.00%|            >>>         inputs, labels = sample['inputs'], sample['labels']
   986|         0|            0|            0|  0.00%|            >>>         optimizer.zero_grad()
   987|         0|            0|            0|  0.00%|            >>>         outputs = net(inputs)
   988|         0|            0|            0|  0.00%|            >>>         loss = criterion(outputs, labels)
   989|         0|            0|            0|  0.00%|            >>>         loss.backward()
   990|         0|            0|            0|  0.00%|            >>>         optimizer.step()
   991|         0|            0|            0|  0.00%|            >>>         scheduler.step(epoch + i / iters)
   992|         0|            0|            0|  0.00%|
   993|         0|            0|            0|  0.00%|        This function can be called in an interleaved way.
   994|         0|            0|            0|  0.00%|
   995|         0|            0|            0|  0.00%|        Example:
   996|         0|            0|            0|  0.00%|            >>> scheduler = CosineAnnealingWarmRestarts(optimizer, T_0, T_mult)
   997|         0|            0|            0|  0.00%|            >>> for epoch in range(20):
   998|         0|            0|            0|  0.00%|            >>>     scheduler.step()
   999|         0|            0|            0|  0.00%|            >>> scheduler.step(26)
  1000|         0|            0|            0|  0.00%|            >>> scheduler.step() # scheduler.step(27), instead of scheduler(20)
  1001|         0|            0|            0|  0.00%|        """
  1002|         0|            0|            0|  0.00%|
  1003|         0|            0|            0|  0.00%|        if epoch is None and self.last_epoch < 0:
  1004|         0|            0|            0|  0.00%|            epoch = 0
  1005|         0|            0|            0|  0.00%|
  1006|         0|            0|            0|  0.00%|        if epoch is None:
  1007|         0|            0|            0|  0.00%|            epoch = self.last_epoch + 1
  1008|         0|            0|            0|  0.00%|            self.T_cur = self.T_cur + 1
  1009|         0|            0|            0|  0.00%|            if self.T_cur >= self.T_i:
  1010|         0|            0|            0|  0.00%|                self.T_cur = self.T_cur - self.T_i
  1011|         0|            0|            0|  0.00%|                self.T_i = self.T_i * self.T_mult
  1012|         0|            0|            0|  0.00%|        else:
  1013|         0|            0|            0|  0.00%|            if epoch < 0:
  1014|         0|            0|            0|  0.00%|                raise ValueError("Expected non-negative epoch, but got {}".format(epoch))
  1015|         0|            0|            0|  0.00%|            if epoch >= self.T_0:
  1016|         0|            0|            0|  0.00%|                if self.T_mult == 1:
  1017|         0|            0|            0|  0.00%|                    self.T_cur = epoch % self.T_0
  1018|         0|            0|            0|  0.00%|                else:
  1019|         0|            0|            0|  0.00%|                    n = int(math.log((epoch / self.T_0 * (self.T_mult - 1) + 1), self.T_mult))
  1020|         0|            0|            0|  0.00%|                    self.T_cur = epoch - self.T_0 * (self.T_mult ** n - 1) / (self.T_mult - 1)
  1021|         0|            0|            0|  0.00%|                    self.T_i = self.T_0 * self.T_mult ** (n)
  1022|         0|            0|            0|  0.00%|            else:
  1023|         0|            0|            0|  0.00%|                self.T_i = self.T_0
  1024|         0|            0|            0|  0.00%|                self.T_cur = epoch
  1025|         0|            0|            0|  0.00%|        self.last_epoch = math.floor(epoch)
  1026|         0|            0|            0|  0.00%|
  1027|         0|            0|            0|  0.00%|        class _enable_get_lr_call:
  1028|         0|            0|            0|  0.00%|
  1029|         0|            0|            0|  0.00%|            def __init__(self, o):
  1030|         0|            0|            0|  0.00%|                self.o = o
  1031|         0|            0|            0|  0.00%|
  1032|         0|            0|            0|  0.00%|            def __enter__(self):
  1033|         0|            0|            0|  0.00%|                self.o._get_lr_called_within_step = True
  1034|         0|            0|            0|  0.00%|                return self
  1035|         0|            0|            0|  0.00%|
  1036|         0|            0|            0|  0.00%|            def __exit__(self, type, value, traceback):
  1037|         0|            0|            0|  0.00%|                self.o._get_lr_called_within_step = False
  1038|         0|            0|            0|  0.00%|                return self
  1039|         0|            0|            0|  0.00%|
  1040|         0|            0|            0|  0.00%|        with _enable_get_lr_call(self):
  1041|         0|            0|            0|  0.00%|            for i, data in enumerate(zip(self.optimizer.param_groups, self.get_lr())):
  1042|         0|            0|            0|  0.00%|                param_group, lr = data
  1043|         0|            0|            0|  0.00%|                param_group['lr'] = lr
  1044|         0|            0|            0|  0.00%|                self.print_lr(self.verbose, i, lr, epoch)
  1045|         0|            0|            0|  0.00%|
  1046|         0|            0|            0|  0.00%|        self._last_lr = [group['lr'] for group in self.optimizer.param_groups]
  1047|         0|            0|            0|  0.00%|
  1048|         0|            0|            0|  0.00%|
  1049|         0|            0|            0|  0.00%|class OneCycleLR(_LRScheduler):
  1050|         0|            0|            0|  0.00%|    r"""Sets the learning rate of each parameter group according to the
  1051|         0|            0|            0|  0.00%|    1cycle learning rate policy. The 1cycle policy anneals the learning
  1052|         0|            0|            0|  0.00%|    rate from an initial learning rate to some maximum learning rate and then
  1053|         0|            0|            0|  0.00%|    from that maximum learning rate to some minimum learning rate much lower
  1054|         0|            0|            0|  0.00%|    than the initial learning rate.
  1055|         0|            0|            0|  0.00%|    This policy was initially described in the paper `Super-Convergence:
  1056|         0|            0|            0|  0.00%|    Very Fast Training of Neural Networks Using Large Learning Rates`_.
  1057|         0|            0|            0|  0.00%|
  1058|         0|            0|            0|  0.00%|    The 1cycle learning rate policy changes the learning rate after every batch.
  1059|         0|            0|            0|  0.00%|    `step` should be called after a batch has been used for training.
  1060|         0|            0|            0|  0.00%|
  1061|         0|            0|            0|  0.00%|    This scheduler is not chainable.
  1062|         0|            0|            0|  0.00%|
  1063|         0|            0|            0|  0.00%|    Note also that the total number of steps in the cycle can be determined in one
  1064|         0|            0|            0|  0.00%|    of two ways (listed in order of precedence):
  1065|         0|            0|            0|  0.00%|
  1066|         0|            0|            0|  0.00%|    #. A value for total_steps is explicitly provided.
  1067|         0|            0|            0|  0.00%|    #. A number of epochs (epochs) and a number of steps per epoch
  1068|         0|            0|            0|  0.00%|       (steps_per_epoch) are provided.
  1069|         0|            0|            0|  0.00%|       In this case, the number of total steps is inferred by
  1070|         0|            0|            0|  0.00%|       total_steps = epochs * steps_per_epoch
  1071|         0|            0|            0|  0.00%|
  1072|         0|            0|            0|  0.00%|    You must either provide a value for total_steps or provide a value for both
  1073|         0|            0|            0|  0.00%|    epochs and steps_per_epoch.
  1074|         0|            0|            0|  0.00%|
  1075|         0|            0|            0|  0.00%|    The default behaviour of this scheduler follows the fastai implementation of 1cycle, which
  1076|         0|            0|            0|  0.00%|    claims that "unpublished work has shown even better results by using only two phases". To
  1077|         0|            0|            0|  0.00%|    mimic the behaviour of the original paper instead, set ``three_phase=True``.
  1078|         0|            0|            0|  0.00%|
  1079|         0|            0|            0|  0.00%|    Args:
  1080|         0|            0|            0|  0.00%|        optimizer (Optimizer): Wrapped optimizer.
  1081|         0|            0|            0|  0.00%|        max_lr (float or list): Upper learning rate boundaries in the cycle
  1082|         0|            0|            0|  0.00%|            for each parameter group.
  1083|         0|            0|            0|  0.00%|        total_steps (int): The total number of steps in the cycle. Note that
  1084|         0|            0|            0|  0.00%|            if a value is not provided here, then it must be inferred by providing
  1085|         0|            0|            0|  0.00%|            a value for epochs and steps_per_epoch.
  1086|         0|            0|            0|  0.00%|            Default: None
  1087|         0|            0|            0|  0.00%|        epochs (int): The number of epochs to train for. This is used along
  1088|         0|            0|            0|  0.00%|            with steps_per_epoch in order to infer the total number of steps in the cycle
  1089|         0|            0|            0|  0.00%|            if a value for total_steps is not provided.
  1090|         0|            0|            0|  0.00%|            Default: None
  1091|         0|            0|            0|  0.00%|        steps_per_epoch (int): The number of steps per epoch to train for. This is
  1092|         0|            0|            0|  0.00%|            used along with epochs in order to infer the total number of steps in the
  1093|         0|            0|            0|  0.00%|            cycle if a value for total_steps is not provided.
  1094|         0|            0|            0|  0.00%|            Default: None
  1095|         0|            0|            0|  0.00%|        pct_start (float): The percentage of the cycle (in number of steps) spent
  1096|         0|            0|            0|  0.00%|            increasing the learning rate.
  1097|         0|            0|            0|  0.00%|            Default: 0.3
  1098|         0|            0|            0|  0.00%|        anneal_strategy (str): {'cos', 'linear'}
  1099|         0|            0|            0|  0.00%|            Specifies the annealing strategy: "cos" for cosine annealing, "linear" for
  1100|         0|            0|            0|  0.00%|            linear annealing.
  1101|         0|            0|            0|  0.00%|            Default: 'cos'
  1102|         0|            0|            0|  0.00%|        cycle_momentum (bool): If ``True``, momentum is cycled inversely
  1103|         0|            0|            0|  0.00%|            to learning rate between 'base_momentum' and 'max_momentum'.
  1104|         0|            0|            0|  0.00%|            Default: True
  1105|         0|            0|            0|  0.00%|        base_momentum (float or list): Lower momentum boundaries in the cycle
  1106|         0|            0|            0|  0.00%|            for each parameter group. Note that momentum is cycled inversely
  1107|         0|            0|            0|  0.00%|            to learning rate; at the peak of a cycle, momentum is
  1108|         0|            0|            0|  0.00%|            'base_momentum' and learning rate is 'max_lr'.
  1109|         0|            0|            0|  0.00%|            Default: 0.85
  1110|         0|            0|            0|  0.00%|        max_momentum (float or list): Upper momentum boundaries in the cycle
  1111|         0|            0|            0|  0.00%|            for each parameter group. Functionally,
  1112|         0|            0|            0|  0.00%|            it defines the cycle amplitude (max_momentum - base_momentum).
  1113|         0|            0|            0|  0.00%|            Note that momentum is cycled inversely
  1114|         0|            0|            0|  0.00%|            to learning rate; at the start of a cycle, momentum is 'max_momentum'
  1115|         0|            0|            0|  0.00%|            and learning rate is 'base_lr'
  1116|         0|            0|            0|  0.00%|            Default: 0.95
  1117|         0|            0|            0|  0.00%|        div_factor (float): Determines the initial learning rate via
  1118|         0|            0|            0|  0.00%|            initial_lr = max_lr/div_factor
  1119|         0|            0|            0|  0.00%|            Default: 25
  1120|         0|            0|            0|  0.00%|        final_div_factor (float): Determines the minimum learning rate via
  1121|         0|            0|            0|  0.00%|            min_lr = initial_lr/final_div_factor
  1122|         0|            0|            0|  0.00%|            Default: 1e4
  1123|         0|            0|            0|  0.00%|        three_phase (bool): If ``True``, use a third phase of the schedule to annihilate the
  1124|         0|            0|            0|  0.00%|            learning rate according to 'final_div_factor' instead of modifying the second
  1125|         0|            0|            0|  0.00%|            phase (the first two phases will be symmetrical about the step indicated by
  1126|         0|            0|            0|  0.00%|            'pct_start').
  1127|         0|            0|            0|  0.00%|        last_epoch (int): The index of the last batch. This parameter is used when
  1128|         0|            0|            0|  0.00%|            resuming a training job. Since `step()` should be invoked after each
  1129|         0|            0|            0|  0.00%|            batch instead of after each epoch, this number represents the total
  1130|         0|            0|            0|  0.00%|            number of *batches* computed, not the total number of epochs computed.
  1131|         0|            0|            0|  0.00%|            When last_epoch=-1, the schedule is started from the beginning.
  1132|         0|            0|            0|  0.00%|            Default: -1
  1133|         0|            0|            0|  0.00%|        verbose (bool): If ``True``, prints a message to stdout for
  1134|         0|            0|            0|  0.00%|            each update. Default: ``False``.
  1135|         0|            0|            0|  0.00%|
  1136|         0|            0|            0|  0.00%|    Example:
  1137|         0|            0|            0|  0.00%|        >>> data_loader = torch.utils.data.DataLoader(...)
  1138|         0|            0|            0|  0.00%|        >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)
  1139|         0|            0|            0|  0.00%|        >>> scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.01, steps_per_epoch=len(data_loader), epochs=10)
  1140|         0|            0|            0|  0.00%|        >>> for epoch in range(10):
  1141|         0|            0|            0|  0.00%|        >>>     for batch in data_loader:
  1142|         0|            0|            0|  0.00%|        >>>         train_batch(...)
  1143|         0|            0|            0|  0.00%|        >>>         scheduler.step()
  1144|         0|            0|            0|  0.00%|
  1145|         0|            0|            0|  0.00%|
  1146|         0|            0|            0|  0.00%|    .. _Super-Convergence\: Very Fast Training of Neural Networks Using Large Learning Rates:
  1147|         0|            0|            0|  0.00%|        https://arxiv.org/abs/1708.07120
  1148|         0|            0|            0|  0.00%|    """
  1149|         0|            0|            0|  0.00%|    def __init__(self,
  1150|         0|            0|            0|  0.00%|                 optimizer,
  1151|         0|            0|            0|  0.00%|                 max_lr,
  1152|         0|            0|            0|  0.00%|                 total_steps=None,
  1153|         0|            0|            0|  0.00%|                 epochs=None,
  1154|         0|            0|            0|  0.00%|                 steps_per_epoch=None,
  1155|         0|            0|            0|  0.00%|                 pct_start=0.3,
  1156|         0|            0|            0|  0.00%|                 anneal_strategy='cos',
  1157|         0|            0|            0|  0.00%|                 cycle_momentum=True,
  1158|         0|            0|            0|  0.00%|                 base_momentum=0.85,
  1159|         0|            0|            0|  0.00%|                 max_momentum=0.95,
  1160|         0|            0|            0|  0.00%|                 div_factor=25.,
  1161|         0|            0|            0|  0.00%|                 final_div_factor=1e4,
  1162|         0|            0|            0|  0.00%|                 three_phase=False,
  1163|         0|            0|            0|  0.00%|                 last_epoch=-1,
  1164|         0|            0|            0|  0.00%|                 verbose=False):
  1165|         0|            0|            0|  0.00%|
  1166|         0|            0|            0|  0.00%|        # Validate optimizer
  1167|         0|            0|            0|  0.00%|        if not isinstance(optimizer, Optimizer):
  1168|         0|            0|            0|  0.00%|            raise TypeError('{} is not an Optimizer'.format(
  1169|         0|            0|            0|  0.00%|                type(optimizer).__name__))
  1170|         0|            0|            0|  0.00%|        self.optimizer = optimizer
  1171|         0|            0|            0|  0.00%|
  1172|         0|            0|            0|  0.00%|        # Validate total_steps
  1173|         0|            0|            0|  0.00%|        if total_steps is None and epochs is None and steps_per_epoch is None:
  1174|         0|            0|            0|  0.00%|            raise ValueError("You must define either total_steps OR (epochs AND steps_per_epoch)")
  1175|         0|            0|            0|  0.00%|        elif total_steps is not None:
  1176|         0|            0|            0|  0.00%|            if total_steps <= 0 or not isinstance(total_steps, int):
  1177|         0|            0|            0|  0.00%|                raise ValueError("Expected positive integer total_steps, but got {}".format(total_steps))
  1178|         0|            0|            0|  0.00%|            self.total_steps = total_steps
  1179|         0|            0|            0|  0.00%|        else:
  1180|         0|            0|            0|  0.00%|            if epochs <= 0 or not isinstance(epochs, int):
  1181|         0|            0|            0|  0.00%|                raise ValueError("Expected positive integer epochs, but got {}".format(epochs))
  1182|         0|            0|            0|  0.00%|            if steps_per_epoch <= 0 or not isinstance(steps_per_epoch, int):
  1183|         0|            0|            0|  0.00%|                raise ValueError("Expected positive integer steps_per_epoch, but got {}".format(steps_per_epoch))
  1184|         0|            0|            0|  0.00%|            self.total_steps = epochs * steps_per_epoch
  1185|         0|            0|            0|  0.00%|
  1186|         0|            0|            0|  0.00%|        if three_phase:
  1187|         0|            0|            0|  0.00%|            self._schedule_phases = [
  1188|         0|            0|            0|  0.00%|                {
  1189|         0|            0|            0|  0.00%|                    'end_step': float(pct_start * self.total_steps) - 1,
  1190|         0|            0|            0|  0.00%|                    'start_lr': 'initial_lr',
  1191|         0|            0|            0|  0.00%|                    'end_lr': 'max_lr',
  1192|         0|            0|            0|  0.00%|                    'start_momentum': 'max_momentum',
  1193|         0|            0|            0|  0.00%|                    'end_momentum': 'base_momentum',
  1194|         0|            0|            0|  0.00%|                },
  1195|         0|            0|            0|  0.00%|                {
  1196|         0|            0|            0|  0.00%|                    'end_step': float(2 * pct_start * self.total_steps) - 2,
  1197|         0|            0|            0|  0.00%|                    'start_lr': 'max_lr',
  1198|         0|            0|            0|  0.00%|                    'end_lr': 'initial_lr',
  1199|         0|            0|            0|  0.00%|                    'start_momentum': 'base_momentum',
  1200|         0|            0|            0|  0.00%|                    'end_momentum': 'max_momentum',
  1201|         0|            0|            0|  0.00%|                },
  1202|         0|            0|            0|  0.00%|                {
  1203|         0|            0|            0|  0.00%|                    'end_step': self.total_steps - 1,
  1204|         0|            0|            0|  0.00%|                    'start_lr': 'initial_lr',
  1205|         0|            0|            0|  0.00%|                    'end_lr': 'min_lr',
  1206|         0|            0|            0|  0.00%|                    'start_momentum': 'max_momentum',
  1207|         0|            0|            0|  0.00%|                    'end_momentum': 'max_momentum',
  1208|         0|            0|            0|  0.00%|                },
  1209|         0|            0|            0|  0.00%|            ]
  1210|         0|            0|            0|  0.00%|        else:
  1211|         0|            0|            0|  0.00%|            self._schedule_phases = [
  1212|         0|            0|            0|  0.00%|                {
  1213|         0|            0|            0|  0.00%|                    'end_step': float(pct_start * self.total_steps) - 1,
  1214|         0|            0|            0|  0.00%|                    'start_lr': 'initial_lr',
  1215|         0|            0|            0|  0.00%|                    'end_lr': 'max_lr',
  1216|         0|            0|            0|  0.00%|                    'start_momentum': 'max_momentum',
  1217|         0|            0|            0|  0.00%|                    'end_momentum': 'base_momentum',
  1218|         0|            0|            0|  0.00%|                },
  1219|         0|            0|            0|  0.00%|                {
  1220|         0|            0|            0|  0.00%|                    'end_step': self.total_steps - 1,
  1221|         0|            0|            0|  0.00%|                    'start_lr': 'max_lr',
  1222|         0|            0|            0|  0.00%|                    'end_lr': 'min_lr',
  1223|         0|            0|            0|  0.00%|                    'start_momentum': 'base_momentum',
  1224|         0|            0|            0|  0.00%|                    'end_momentum': 'max_momentum',
  1225|         0|            0|            0|  0.00%|                },
  1226|         0|            0|            0|  0.00%|            ]
  1227|         0|            0|            0|  0.00%|
  1228|         0|            0|            0|  0.00%|        # Validate pct_start
  1229|         0|            0|            0|  0.00%|        if pct_start < 0 or pct_start > 1 or not isinstance(pct_start, float):
  1230|         0|            0|            0|  0.00%|            raise ValueError("Expected float between 0 and 1 pct_start, but got {}".format(pct_start))
  1231|         0|            0|            0|  0.00%|
  1232|         0|            0|            0|  0.00%|        # Validate anneal_strategy
  1233|         0|            0|            0|  0.00%|        if anneal_strategy not in ['cos', 'linear']:
  1234|         0|            0|            0|  0.00%|            raise ValueError("anneal_strategy must by one of 'cos' or 'linear', instead got {}".format(anneal_strategy))
  1235|         0|            0|            0|  0.00%|        elif anneal_strategy == 'cos':
  1236|         0|            0|            0|  0.00%|            self.anneal_func = self._annealing_cos
  1237|         0|            0|            0|  0.00%|        elif anneal_strategy == 'linear':
  1238|         0|            0|            0|  0.00%|            self.anneal_func = self._annealing_linear
  1239|         0|            0|            0|  0.00%|
  1240|         0|            0|            0|  0.00%|        # Initialize learning rate variables
  1241|         0|            0|            0|  0.00%|        max_lrs = self._format_param('max_lr', self.optimizer, max_lr)
  1242|         0|            0|            0|  0.00%|        if last_epoch == -1:
  1243|         0|            0|            0|  0.00%|            for idx, group in enumerate(self.optimizer.param_groups):
  1244|         0|            0|            0|  0.00%|                group['initial_lr'] = max_lrs[idx] / div_factor
  1245|         0|            0|            0|  0.00%|                group['max_lr'] = max_lrs[idx]
  1246|         0|            0|            0|  0.00%|                group['min_lr'] = group['initial_lr'] / final_div_factor
  1247|         0|            0|            0|  0.00%|
  1248|         0|            0|            0|  0.00%|        # Initialize momentum variables
  1249|         0|            0|            0|  0.00%|        self.cycle_momentum = cycle_momentum
  1250|         0|            0|            0|  0.00%|        if self.cycle_momentum:
  1251|         0|            0|            0|  0.00%|            if 'momentum' not in self.optimizer.defaults and 'betas' not in self.optimizer.defaults:
  1252|         0|            0|            0|  0.00%|                raise ValueError('optimizer must support momentum with `cycle_momentum` option enabled')
  1253|         0|            0|            0|  0.00%|            self.use_beta1 = 'betas' in self.optimizer.defaults
  1254|         0|            0|            0|  0.00%|            max_momentums = self._format_param('max_momentum', optimizer, max_momentum)
  1255|         0|            0|            0|  0.00%|            base_momentums = self._format_param('base_momentum', optimizer, base_momentum)
  1256|         0|            0|            0|  0.00%|            if last_epoch == -1:
  1257|         0|            0|            0|  0.00%|                for m_momentum, b_momentum, group in zip(max_momentums, base_momentums, optimizer.param_groups):
  1258|         0|            0|            0|  0.00%|                    if self.use_beta1:
  1259|         0|            0|            0|  0.00%|                        _, beta2 = group['betas']
  1260|         0|            0|            0|  0.00%|                        group['betas'] = (m_momentum, beta2)
  1261|         0|            0|            0|  0.00%|                    else:
  1262|         0|            0|            0|  0.00%|                        group['momentum'] = m_momentum
  1263|         0|            0|            0|  0.00%|                    group['max_momentum'] = m_momentum
  1264|         0|            0|            0|  0.00%|                    group['base_momentum'] = b_momentum
  1265|         0|            0|            0|  0.00%|
  1266|         0|            0|            0|  0.00%|        super(OneCycleLR, self).__init__(optimizer, last_epoch, verbose)
  1267|         0|            0|            0|  0.00%|
  1268|         0|            0|            0|  0.00%|    def _format_param(self, name, optimizer, param):
  1269|         0|            0|            0|  0.00%|        """Return correctly formatted lr/momentum for each param group."""
  1270|         0|            0|            0|  0.00%|        if isinstance(param, (list, tuple)):
  1271|         0|            0|            0|  0.00%|            if len(param) != len(optimizer.param_groups):
  1272|         0|            0|            0|  0.00%|                raise ValueError("expected {} values for {}, got {}".format(
  1273|         0|            0|            0|  0.00%|                    len(optimizer.param_groups), name, len(param)))
  1274|         0|            0|            0|  0.00%|            return param
  1275|         0|            0|            0|  0.00%|        else:
  1276|         0|            0|            0|  0.00%|            return [param] * len(optimizer.param_groups)
  1277|         0|            0|            0|  0.00%|
  1278|         0|            0|            0|  0.00%|    def _annealing_cos(self, start, end, pct):
  1279|         0|            0|            0|  0.00%|        "Cosine anneal from `start` to `end` as pct goes from 0.0 to 1.0."
  1280|         0|            0|            0|  0.00%|        cos_out = math.cos(math.pi * pct) + 1
  1281|         0|            0|            0|  0.00%|        return end + (start - end) / 2.0 * cos_out
  1282|         0|            0|            0|  0.00%|
  1283|         0|            0|            0|  0.00%|    def _annealing_linear(self, start, end, pct):
  1284|         0|            0|            0|  0.00%|        "Linearly anneal from `start` to `end` as pct goes from 0.0 to 1.0."
  1285|         0|            0|            0|  0.00%|        return (end - start) * pct + start
  1286|         0|            0|            0|  0.00%|
  1287|         0|            0|            0|  0.00%|    def get_lr(self):
  1288|         0|            0|            0|  0.00%|        if not self._get_lr_called_within_step:
  1289|         0|            0|            0|  0.00%|            warnings.warn("To get the last learning rate computed by the scheduler, "
  1290|         0|            0|            0|  0.00%|                          "please use `get_last_lr()`.", UserWarning)
  1291|         0|            0|            0|  0.00%|
  1292|         0|            0|            0|  0.00%|        lrs = []
  1293|         0|            0|            0|  0.00%|        step_num = self.last_epoch
  1294|         0|            0|            0|  0.00%|
  1295|         0|            0|            0|  0.00%|        if step_num > self.total_steps:
  1296|         0|            0|            0|  0.00%|            raise ValueError("Tried to step {} times. The specified number of total steps is {}"
  1297|         0|            0|            0|  0.00%|                             .format(step_num + 1, self.total_steps))
  1298|         0|            0|            0|  0.00%|
  1299|         0|            0|            0|  0.00%|        for group in self.optimizer.param_groups:
  1300|         0|            0|            0|  0.00%|            start_step = 0
  1301|         0|            0|            0|  0.00%|            for i, phase in enumerate(self._schedule_phases):
  1302|         0|            0|            0|  0.00%|                end_step = phase['end_step']
  1303|         0|            0|            0|  0.00%|                if step_num <= end_step or i == len(self._schedule_phases) - 1:
  1304|         0|            0|            0|  0.00%|                    pct = (step_num - start_step) / (end_step - start_step)
  1305|         0|            0|            0|  0.00%|                    computed_lr = self.anneal_func(group[phase['start_lr']], group[phase['end_lr']], pct)
  1306|         0|            0|            0|  0.00%|                    if self.cycle_momentum:
  1307|         0|            0|            0|  0.00%|                        computed_momentum = self.anneal_func(group[phase['start_momentum']], group[phase['end_momentum']], pct)
  1308|         0|            0|            0|  0.00%|                    break
  1309|         0|            0|            0|  0.00%|                start_step = phase['end_step']
  1310|         0|            0|            0|  0.00%|
  1311|         0|            0|            0|  0.00%|            lrs.append(computed_lr)
  1312|         0|            0|            0|  0.00%|            if self.cycle_momentum:
  1313|         0|            0|            0|  0.00%|                if self.use_beta1:
  1314|         0|            0|            0|  0.00%|                    _, beta2 = group['betas']
  1315|         0|            0|            0|  0.00%|                    group['betas'] = (computed_momentum, beta2)
  1316|         0|            0|            0|  0.00%|                else:
  1317|         0|            0|            0|  0.00%|                    group['momentum'] = computed_momentum
  1318|         0|            0|            0|  0.00%|
  1319|         0|            0|            0|  0.00%|        return lrs
File: /opt/conda/lib/python3.8/logging/__init__.py
File duration: 0.00375724s (0.01%)
Line #|      Hits|         Time| Time per hit|      %|Source code
------+----------+-------------+-------------+-------+-----------
     1|         0|            0|            0|  0.00%|# Copyright 2001-2017 by Vinay Sajip. All Rights Reserved.
     2|         0|            0|            0|  0.00%|#
     3|         0|            0|            0|  0.00%|# Permission to use, copy, modify, and distribute this software and its
     4|         0|            0|            0|  0.00%|# documentation for any purpose and without fee is hereby granted,
     5|         0|            0|            0|  0.00%|# provided that the above copyright notice appear in all copies and that
     6|         0|            0|            0|  0.00%|# both that copyright notice and this permission notice appear in
     7|         0|            0|            0|  0.00%|# supporting documentation, and that the name of Vinay Sajip
     8|         0|            0|            0|  0.00%|# not be used in advertising or publicity pertaining to distribution
     9|         0|            0|            0|  0.00%|# of the software without specific, written prior permission.
    10|         0|            0|            0|  0.00%|# VINAY SAJIP DISCLAIMS ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING
    11|         0|            0|            0|  0.00%|# ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL
    12|         0|            0|            0|  0.00%|# VINAY SAJIP BE LIABLE FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR
    13|         0|            0|            0|  0.00%|# ANY DAMAGES WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER
    14|         0|            0|            0|  0.00%|# IN AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT
    15|         0|            0|            0|  0.00%|# OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
    16|         0|            0|            0|  0.00%|
    17|         0|            0|            0|  0.00%|"""
    18|         0|            0|            0|  0.00%|Logging package for Python. Based on PEP 282 and comments thereto in
    19|         0|            0|            0|  0.00%|comp.lang.python.
    20|         0|            0|            0|  0.00%|
    21|         0|            0|            0|  0.00%|Copyright (C) 2001-2017 Vinay Sajip. All Rights Reserved.
    22|         0|            0|            0|  0.00%|
    23|         0|            0|            0|  0.00%|To use, simply 'import logging' and log away!
    24|         0|            0|            0|  0.00%|"""
    25|         0|            0|            0|  0.00%|
    26|         0|            0|            0|  0.00%|import sys, os, time, io, re, traceback, warnings, weakref, collections.abc
    27|         0|            0|            0|  0.00%|
    28|         0|            0|            0|  0.00%|from string import Template
    29|         0|            0|            0|  0.00%|from string import Formatter as StrFormatter
    30|         0|            0|            0|  0.00%|
    31|         0|            0|            0|  0.00%|
    32|         0|            0|            0|  0.00%|__all__ = ['BASIC_FORMAT', 'BufferingFormatter', 'CRITICAL', 'DEBUG', 'ERROR',
    33|         0|            0|            0|  0.00%|           'FATAL', 'FileHandler', 'Filter', 'Formatter', 'Handler', 'INFO',
    34|         0|            0|            0|  0.00%|           'LogRecord', 'Logger', 'LoggerAdapter', 'NOTSET', 'NullHandler',
    35|         0|            0|            0|  0.00%|           'StreamHandler', 'WARN', 'WARNING', 'addLevelName', 'basicConfig',
    36|         0|            0|            0|  0.00%|           'captureWarnings', 'critical', 'debug', 'disable', 'error',
    37|         0|            0|            0|  0.00%|           'exception', 'fatal', 'getLevelName', 'getLogger', 'getLoggerClass',
    38|         0|            0|            0|  0.00%|           'info', 'log', 'makeLogRecord', 'setLoggerClass', 'shutdown',
    39|         0|            0|            0|  0.00%|           'warn', 'warning', 'getLogRecordFactory', 'setLogRecordFactory',
    40|         0|            0|            0|  0.00%|           'lastResort', 'raiseExceptions']
    41|         0|            0|            0|  0.00%|
    42|         0|            0|            0|  0.00%|import threading
    43|         0|            0|            0|  0.00%|
    44|         0|            0|            0|  0.00%|__author__  = "Vinay Sajip <vinay_sajip@red-dove.com>"
    45|         0|            0|            0|  0.00%|__status__  = "production"
    46|         0|            0|            0|  0.00%|# The following module attributes are no longer updated.
    47|         0|            0|            0|  0.00%|__version__ = "0.5.1.2"
    48|         0|            0|            0|  0.00%|__date__    = "07 February 2010"
    49|         0|            0|            0|  0.00%|
    50|         0|            0|            0|  0.00%|#---------------------------------------------------------------------------
    51|         0|            0|            0|  0.00%|#   Miscellaneous module data
    52|         0|            0|            0|  0.00%|#---------------------------------------------------------------------------
    53|         0|            0|            0|  0.00%|
    54|         0|            0|            0|  0.00%|#
    55|         0|            0|            0|  0.00%|#_startTime is used as the base when calculating the relative time of events
    56|         0|            0|            0|  0.00%|#
    57|         0|            0|            0|  0.00%|_startTime = time.time()
    58|         0|            0|            0|  0.00%|
    59|         0|            0|            0|  0.00%|#
    60|         0|            0|            0|  0.00%|#raiseExceptions is used to see if exceptions during handling should be
    61|         0|            0|            0|  0.00%|#propagated
    62|         0|            0|            0|  0.00%|#
    63|         0|            0|            0|  0.00%|raiseExceptions = True
    64|         0|            0|            0|  0.00%|
    65|         0|            0|            0|  0.00%|#
    66|         0|            0|            0|  0.00%|# If you don't want threading information in the log, set this to zero
    67|         0|            0|            0|  0.00%|#
    68|         0|            0|            0|  0.00%|logThreads = True
    69|         0|            0|            0|  0.00%|
    70|         0|            0|            0|  0.00%|#
    71|         0|            0|            0|  0.00%|# If you don't want multiprocessing information in the log, set this to zero
    72|         0|            0|            0|  0.00%|#
    73|         0|            0|            0|  0.00%|logMultiprocessing = True
    74|         0|            0|            0|  0.00%|
    75|         0|            0|            0|  0.00%|#
    76|         0|            0|            0|  0.00%|# If you don't want process information in the log, set this to zero
    77|         0|            0|            0|  0.00%|#
    78|         0|            0|            0|  0.00%|logProcesses = True
    79|         0|            0|            0|  0.00%|
    80|         0|            0|            0|  0.00%|#---------------------------------------------------------------------------
    81|         0|            0|            0|  0.00%|#   Level related stuff
    82|         0|            0|            0|  0.00%|#---------------------------------------------------------------------------
    83|         0|            0|            0|  0.00%|#
    84|         0|            0|            0|  0.00%|# Default levels and level names, these can be replaced with any positive set
    85|         0|            0|            0|  0.00%|# of values having corresponding names. There is a pseudo-level, NOTSET, which
    86|         0|            0|            0|  0.00%|# is only really there as a lower limit for user-defined levels. Handlers and
    87|         0|            0|            0|  0.00%|# loggers are initialized with NOTSET so that they will log all messages, even
    88|         0|            0|            0|  0.00%|# at user-defined levels.
    89|         0|            0|            0|  0.00%|#
    90|         0|            0|            0|  0.00%|
    91|         0|            0|            0|  0.00%|CRITICAL = 50
    92|         0|            0|            0|  0.00%|FATAL = CRITICAL
    93|         0|            0|            0|  0.00%|ERROR = 40
    94|         0|            0|            0|  0.00%|WARNING = 30
    95|         0|            0|            0|  0.00%|WARN = WARNING
    96|         0|            0|            0|  0.00%|INFO = 20
    97|         0|            0|            0|  0.00%|DEBUG = 10
    98|         0|            0|            0|  0.00%|NOTSET = 0
    99|         0|            0|            0|  0.00%|
   100|         0|            0|            0|  0.00%|_levelToName = {
   101|         0|            0|            0|  0.00%|    CRITICAL: 'CRITICAL',
   102|         0|            0|            0|  0.00%|    ERROR: 'ERROR',
   103|         0|            0|            0|  0.00%|    WARNING: 'WARNING',
   104|         0|            0|            0|  0.00%|    INFO: 'INFO',
   105|         0|            0|            0|  0.00%|    DEBUG: 'DEBUG',
   106|         0|            0|            0|  0.00%|    NOTSET: 'NOTSET',
   107|         0|            0|            0|  0.00%|}
   108|         0|            0|            0|  0.00%|_nameToLevel = {
   109|         0|            0|            0|  0.00%|    'CRITICAL': CRITICAL,
   110|         0|            0|            0|  0.00%|    'FATAL': FATAL,
   111|         0|            0|            0|  0.00%|    'ERROR': ERROR,
   112|         0|            0|            0|  0.00%|    'WARN': WARNING,
   113|         0|            0|            0|  0.00%|    'WARNING': WARNING,
   114|         0|            0|            0|  0.00%|    'INFO': INFO,
   115|         0|            0|            0|  0.00%|    'DEBUG': DEBUG,
   116|         0|            0|            0|  0.00%|    'NOTSET': NOTSET,
   117|         0|            0|            0|  0.00%|}
   118|         0|            0|            0|  0.00%|
   119|         0|            0|            0|  0.00%|def getLevelName(level):
   120|         0|            0|            0|  0.00%|    """
   121|         0|            0|            0|  0.00%|    Return the textual representation of logging level 'level'.
   122|         0|            0|            0|  0.00%|
   123|         0|            0|            0|  0.00%|    If the level is one of the predefined levels (CRITICAL, ERROR, WARNING,
   124|         0|            0|            0|  0.00%|    INFO, DEBUG) then you get the corresponding string. If you have
   125|         0|            0|            0|  0.00%|    associated levels with names using addLevelName then the name you have
   126|         0|            0|            0|  0.00%|    associated with 'level' is returned.
   127|         0|            0|            0|  0.00%|
   128|         0|            0|            0|  0.00%|    If a numeric value corresponding to one of the defined levels is passed
   129|         0|            0|            0|  0.00%|    in, the corresponding string representation is returned.
   130|         0|            0|            0|  0.00%|
   131|         0|            0|            0|  0.00%|    Otherwise, the string "Level %s" % level is returned.
   132|         0|            0|            0|  0.00%|    """
   133|         0|            0|            0|  0.00%|    # See Issues #22386, #27937 and #29220 for why it's this way
   134|         0|            0|            0|  0.00%|    result = _levelToName.get(level)
   135|         0|            0|            0|  0.00%|    if result is not None:
   136|         0|            0|            0|  0.00%|        return result
   137|         0|            0|            0|  0.00%|    result = _nameToLevel.get(level)
   138|         0|            0|            0|  0.00%|    if result is not None:
   139|         0|            0|            0|  0.00%|        return result
   140|         0|            0|            0|  0.00%|    return "Level %s" % level
   141|         0|            0|            0|  0.00%|
   142|         0|            0|            0|  0.00%|def addLevelName(level, levelName):
   143|         0|            0|            0|  0.00%|    """
   144|         0|            0|            0|  0.00%|    Associate 'levelName' with 'level'.
   145|         0|            0|            0|  0.00%|
   146|         0|            0|            0|  0.00%|    This is used when converting levels to text during message formatting.
   147|         0|            0|            0|  0.00%|    """
   148|         0|            0|            0|  0.00%|    _acquireLock()
   149|         0|            0|            0|  0.00%|    try:    #unlikely to cause an exception, but you never know...
   150|         0|            0|            0|  0.00%|        _levelToName[level] = levelName
   151|         0|            0|            0|  0.00%|        _nameToLevel[levelName] = level
   152|         0|            0|            0|  0.00%|    finally:
   153|         0|            0|            0|  0.00%|        _releaseLock()
   154|         0|            0|            0|  0.00%|
   155|         0|            0|            0|  0.00%|if hasattr(sys, '_getframe'):
   156|         0|            0|            0|  0.00%|    currentframe = lambda: sys._getframe(3)
   157|         0|            0|            0|  0.00%|else: #pragma: no cover
   158|         0|            0|            0|  0.00%|    def currentframe():
   159|         0|            0|            0|  0.00%|        """Return the frame object for the caller's stack frame."""
   160|         0|            0|            0|  0.00%|        try:
   161|         0|            0|            0|  0.00%|            raise Exception
   162|         0|            0|            0|  0.00%|        except Exception:
   163|         0|            0|            0|  0.00%|            return sys.exc_info()[2].tb_frame.f_back
   164|         0|            0|            0|  0.00%|
   165|         0|            0|            0|  0.00%|#
   166|         0|            0|            0|  0.00%|# _srcfile is used when walking the stack to check when we've got the first
   167|         0|            0|            0|  0.00%|# caller stack frame, by skipping frames whose filename is that of this
   168|         0|            0|            0|  0.00%|# module's source. It therefore should contain the filename of this module's
   169|         0|            0|            0|  0.00%|# source file.
   170|         0|            0|            0|  0.00%|#
   171|         0|            0|            0|  0.00%|# Ordinarily we would use __file__ for this, but frozen modules don't always
   172|         0|            0|            0|  0.00%|# have __file__ set, for some reason (see Issue #21736). Thus, we get the
   173|         0|            0|            0|  0.00%|# filename from a handy code object from a function defined in this module.
   174|         0|            0|            0|  0.00%|# (There's no particular reason for picking addLevelName.)
   175|         0|            0|            0|  0.00%|#
   176|         0|            0|            0|  0.00%|
   177|         0|            0|            0|  0.00%|_srcfile = os.path.normcase(addLevelName.__code__.co_filename)
   178|         0|            0|            0|  0.00%|
   179|         0|            0|            0|  0.00%|# _srcfile is only used in conjunction with sys._getframe().
   180|         0|            0|            0|  0.00%|# To provide compatibility with older versions of Python, set _srcfile
   181|         0|            0|            0|  0.00%|# to None if _getframe() is not available; this value will prevent
   182|         0|            0|            0|  0.00%|# findCaller() from being called. You can also do this if you want to avoid
   183|         0|            0|            0|  0.00%|# the overhead of fetching caller information, even when _getframe() is
   184|         0|            0|            0|  0.00%|# available.
   185|         0|            0|            0|  0.00%|#if not hasattr(sys, '_getframe'):
   186|         0|            0|            0|  0.00%|#    _srcfile = None
   187|         0|            0|            0|  0.00%|
   188|         0|            0|            0|  0.00%|
   189|         0|            0|            0|  0.00%|def _checkLevel(level):
   190|         0|            0|            0|  0.00%|    if isinstance(level, int):
   191|         0|            0|            0|  0.00%|        rv = level
   192|         0|            0|            0|  0.00%|    elif str(level) == level:
   193|         0|            0|            0|  0.00%|        if level not in _nameToLevel:
   194|         0|            0|            0|  0.00%|            raise ValueError("Unknown level: %r" % level)
   195|         0|            0|            0|  0.00%|        rv = _nameToLevel[level]
   196|         0|            0|            0|  0.00%|    else:
   197|         0|            0|            0|  0.00%|        raise TypeError("Level not an integer or a valid string: %r" % level)
   198|         0|            0|            0|  0.00%|    return rv
   199|         0|            0|            0|  0.00%|
   200|         0|            0|            0|  0.00%|#---------------------------------------------------------------------------
   201|         0|            0|            0|  0.00%|#   Thread-related stuff
   202|         0|            0|            0|  0.00%|#---------------------------------------------------------------------------
   203|         0|            0|            0|  0.00%|
   204|         0|            0|            0|  0.00%|#
   205|         0|            0|            0|  0.00%|#_lock is used to serialize access to shared data structures in this module.
   206|         0|            0|            0|  0.00%|#This needs to be an RLock because fileConfig() creates and configures
   207|         0|            0|            0|  0.00%|#Handlers, and so might arbitrary user threads. Since Handler code updates the
   208|         0|            0|            0|  0.00%|#shared dictionary _handlers, it needs to acquire the lock. But if configuring,
   209|         0|            0|            0|  0.00%|#the lock would already have been acquired - so we need an RLock.
   210|         0|            0|            0|  0.00%|#The same argument applies to Loggers and Manager.loggerDict.
   211|         0|            0|            0|  0.00%|#
   212|         0|            0|            0|  0.00%|_lock = threading.RLock()
   213|         0|            0|            0|  0.00%|
   214|         8|  5.03063e-05|  6.28829e-06|  0.00%|def _acquireLock():
   215|         0|            0|            0|  0.00%|    """
   216|         0|            0|            0|  0.00%|    Acquire the module-level lock for serializing access to shared data.
   217|         0|            0|            0|  0.00%|
   218|         0|            0|            0|  0.00%|    This should be released with _releaseLock().
   219|         0|            0|            0|  0.00%|    """
   220|         8|  4.86374e-05|  6.07967e-06|  0.00%|    if _lock:
   221|         8|  8.32081e-05|   1.0401e-05|  0.00%|        _lock.acquire()
   222|         0|            0|            0|  0.00%|
   223|         8|    0.0019145|  0.000239313|  0.00%|def _releaseLock():
   224|         0|            0|            0|  0.00%|    """
   225|         0|            0|            0|  0.00%|    Release the module-level lock acquired by calling _acquireLock().
   226|         0|            0|            0|  0.00%|    """
   227|         8|    0.0011611|  0.000145137|  0.00%|    if _lock:
   228|         8|  0.000499487|  6.24359e-05|  0.00%|        _lock.release()
   229|         0|            0|            0|  0.00%|
   230|         0|            0|            0|  0.00%|
   231|         0|            0|            0|  0.00%|# Prevent a held logging lock from blocking a child from logging.
   232|         0|            0|            0|  0.00%|
   233|         0|            0|            0|  0.00%|if not hasattr(os, 'register_at_fork'):  # Windows and friends.
   234|         0|            0|            0|  0.00%|    def _register_at_fork_reinit_lock(instance):
   235|         0|            0|            0|  0.00%|        pass  # no-op when os.register_at_fork does not exist.
   236|         0|            0|            0|  0.00%|else:
   237|         0|            0|            0|  0.00%|    # A collection of instances with a createLock method (logging.Handler)
   238|         0|            0|            0|  0.00%|    # to be called in the child after forking.  The weakref avoids us keeping
   239|         0|            0|            0|  0.00%|    # discarded Handler instances alive.  A set is used to avoid accumulating
   240|         0|            0|            0|  0.00%|    # duplicate registrations as createLock() is responsible for registering
   241|         0|            0|            0|  0.00%|    # a new Handler instance with this set in the first place.
   242|         0|            0|            0|  0.00%|    _at_fork_reinit_lock_weakset = weakref.WeakSet()
   243|         0|            0|            0|  0.00%|
   244|         0|            0|            0|  0.00%|    def _register_at_fork_reinit_lock(instance):
   245|         0|            0|            0|  0.00%|        _acquireLock()
   246|         0|            0|            0|  0.00%|        try:
   247|         0|            0|            0|  0.00%|            _at_fork_reinit_lock_weakset.add(instance)
   248|         0|            0|            0|  0.00%|        finally:
   249|         0|            0|            0|  0.00%|            _releaseLock()
   250|         0|            0|            0|  0.00%|
   251|         0|            0|            0|  0.00%|    def _after_at_fork_child_reinit_locks():
   252|         0|            0|            0|  0.00%|        # _acquireLock() was called in the parent before forking.
   253|         0|            0|            0|  0.00%|        for handler in _at_fork_reinit_lock_weakset:
   254|         0|            0|            0|  0.00%|            try:
   255|         0|            0|            0|  0.00%|                handler.createLock()
   256|         0|            0|            0|  0.00%|            except Exception as err:
   257|         0|            0|            0|  0.00%|                # Similar to what PyErr_WriteUnraisable does.
   258|         0|            0|            0|  0.00%|                print("Ignoring exception from logging atfork", instance,
   259|         0|            0|            0|  0.00%|                      "._reinit_lock() method:", err, file=sys.stderr)
   260|         0|            0|            0|  0.00%|        _releaseLock()  # Acquired by os.register_at_fork(before=.
   261|         0|            0|            0|  0.00%|
   262|         0|            0|            0|  0.00%|
   263|         0|            0|            0|  0.00%|    os.register_at_fork(before=_acquireLock,
   264|         0|            0|            0|  0.00%|                        after_in_child=_after_at_fork_child_reinit_locks,
   265|         0|            0|            0|  0.00%|                        after_in_parent=_releaseLock)
   266|         0|            0|            0|  0.00%|
   267|         0|            0|            0|  0.00%|
   268|         0|            0|            0|  0.00%|#---------------------------------------------------------------------------
   269|         0|            0|            0|  0.00%|#   The logging record
   270|         0|            0|            0|  0.00%|#---------------------------------------------------------------------------
   271|         0|            0|            0|  0.00%|
   272|         0|            0|            0|  0.00%|class LogRecord(object):
   273|         0|            0|            0|  0.00%|    """
   274|         0|            0|            0|  0.00%|    A LogRecord instance represents an event being logged.
   275|         0|            0|            0|  0.00%|
   276|         0|            0|            0|  0.00%|    LogRecord instances are created every time something is logged. They
   277|         0|            0|            0|  0.00%|    contain all the information pertinent to the event being logged. The
   278|         0|            0|            0|  0.00%|    main information passed in is in msg and args, which are combined
   279|         0|            0|            0|  0.00%|    using str(msg) % args to create the message field of the record. The
   280|         0|            0|            0|  0.00%|    record also includes information such as when the record was created,
   281|         0|            0|            0|  0.00%|    the source line where the logging call was made, and any exception
   282|         0|            0|            0|  0.00%|    information to be logged.
   283|         0|            0|            0|  0.00%|    """
   284|         0|            0|            0|  0.00%|    def __init__(self, name, level, pathname, lineno,
   285|         0|            0|            0|  0.00%|                 msg, args, exc_info, func=None, sinfo=None, **kwargs):
   286|         0|            0|            0|  0.00%|        """
   287|         0|            0|            0|  0.00%|        Initialize a logging record with interesting information.
   288|         0|            0|            0|  0.00%|        """
   289|         0|            0|            0|  0.00%|        ct = time.time()
   290|         0|            0|            0|  0.00%|        self.name = name
   291|         0|            0|            0|  0.00%|        self.msg = msg
   292|         0|            0|            0|  0.00%|        #
   293|         0|            0|            0|  0.00%|        # The following statement allows passing of a dictionary as a sole
   294|         0|            0|            0|  0.00%|        # argument, so that you can do something like
   295|         0|            0|            0|  0.00%|        #  logging.debug("a %(a)d b %(b)s", {'a':1, 'b':2})
   296|         0|            0|            0|  0.00%|        # Suggested by Stefan Behnel.
   297|         0|            0|            0|  0.00%|        # Note that without the test for args[0], we get a problem because
   298|         0|            0|            0|  0.00%|        # during formatting, we test to see if the arg is present using
   299|         0|            0|            0|  0.00%|        # 'if self.args:'. If the event being logged is e.g. 'Value is %d'
   300|         0|            0|            0|  0.00%|        # and if the passed arg fails 'if self.args:' then no formatting
   301|         0|            0|            0|  0.00%|        # is done. For example, logger.warning('Value is %d', 0) would log
   302|         0|            0|            0|  0.00%|        # 'Value is %d' instead of 'Value is 0'.
   303|         0|            0|            0|  0.00%|        # For the use case of passing a dictionary, this should not be a
   304|         0|            0|            0|  0.00%|        # problem.
   305|         0|            0|            0|  0.00%|        # Issue #21172: a request was made to relax the isinstance check
   306|         0|            0|            0|  0.00%|        # to hasattr(args[0], '__getitem__'). However, the docs on string
   307|         0|            0|            0|  0.00%|        # formatting still seem to suggest a mapping object is required.
   308|         0|            0|            0|  0.00%|        # Thus, while not removing the isinstance check, it does now look
   309|         0|            0|            0|  0.00%|        # for collections.abc.Mapping rather than, as before, dict.
   310|         0|            0|            0|  0.00%|        if (args and len(args) == 1 and isinstance(args[0], collections.abc.Mapping)
   311|         0|            0|            0|  0.00%|            and args[0]):
   312|         0|            0|            0|  0.00%|            args = args[0]
   313|         0|            0|            0|  0.00%|        self.args = args
   314|         0|            0|            0|  0.00%|        self.levelname = getLevelName(level)
   315|         0|            0|            0|  0.00%|        self.levelno = level
   316|         0|            0|            0|  0.00%|        self.pathname = pathname
   317|         0|            0|            0|  0.00%|        try:
   318|         0|            0|            0|  0.00%|            self.filename = os.path.basename(pathname)
   319|         0|            0|            0|  0.00%|            self.module = os.path.splitext(self.filename)[0]
   320|         0|            0|            0|  0.00%|        except (TypeError, ValueError, AttributeError):
   321|         0|            0|            0|  0.00%|            self.filename = pathname
   322|         0|            0|            0|  0.00%|            self.module = "Unknown module"
   323|         0|            0|            0|  0.00%|        self.exc_info = exc_info
   324|         0|            0|            0|  0.00%|        self.exc_text = None      # used to cache the traceback text
   325|         0|            0|            0|  0.00%|        self.stack_info = sinfo
   326|         0|            0|            0|  0.00%|        self.lineno = lineno
   327|         0|            0|            0|  0.00%|        self.funcName = func
   328|         0|            0|            0|  0.00%|        self.created = ct
   329|         0|            0|            0|  0.00%|        self.msecs = (ct - int(ct)) * 1000
   330|         0|            0|            0|  0.00%|        self.relativeCreated = (self.created - _startTime) * 1000
   331|         0|            0|            0|  0.00%|        if logThreads:
   332|         0|            0|            0|  0.00%|            self.thread = threading.get_ident()
   333|         0|            0|            0|  0.00%|            self.threadName = threading.current_thread().name
   334|         0|            0|            0|  0.00%|        else: # pragma: no cover
   335|         0|            0|            0|  0.00%|            self.thread = None
   336|         0|            0|            0|  0.00%|            self.threadName = None
   337|         0|            0|            0|  0.00%|        if not logMultiprocessing: # pragma: no cover
   338|         0|            0|            0|  0.00%|            self.processName = None
   339|         0|            0|            0|  0.00%|        else:
   340|         0|            0|            0|  0.00%|            self.processName = 'MainProcess'
   341|         0|            0|            0|  0.00%|            mp = sys.modules.get('multiprocessing')
   342|         0|            0|            0|  0.00%|            if mp is not None:
   343|         0|            0|            0|  0.00%|                # Errors may occur if multiprocessing has not finished loading
   344|         0|            0|            0|  0.00%|                # yet - e.g. if a custom import hook causes third-party code
   345|         0|            0|            0|  0.00%|                # to run when multiprocessing calls import. See issue 8200
   346|         0|            0|            0|  0.00%|                # for an example
   347|         0|            0|            0|  0.00%|                try:
   348|         0|            0|            0|  0.00%|                    self.processName = mp.current_process().name
   349|         0|            0|            0|  0.00%|                except Exception: #pragma: no cover
   350|         0|            0|            0|  0.00%|                    pass
   351|         0|            0|            0|  0.00%|        if logProcesses and hasattr(os, 'getpid'):
   352|         0|            0|            0|  0.00%|            self.process = os.getpid()
   353|         0|            0|            0|  0.00%|        else:
   354|         0|            0|            0|  0.00%|            self.process = None
   355|         0|            0|            0|  0.00%|
   356|         0|            0|            0|  0.00%|    def __repr__(self):
   357|         0|            0|            0|  0.00%|        return '<LogRecord: %s, %s, %s, %s, "%s">'%(self.name, self.levelno,
   358|         0|            0|            0|  0.00%|            self.pathname, self.lineno, self.msg)
   359|         0|            0|            0|  0.00%|
   360|         0|            0|            0|  0.00%|    def getMessage(self):
   361|         0|            0|            0|  0.00%|        """
   362|         0|            0|            0|  0.00%|        Return the message for this LogRecord.
   363|         0|            0|            0|  0.00%|
   364|         0|            0|            0|  0.00%|        Return the message for this LogRecord after merging any user-supplied
   365|         0|            0|            0|  0.00%|        arguments with the message.
   366|         0|            0|            0|  0.00%|        """
   367|         0|            0|            0|  0.00%|        msg = str(self.msg)
   368|         0|            0|            0|  0.00%|        if self.args:
   369|         0|            0|            0|  0.00%|            msg = msg % self.args
   370|         0|            0|            0|  0.00%|        return msg
   371|         0|            0|            0|  0.00%|
   372|         0|            0|            0|  0.00%|#
   373|         0|            0|            0|  0.00%|#   Determine which class to use when instantiating log records.
   374|         0|            0|            0|  0.00%|#
   375|         0|            0|            0|  0.00%|_logRecordFactory = LogRecord
   376|         0|            0|            0|  0.00%|
   377|         0|            0|            0|  0.00%|def setLogRecordFactory(factory):
   378|         0|            0|            0|  0.00%|    """
   379|         0|            0|            0|  0.00%|    Set the factory to be used when instantiating a log record.
   380|         0|            0|            0|  0.00%|
   381|         0|            0|            0|  0.00%|    :param factory: A callable which will be called to instantiate
   382|         0|            0|            0|  0.00%|    a log record.
   383|         0|            0|            0|  0.00%|    """
   384|         0|            0|            0|  0.00%|    global _logRecordFactory
   385|         0|            0|            0|  0.00%|    _logRecordFactory = factory
   386|         0|            0|            0|  0.00%|
   387|         0|            0|            0|  0.00%|def getLogRecordFactory():
   388|         0|            0|            0|  0.00%|    """
   389|         0|            0|            0|  0.00%|    Return the factory to be used when instantiating a log record.
   390|         0|            0|            0|  0.00%|    """
   391|         0|            0|            0|  0.00%|
   392|         0|            0|            0|  0.00%|    return _logRecordFactory
   393|         0|            0|            0|  0.00%|
   394|         0|            0|            0|  0.00%|def makeLogRecord(dict):
   395|         0|            0|            0|  0.00%|    """
   396|         0|            0|            0|  0.00%|    Make a LogRecord whose attributes are defined by the specified dictionary,
   397|         0|            0|            0|  0.00%|    This function is useful for converting a logging event received over
   398|         0|            0|            0|  0.00%|    a socket connection (which is sent as a dictionary) into a LogRecord
   399|         0|            0|            0|  0.00%|    instance.
   400|         0|            0|            0|  0.00%|    """
   401|         0|            0|            0|  0.00%|    rv = _logRecordFactory(None, None, "", 0, "", (), None, None)
   402|         0|            0|            0|  0.00%|    rv.__dict__.update(dict)
   403|         0|            0|            0|  0.00%|    return rv
   404|         0|            0|            0|  0.00%|
   405|         0|            0|            0|  0.00%|
   406|         0|            0|            0|  0.00%|#---------------------------------------------------------------------------
   407|         0|            0|            0|  0.00%|#   Formatter classes and functions
   408|         0|            0|            0|  0.00%|#---------------------------------------------------------------------------
   409|         0|            0|            0|  0.00%|_str_formatter = StrFormatter()
   410|         0|            0|            0|  0.00%|del StrFormatter
   411|         0|            0|            0|  0.00%|
   412|         0|            0|            0|  0.00%|
   413|         0|            0|            0|  0.00%|class PercentStyle(object):
   414|         0|            0|            0|  0.00%|
   415|         0|            0|            0|  0.00%|    default_format = '%(message)s'
   416|         0|            0|            0|  0.00%|    asctime_format = '%(asctime)s'
   417|         0|            0|            0|  0.00%|    asctime_search = '%(asctime)'
   418|         0|            0|            0|  0.00%|    validation_pattern = re.compile(r'%\(\w+\)[#0+ -]*(\*|\d+)?(\.(\*|\d+))?[diouxefgcrsa%]', re.I)
   419|         0|            0|            0|  0.00%|
   420|         0|            0|            0|  0.00%|    def __init__(self, fmt):
   421|         0|            0|            0|  0.00%|        self._fmt = fmt or self.default_format
   422|         0|            0|            0|  0.00%|
   423|         0|            0|            0|  0.00%|    def usesTime(self):
   424|         0|            0|            0|  0.00%|        return self._fmt.find(self.asctime_search) >= 0
   425|         0|            0|            0|  0.00%|
   426|         0|            0|            0|  0.00%|    def validate(self):
   427|         0|            0|            0|  0.00%|        """Validate the input format, ensure it matches the correct style"""
   428|         0|            0|            0|  0.00%|        if not self.validation_pattern.search(self._fmt):
   429|         0|            0|            0|  0.00%|            raise ValueError("Invalid format '%s' for '%s' style" % (self._fmt, self.default_format[0]))
   430|         0|            0|            0|  0.00%|
   431|         0|            0|            0|  0.00%|    def _format(self, record):
   432|         0|            0|            0|  0.00%|        return self._fmt % record.__dict__
   433|         0|            0|            0|  0.00%|
   434|         0|            0|            0|  0.00%|    def format(self, record):
   435|         0|            0|            0|  0.00%|        try:
   436|         0|            0|            0|  0.00%|            return self._format(record)
   437|         0|            0|            0|  0.00%|        except KeyError as e:
   438|         0|            0|            0|  0.00%|            raise ValueError('Formatting field not found in record: %s' % e)
   439|         0|            0|            0|  0.00%|
   440|         0|            0|            0|  0.00%|
   441|         0|            0|            0|  0.00%|class StrFormatStyle(PercentStyle):
   442|         0|            0|            0|  0.00%|    default_format = '{message}'
   443|         0|            0|            0|  0.00%|    asctime_format = '{asctime}'
   444|         0|            0|            0|  0.00%|    asctime_search = '{asctime'
   445|         0|            0|            0|  0.00%|
   446|         0|            0|            0|  0.00%|    fmt_spec = re.compile(r'^(.?[<>=^])?[+ -]?#?0?(\d+|{\w+})?[,_]?(\.(\d+|{\w+}))?[bcdefgnosx%]?$', re.I)
   447|         0|            0|            0|  0.00%|    field_spec = re.compile(r'^(\d+|\w+)(\.\w+|\[[^]]+\])*$')
   448|         0|            0|            0|  0.00%|
   449|         0|            0|            0|  0.00%|    def _format(self, record):
   450|         0|            0|            0|  0.00%|        return self._fmt.format(**record.__dict__)
   451|         0|            0|            0|  0.00%|
   452|         0|            0|            0|  0.00%|    def validate(self):
   453|         0|            0|            0|  0.00%|        """Validate the input format, ensure it is the correct string formatting style"""
   454|         0|            0|            0|  0.00%|        fields = set()
   455|         0|            0|            0|  0.00%|        try:
   456|         0|            0|            0|  0.00%|            for _, fieldname, spec, conversion in _str_formatter.parse(self._fmt):
   457|         0|            0|            0|  0.00%|                if fieldname:
   458|         0|            0|            0|  0.00%|                    if not self.field_spec.match(fieldname):
   459|         0|            0|            0|  0.00%|                        raise ValueError('invalid field name/expression: %r' % fieldname)
   460|         0|            0|            0|  0.00%|                    fields.add(fieldname)
   461|         0|            0|            0|  0.00%|                if conversion and conversion not in 'rsa':
   462|         0|            0|            0|  0.00%|                    raise ValueError('invalid conversion: %r' % conversion)
   463|         0|            0|            0|  0.00%|                if spec and not self.fmt_spec.match(spec):
   464|         0|            0|            0|  0.00%|                    raise ValueError('bad specifier: %r' % spec)
   465|         0|            0|            0|  0.00%|        except ValueError as e:
   466|         0|            0|            0|  0.00%|            raise ValueError('invalid format: %s' % e)
   467|         0|            0|            0|  0.00%|        if not fields:
   468|         0|            0|            0|  0.00%|            raise ValueError('invalid format: no fields')
   469|         0|            0|            0|  0.00%|
   470|         0|            0|            0|  0.00%|
   471|         0|            0|            0|  0.00%|class StringTemplateStyle(PercentStyle):
   472|         0|            0|            0|  0.00%|    default_format = '${message}'
   473|         0|            0|            0|  0.00%|    asctime_format = '${asctime}'
   474|         0|            0|            0|  0.00%|    asctime_search = '${asctime}'
   475|         0|            0|            0|  0.00%|
   476|         0|            0|            0|  0.00%|    def __init__(self, fmt):
   477|         0|            0|            0|  0.00%|        self._fmt = fmt or self.default_format
   478|         0|            0|            0|  0.00%|        self._tpl = Template(self._fmt)
   479|         0|            0|            0|  0.00%|
   480|         0|            0|            0|  0.00%|    def usesTime(self):
   481|         0|            0|            0|  0.00%|        fmt = self._fmt
   482|         0|            0|            0|  0.00%|        return fmt.find('$asctime') >= 0 or fmt.find(self.asctime_format) >= 0
   483|         0|            0|            0|  0.00%|
   484|         0|            0|            0|  0.00%|    def validate(self):
   485|         0|            0|            0|  0.00%|        pattern = Template.pattern
   486|         0|            0|            0|  0.00%|        fields = set()
   487|         0|            0|            0|  0.00%|        for m in pattern.finditer(self._fmt):
   488|         0|            0|            0|  0.00%|            d = m.groupdict()
   489|         0|            0|            0|  0.00%|            if d['named']:
   490|         0|            0|            0|  0.00%|                fields.add(d['named'])
   491|         0|            0|            0|  0.00%|            elif d['braced']:
   492|         0|            0|            0|  0.00%|                fields.add(d['braced'])
   493|         0|            0|            0|  0.00%|            elif m.group(0) == '$':
   494|         0|            0|            0|  0.00%|                raise ValueError('invalid format: bare \'$\' not allowed')
   495|         0|            0|            0|  0.00%|        if not fields:
   496|         0|            0|            0|  0.00%|            raise ValueError('invalid format: no fields')
   497|         0|            0|            0|  0.00%|
   498|         0|            0|            0|  0.00%|    def _format(self, record):
   499|         0|            0|            0|  0.00%|        return self._tpl.substitute(**record.__dict__)
   500|         0|            0|            0|  0.00%|
   501|         0|            0|            0|  0.00%|
   502|         0|            0|            0|  0.00%|BASIC_FORMAT = "%(levelname)s:%(name)s:%(message)s"
   503|         0|            0|            0|  0.00%|
   504|         0|            0|            0|  0.00%|_STYLES = {
   505|         0|            0|            0|  0.00%|    '%': (PercentStyle, BASIC_FORMAT),
   506|         0|            0|            0|  0.00%|    '{': (StrFormatStyle, '{levelname}:{name}:{message}'),
   507|         0|            0|            0|  0.00%|    '$': (StringTemplateStyle, '${levelname}:${name}:${message}'),
   508|         0|            0|            0|  0.00%|}
   509|         0|            0|            0|  0.00%|
   510|         0|            0|            0|  0.00%|class Formatter(object):
   511|         0|            0|            0|  0.00%|    """
   512|         0|            0|            0|  0.00%|    Formatter instances are used to convert a LogRecord to text.
   513|         0|            0|            0|  0.00%|
   514|         0|            0|            0|  0.00%|    Formatters need to know how a LogRecord is constructed. They are
   515|         0|            0|            0|  0.00%|    responsible for converting a LogRecord to (usually) a string which can
   516|         0|            0|            0|  0.00%|    be interpreted by either a human or an external system. The base Formatter
   517|         0|            0|            0|  0.00%|    allows a formatting string to be specified. If none is supplied, the
   518|         0|            0|            0|  0.00%|    style-dependent default value, "%(message)s", "{message}", or
   519|         0|            0|            0|  0.00%|    "${message}", is used.
   520|         0|            0|            0|  0.00%|
   521|         0|            0|            0|  0.00%|    The Formatter can be initialized with a format string which makes use of
   522|         0|            0|            0|  0.00%|    knowledge of the LogRecord attributes - e.g. the default value mentioned
   523|         0|            0|            0|  0.00%|    above makes use of the fact that the user's message and arguments are pre-
   524|         0|            0|            0|  0.00%|    formatted into a LogRecord's message attribute. Currently, the useful
   525|         0|            0|            0|  0.00%|    attributes in a LogRecord are described by:
   526|         0|            0|            0|  0.00%|
   527|         0|            0|            0|  0.00%|    %(name)s            Name of the logger (logging channel)
   528|         0|            0|            0|  0.00%|    %(levelno)s         Numeric logging level for the message (DEBUG, INFO,
   529|         0|            0|            0|  0.00%|                        WARNING, ERROR, CRITICAL)
   530|         0|            0|            0|  0.00%|    %(levelname)s       Text logging level for the message ("DEBUG", "INFO",
   531|         0|            0|            0|  0.00%|                        "WARNING", "ERROR", "CRITICAL")
   532|         0|            0|            0|  0.00%|    %(pathname)s        Full pathname of the source file where the logging
   533|         0|            0|            0|  0.00%|                        call was issued (if available)
   534|         0|            0|            0|  0.00%|    %(filename)s        Filename portion of pathname
   535|         0|            0|            0|  0.00%|    %(module)s          Module (name portion of filename)
   536|         0|            0|            0|  0.00%|    %(lineno)d          Source line number where the logging call was issued
   537|         0|            0|            0|  0.00%|                        (if available)
   538|         0|            0|            0|  0.00%|    %(funcName)s        Function name
   539|         0|            0|            0|  0.00%|    %(created)f         Time when the LogRecord was created (time.time()
   540|         0|            0|            0|  0.00%|                        return value)
   541|         0|            0|            0|  0.00%|    %(asctime)s         Textual time when the LogRecord was created
   542|         0|            0|            0|  0.00%|    %(msecs)d           Millisecond portion of the creation time
   543|         0|            0|            0|  0.00%|    %(relativeCreated)d Time in milliseconds when the LogRecord was created,
   544|         0|            0|            0|  0.00%|                        relative to the time the logging module was loaded
   545|         0|            0|            0|  0.00%|                        (typically at application startup time)
   546|         0|            0|            0|  0.00%|    %(thread)d          Thread ID (if available)
   547|         0|            0|            0|  0.00%|    %(threadName)s      Thread name (if available)
   548|         0|            0|            0|  0.00%|    %(process)d         Process ID (if available)
   549|         0|            0|            0|  0.00%|    %(message)s         The result of record.getMessage(), computed just as
   550|         0|            0|            0|  0.00%|                        the record is emitted
   551|         0|            0|            0|  0.00%|    """
   552|         0|            0|            0|  0.00%|
   553|         0|            0|            0|  0.00%|    converter = time.localtime
   554|         0|            0|            0|  0.00%|
   555|         0|            0|            0|  0.00%|    def __init__(self, fmt=None, datefmt=None, style='%', validate=True):
   556|         0|            0|            0|  0.00%|        """
   557|         0|            0|            0|  0.00%|        Initialize the formatter with specified format strings.
   558|         0|            0|            0|  0.00%|
   559|         0|            0|            0|  0.00%|        Initialize the formatter either with the specified format string, or a
   560|         0|            0|            0|  0.00%|        default as described above. Allow for specialized date formatting with
   561|         0|            0|            0|  0.00%|        the optional datefmt argument. If datefmt is omitted, you get an
   562|         0|            0|            0|  0.00%|        ISO8601-like (or RFC 3339-like) format.
   563|         0|            0|            0|  0.00%|
   564|         0|            0|            0|  0.00%|        Use a style parameter of '%', '{' or '$' to specify that you want to
   565|         0|            0|            0|  0.00%|        use one of %-formatting, :meth:`str.format` (``{}``) formatting or
   566|         0|            0|            0|  0.00%|        :class:`string.Template` formatting in your format string.
   567|         0|            0|            0|  0.00%|
   568|         0|            0|            0|  0.00%|        .. versionchanged:: 3.2
   569|         0|            0|            0|  0.00%|           Added the ``style`` parameter.
   570|         0|            0|            0|  0.00%|        """
   571|         0|            0|            0|  0.00%|        if style not in _STYLES:
   572|         0|            0|            0|  0.00%|            raise ValueError('Style must be one of: %s' % ','.join(
   573|         0|            0|            0|  0.00%|                             _STYLES.keys()))
   574|         0|            0|            0|  0.00%|        self._style = _STYLES[style][0](fmt)
   575|         0|            0|            0|  0.00%|        if validate:
   576|         0|            0|            0|  0.00%|            self._style.validate()
   577|         0|            0|            0|  0.00%|
   578|         0|            0|            0|  0.00%|        self._fmt = self._style._fmt
   579|         0|            0|            0|  0.00%|        self.datefmt = datefmt
   580|         0|            0|            0|  0.00%|
   581|         0|            0|            0|  0.00%|    default_time_format = '%Y-%m-%d %H:%M:%S'
   582|         0|            0|            0|  0.00%|    default_msec_format = '%s,%03d'
   583|         0|            0|            0|  0.00%|
   584|         0|            0|            0|  0.00%|    def formatTime(self, record, datefmt=None):
   585|         0|            0|            0|  0.00%|        """
   586|         0|            0|            0|  0.00%|        Return the creation time of the specified LogRecord as formatted text.
   587|         0|            0|            0|  0.00%|
   588|         0|            0|            0|  0.00%|        This method should be called from format() by a formatter which
   589|         0|            0|            0|  0.00%|        wants to make use of a formatted time. This method can be overridden
   590|         0|            0|            0|  0.00%|        in formatters to provide for any specific requirement, but the
   591|         0|            0|            0|  0.00%|        basic behaviour is as follows: if datefmt (a string) is specified,
   592|         0|            0|            0|  0.00%|        it is used with time.strftime() to format the creation time of the
   593|         0|            0|            0|  0.00%|        record. Otherwise, an ISO8601-like (or RFC 3339-like) format is used.
   594|         0|            0|            0|  0.00%|        The resulting string is returned. This function uses a user-configurable
   595|         0|            0|            0|  0.00%|        function to convert the creation time to a tuple. By default,
   596|         0|            0|            0|  0.00%|        time.localtime() is used; to change this for a particular formatter
   597|         0|            0|            0|  0.00%|        instance, set the 'converter' attribute to a function with the same
   598|         0|            0|            0|  0.00%|        signature as time.localtime() or time.gmtime(). To change it for all
   599|         0|            0|            0|  0.00%|        formatters, for example if you want all logging times to be shown in GMT,
   600|         0|            0|            0|  0.00%|        set the 'converter' attribute in the Formatter class.
   601|         0|            0|            0|  0.00%|        """
   602|         0|            0|            0|  0.00%|        ct = self.converter(record.created)
   603|         0|            0|            0|  0.00%|        if datefmt:
   604|         0|            0|            0|  0.00%|            s = time.strftime(datefmt, ct)
   605|         0|            0|            0|  0.00%|        else:
   606|         0|            0|            0|  0.00%|            t = time.strftime(self.default_time_format, ct)
   607|         0|            0|            0|  0.00%|            s = self.default_msec_format % (t, record.msecs)
   608|         0|            0|            0|  0.00%|        return s
   609|         0|            0|            0|  0.00%|
   610|         0|            0|            0|  0.00%|    def formatException(self, ei):
   611|         0|            0|            0|  0.00%|        """
   612|         0|            0|            0|  0.00%|        Format and return the specified exception information as a string.
   613|         0|            0|            0|  0.00%|
   614|         0|            0|            0|  0.00%|        This default implementation just uses
   615|         0|            0|            0|  0.00%|        traceback.print_exception()
   616|         0|            0|            0|  0.00%|        """
   617|         0|            0|            0|  0.00%|        sio = io.StringIO()
   618|         0|            0|            0|  0.00%|        tb = ei[2]
   619|         0|            0|            0|  0.00%|        # See issues #9427, #1553375. Commented out for now.
   620|         0|            0|            0|  0.00%|        #if getattr(self, 'fullstack', False):
   621|         0|            0|            0|  0.00%|        #    traceback.print_stack(tb.tb_frame.f_back, file=sio)
   622|         0|            0|            0|  0.00%|        traceback.print_exception(ei[0], ei[1], tb, None, sio)
   623|         0|            0|            0|  0.00%|        s = sio.getvalue()
   624|         0|            0|            0|  0.00%|        sio.close()
   625|         0|            0|            0|  0.00%|        if s[-1:] == "\n":
   626|         0|            0|            0|  0.00%|            s = s[:-1]
   627|         0|            0|            0|  0.00%|        return s
   628|         0|            0|            0|  0.00%|
   629|         0|            0|            0|  0.00%|    def usesTime(self):
   630|         0|            0|            0|  0.00%|        """
   631|         0|            0|            0|  0.00%|        Check if the format uses the creation time of the record.
   632|         0|            0|            0|  0.00%|        """
   633|         0|            0|            0|  0.00%|        return self._style.usesTime()
   634|         0|            0|            0|  0.00%|
   635|         0|            0|            0|  0.00%|    def formatMessage(self, record):
   636|         0|            0|            0|  0.00%|        return self._style.format(record)
   637|         0|            0|            0|  0.00%|
   638|         0|            0|            0|  0.00%|    def formatStack(self, stack_info):
   639|         0|            0|            0|  0.00%|        """
   640|         0|            0|            0|  0.00%|        This method is provided as an extension point for specialized
   641|         0|            0|            0|  0.00%|        formatting of stack information.
   642|         0|            0|            0|  0.00%|
   643|         0|            0|            0|  0.00%|        The input data is a string as returned from a call to
   644|         0|            0|            0|  0.00%|        :func:`traceback.print_stack`, but with the last trailing newline
   645|         0|            0|            0|  0.00%|        removed.
   646|         0|            0|            0|  0.00%|
   647|         0|            0|            0|  0.00%|        The base implementation just returns the value passed in.
   648|         0|            0|            0|  0.00%|        """
   649|         0|            0|            0|  0.00%|        return stack_info
   650|         0|            0|            0|  0.00%|
   651|         0|            0|            0|  0.00%|    def format(self, record):
   652|         0|            0|            0|  0.00%|        """
   653|         0|            0|            0|  0.00%|        Format the specified record as text.
   654|         0|            0|            0|  0.00%|
   655|         0|            0|            0|  0.00%|        The record's attribute dictionary is used as the operand to a
   656|         0|            0|            0|  0.00%|        string formatting operation which yields the returned string.
   657|         0|            0|            0|  0.00%|        Before formatting the dictionary, a couple of preparatory steps
   658|         0|            0|            0|  0.00%|        are carried out. The message attribute of the record is computed
   659|         0|            0|            0|  0.00%|        using LogRecord.getMessage(). If the formatting string uses the
   660|         0|            0|            0|  0.00%|        time (as determined by a call to usesTime(), formatTime() is
   661|         0|            0|            0|  0.00%|        called to format the event time. If there is exception information,
   662|         0|            0|            0|  0.00%|        it is formatted using formatException() and appended to the message.
   663|         0|            0|            0|  0.00%|        """
   664|         0|            0|            0|  0.00%|        record.message = record.getMessage()
   665|         0|            0|            0|  0.00%|        if self.usesTime():
   666|         0|            0|            0|  0.00%|            record.asctime = self.formatTime(record, self.datefmt)
   667|         0|            0|            0|  0.00%|        s = self.formatMessage(record)
   668|         0|            0|            0|  0.00%|        if record.exc_info:
   669|         0|            0|            0|  0.00%|            # Cache the traceback text to avoid converting it multiple times
   670|         0|            0|            0|  0.00%|            # (it's constant anyway)
   671|         0|            0|            0|  0.00%|            if not record.exc_text:
   672|         0|            0|            0|  0.00%|                record.exc_text = self.formatException(record.exc_info)
   673|         0|            0|            0|  0.00%|        if record.exc_text:
   674|         0|            0|            0|  0.00%|            if s[-1:] != "\n":
   675|         0|            0|            0|  0.00%|                s = s + "\n"
   676|         0|            0|            0|  0.00%|            s = s + record.exc_text
   677|         0|            0|            0|  0.00%|        if record.stack_info:
   678|         0|            0|            0|  0.00%|            if s[-1:] != "\n":
   679|         0|            0|            0|  0.00%|                s = s + "\n"
   680|         0|            0|            0|  0.00%|            s = s + self.formatStack(record.stack_info)
   681|         0|            0|            0|  0.00%|        return s
   682|         0|            0|            0|  0.00%|
   683|         0|            0|            0|  0.00%|#
   684|         0|            0|            0|  0.00%|#   The default formatter to use when no other is specified
   685|         0|            0|            0|  0.00%|#
   686|         0|            0|            0|  0.00%|_defaultFormatter = Formatter()
   687|         0|            0|            0|  0.00%|
   688|         0|            0|            0|  0.00%|class BufferingFormatter(object):
   689|         0|            0|            0|  0.00%|    """
   690|         0|            0|            0|  0.00%|    A formatter suitable for formatting a number of records.
   691|         0|            0|            0|  0.00%|    """
   692|         0|            0|            0|  0.00%|    def __init__(self, linefmt=None):
   693|         0|            0|            0|  0.00%|        """
   694|         0|            0|            0|  0.00%|        Optionally specify a formatter which will be used to format each
   695|         0|            0|            0|  0.00%|        individual record.
   696|         0|            0|            0|  0.00%|        """
   697|         0|            0|            0|  0.00%|        if linefmt:
   698|         0|            0|            0|  0.00%|            self.linefmt = linefmt
   699|         0|            0|            0|  0.00%|        else:
   700|         0|            0|            0|  0.00%|            self.linefmt = _defaultFormatter
   701|         0|            0|            0|  0.00%|
   702|         0|            0|            0|  0.00%|    def formatHeader(self, records):
   703|         0|            0|            0|  0.00%|        """
   704|         0|            0|            0|  0.00%|        Return the header string for the specified records.
   705|         0|            0|            0|  0.00%|        """
   706|         0|            0|            0|  0.00%|        return ""
   707|         0|            0|            0|  0.00%|
   708|         0|            0|            0|  0.00%|    def formatFooter(self, records):
   709|         0|            0|            0|  0.00%|        """
   710|         0|            0|            0|  0.00%|        Return the footer string for the specified records.
   711|         0|            0|            0|  0.00%|        """
   712|         0|            0|            0|  0.00%|        return ""
   713|         0|            0|            0|  0.00%|
   714|         0|            0|            0|  0.00%|    def format(self, records):
   715|         0|            0|            0|  0.00%|        """
   716|         0|            0|            0|  0.00%|        Format the specified records and return the result as a string.
   717|         0|            0|            0|  0.00%|        """
   718|         0|            0|            0|  0.00%|        rv = ""
   719|         0|            0|            0|  0.00%|        if len(records) > 0:
   720|         0|            0|            0|  0.00%|            rv = rv + self.formatHeader(records)
   721|         0|            0|            0|  0.00%|            for record in records:
   722|         0|            0|            0|  0.00%|                rv = rv + self.linefmt.format(record)
   723|         0|            0|            0|  0.00%|            rv = rv + self.formatFooter(records)
   724|         0|            0|            0|  0.00%|        return rv
   725|         0|            0|            0|  0.00%|
   726|         0|            0|            0|  0.00%|#---------------------------------------------------------------------------
   727|         0|            0|            0|  0.00%|#   Filter classes and functions
   728|         0|            0|            0|  0.00%|#---------------------------------------------------------------------------
   729|         0|            0|            0|  0.00%|
   730|         0|            0|            0|  0.00%|class Filter(object):
   731|         0|            0|            0|  0.00%|    """
   732|         0|            0|            0|  0.00%|    Filter instances are used to perform arbitrary filtering of LogRecords.
   733|         0|            0|            0|  0.00%|
   734|         0|            0|            0|  0.00%|    Loggers and Handlers can optionally use Filter instances to filter
   735|         0|            0|            0|  0.00%|    records as desired. The base filter class only allows events which are
   736|         0|            0|            0|  0.00%|    below a certain point in the logger hierarchy. For example, a filter
   737|         0|            0|            0|  0.00%|    initialized with "A.B" will allow events logged by loggers "A.B",
   738|         0|            0|            0|  0.00%|    "A.B.C", "A.B.C.D", "A.B.D" etc. but not "A.BB", "B.A.B" etc. If
   739|         0|            0|            0|  0.00%|    initialized with the empty string, all events are passed.
   740|         0|            0|            0|  0.00%|    """
   741|         0|            0|            0|  0.00%|    def __init__(self, name=''):
   742|         0|            0|            0|  0.00%|        """
   743|         0|            0|            0|  0.00%|        Initialize a filter.
   744|         0|            0|            0|  0.00%|
   745|         0|            0|            0|  0.00%|        Initialize with the name of the logger which, together with its
   746|         0|            0|            0|  0.00%|        children, will have its events allowed through the filter. If no
   747|         0|            0|            0|  0.00%|        name is specified, allow every event.
   748|         0|            0|            0|  0.00%|        """
   749|         0|            0|            0|  0.00%|        self.name = name
   750|         0|            0|            0|  0.00%|        self.nlen = len(name)
   751|         0|            0|            0|  0.00%|
   752|         0|            0|            0|  0.00%|    def filter(self, record):
   753|         0|            0|            0|  0.00%|        """
   754|         0|            0|            0|  0.00%|        Determine if the specified record is to be logged.
   755|         0|            0|            0|  0.00%|
   756|         0|            0|            0|  0.00%|        Returns True if the record should be logged, or False otherwise.
   757|         0|            0|            0|  0.00%|        If deemed appropriate, the record may be modified in-place.
   758|         0|            0|            0|  0.00%|        """
   759|         0|            0|            0|  0.00%|        if self.nlen == 0:
   760|         0|            0|            0|  0.00%|            return True
   761|         0|            0|            0|  0.00%|        elif self.name == record.name:
   762|         0|            0|            0|  0.00%|            return True
   763|         0|            0|            0|  0.00%|        elif record.name.find(self.name, 0, self.nlen) != 0:
   764|         0|            0|            0|  0.00%|            return False
   765|         0|            0|            0|  0.00%|        return (record.name[self.nlen] == ".")
   766|         0|            0|            0|  0.00%|
   767|         0|            0|            0|  0.00%|class Filterer(object):
   768|         0|            0|            0|  0.00%|    """
   769|         0|            0|            0|  0.00%|    A base class for loggers and handlers which allows them to share
   770|         0|            0|            0|  0.00%|    common code.
   771|         0|            0|            0|  0.00%|    """
   772|         0|            0|            0|  0.00%|    def __init__(self):
   773|         0|            0|            0|  0.00%|        """
   774|         0|            0|            0|  0.00%|        Initialize the list of filters to be an empty list.
   775|         0|            0|            0|  0.00%|        """
   776|         0|            0|            0|  0.00%|        self.filters = []
   777|         0|            0|            0|  0.00%|
   778|         0|            0|            0|  0.00%|    def addFilter(self, filter):
   779|         0|            0|            0|  0.00%|        """
   780|         0|            0|            0|  0.00%|        Add the specified filter to this handler.
   781|         0|            0|            0|  0.00%|        """
   782|         0|            0|            0|  0.00%|        if not (filter in self.filters):
   783|         0|            0|            0|  0.00%|            self.filters.append(filter)
   784|         0|            0|            0|  0.00%|
   785|         0|            0|            0|  0.00%|    def removeFilter(self, filter):
   786|         0|            0|            0|  0.00%|        """
   787|         0|            0|            0|  0.00%|        Remove the specified filter from this handler.
   788|         0|            0|            0|  0.00%|        """
   789|         0|            0|            0|  0.00%|        if filter in self.filters:
   790|         0|            0|            0|  0.00%|            self.filters.remove(filter)
   791|         0|            0|            0|  0.00%|
   792|         0|            0|            0|  0.00%|    def filter(self, record):
   793|         0|            0|            0|  0.00%|        """
   794|         0|            0|            0|  0.00%|        Determine if a record is loggable by consulting all the filters.
   795|         0|            0|            0|  0.00%|
   796|         0|            0|            0|  0.00%|        The default is to allow the record to be logged; any filter can veto
   797|         0|            0|            0|  0.00%|        this and the record is then dropped. Returns a zero value if a record
   798|         0|            0|            0|  0.00%|        is to be dropped, else non-zero.
   799|         0|            0|            0|  0.00%|
   800|         0|            0|            0|  0.00%|        .. versionchanged:: 3.2
   801|         0|            0|            0|  0.00%|
   802|         0|            0|            0|  0.00%|           Allow filters to be just callables.
   803|         0|            0|            0|  0.00%|        """
   804|         0|            0|            0|  0.00%|        rv = True
   805|         0|            0|            0|  0.00%|        for f in self.filters:
   806|         0|            0|            0|  0.00%|            if hasattr(f, 'filter'):
   807|         0|            0|            0|  0.00%|                result = f.filter(record)
   808|         0|            0|            0|  0.00%|            else:
   809|         0|            0|            0|  0.00%|                result = f(record) # assume callable - will raise if not
   810|         0|            0|            0|  0.00%|            if not result:
   811|         0|            0|            0|  0.00%|                rv = False
   812|         0|            0|            0|  0.00%|                break
   813|         0|            0|            0|  0.00%|        return rv
   814|         0|            0|            0|  0.00%|
   815|         0|            0|            0|  0.00%|#---------------------------------------------------------------------------
   816|         0|            0|            0|  0.00%|#   Handler classes and functions
   817|         0|            0|            0|  0.00%|#---------------------------------------------------------------------------
   818|         0|            0|            0|  0.00%|
   819|         0|            0|            0|  0.00%|_handlers = weakref.WeakValueDictionary()  #map of handler names to handlers
   820|         0|            0|            0|  0.00%|_handlerList = [] # added to allow handlers to be removed in reverse of order initialized
   821|         0|            0|            0|  0.00%|
   822|         0|            0|            0|  0.00%|def _removeHandlerRef(wr):
   823|         0|            0|            0|  0.00%|    """
   824|         0|            0|            0|  0.00%|    Remove a handler reference from the internal cleanup list.
   825|         0|            0|            0|  0.00%|    """
   826|         0|            0|            0|  0.00%|    # This function can be called during module teardown, when globals are
   827|         0|            0|            0|  0.00%|    # set to None. It can also be called from another thread. So we need to
   828|         0|            0|            0|  0.00%|    # pre-emptively grab the necessary globals and check if they're None,
   829|         0|            0|            0|  0.00%|    # to prevent race conditions and failures during interpreter shutdown.
   830|         0|            0|            0|  0.00%|    acquire, release, handlers = _acquireLock, _releaseLock, _handlerList
   831|         0|            0|            0|  0.00%|    if acquire and release and handlers:
   832|         0|            0|            0|  0.00%|        acquire()
   833|         0|            0|            0|  0.00%|        try:
   834|         0|            0|            0|  0.00%|            if wr in handlers:
   835|         0|            0|            0|  0.00%|                handlers.remove(wr)
   836|         0|            0|            0|  0.00%|        finally:
   837|         0|            0|            0|  0.00%|            release()
   838|         0|            0|            0|  0.00%|
   839|         0|            0|            0|  0.00%|def _addHandlerRef(handler):
   840|         0|            0|            0|  0.00%|    """
   841|         0|            0|            0|  0.00%|    Add a handler to the internal cleanup list using a weak reference.
   842|         0|            0|            0|  0.00%|    """
   843|         0|            0|            0|  0.00%|    _acquireLock()
   844|         0|            0|            0|  0.00%|    try:
   845|         0|            0|            0|  0.00%|        _handlerList.append(weakref.ref(handler, _removeHandlerRef))
   846|         0|            0|            0|  0.00%|    finally:
   847|         0|            0|            0|  0.00%|        _releaseLock()
   848|         0|            0|            0|  0.00%|
   849|         0|            0|            0|  0.00%|class Handler(Filterer):
   850|         0|            0|            0|  0.00%|    """
   851|         0|            0|            0|  0.00%|    Handler instances dispatch logging events to specific destinations.
   852|         0|            0|            0|  0.00%|
   853|         0|            0|            0|  0.00%|    The base handler class. Acts as a placeholder which defines the Handler
   854|         0|            0|            0|  0.00%|    interface. Handlers can optionally use Formatter instances to format
   855|         0|            0|            0|  0.00%|    records as desired. By default, no formatter is specified; in this case,
   856|         0|            0|            0|  0.00%|    the 'raw' message as determined by record.message is logged.
   857|         0|            0|            0|  0.00%|    """
   858|         0|            0|            0|  0.00%|    def __init__(self, level=NOTSET):
   859|         0|            0|            0|  0.00%|        """
   860|         0|            0|            0|  0.00%|        Initializes the instance - basically setting the formatter to None
   861|         0|            0|            0|  0.00%|        and the filter list to empty.
   862|         0|            0|            0|  0.00%|        """
   863|         0|            0|            0|  0.00%|        Filterer.__init__(self)
   864|         0|            0|            0|  0.00%|        self._name = None
   865|         0|            0|            0|  0.00%|        self.level = _checkLevel(level)
   866|         0|            0|            0|  0.00%|        self.formatter = None
   867|         0|            0|            0|  0.00%|        # Add the handler to the global _handlerList (for cleanup on shutdown)
   868|         0|            0|            0|  0.00%|        _addHandlerRef(self)
   869|         0|            0|            0|  0.00%|        self.createLock()
   870|         0|            0|            0|  0.00%|
   871|         0|            0|            0|  0.00%|    def get_name(self):
   872|         0|            0|            0|  0.00%|        return self._name
   873|         0|            0|            0|  0.00%|
   874|         0|            0|            0|  0.00%|    def set_name(self, name):
   875|         0|            0|            0|  0.00%|        _acquireLock()
   876|         0|            0|            0|  0.00%|        try:
   877|         0|            0|            0|  0.00%|            if self._name in _handlers:
   878|         0|            0|            0|  0.00%|                del _handlers[self._name]
   879|         0|            0|            0|  0.00%|            self._name = name
   880|         0|            0|            0|  0.00%|            if name:
   881|         0|            0|            0|  0.00%|                _handlers[name] = self
   882|         0|            0|            0|  0.00%|        finally:
   883|         0|            0|            0|  0.00%|            _releaseLock()
   884|         0|            0|            0|  0.00%|
   885|         0|            0|            0|  0.00%|    name = property(get_name, set_name)
   886|         0|            0|            0|  0.00%|
   887|         0|            0|            0|  0.00%|    def createLock(self):
   888|         0|            0|            0|  0.00%|        """
   889|         0|            0|            0|  0.00%|        Acquire a thread lock for serializing access to the underlying I/O.
   890|         0|            0|            0|  0.00%|        """
   891|         0|            0|            0|  0.00%|        self.lock = threading.RLock()
   892|         0|            0|            0|  0.00%|        _register_at_fork_reinit_lock(self)
   893|         0|            0|            0|  0.00%|
   894|         0|            0|            0|  0.00%|    def acquire(self):
   895|         0|            0|            0|  0.00%|        """
   896|         0|            0|            0|  0.00%|        Acquire the I/O thread lock.
   897|         0|            0|            0|  0.00%|        """
   898|         0|            0|            0|  0.00%|        if self.lock:
   899|         0|            0|            0|  0.00%|            self.lock.acquire()
   900|         0|            0|            0|  0.00%|
   901|         0|            0|            0|  0.00%|    def release(self):
   902|         0|            0|            0|  0.00%|        """
   903|         0|            0|            0|  0.00%|        Release the I/O thread lock.
   904|         0|            0|            0|  0.00%|        """
   905|         0|            0|            0|  0.00%|        if self.lock:
   906|         0|            0|            0|  0.00%|            self.lock.release()
   907|         0|            0|            0|  0.00%|
   908|         0|            0|            0|  0.00%|    def setLevel(self, level):
   909|         0|            0|            0|  0.00%|        """
   910|         0|            0|            0|  0.00%|        Set the logging level of this handler.  level must be an int or a str.
   911|         0|            0|            0|  0.00%|        """
   912|         0|            0|            0|  0.00%|        self.level = _checkLevel(level)
   913|         0|            0|            0|  0.00%|
   914|         0|            0|            0|  0.00%|    def format(self, record):
   915|         0|            0|            0|  0.00%|        """
   916|         0|            0|            0|  0.00%|        Format the specified record.
   917|         0|            0|            0|  0.00%|
   918|         0|            0|            0|  0.00%|        If a formatter is set, use it. Otherwise, use the default formatter
   919|         0|            0|            0|  0.00%|        for the module.
   920|         0|            0|            0|  0.00%|        """
   921|         0|            0|            0|  0.00%|        if self.formatter:
   922|         0|            0|            0|  0.00%|            fmt = self.formatter
   923|         0|            0|            0|  0.00%|        else:
   924|         0|            0|            0|  0.00%|            fmt = _defaultFormatter
   925|         0|            0|            0|  0.00%|        return fmt.format(record)
   926|         0|            0|            0|  0.00%|
   927|         0|            0|            0|  0.00%|    def emit(self, record):
   928|         0|            0|            0|  0.00%|        """
   929|         0|            0|            0|  0.00%|        Do whatever it takes to actually log the specified logging record.
   930|         0|            0|            0|  0.00%|
   931|         0|            0|            0|  0.00%|        This version is intended to be implemented by subclasses and so
   932|         0|            0|            0|  0.00%|        raises a NotImplementedError.
   933|         0|            0|            0|  0.00%|        """
   934|         0|            0|            0|  0.00%|        raise NotImplementedError('emit must be implemented '
   935|         0|            0|            0|  0.00%|                                  'by Handler subclasses')
   936|         0|            0|            0|  0.00%|
   937|         0|            0|            0|  0.00%|    def handle(self, record):
   938|         0|            0|            0|  0.00%|        """
   939|         0|            0|            0|  0.00%|        Conditionally emit the specified logging record.
   940|         0|            0|            0|  0.00%|
   941|         0|            0|            0|  0.00%|        Emission depends on filters which may have been added to the handler.
   942|         0|            0|            0|  0.00%|        Wrap the actual emission of the record with acquisition/release of
   943|         0|            0|            0|  0.00%|        the I/O thread lock. Returns whether the filter passed the record for
   944|         0|            0|            0|  0.00%|        emission.
   945|         0|            0|            0|  0.00%|        """
   946|         0|            0|            0|  0.00%|        rv = self.filter(record)
   947|         0|            0|            0|  0.00%|        if rv:
   948|         0|            0|            0|  0.00%|            self.acquire()
   949|         0|            0|            0|  0.00%|            try:
   950|         0|            0|            0|  0.00%|                self.emit(record)
   951|         0|            0|            0|  0.00%|            finally:
   952|         0|            0|            0|  0.00%|                self.release()
   953|         0|            0|            0|  0.00%|        return rv
   954|         0|            0|            0|  0.00%|
   955|         0|            0|            0|  0.00%|    def setFormatter(self, fmt):
   956|         0|            0|            0|  0.00%|        """
   957|         0|            0|            0|  0.00%|        Set the formatter for this handler.
   958|         0|            0|            0|  0.00%|        """
   959|         0|            0|            0|  0.00%|        self.formatter = fmt
   960|         0|            0|            0|  0.00%|
   961|         0|            0|            0|  0.00%|    def flush(self):
   962|         0|            0|            0|  0.00%|        """
   963|         0|            0|            0|  0.00%|        Ensure all logging output has been flushed.
   964|         0|            0|            0|  0.00%|
   965|         0|            0|            0|  0.00%|        This version does nothing and is intended to be implemented by
   966|         0|            0|            0|  0.00%|        subclasses.
   967|         0|            0|            0|  0.00%|        """
   968|         0|            0|            0|  0.00%|        pass
   969|         0|            0|            0|  0.00%|
   970|         0|            0|            0|  0.00%|    def close(self):
   971|         0|            0|            0|  0.00%|        """
   972|         0|            0|            0|  0.00%|        Tidy up any resources used by the handler.
   973|         0|            0|            0|  0.00%|
   974|         0|            0|            0|  0.00%|        This version removes the handler from an internal map of handlers,
   975|         0|            0|            0|  0.00%|        _handlers, which is used for handler lookup by name. Subclasses
   976|         0|            0|            0|  0.00%|        should ensure that this gets called from overridden close()
   977|         0|            0|            0|  0.00%|        methods.
   978|         0|            0|            0|  0.00%|        """
   979|         0|            0|            0|  0.00%|        #get the module data lock, as we're updating a shared structure.
   980|         0|            0|            0|  0.00%|        _acquireLock()
   981|         0|            0|            0|  0.00%|        try:    #unlikely to raise an exception, but you never know...
   982|         0|            0|            0|  0.00%|            if self._name and self._name in _handlers:
   983|         0|            0|            0|  0.00%|                del _handlers[self._name]
   984|         0|            0|            0|  0.00%|        finally:
   985|         0|            0|            0|  0.00%|            _releaseLock()
   986|         0|            0|            0|  0.00%|
   987|         0|            0|            0|  0.00%|    def handleError(self, record):
   988|         0|            0|            0|  0.00%|        """
   989|         0|            0|            0|  0.00%|        Handle errors which occur during an emit() call.
   990|         0|            0|            0|  0.00%|
   991|         0|            0|            0|  0.00%|        This method should be called from handlers when an exception is
   992|         0|            0|            0|  0.00%|        encountered during an emit() call. If raiseExceptions is false,
   993|         0|            0|            0|  0.00%|        exceptions get silently ignored. This is what is mostly wanted
   994|         0|            0|            0|  0.00%|        for a logging system - most users will not care about errors in
   995|         0|            0|            0|  0.00%|        the logging system, they are more interested in application errors.
   996|         0|            0|            0|  0.00%|        You could, however, replace this with a custom handler if you wish.
   997|         0|            0|            0|  0.00%|        The record which was being processed is passed in to this method.
   998|         0|            0|            0|  0.00%|        """
   999|         0|            0|            0|  0.00%|        if raiseExceptions and sys.stderr:  # see issue 13807
  1000|         0|            0|            0|  0.00%|            t, v, tb = sys.exc_info()
  1001|         0|            0|            0|  0.00%|            try:
  1002|         0|            0|            0|  0.00%|                sys.stderr.write('--- Logging error ---\n')
  1003|         0|            0|            0|  0.00%|                traceback.print_exception(t, v, tb, None, sys.stderr)
  1004|         0|            0|            0|  0.00%|                sys.stderr.write('Call stack:\n')
  1005|         0|            0|            0|  0.00%|                # Walk the stack frame up until we're out of logging,
  1006|         0|            0|            0|  0.00%|                # so as to print the calling context.
  1007|         0|            0|            0|  0.00%|                frame = tb.tb_frame
  1008|         0|            0|            0|  0.00%|                while (frame and os.path.dirname(frame.f_code.co_filename) ==
  1009|         0|            0|            0|  0.00%|                       __path__[0]):
  1010|         0|            0|            0|  0.00%|                    frame = frame.f_back
  1011|         0|            0|            0|  0.00%|                if frame:
  1012|         0|            0|            0|  0.00%|                    traceback.print_stack(frame, file=sys.stderr)
  1013|         0|            0|            0|  0.00%|                else:
  1014|         0|            0|            0|  0.00%|                    # couldn't find the right stack frame, for some reason
  1015|         0|            0|            0|  0.00%|                    sys.stderr.write('Logged from file %s, line %s\n' % (
  1016|         0|            0|            0|  0.00%|                                     record.filename, record.lineno))
  1017|         0|            0|            0|  0.00%|                # Issue 18671: output logging message and arguments
  1018|         0|            0|            0|  0.00%|                try:
  1019|         0|            0|            0|  0.00%|                    sys.stderr.write('Message: %r\n'
  1020|         0|            0|            0|  0.00%|                                     'Arguments: %s\n' % (record.msg,
  1021|         0|            0|            0|  0.00%|                                                          record.args))
  1022|         0|            0|            0|  0.00%|                except RecursionError:  # See issue 36272
  1023|         0|            0|            0|  0.00%|                    raise
  1024|         0|            0|            0|  0.00%|                except Exception:
  1025|         0|            0|            0|  0.00%|                    sys.stderr.write('Unable to print the message and arguments'
  1026|         0|            0|            0|  0.00%|                                     ' - possible formatting error.\nUse the'
  1027|         0|            0|            0|  0.00%|                                     ' traceback above to help find the error.\n'
  1028|         0|            0|            0|  0.00%|                                    )
  1029|         0|            0|            0|  0.00%|            except OSError: #pragma: no cover
  1030|         0|            0|            0|  0.00%|                pass    # see issue 5971
  1031|         0|            0|            0|  0.00%|            finally:
  1032|         0|            0|            0|  0.00%|                del t, v, tb
  1033|         0|            0|            0|  0.00%|
  1034|         0|            0|            0|  0.00%|    def __repr__(self):
  1035|         0|            0|            0|  0.00%|        level = getLevelName(self.level)
  1036|         0|            0|            0|  0.00%|        return '<%s (%s)>' % (self.__class__.__name__, level)
  1037|         0|            0|            0|  0.00%|
  1038|         0|            0|            0|  0.00%|class StreamHandler(Handler):
  1039|         0|            0|            0|  0.00%|    """
  1040|         0|            0|            0|  0.00%|    A handler class which writes logging records, appropriately formatted,
  1041|         0|            0|            0|  0.00%|    to a stream. Note that this class does not close the stream, as
  1042|         0|            0|            0|  0.00%|    sys.stdout or sys.stderr may be used.
  1043|         0|            0|            0|  0.00%|    """
  1044|         0|            0|            0|  0.00%|
  1045|         0|            0|            0|  0.00%|    terminator = '\n'
  1046|         0|            0|            0|  0.00%|
  1047|         0|            0|            0|  0.00%|    def __init__(self, stream=None):
  1048|         0|            0|            0|  0.00%|        """
  1049|         0|            0|            0|  0.00%|        Initialize the handler.
  1050|         0|            0|            0|  0.00%|
  1051|         0|            0|            0|  0.00%|        If stream is not specified, sys.stderr is used.
  1052|         0|            0|            0|  0.00%|        """
  1053|         0|            0|            0|  0.00%|        Handler.__init__(self)
  1054|         0|            0|            0|  0.00%|        if stream is None:
  1055|         0|            0|            0|  0.00%|            stream = sys.stderr
  1056|         0|            0|            0|  0.00%|        self.stream = stream
  1057|         0|            0|            0|  0.00%|
  1058|         0|            0|            0|  0.00%|    def flush(self):
  1059|         0|            0|            0|  0.00%|        """
  1060|         0|            0|            0|  0.00%|        Flushes the stream.
  1061|         0|            0|            0|  0.00%|        """
  1062|         0|            0|            0|  0.00%|        self.acquire()
  1063|         0|            0|            0|  0.00%|        try:
  1064|         0|            0|            0|  0.00%|            if self.stream and hasattr(self.stream, "flush"):
  1065|         0|            0|            0|  0.00%|                self.stream.flush()
  1066|         0|            0|            0|  0.00%|        finally:
  1067|         0|            0|            0|  0.00%|            self.release()
  1068|         0|            0|            0|  0.00%|
  1069|         0|            0|            0|  0.00%|    def emit(self, record):
  1070|         0|            0|            0|  0.00%|        """
  1071|         0|            0|            0|  0.00%|        Emit a record.
  1072|         0|            0|            0|  0.00%|
  1073|         0|            0|            0|  0.00%|        If a formatter is specified, it is used to format the record.
  1074|         0|            0|            0|  0.00%|        The record is then written to the stream with a trailing newline.  If
  1075|         0|            0|            0|  0.00%|        exception information is present, it is formatted using
  1076|         0|            0|            0|  0.00%|        traceback.print_exception and appended to the stream.  If the stream
  1077|         0|            0|            0|  0.00%|        has an 'encoding' attribute, it is used to determine how to do the
  1078|         0|            0|            0|  0.00%|        output to the stream.
  1079|         0|            0|            0|  0.00%|        """
  1080|         0|            0|            0|  0.00%|        try:
  1081|         0|            0|            0|  0.00%|            msg = self.format(record)
  1082|         0|            0|            0|  0.00%|            stream = self.stream
  1083|         0|            0|            0|  0.00%|            # issue 35046: merged two stream.writes into one.
  1084|         0|            0|            0|  0.00%|            stream.write(msg + self.terminator)
  1085|         0|            0|            0|  0.00%|            self.flush()
  1086|         0|            0|            0|  0.00%|        except RecursionError:  # See issue 36272
  1087|         0|            0|            0|  0.00%|            raise
  1088|         0|            0|            0|  0.00%|        except Exception:
  1089|         0|            0|            0|  0.00%|            self.handleError(record)
  1090|         0|            0|            0|  0.00%|
  1091|         0|            0|            0|  0.00%|    def setStream(self, stream):
  1092|         0|            0|            0|  0.00%|        """
  1093|         0|            0|            0|  0.00%|        Sets the StreamHandler's stream to the specified value,
  1094|         0|            0|            0|  0.00%|        if it is different.
  1095|         0|            0|            0|  0.00%|
  1096|         0|            0|            0|  0.00%|        Returns the old stream, if the stream was changed, or None
  1097|         0|            0|            0|  0.00%|        if it wasn't.
  1098|         0|            0|            0|  0.00%|        """
  1099|         0|            0|            0|  0.00%|        if stream is self.stream:
  1100|         0|            0|            0|  0.00%|            result = None
  1101|         0|            0|            0|  0.00%|        else:
  1102|         0|            0|            0|  0.00%|            result = self.stream
  1103|         0|            0|            0|  0.00%|            self.acquire()
  1104|         0|            0|            0|  0.00%|            try:
  1105|         0|            0|            0|  0.00%|                self.flush()
  1106|         0|            0|            0|  0.00%|                self.stream = stream
  1107|         0|            0|            0|  0.00%|            finally:
  1108|         0|            0|            0|  0.00%|                self.release()
  1109|         0|            0|            0|  0.00%|        return result
  1110|         0|            0|            0|  0.00%|
  1111|         0|            0|            0|  0.00%|    def __repr__(self):
  1112|         0|            0|            0|  0.00%|        level = getLevelName(self.level)
  1113|         0|            0|            0|  0.00%|        name = getattr(self.stream, 'name', '')
  1114|         0|            0|            0|  0.00%|        #  bpo-36015: name can be an int
  1115|         0|            0|            0|  0.00%|        name = str(name)
  1116|         0|            0|            0|  0.00%|        if name:
  1117|         0|            0|            0|  0.00%|            name += ' '
  1118|         0|            0|            0|  0.00%|        return '<%s %s(%s)>' % (self.__class__.__name__, name, level)
  1119|         0|            0|            0|  0.00%|
  1120|         0|            0|            0|  0.00%|
  1121|         0|            0|            0|  0.00%|class FileHandler(StreamHandler):
  1122|         0|            0|            0|  0.00%|    """
  1123|         0|            0|            0|  0.00%|    A handler class which writes formatted logging records to disk files.
  1124|         0|            0|            0|  0.00%|    """
  1125|         0|            0|            0|  0.00%|    def __init__(self, filename, mode='a', encoding=None, delay=False):
  1126|         0|            0|            0|  0.00%|        """
  1127|         0|            0|            0|  0.00%|        Open the specified file and use it as the stream for logging.
  1128|         0|            0|            0|  0.00%|        """
  1129|         0|            0|            0|  0.00%|        # Issue #27493: add support for Path objects to be passed in
  1130|         0|            0|            0|  0.00%|        filename = os.fspath(filename)
  1131|         0|            0|            0|  0.00%|        #keep the absolute path, otherwise derived classes which use this
  1132|         0|            0|            0|  0.00%|        #may come a cropper when the current directory changes
  1133|         0|            0|            0|  0.00%|        self.baseFilename = os.path.abspath(filename)
  1134|         0|            0|            0|  0.00%|        self.mode = mode
  1135|         0|            0|            0|  0.00%|        self.encoding = encoding
  1136|         0|            0|            0|  0.00%|        self.delay = delay
  1137|         0|            0|            0|  0.00%|        if delay:
  1138|         0|            0|            0|  0.00%|            #We don't open the stream, but we still need to call the
  1139|         0|            0|            0|  0.00%|            #Handler constructor to set level, formatter, lock etc.
  1140|         0|            0|            0|  0.00%|            Handler.__init__(self)
  1141|         0|            0|            0|  0.00%|            self.stream = None
  1142|         0|            0|            0|  0.00%|        else:
  1143|         0|            0|            0|  0.00%|            StreamHandler.__init__(self, self._open())
  1144|         0|            0|            0|  0.00%|
  1145|         0|            0|            0|  0.00%|    def close(self):
  1146|         0|            0|            0|  0.00%|        """
  1147|         0|            0|            0|  0.00%|        Closes the stream.
  1148|         0|            0|            0|  0.00%|        """
  1149|         0|            0|            0|  0.00%|        self.acquire()
  1150|         0|            0|            0|  0.00%|        try:
  1151|         0|            0|            0|  0.00%|            try:
  1152|         0|            0|            0|  0.00%|                if self.stream:
  1153|         0|            0|            0|  0.00%|                    try:
  1154|         0|            0|            0|  0.00%|                        self.flush()
  1155|         0|            0|            0|  0.00%|                    finally:
  1156|         0|            0|            0|  0.00%|                        stream = self.stream
  1157|         0|            0|            0|  0.00%|                        self.stream = None
  1158|         0|            0|            0|  0.00%|                        if hasattr(stream, "close"):
  1159|         0|            0|            0|  0.00%|                            stream.close()
  1160|         0|            0|            0|  0.00%|            finally:
  1161|         0|            0|            0|  0.00%|                # Issue #19523: call unconditionally to
  1162|         0|            0|            0|  0.00%|                # prevent a handler leak when delay is set
  1163|         0|            0|            0|  0.00%|                StreamHandler.close(self)
  1164|         0|            0|            0|  0.00%|        finally:
  1165|         0|            0|            0|  0.00%|            self.release()
  1166|         0|            0|            0|  0.00%|
  1167|         0|            0|            0|  0.00%|    def _open(self):
  1168|         0|            0|            0|  0.00%|        """
  1169|         0|            0|            0|  0.00%|        Open the current base file with the (original) mode and encoding.
  1170|         0|            0|            0|  0.00%|        Return the resulting stream.
  1171|         0|            0|            0|  0.00%|        """
  1172|         0|            0|            0|  0.00%|        return open(self.baseFilename, self.mode, encoding=self.encoding)
  1173|         0|            0|            0|  0.00%|
  1174|         0|            0|            0|  0.00%|    def emit(self, record):
  1175|         0|            0|            0|  0.00%|        """
  1176|         0|            0|            0|  0.00%|        Emit a record.
  1177|         0|            0|            0|  0.00%|
  1178|         0|            0|            0|  0.00%|        If the stream was not opened because 'delay' was specified in the
  1179|         0|            0|            0|  0.00%|        constructor, open it before calling the superclass's emit.
  1180|         0|            0|            0|  0.00%|        """
  1181|         0|            0|            0|  0.00%|        if self.stream is None:
  1182|         0|            0|            0|  0.00%|            self.stream = self._open()
  1183|         0|            0|            0|  0.00%|        StreamHandler.emit(self, record)
  1184|         0|            0|            0|  0.00%|
  1185|         0|            0|            0|  0.00%|    def __repr__(self):
  1186|         0|            0|            0|  0.00%|        level = getLevelName(self.level)
  1187|         0|            0|            0|  0.00%|        return '<%s %s (%s)>' % (self.__class__.__name__, self.baseFilename, level)
  1188|         0|            0|            0|  0.00%|
  1189|         0|            0|            0|  0.00%|
  1190|         0|            0|            0|  0.00%|class _StderrHandler(StreamHandler):
  1191|         0|            0|            0|  0.00%|    """
  1192|         0|            0|            0|  0.00%|    This class is like a StreamHandler using sys.stderr, but always uses
  1193|         0|            0|            0|  0.00%|    whatever sys.stderr is currently set to rather than the value of
  1194|         0|            0|            0|  0.00%|    sys.stderr at handler construction time.
  1195|         0|            0|            0|  0.00%|    """
  1196|         0|            0|            0|  0.00%|    def __init__(self, level=NOTSET):
  1197|         0|            0|            0|  0.00%|        """
  1198|         0|            0|            0|  0.00%|        Initialize the handler.
  1199|         0|            0|            0|  0.00%|        """
  1200|         0|            0|            0|  0.00%|        Handler.__init__(self, level)
  1201|         0|            0|            0|  0.00%|
  1202|         0|            0|            0|  0.00%|    @property
  1203|         0|            0|            0|  0.00%|    def stream(self):
  1204|         0|            0|            0|  0.00%|        return sys.stderr
  1205|         0|            0|            0|  0.00%|
  1206|         0|            0|            0|  0.00%|
  1207|         0|            0|            0|  0.00%|_defaultLastResort = _StderrHandler(WARNING)
  1208|         0|            0|            0|  0.00%|lastResort = _defaultLastResort
  1209|         0|            0|            0|  0.00%|
  1210|         0|            0|            0|  0.00%|#---------------------------------------------------------------------------
  1211|         0|            0|            0|  0.00%|#   Manager classes and functions
  1212|         0|            0|            0|  0.00%|#---------------------------------------------------------------------------
  1213|         0|            0|            0|  0.00%|
  1214|         0|            0|            0|  0.00%|class PlaceHolder(object):
  1215|         0|            0|            0|  0.00%|    """
  1216|         0|            0|            0|  0.00%|    PlaceHolder instances are used in the Manager logger hierarchy to take
  1217|         0|            0|            0|  0.00%|    the place of nodes for which no loggers have been defined. This class is
  1218|         0|            0|            0|  0.00%|    intended for internal use only and not as part of the public API.
  1219|         0|            0|            0|  0.00%|    """
  1220|         0|            0|            0|  0.00%|    def __init__(self, alogger):
  1221|         0|            0|            0|  0.00%|        """
  1222|         0|            0|            0|  0.00%|        Initialize with the specified logger being a child of this placeholder.
  1223|         0|            0|            0|  0.00%|        """
  1224|         0|            0|            0|  0.00%|        self.loggerMap = { alogger : None }
  1225|         0|            0|            0|  0.00%|
  1226|         0|            0|            0|  0.00%|    def append(self, alogger):
  1227|         0|            0|            0|  0.00%|        """
  1228|         0|            0|            0|  0.00%|        Add the specified logger as a child of this placeholder.
  1229|         0|            0|            0|  0.00%|        """
  1230|         0|            0|            0|  0.00%|        if alogger not in self.loggerMap:
  1231|         0|            0|            0|  0.00%|            self.loggerMap[alogger] = None
  1232|         0|            0|            0|  0.00%|
  1233|         0|            0|            0|  0.00%|#
  1234|         0|            0|            0|  0.00%|#   Determine which class to use when instantiating loggers.
  1235|         0|            0|            0|  0.00%|#
  1236|         0|            0|            0|  0.00%|
  1237|         0|            0|            0|  0.00%|def setLoggerClass(klass):
  1238|         0|            0|            0|  0.00%|    """
  1239|         0|            0|            0|  0.00%|    Set the class to be used when instantiating a logger. The class should
  1240|         0|            0|            0|  0.00%|    define __init__() such that only a name argument is required, and the
  1241|         0|            0|            0|  0.00%|    __init__() should call Logger.__init__()
  1242|         0|            0|            0|  0.00%|    """
  1243|         0|            0|            0|  0.00%|    if klass != Logger:
  1244|         0|            0|            0|  0.00%|        if not issubclass(klass, Logger):
  1245|         0|            0|            0|  0.00%|            raise TypeError("logger not derived from logging.Logger: "
  1246|         0|            0|            0|  0.00%|                            + klass.__name__)
  1247|         0|            0|            0|  0.00%|    global _loggerClass
  1248|         0|            0|            0|  0.00%|    _loggerClass = klass
  1249|         0|            0|            0|  0.00%|
  1250|         0|            0|            0|  0.00%|def getLoggerClass():
  1251|         0|            0|            0|  0.00%|    """
  1252|         0|            0|            0|  0.00%|    Return the class to be used when instantiating a logger.
  1253|         0|            0|            0|  0.00%|    """
  1254|         0|            0|            0|  0.00%|    return _loggerClass
  1255|         0|            0|            0|  0.00%|
  1256|         0|            0|            0|  0.00%|class Manager(object):
  1257|         0|            0|            0|  0.00%|    """
  1258|         0|            0|            0|  0.00%|    There is [under normal circumstances] just one Manager instance, which
  1259|         0|            0|            0|  0.00%|    holds the hierarchy of loggers.
  1260|         0|            0|            0|  0.00%|    """
  1261|         0|            0|            0|  0.00%|    def __init__(self, rootnode):
  1262|         0|            0|            0|  0.00%|        """
  1263|         0|            0|            0|  0.00%|        Initialize the manager with the root node of the logger hierarchy.
  1264|         0|            0|            0|  0.00%|        """
  1265|         0|            0|            0|  0.00%|        self.root = rootnode
  1266|         0|            0|            0|  0.00%|        self.disable = 0
  1267|         0|            0|            0|  0.00%|        self.emittedNoHandlerWarning = False
  1268|         0|            0|            0|  0.00%|        self.loggerDict = {}
  1269|         0|            0|            0|  0.00%|        self.loggerClass = None
  1270|         0|            0|            0|  0.00%|        self.logRecordFactory = None
  1271|         0|            0|            0|  0.00%|
  1272|         0|            0|            0|  0.00%|    @property
  1273|         0|            0|            0|  0.00%|    def disable(self):
  1274|         0|            0|            0|  0.00%|        return self._disable
  1275|         0|            0|            0|  0.00%|
  1276|         0|            0|            0|  0.00%|    @disable.setter
  1277|         0|            0|            0|  0.00%|    def disable(self, value):
  1278|         0|            0|            0|  0.00%|        self._disable = _checkLevel(value)
  1279|         0|            0|            0|  0.00%|
  1280|         0|            0|            0|  0.00%|    def getLogger(self, name):
  1281|         0|            0|            0|  0.00%|        """
  1282|         0|            0|            0|  0.00%|        Get a logger with the specified name (channel name), creating it
  1283|         0|            0|            0|  0.00%|        if it doesn't yet exist. This name is a dot-separated hierarchical
  1284|         0|            0|            0|  0.00%|        name, such as "a", "a.b", "a.b.c" or similar.
  1285|         0|            0|            0|  0.00%|
  1286|         0|            0|            0|  0.00%|        If a PlaceHolder existed for the specified name [i.e. the logger
  1287|         0|            0|            0|  0.00%|        didn't exist but a child of it did], replace it with the created
  1288|         0|            0|            0|  0.00%|        logger and fix up the parent/child references which pointed to the
  1289|         0|            0|            0|  0.00%|        placeholder to now point to the logger.
  1290|         0|            0|            0|  0.00%|        """
  1291|         0|            0|            0|  0.00%|        rv = None
  1292|         0|            0|            0|  0.00%|        if not isinstance(name, str):
  1293|         0|            0|            0|  0.00%|            raise TypeError('A logger name must be a string')
  1294|         0|            0|            0|  0.00%|        _acquireLock()
  1295|         0|            0|            0|  0.00%|        try:
  1296|         0|            0|            0|  0.00%|            if name in self.loggerDict:
  1297|         0|            0|            0|  0.00%|                rv = self.loggerDict[name]
  1298|         0|            0|            0|  0.00%|                if isinstance(rv, PlaceHolder):
  1299|         0|            0|            0|  0.00%|                    ph = rv
  1300|         0|            0|            0|  0.00%|                    rv = (self.loggerClass or _loggerClass)(name)
  1301|         0|            0|            0|  0.00%|                    rv.manager = self
  1302|         0|            0|            0|  0.00%|                    self.loggerDict[name] = rv
  1303|         0|            0|            0|  0.00%|                    self._fixupChildren(ph, rv)
  1304|         0|            0|            0|  0.00%|                    self._fixupParents(rv)
  1305|         0|            0|            0|  0.00%|            else:
  1306|         0|            0|            0|  0.00%|                rv = (self.loggerClass or _loggerClass)(name)
  1307|         0|            0|            0|  0.00%|                rv.manager = self
  1308|         0|            0|            0|  0.00%|                self.loggerDict[name] = rv
  1309|         0|            0|            0|  0.00%|                self._fixupParents(rv)
  1310|         0|            0|            0|  0.00%|        finally:
  1311|         0|            0|            0|  0.00%|            _releaseLock()
  1312|         0|            0|            0|  0.00%|        return rv
  1313|         0|            0|            0|  0.00%|
  1314|         0|            0|            0|  0.00%|    def setLoggerClass(self, klass):
  1315|         0|            0|            0|  0.00%|        """
  1316|         0|            0|            0|  0.00%|        Set the class to be used when instantiating a logger with this Manager.
  1317|         0|            0|            0|  0.00%|        """
  1318|         0|            0|            0|  0.00%|        if klass != Logger:
  1319|         0|            0|            0|  0.00%|            if not issubclass(klass, Logger):
  1320|         0|            0|            0|  0.00%|                raise TypeError("logger not derived from logging.Logger: "
  1321|         0|            0|            0|  0.00%|                                + klass.__name__)
  1322|         0|            0|            0|  0.00%|        self.loggerClass = klass
  1323|         0|            0|            0|  0.00%|
  1324|         0|            0|            0|  0.00%|    def setLogRecordFactory(self, factory):
  1325|         0|            0|            0|  0.00%|        """
  1326|         0|            0|            0|  0.00%|        Set the factory to be used when instantiating a log record with this
  1327|         0|            0|            0|  0.00%|        Manager.
  1328|         0|            0|            0|  0.00%|        """
  1329|         0|            0|            0|  0.00%|        self.logRecordFactory = factory
  1330|         0|            0|            0|  0.00%|
  1331|         0|            0|            0|  0.00%|    def _fixupParents(self, alogger):
  1332|         0|            0|            0|  0.00%|        """
  1333|         0|            0|            0|  0.00%|        Ensure that there are either loggers or placeholders all the way
  1334|         0|            0|            0|  0.00%|        from the specified logger to the root of the logger hierarchy.
  1335|         0|            0|            0|  0.00%|        """
  1336|         0|            0|            0|  0.00%|        name = alogger.name
  1337|         0|            0|            0|  0.00%|        i = name.rfind(".")
  1338|         0|            0|            0|  0.00%|        rv = None
  1339|         0|            0|            0|  0.00%|        while (i > 0) and not rv:
  1340|         0|            0|            0|  0.00%|            substr = name[:i]
  1341|         0|            0|            0|  0.00%|            if substr not in self.loggerDict:
  1342|         0|            0|            0|  0.00%|                self.loggerDict[substr] = PlaceHolder(alogger)
  1343|         0|            0|            0|  0.00%|            else:
  1344|         0|            0|            0|  0.00%|                obj = self.loggerDict[substr]
  1345|         0|            0|            0|  0.00%|                if isinstance(obj, Logger):
  1346|         0|            0|            0|  0.00%|                    rv = obj
  1347|         0|            0|            0|  0.00%|                else:
  1348|         0|            0|            0|  0.00%|                    assert isinstance(obj, PlaceHolder)
  1349|         0|            0|            0|  0.00%|                    obj.append(alogger)
  1350|         0|            0|            0|  0.00%|            i = name.rfind(".", 0, i - 1)
  1351|         0|            0|            0|  0.00%|        if not rv:
  1352|         0|            0|            0|  0.00%|            rv = self.root
  1353|         0|            0|            0|  0.00%|        alogger.parent = rv
  1354|         0|            0|            0|  0.00%|
  1355|         0|            0|            0|  0.00%|    def _fixupChildren(self, ph, alogger):
  1356|         0|            0|            0|  0.00%|        """
  1357|         0|            0|            0|  0.00%|        Ensure that children of the placeholder ph are connected to the
  1358|         0|            0|            0|  0.00%|        specified logger.
  1359|         0|            0|            0|  0.00%|        """
  1360|         0|            0|            0|  0.00%|        name = alogger.name
  1361|         0|            0|            0|  0.00%|        namelen = len(name)
  1362|         0|            0|            0|  0.00%|        for c in ph.loggerMap.keys():
  1363|         0|            0|            0|  0.00%|            #The if means ... if not c.parent.name.startswith(nm)
  1364|         0|            0|            0|  0.00%|            if c.parent.name[:namelen] != name:
  1365|         0|            0|            0|  0.00%|                alogger.parent = c.parent
  1366|         0|            0|            0|  0.00%|                c.parent = alogger
  1367|         0|            0|            0|  0.00%|
  1368|         0|            0|            0|  0.00%|    def _clear_cache(self):
  1369|         0|            0|            0|  0.00%|        """
  1370|         0|            0|            0|  0.00%|        Clear the cache for all loggers in loggerDict
  1371|         0|            0|            0|  0.00%|        Called when level changes are made
  1372|         0|            0|            0|  0.00%|        """
  1373|         0|            0|            0|  0.00%|
  1374|         0|            0|            0|  0.00%|        _acquireLock()
  1375|         0|            0|            0|  0.00%|        for logger in self.loggerDict.values():
  1376|         0|            0|            0|  0.00%|            if isinstance(logger, Logger):
  1377|         0|            0|            0|  0.00%|                logger._cache.clear()
  1378|         0|            0|            0|  0.00%|        self.root._cache.clear()
  1379|         0|            0|            0|  0.00%|        _releaseLock()
  1380|         0|            0|            0|  0.00%|
  1381|         0|            0|            0|  0.00%|#---------------------------------------------------------------------------
  1382|         0|            0|            0|  0.00%|#   Logger classes and functions
  1383|         0|            0|            0|  0.00%|#---------------------------------------------------------------------------
  1384|         0|            0|            0|  0.00%|
  1385|         0|            0|            0|  0.00%|class Logger(Filterer):
  1386|         0|            0|            0|  0.00%|    """
  1387|         0|            0|            0|  0.00%|    Instances of the Logger class represent a single logging channel. A
  1388|         0|            0|            0|  0.00%|    "logging channel" indicates an area of an application. Exactly how an
  1389|         0|            0|            0|  0.00%|    "area" is defined is up to the application developer. Since an
  1390|         0|            0|            0|  0.00%|    application can have any number of areas, logging channels are identified
  1391|         0|            0|            0|  0.00%|    by a unique string. Application areas can be nested (e.g. an area
  1392|         0|            0|            0|  0.00%|    of "input processing" might include sub-areas "read CSV files", "read
  1393|         0|            0|            0|  0.00%|    XLS files" and "read Gnumeric files"). To cater for this natural nesting,
  1394|         0|            0|            0|  0.00%|    channel names are organized into a namespace hierarchy where levels are
  1395|         0|            0|            0|  0.00%|    separated by periods, much like the Java or Python package namespace. So
  1396|         0|            0|            0|  0.00%|    in the instance given above, channel names might be "input" for the upper
  1397|         0|            0|            0|  0.00%|    level, and "input.csv", "input.xls" and "input.gnu" for the sub-levels.
  1398|         0|            0|            0|  0.00%|    There is no arbitrary limit to the depth of nesting.
  1399|         0|            0|            0|  0.00%|    """
  1400|         0|            0|            0|  0.00%|    def __init__(self, name, level=NOTSET):
  1401|         0|            0|            0|  0.00%|        """
  1402|         0|            0|            0|  0.00%|        Initialize the logger with a name and an optional level.
  1403|         0|            0|            0|  0.00%|        """
  1404|         0|            0|            0|  0.00%|        Filterer.__init__(self)
  1405|         0|            0|            0|  0.00%|        self.name = name
  1406|         0|            0|            0|  0.00%|        self.level = _checkLevel(level)
  1407|         0|            0|            0|  0.00%|        self.parent = None
  1408|         0|            0|            0|  0.00%|        self.propagate = True
  1409|         0|            0|            0|  0.00%|        self.handlers = []
  1410|         0|            0|            0|  0.00%|        self.disabled = False
  1411|         0|            0|            0|  0.00%|        self._cache = {}
  1412|         0|            0|            0|  0.00%|
  1413|         0|            0|            0|  0.00%|    def setLevel(self, level):
  1414|         0|            0|            0|  0.00%|        """
  1415|         0|            0|            0|  0.00%|        Set the logging level of this logger.  level must be an int or a str.
  1416|         0|            0|            0|  0.00%|        """
  1417|         0|            0|            0|  0.00%|        self.level = _checkLevel(level)
  1418|         0|            0|            0|  0.00%|        self.manager._clear_cache()
  1419|         0|            0|            0|  0.00%|
  1420|         0|            0|            0|  0.00%|    def debug(self, msg, *args, **kwargs):
  1421|         0|            0|            0|  0.00%|        """
  1422|         0|            0|            0|  0.00%|        Log 'msg % args' with severity 'DEBUG'.
  1423|         0|            0|            0|  0.00%|
  1424|         0|            0|            0|  0.00%|        To pass exception information, use the keyword argument exc_info with
  1425|         0|            0|            0|  0.00%|        a true value, e.g.
  1426|         0|            0|            0|  0.00%|
  1427|         0|            0|            0|  0.00%|        logger.debug("Houston, we have a %s", "thorny problem", exc_info=1)
  1428|         0|            0|            0|  0.00%|        """
  1429|         0|            0|            0|  0.00%|        if self.isEnabledFor(DEBUG):
  1430|         0|            0|            0|  0.00%|            self._log(DEBUG, msg, args, **kwargs)
  1431|         0|            0|            0|  0.00%|
  1432|         0|            0|            0|  0.00%|    def info(self, msg, *args, **kwargs):
  1433|         0|            0|            0|  0.00%|        """
  1434|         0|            0|            0|  0.00%|        Log 'msg % args' with severity 'INFO'.
  1435|         0|            0|            0|  0.00%|
  1436|         0|            0|            0|  0.00%|        To pass exception information, use the keyword argument exc_info with
  1437|         0|            0|            0|  0.00%|        a true value, e.g.
  1438|         0|            0|            0|  0.00%|
  1439|         0|            0|            0|  0.00%|        logger.info("Houston, we have a %s", "interesting problem", exc_info=1)
  1440|         0|            0|            0|  0.00%|        """
  1441|         0|            0|            0|  0.00%|        if self.isEnabledFor(INFO):
  1442|         0|            0|            0|  0.00%|            self._log(INFO, msg, args, **kwargs)
  1443|         0|            0|            0|  0.00%|
  1444|         0|            0|            0|  0.00%|    def warning(self, msg, *args, **kwargs):
  1445|         0|            0|            0|  0.00%|        """
  1446|         0|            0|            0|  0.00%|        Log 'msg % args' with severity 'WARNING'.
  1447|         0|            0|            0|  0.00%|
  1448|         0|            0|            0|  0.00%|        To pass exception information, use the keyword argument exc_info with
  1449|         0|            0|            0|  0.00%|        a true value, e.g.
  1450|         0|            0|            0|  0.00%|
  1451|         0|            0|            0|  0.00%|        logger.warning("Houston, we have a %s", "bit of a problem", exc_info=1)
  1452|         0|            0|            0|  0.00%|        """
  1453|         0|            0|            0|  0.00%|        if self.isEnabledFor(WARNING):
  1454|         0|            0|            0|  0.00%|            self._log(WARNING, msg, args, **kwargs)
  1455|         0|            0|            0|  0.00%|
  1456|         0|            0|            0|  0.00%|    def warn(self, msg, *args, **kwargs):
  1457|         0|            0|            0|  0.00%|        warnings.warn("The 'warn' method is deprecated, "
  1458|         0|            0|            0|  0.00%|            "use 'warning' instead", DeprecationWarning, 2)
  1459|         0|            0|            0|  0.00%|        self.warning(msg, *args, **kwargs)
  1460|         0|            0|            0|  0.00%|
  1461|         0|            0|            0|  0.00%|    def error(self, msg, *args, **kwargs):
  1462|         0|            0|            0|  0.00%|        """
  1463|         0|            0|            0|  0.00%|        Log 'msg % args' with severity 'ERROR'.
  1464|         0|            0|            0|  0.00%|
  1465|         0|            0|            0|  0.00%|        To pass exception information, use the keyword argument exc_info with
  1466|         0|            0|            0|  0.00%|        a true value, e.g.
  1467|         0|            0|            0|  0.00%|
  1468|         0|            0|            0|  0.00%|        logger.error("Houston, we have a %s", "major problem", exc_info=1)
  1469|         0|            0|            0|  0.00%|        """
  1470|         0|            0|            0|  0.00%|        if self.isEnabledFor(ERROR):
  1471|         0|            0|            0|  0.00%|            self._log(ERROR, msg, args, **kwargs)
  1472|         0|            0|            0|  0.00%|
  1473|         0|            0|            0|  0.00%|    def exception(self, msg, *args, exc_info=True, **kwargs):
  1474|         0|            0|            0|  0.00%|        """
  1475|         0|            0|            0|  0.00%|        Convenience method for logging an ERROR with exception information.
  1476|         0|            0|            0|  0.00%|        """
  1477|         0|            0|            0|  0.00%|        self.error(msg, *args, exc_info=exc_info, **kwargs)
  1478|         0|            0|            0|  0.00%|
  1479|         0|            0|            0|  0.00%|    def critical(self, msg, *args, **kwargs):
  1480|         0|            0|            0|  0.00%|        """
  1481|         0|            0|            0|  0.00%|        Log 'msg % args' with severity 'CRITICAL'.
  1482|         0|            0|            0|  0.00%|
  1483|         0|            0|            0|  0.00%|        To pass exception information, use the keyword argument exc_info with
  1484|         0|            0|            0|  0.00%|        a true value, e.g.
  1485|         0|            0|            0|  0.00%|
  1486|         0|            0|            0|  0.00%|        logger.critical("Houston, we have a %s", "major disaster", exc_info=1)
  1487|         0|            0|            0|  0.00%|        """
  1488|         0|            0|            0|  0.00%|        if self.isEnabledFor(CRITICAL):
  1489|         0|            0|            0|  0.00%|            self._log(CRITICAL, msg, args, **kwargs)
  1490|         0|            0|            0|  0.00%|
  1491|         0|            0|            0|  0.00%|    fatal = critical
  1492|         0|            0|            0|  0.00%|
  1493|         0|            0|            0|  0.00%|    def log(self, level, msg, *args, **kwargs):
  1494|         0|            0|            0|  0.00%|        """
  1495|         0|            0|            0|  0.00%|        Log 'msg % args' with the integer severity 'level'.
  1496|         0|            0|            0|  0.00%|
  1497|         0|            0|            0|  0.00%|        To pass exception information, use the keyword argument exc_info with
  1498|         0|            0|            0|  0.00%|        a true value, e.g.
  1499|         0|            0|            0|  0.00%|
  1500|         0|            0|            0|  0.00%|        logger.log(level, "We have a %s", "mysterious problem", exc_info=1)
  1501|         0|            0|            0|  0.00%|        """
  1502|         0|            0|            0|  0.00%|        if not isinstance(level, int):
  1503|         0|            0|            0|  0.00%|            if raiseExceptions:
  1504|         0|            0|            0|  0.00%|                raise TypeError("level must be an integer")
  1505|         0|            0|            0|  0.00%|            else:
  1506|         0|            0|            0|  0.00%|                return
  1507|         0|            0|            0|  0.00%|        if self.isEnabledFor(level):
  1508|         0|            0|            0|  0.00%|            self._log(level, msg, args, **kwargs)
  1509|         0|            0|            0|  0.00%|
  1510|         0|            0|            0|  0.00%|    def findCaller(self, stack_info=False, stacklevel=1):
  1511|         0|            0|            0|  0.00%|        """
  1512|         0|            0|            0|  0.00%|        Find the stack frame of the caller so that we can note the source
  1513|         0|            0|            0|  0.00%|        file name, line number and function name.
  1514|         0|            0|            0|  0.00%|        """
  1515|         0|            0|            0|  0.00%|        f = currentframe()
  1516|         0|            0|            0|  0.00%|        #On some versions of IronPython, currentframe() returns None if
  1517|         0|            0|            0|  0.00%|        #IronPython isn't run with -X:Frames.
  1518|         0|            0|            0|  0.00%|        if f is not None:
  1519|         0|            0|            0|  0.00%|            f = f.f_back
  1520|         0|            0|            0|  0.00%|        orig_f = f
  1521|         0|            0|            0|  0.00%|        while f and stacklevel > 1:
  1522|         0|            0|            0|  0.00%|            f = f.f_back
  1523|         0|            0|            0|  0.00%|            stacklevel -= 1
  1524|         0|            0|            0|  0.00%|        if not f:
  1525|         0|            0|            0|  0.00%|            f = orig_f
  1526|         0|            0|            0|  0.00%|        rv = "(unknown file)", 0, "(unknown function)", None
  1527|         0|            0|            0|  0.00%|        while hasattr(f, "f_code"):
  1528|         0|            0|            0|  0.00%|            co = f.f_code
  1529|         0|            0|            0|  0.00%|            filename = os.path.normcase(co.co_filename)
  1530|         0|            0|            0|  0.00%|            if filename == _srcfile:
  1531|         0|            0|            0|  0.00%|                f = f.f_back
  1532|         0|            0|            0|  0.00%|                continue
  1533|         0|            0|            0|  0.00%|            sinfo = None
  1534|         0|            0|            0|  0.00%|            if stack_info:
  1535|         0|            0|            0|  0.00%|                sio = io.StringIO()
  1536|         0|            0|            0|  0.00%|                sio.write('Stack (most recent call last):\n')
  1537|         0|            0|            0|  0.00%|                traceback.print_stack(f, file=sio)
  1538|         0|            0|            0|  0.00%|                sinfo = sio.getvalue()
  1539|         0|            0|            0|  0.00%|                if sinfo[-1] == '\n':
  1540|         0|            0|            0|  0.00%|                    sinfo = sinfo[:-1]
  1541|         0|            0|            0|  0.00%|                sio.close()
  1542|         0|            0|            0|  0.00%|            rv = (co.co_filename, f.f_lineno, co.co_name, sinfo)
  1543|         0|            0|            0|  0.00%|            break
  1544|         0|            0|            0|  0.00%|        return rv
  1545|         0|            0|            0|  0.00%|
  1546|         0|            0|            0|  0.00%|    def makeRecord(self, name, level, fn, lno, msg, args, exc_info,
  1547|         0|            0|            0|  0.00%|                   func=None, extra=None, sinfo=None):
  1548|         0|            0|            0|  0.00%|        """
  1549|         0|            0|            0|  0.00%|        A factory method which can be overridden in subclasses to create
  1550|         0|            0|            0|  0.00%|        specialized LogRecords.
  1551|         0|            0|            0|  0.00%|        """
  1552|         0|            0|            0|  0.00%|        rv = _logRecordFactory(name, level, fn, lno, msg, args, exc_info, func,
  1553|         0|            0|            0|  0.00%|                             sinfo)
  1554|         0|            0|            0|  0.00%|        if extra is not None:
  1555|         0|            0|            0|  0.00%|            for key in extra:
  1556|         0|            0|            0|  0.00%|                if (key in ["message", "asctime"]) or (key in rv.__dict__):
  1557|         0|            0|            0|  0.00%|                    raise KeyError("Attempt to overwrite %r in LogRecord" % key)
  1558|         0|            0|            0|  0.00%|                rv.__dict__[key] = extra[key]
  1559|         0|            0|            0|  0.00%|        return rv
  1560|         0|            0|            0|  0.00%|
  1561|         0|            0|            0|  0.00%|    def _log(self, level, msg, args, exc_info=None, extra=None, stack_info=False,
  1562|         0|            0|            0|  0.00%|             stacklevel=1):
  1563|         0|            0|            0|  0.00%|        """
  1564|         0|            0|            0|  0.00%|        Low-level logging routine which creates a LogRecord and then calls
  1565|         0|            0|            0|  0.00%|        all the handlers of this logger to handle the record.
  1566|         0|            0|            0|  0.00%|        """
  1567|         0|            0|            0|  0.00%|        sinfo = None
  1568|         0|            0|            0|  0.00%|        if _srcfile:
  1569|         0|            0|            0|  0.00%|            #IronPython doesn't track Python frames, so findCaller raises an
  1570|         0|            0|            0|  0.00%|            #exception on some versions of IronPython. We trap it here so that
  1571|         0|            0|            0|  0.00%|            #IronPython can use logging.
  1572|         0|            0|            0|  0.00%|            try:
  1573|         0|            0|            0|  0.00%|                fn, lno, func, sinfo = self.findCaller(stack_info, stacklevel)
  1574|         0|            0|            0|  0.00%|            except ValueError: # pragma: no cover
  1575|         0|            0|            0|  0.00%|                fn, lno, func = "(unknown file)", 0, "(unknown function)"
  1576|         0|            0|            0|  0.00%|        else: # pragma: no cover
  1577|         0|            0|            0|  0.00%|            fn, lno, func = "(unknown file)", 0, "(unknown function)"
  1578|         0|            0|            0|  0.00%|        if exc_info:
  1579|         0|            0|            0|  0.00%|            if isinstance(exc_info, BaseException):
  1580|         0|            0|            0|  0.00%|                exc_info = (type(exc_info), exc_info, exc_info.__traceback__)
  1581|         0|            0|            0|  0.00%|            elif not isinstance(exc_info, tuple):
  1582|         0|            0|            0|  0.00%|                exc_info = sys.exc_info()
  1583|         0|            0|            0|  0.00%|        record = self.makeRecord(self.name, level, fn, lno, msg, args,
  1584|         0|            0|            0|  0.00%|                                 exc_info, func, extra, sinfo)
  1585|         0|            0|            0|  0.00%|        self.handle(record)
  1586|         0|            0|            0|  0.00%|
  1587|         0|            0|            0|  0.00%|    def handle(self, record):
  1588|         0|            0|            0|  0.00%|        """
  1589|         0|            0|            0|  0.00%|        Call the handlers for the specified record.
  1590|         0|            0|            0|  0.00%|
  1591|         0|            0|            0|  0.00%|        This method is used for unpickled records received from a socket, as
  1592|         0|            0|            0|  0.00%|        well as those created locally. Logger-level filtering is applied.
  1593|         0|            0|            0|  0.00%|        """
  1594|         0|            0|            0|  0.00%|        if (not self.disabled) and self.filter(record):
  1595|         0|            0|            0|  0.00%|            self.callHandlers(record)
  1596|         0|            0|            0|  0.00%|
  1597|         0|            0|            0|  0.00%|    def addHandler(self, hdlr):
  1598|         0|            0|            0|  0.00%|        """
  1599|         0|            0|            0|  0.00%|        Add the specified handler to this logger.
  1600|         0|            0|            0|  0.00%|        """
  1601|         0|            0|            0|  0.00%|        _acquireLock()
  1602|         0|            0|            0|  0.00%|        try:
  1603|         0|            0|            0|  0.00%|            if not (hdlr in self.handlers):
  1604|         0|            0|            0|  0.00%|                self.handlers.append(hdlr)
  1605|         0|            0|            0|  0.00%|        finally:
  1606|         0|            0|            0|  0.00%|            _releaseLock()
  1607|         0|            0|            0|  0.00%|
  1608|         0|            0|            0|  0.00%|    def removeHandler(self, hdlr):
  1609|         0|            0|            0|  0.00%|        """
  1610|         0|            0|            0|  0.00%|        Remove the specified handler from this logger.
  1611|         0|            0|            0|  0.00%|        """
  1612|         0|            0|            0|  0.00%|        _acquireLock()
  1613|         0|            0|            0|  0.00%|        try:
  1614|         0|            0|            0|  0.00%|            if hdlr in self.handlers:
  1615|         0|            0|            0|  0.00%|                self.handlers.remove(hdlr)
  1616|         0|            0|            0|  0.00%|        finally:
  1617|         0|            0|            0|  0.00%|            _releaseLock()
  1618|         0|            0|            0|  0.00%|
  1619|         0|            0|            0|  0.00%|    def hasHandlers(self):
  1620|         0|            0|            0|  0.00%|        """
  1621|         0|            0|            0|  0.00%|        See if this logger has any handlers configured.
  1622|         0|            0|            0|  0.00%|
  1623|         0|            0|            0|  0.00%|        Loop through all handlers for this logger and its parents in the
  1624|         0|            0|            0|  0.00%|        logger hierarchy. Return True if a handler was found, else False.
  1625|         0|            0|            0|  0.00%|        Stop searching up the hierarchy whenever a logger with the "propagate"
  1626|         0|            0|            0|  0.00%|        attribute set to zero is found - that will be the last logger which
  1627|         0|            0|            0|  0.00%|        is checked for the existence of handlers.
  1628|         0|            0|            0|  0.00%|        """
  1629|         0|            0|            0|  0.00%|        c = self
  1630|         0|            0|            0|  0.00%|        rv = False
  1631|         0|            0|            0|  0.00%|        while c:
  1632|         0|            0|            0|  0.00%|            if c.handlers:
  1633|         0|            0|            0|  0.00%|                rv = True
  1634|         0|            0|            0|  0.00%|                break
  1635|         0|            0|            0|  0.00%|            if not c.propagate:
  1636|         0|            0|            0|  0.00%|                break
  1637|         0|            0|            0|  0.00%|            else:
  1638|         0|            0|            0|  0.00%|                c = c.parent
  1639|         0|            0|            0|  0.00%|        return rv
  1640|         0|            0|            0|  0.00%|
  1641|         0|            0|            0|  0.00%|    def callHandlers(self, record):
  1642|         0|            0|            0|  0.00%|        """
  1643|         0|            0|            0|  0.00%|        Pass a record to all relevant handlers.
  1644|         0|            0|            0|  0.00%|
  1645|         0|            0|            0|  0.00%|        Loop through all handlers for this logger and its parents in the
  1646|         0|            0|            0|  0.00%|        logger hierarchy. If no handler was found, output a one-off error
  1647|         0|            0|            0|  0.00%|        message to sys.stderr. Stop searching up the hierarchy whenever a
  1648|         0|            0|            0|  0.00%|        logger with the "propagate" attribute set to zero is found - that
  1649|         0|            0|            0|  0.00%|        will be the last logger whose handlers are called.
  1650|         0|            0|            0|  0.00%|        """
  1651|         0|            0|            0|  0.00%|        c = self
  1652|         0|            0|            0|  0.00%|        found = 0
  1653|         0|            0|            0|  0.00%|        while c:
  1654|         0|            0|            0|  0.00%|            for hdlr in c.handlers:
  1655|         0|            0|            0|  0.00%|                found = found + 1
  1656|         0|            0|            0|  0.00%|                if record.levelno >= hdlr.level:
  1657|         0|            0|            0|  0.00%|                    hdlr.handle(record)
  1658|         0|            0|            0|  0.00%|            if not c.propagate:
  1659|         0|            0|            0|  0.00%|                c = None    #break out
  1660|         0|            0|            0|  0.00%|            else:
  1661|         0|            0|            0|  0.00%|                c = c.parent
  1662|         0|            0|            0|  0.00%|        if (found == 0):
  1663|         0|            0|            0|  0.00%|            if lastResort:
  1664|         0|            0|            0|  0.00%|                if record.levelno >= lastResort.level:
  1665|         0|            0|            0|  0.00%|                    lastResort.handle(record)
  1666|         0|            0|            0|  0.00%|            elif raiseExceptions and not self.manager.emittedNoHandlerWarning:
  1667|         0|            0|            0|  0.00%|                sys.stderr.write("No handlers could be found for logger"
  1668|         0|            0|            0|  0.00%|                                 " \"%s\"\n" % self.name)
  1669|         0|            0|            0|  0.00%|                self.manager.emittedNoHandlerWarning = True
  1670|         0|            0|            0|  0.00%|
  1671|         0|            0|            0|  0.00%|    def getEffectiveLevel(self):
  1672|         0|            0|            0|  0.00%|        """
  1673|         0|            0|            0|  0.00%|        Get the effective level for this logger.
  1674|         0|            0|            0|  0.00%|
  1675|         0|            0|            0|  0.00%|        Loop through this logger and its parents in the logger hierarchy,
  1676|         0|            0|            0|  0.00%|        looking for a non-zero logging level. Return the first one found.
  1677|         0|            0|            0|  0.00%|        """
  1678|         0|            0|            0|  0.00%|        logger = self
  1679|         0|            0|            0|  0.00%|        while logger:
  1680|         0|            0|            0|  0.00%|            if logger.level:
  1681|         0|            0|            0|  0.00%|                return logger.level
  1682|         0|            0|            0|  0.00%|            logger = logger.parent
  1683|         0|            0|            0|  0.00%|        return NOTSET
  1684|         0|            0|            0|  0.00%|
  1685|         0|            0|            0|  0.00%|    def isEnabledFor(self, level):
  1686|         0|            0|            0|  0.00%|        """
  1687|         0|            0|            0|  0.00%|        Is this logger enabled for level 'level'?
  1688|         0|            0|            0|  0.00%|        """
  1689|         0|            0|            0|  0.00%|        if self.disabled:
  1690|         0|            0|            0|  0.00%|            return False
  1691|         0|            0|            0|  0.00%|
  1692|         0|            0|            0|  0.00%|        try:
  1693|         0|            0|            0|  0.00%|            return self._cache[level]
  1694|         0|            0|            0|  0.00%|        except KeyError:
  1695|         0|            0|            0|  0.00%|            _acquireLock()
  1696|         0|            0|            0|  0.00%|            try:
  1697|         0|            0|            0|  0.00%|                if self.manager.disable >= level:
  1698|         0|            0|            0|  0.00%|                    is_enabled = self._cache[level] = False
  1699|         0|            0|            0|  0.00%|                else:
  1700|         0|            0|            0|  0.00%|                    is_enabled = self._cache[level] = (
  1701|         0|            0|            0|  0.00%|                        level >= self.getEffectiveLevel()
  1702|         0|            0|            0|  0.00%|                    )
  1703|         0|            0|            0|  0.00%|            finally:
  1704|         0|            0|            0|  0.00%|                _releaseLock()
  1705|         0|            0|            0|  0.00%|            return is_enabled
  1706|         0|            0|            0|  0.00%|
  1707|         0|            0|            0|  0.00%|    def getChild(self, suffix):
  1708|         0|            0|            0|  0.00%|        """
  1709|         0|            0|            0|  0.00%|        Get a logger which is a descendant to this one.
  1710|         0|            0|            0|  0.00%|
  1711|         0|            0|            0|  0.00%|        This is a convenience method, such that
  1712|         0|            0|            0|  0.00%|
  1713|         0|            0|            0|  0.00%|        logging.getLogger('abc').getChild('def.ghi')
  1714|         0|            0|            0|  0.00%|
  1715|         0|            0|            0|  0.00%|        is the same as
  1716|         0|            0|            0|  0.00%|
  1717|         0|            0|            0|  0.00%|        logging.getLogger('abc.def.ghi')
  1718|         0|            0|            0|  0.00%|
  1719|         0|            0|            0|  0.00%|        It's useful, for example, when the parent logger is named using
  1720|         0|            0|            0|  0.00%|        __name__ rather than a literal string.
  1721|         0|            0|            0|  0.00%|        """
  1722|         0|            0|            0|  0.00%|        if self.root is not self:
  1723|         0|            0|            0|  0.00%|            suffix = '.'.join((self.name, suffix))
  1724|         0|            0|            0|  0.00%|        return self.manager.getLogger(suffix)
  1725|         0|            0|            0|  0.00%|
  1726|         0|            0|            0|  0.00%|    def __repr__(self):
  1727|         0|            0|            0|  0.00%|        level = getLevelName(self.getEffectiveLevel())
  1728|         0|            0|            0|  0.00%|        return '<%s %s (%s)>' % (self.__class__.__name__, self.name, level)
  1729|         0|            0|            0|  0.00%|
  1730|         0|            0|            0|  0.00%|    def __reduce__(self):
  1731|         0|            0|            0|  0.00%|        # In general, only the root logger will not be accessible via its name.
  1732|         0|            0|            0|  0.00%|        # However, the root logger's class has its own __reduce__ method.
  1733|         0|            0|            0|  0.00%|        if getLogger(self.name) is not self:
  1734|         0|            0|            0|  0.00%|            import pickle
  1735|         0|            0|            0|  0.00%|            raise pickle.PicklingError('logger cannot be pickled')
  1736|         0|            0|            0|  0.00%|        return getLogger, (self.name,)
  1737|         0|            0|            0|  0.00%|
  1738|         0|            0|            0|  0.00%|
  1739|         0|            0|            0|  0.00%|class RootLogger(Logger):
  1740|         0|            0|            0|  0.00%|    """
  1741|         0|            0|            0|  0.00%|    A root logger is not that different to any other logger, except that
  1742|         0|            0|            0|  0.00%|    it must have a logging level and there is only one instance of it in
  1743|         0|            0|            0|  0.00%|    the hierarchy.
  1744|         0|            0|            0|  0.00%|    """
  1745|         0|            0|            0|  0.00%|    def __init__(self, level):
  1746|         0|            0|            0|  0.00%|        """
  1747|         0|            0|            0|  0.00%|        Initialize the logger with the name "root".
  1748|         0|            0|            0|  0.00%|        """
  1749|         0|            0|            0|  0.00%|        Logger.__init__(self, "root", level)
  1750|         0|            0|            0|  0.00%|
  1751|         0|            0|            0|  0.00%|    def __reduce__(self):
  1752|         0|            0|            0|  0.00%|        return getLogger, ()
  1753|         0|            0|            0|  0.00%|
  1754|         0|            0|            0|  0.00%|_loggerClass = Logger
  1755|         0|            0|            0|  0.00%|
  1756|         0|            0|            0|  0.00%|class LoggerAdapter(object):
  1757|         0|            0|            0|  0.00%|    """
  1758|         0|            0|            0|  0.00%|    An adapter for loggers which makes it easier to specify contextual
  1759|         0|            0|            0|  0.00%|    information in logging output.
  1760|         0|            0|            0|  0.00%|    """
  1761|         0|            0|            0|  0.00%|
  1762|         0|            0|            0|  0.00%|    def __init__(self, logger, extra):
  1763|         0|            0|            0|  0.00%|        """
  1764|         0|            0|            0|  0.00%|        Initialize the adapter with a logger and a dict-like object which
  1765|         0|            0|            0|  0.00%|        provides contextual information. This constructor signature allows
  1766|         0|            0|            0|  0.00%|        easy stacking of LoggerAdapters, if so desired.
  1767|         0|            0|            0|  0.00%|
  1768|         0|            0|            0|  0.00%|        You can effectively pass keyword arguments as shown in the
  1769|         0|            0|            0|  0.00%|        following example:
  1770|         0|            0|            0|  0.00%|
  1771|         0|            0|            0|  0.00%|        adapter = LoggerAdapter(someLogger, dict(p1=v1, p2="v2"))
  1772|         0|            0|            0|  0.00%|        """
  1773|         0|            0|            0|  0.00%|        self.logger = logger
  1774|         0|            0|            0|  0.00%|        self.extra = extra
  1775|         0|            0|            0|  0.00%|
  1776|         0|            0|            0|  0.00%|    def process(self, msg, kwargs):
  1777|         0|            0|            0|  0.00%|        """
  1778|         0|            0|            0|  0.00%|        Process the logging message and keyword arguments passed in to
  1779|         0|            0|            0|  0.00%|        a logging call to insert contextual information. You can either
  1780|         0|            0|            0|  0.00%|        manipulate the message itself, the keyword args or both. Return
  1781|         0|            0|            0|  0.00%|        the message and kwargs modified (or not) to suit your needs.
  1782|         0|            0|            0|  0.00%|
  1783|         0|            0|            0|  0.00%|        Normally, you'll only need to override this one method in a
  1784|         0|            0|            0|  0.00%|        LoggerAdapter subclass for your specific needs.
  1785|         0|            0|            0|  0.00%|        """
  1786|         0|            0|            0|  0.00%|        kwargs["extra"] = self.extra
  1787|         0|            0|            0|  0.00%|        return msg, kwargs
  1788|         0|            0|            0|  0.00%|
  1789|         0|            0|            0|  0.00%|    #
  1790|         0|            0|            0|  0.00%|    # Boilerplate convenience methods
  1791|         0|            0|            0|  0.00%|    #
  1792|         0|            0|            0|  0.00%|    def debug(self, msg, *args, **kwargs):
  1793|         0|            0|            0|  0.00%|        """
  1794|         0|            0|            0|  0.00%|        Delegate a debug call to the underlying logger.
  1795|         0|            0|            0|  0.00%|        """
  1796|         0|            0|            0|  0.00%|        self.log(DEBUG, msg, *args, **kwargs)
  1797|         0|            0|            0|  0.00%|
  1798|         0|            0|            0|  0.00%|    def info(self, msg, *args, **kwargs):
  1799|         0|            0|            0|  0.00%|        """
  1800|         0|            0|            0|  0.00%|        Delegate an info call to the underlying logger.
  1801|         0|            0|            0|  0.00%|        """
  1802|         0|            0|            0|  0.00%|        self.log(INFO, msg, *args, **kwargs)
  1803|         0|            0|            0|  0.00%|
  1804|         0|            0|            0|  0.00%|    def warning(self, msg, *args, **kwargs):
  1805|         0|            0|            0|  0.00%|        """
  1806|         0|            0|            0|  0.00%|        Delegate a warning call to the underlying logger.
  1807|         0|            0|            0|  0.00%|        """
  1808|         0|            0|            0|  0.00%|        self.log(WARNING, msg, *args, **kwargs)
  1809|         0|            0|            0|  0.00%|
  1810|         0|            0|            0|  0.00%|    def warn(self, msg, *args, **kwargs):
  1811|         0|            0|            0|  0.00%|        warnings.warn("The 'warn' method is deprecated, "
  1812|         0|            0|            0|  0.00%|            "use 'warning' instead", DeprecationWarning, 2)
  1813|         0|            0|            0|  0.00%|        self.warning(msg, *args, **kwargs)
  1814|         0|            0|            0|  0.00%|
  1815|         0|            0|            0|  0.00%|    def error(self, msg, *args, **kwargs):
  1816|         0|            0|            0|  0.00%|        """
  1817|         0|            0|            0|  0.00%|        Delegate an error call to the underlying logger.
  1818|         0|            0|            0|  0.00%|        """
  1819|         0|            0|            0|  0.00%|        self.log(ERROR, msg, *args, **kwargs)
  1820|         0|            0|            0|  0.00%|
  1821|         0|            0|            0|  0.00%|    def exception(self, msg, *args, exc_info=True, **kwargs):
  1822|         0|            0|            0|  0.00%|        """
  1823|         0|            0|            0|  0.00%|        Delegate an exception call to the underlying logger.
  1824|         0|            0|            0|  0.00%|        """
  1825|         0|            0|            0|  0.00%|        self.log(ERROR, msg, *args, exc_info=exc_info, **kwargs)
  1826|         0|            0|            0|  0.00%|
  1827|         0|            0|            0|  0.00%|    def critical(self, msg, *args, **kwargs):
  1828|         0|            0|            0|  0.00%|        """
  1829|         0|            0|            0|  0.00%|        Delegate a critical call to the underlying logger.
  1830|         0|            0|            0|  0.00%|        """
  1831|         0|            0|            0|  0.00%|        self.log(CRITICAL, msg, *args, **kwargs)
  1832|         0|            0|            0|  0.00%|
  1833|         0|            0|            0|  0.00%|    def log(self, level, msg, *args, **kwargs):
  1834|         0|            0|            0|  0.00%|        """
  1835|         0|            0|            0|  0.00%|        Delegate a log call to the underlying logger, after adding
  1836|         0|            0|            0|  0.00%|        contextual information from this adapter instance.
  1837|         0|            0|            0|  0.00%|        """
  1838|         0|            0|            0|  0.00%|        if self.isEnabledFor(level):
  1839|         0|            0|            0|  0.00%|            msg, kwargs = self.process(msg, kwargs)
  1840|         0|            0|            0|  0.00%|            self.logger.log(level, msg, *args, **kwargs)
  1841|         0|            0|            0|  0.00%|
  1842|         0|            0|            0|  0.00%|    def isEnabledFor(self, level):
  1843|         0|            0|            0|  0.00%|        """
  1844|         0|            0|            0|  0.00%|        Is this logger enabled for level 'level'?
  1845|         0|            0|            0|  0.00%|        """
  1846|         0|            0|            0|  0.00%|        return self.logger.isEnabledFor(level)
  1847|         0|            0|            0|  0.00%|
  1848|         0|            0|            0|  0.00%|    def setLevel(self, level):
  1849|         0|            0|            0|  0.00%|        """
  1850|         0|            0|            0|  0.00%|        Set the specified level on the underlying logger.
  1851|         0|            0|            0|  0.00%|        """
  1852|         0|            0|            0|  0.00%|        self.logger.setLevel(level)
  1853|         0|            0|            0|  0.00%|
  1854|         0|            0|            0|  0.00%|    def getEffectiveLevel(self):
  1855|         0|            0|            0|  0.00%|        """
  1856|         0|            0|            0|  0.00%|        Get the effective level for the underlying logger.
  1857|         0|            0|            0|  0.00%|        """
  1858|         0|            0|            0|  0.00%|        return self.logger.getEffectiveLevel()
  1859|         0|            0|            0|  0.00%|
  1860|         0|            0|            0|  0.00%|    def hasHandlers(self):
  1861|         0|            0|            0|  0.00%|        """
  1862|         0|            0|            0|  0.00%|        See if the underlying logger has any handlers.
  1863|         0|            0|            0|  0.00%|        """
  1864|         0|            0|            0|  0.00%|        return self.logger.hasHandlers()
  1865|         0|            0|            0|  0.00%|
  1866|         0|            0|            0|  0.00%|    def _log(self, level, msg, args, exc_info=None, extra=None, stack_info=False):
  1867|         0|            0|            0|  0.00%|        """
  1868|         0|            0|            0|  0.00%|        Low-level log implementation, proxied to allow nested logger adapters.
  1869|         0|            0|            0|  0.00%|        """
  1870|         0|            0|            0|  0.00%|        return self.logger._log(
  1871|         0|            0|            0|  0.00%|            level,
  1872|         0|            0|            0|  0.00%|            msg,
  1873|         0|            0|            0|  0.00%|            args,
  1874|         0|            0|            0|  0.00%|            exc_info=exc_info,
  1875|         0|            0|            0|  0.00%|            extra=extra,
  1876|         0|            0|            0|  0.00%|            stack_info=stack_info,
  1877|         0|            0|            0|  0.00%|        )
  1878|         0|            0|            0|  0.00%|
  1879|         0|            0|            0|  0.00%|    @property
  1880|         0|            0|            0|  0.00%|    def manager(self):
  1881|         0|            0|            0|  0.00%|        return self.logger.manager
  1882|         0|            0|            0|  0.00%|
  1883|         0|            0|            0|  0.00%|    @manager.setter
  1884|         0|            0|            0|  0.00%|    def manager(self, value):
  1885|         0|            0|            0|  0.00%|        self.logger.manager = value
  1886|         0|            0|            0|  0.00%|
  1887|         0|            0|            0|  0.00%|    @property
  1888|         0|            0|            0|  0.00%|    def name(self):
  1889|         0|            0|            0|  0.00%|        return self.logger.name
  1890|         0|            0|            0|  0.00%|
  1891|         0|            0|            0|  0.00%|    def __repr__(self):
  1892|         0|            0|            0|  0.00%|        logger = self.logger
  1893|         0|            0|            0|  0.00%|        level = getLevelName(logger.getEffectiveLevel())
  1894|         0|            0|            0|  0.00%|        return '<%s %s (%s)>' % (self.__class__.__name__, logger.name, level)
  1895|         0|            0|            0|  0.00%|
  1896|         0|            0|            0|  0.00%|root = RootLogger(WARNING)
  1897|         0|            0|            0|  0.00%|Logger.root = root
  1898|         0|            0|            0|  0.00%|Logger.manager = Manager(Logger.root)
  1899|         0|            0|            0|  0.00%|
  1900|         0|            0|            0|  0.00%|#---------------------------------------------------------------------------
  1901|         0|            0|            0|  0.00%|# Configuration classes and functions
  1902|         0|            0|            0|  0.00%|#---------------------------------------------------------------------------
  1903|         0|            0|            0|  0.00%|
  1904|         0|            0|            0|  0.00%|def basicConfig(**kwargs):
  1905|         0|            0|            0|  0.00%|    """
  1906|         0|            0|            0|  0.00%|    Do basic configuration for the logging system.
  1907|         0|            0|            0|  0.00%|
  1908|         0|            0|            0|  0.00%|    This function does nothing if the root logger already has handlers
  1909|         0|            0|            0|  0.00%|    configured, unless the keyword argument *force* is set to ``True``.
  1910|         0|            0|            0|  0.00%|    It is a convenience method intended for use by simple scripts
  1911|         0|            0|            0|  0.00%|    to do one-shot configuration of the logging package.
  1912|         0|            0|            0|  0.00%|
  1913|         0|            0|            0|  0.00%|    The default behaviour is to create a StreamHandler which writes to
  1914|         0|            0|            0|  0.00%|    sys.stderr, set a formatter using the BASIC_FORMAT format string, and
  1915|         0|            0|            0|  0.00%|    add the handler to the root logger.
  1916|         0|            0|            0|  0.00%|
  1917|         0|            0|            0|  0.00%|    A number of optional keyword arguments may be specified, which can alter
  1918|         0|            0|            0|  0.00%|    the default behaviour.
  1919|         0|            0|            0|  0.00%|
  1920|         0|            0|            0|  0.00%|    filename  Specifies that a FileHandler be created, using the specified
  1921|         0|            0|            0|  0.00%|              filename, rather than a StreamHandler.
  1922|         0|            0|            0|  0.00%|    filemode  Specifies the mode to open the file, if filename is specified
  1923|         0|            0|            0|  0.00%|              (if filemode is unspecified, it defaults to 'a').
  1924|         0|            0|            0|  0.00%|    format    Use the specified format string for the handler.
  1925|         0|            0|            0|  0.00%|    datefmt   Use the specified date/time format.
  1926|         0|            0|            0|  0.00%|    style     If a format string is specified, use this to specify the
  1927|         0|            0|            0|  0.00%|              type of format string (possible values '%', '{', '$', for
  1928|         0|            0|            0|  0.00%|              %-formatting, :meth:`str.format` and :class:`string.Template`
  1929|         0|            0|            0|  0.00%|              - defaults to '%').
  1930|         0|            0|            0|  0.00%|    level     Set the root logger level to the specified level.
  1931|         0|            0|            0|  0.00%|    stream    Use the specified stream to initialize the StreamHandler. Note
  1932|         0|            0|            0|  0.00%|              that this argument is incompatible with 'filename' - if both
  1933|         0|            0|            0|  0.00%|              are present, 'stream' is ignored.
  1934|         0|            0|            0|  0.00%|    handlers  If specified, this should be an iterable of already created
  1935|         0|            0|            0|  0.00%|              handlers, which will be added to the root handler. Any handler
  1936|         0|            0|            0|  0.00%|              in the list which does not have a formatter assigned will be
  1937|         0|            0|            0|  0.00%|              assigned the formatter created in this function.
  1938|         0|            0|            0|  0.00%|    force     If this keyword  is specified as true, any existing handlers
  1939|         0|            0|            0|  0.00%|              attached to the root logger are removed and closed, before
  1940|         0|            0|            0|  0.00%|              carrying out the configuration as specified by the other
  1941|         0|            0|            0|  0.00%|              arguments.
  1942|         0|            0|            0|  0.00%|    Note that you could specify a stream created using open(filename, mode)
  1943|         0|            0|            0|  0.00%|    rather than passing the filename and mode in. However, it should be
  1944|         0|            0|            0|  0.00%|    remembered that StreamHandler does not close its stream (since it may be
  1945|         0|            0|            0|  0.00%|    using sys.stdout or sys.stderr), whereas FileHandler closes its stream
  1946|         0|            0|            0|  0.00%|    when the handler is closed.
  1947|         0|            0|            0|  0.00%|
  1948|         0|            0|            0|  0.00%|    .. versionchanged:: 3.8
  1949|         0|            0|            0|  0.00%|       Added the ``force`` parameter.
  1950|         0|            0|            0|  0.00%|
  1951|         0|            0|            0|  0.00%|    .. versionchanged:: 3.2
  1952|         0|            0|            0|  0.00%|       Added the ``style`` parameter.
  1953|         0|            0|            0|  0.00%|
  1954|         0|            0|            0|  0.00%|    .. versionchanged:: 3.3
  1955|         0|            0|            0|  0.00%|       Added the ``handlers`` parameter. A ``ValueError`` is now thrown for
  1956|         0|            0|            0|  0.00%|       incompatible arguments (e.g. ``handlers`` specified together with
  1957|         0|            0|            0|  0.00%|       ``filename``/``filemode``, or ``filename``/``filemode`` specified
  1958|         0|            0|            0|  0.00%|       together with ``stream``, or ``handlers`` specified together with
  1959|         0|            0|            0|  0.00%|       ``stream``.
  1960|         0|            0|            0|  0.00%|    """
  1961|         0|            0|            0|  0.00%|    # Add thread safety in case someone mistakenly calls
  1962|         0|            0|            0|  0.00%|    # basicConfig() from multiple threads
  1963|         0|            0|            0|  0.00%|    _acquireLock()
  1964|         0|            0|            0|  0.00%|    try:
  1965|         0|            0|            0|  0.00%|        force = kwargs.pop('force', False)
  1966|         0|            0|            0|  0.00%|        if force:
  1967|         0|            0|            0|  0.00%|            for h in root.handlers[:]:
  1968|         0|            0|            0|  0.00%|                root.removeHandler(h)
  1969|         0|            0|            0|  0.00%|                h.close()
  1970|         0|            0|            0|  0.00%|        if len(root.handlers) == 0:
  1971|         0|            0|            0|  0.00%|            handlers = kwargs.pop("handlers", None)
  1972|         0|            0|            0|  0.00%|            if handlers is None:
  1973|         0|            0|            0|  0.00%|                if "stream" in kwargs and "filename" in kwargs:
  1974|         0|            0|            0|  0.00%|                    raise ValueError("'stream' and 'filename' should not be "
  1975|         0|            0|            0|  0.00%|                                     "specified together")
  1976|         0|            0|            0|  0.00%|            else:
  1977|         0|            0|            0|  0.00%|                if "stream" in kwargs or "filename" in kwargs:
  1978|         0|            0|            0|  0.00%|                    raise ValueError("'stream' or 'filename' should not be "
  1979|         0|            0|            0|  0.00%|                                     "specified together with 'handlers'")
  1980|         0|            0|            0|  0.00%|            if handlers is None:
  1981|         0|            0|            0|  0.00%|                filename = kwargs.pop("filename", None)
  1982|         0|            0|            0|  0.00%|                mode = kwargs.pop("filemode", 'a')
  1983|         0|            0|            0|  0.00%|                if filename:
  1984|         0|            0|            0|  0.00%|                    h = FileHandler(filename, mode)
  1985|         0|            0|            0|  0.00%|                else:
  1986|         0|            0|            0|  0.00%|                    stream = kwargs.pop("stream", None)
  1987|         0|            0|            0|  0.00%|                    h = StreamHandler(stream)
  1988|         0|            0|            0|  0.00%|                handlers = [h]
  1989|         0|            0|            0|  0.00%|            dfs = kwargs.pop("datefmt", None)
  1990|         0|            0|            0|  0.00%|            style = kwargs.pop("style", '%')
  1991|         0|            0|            0|  0.00%|            if style not in _STYLES:
  1992|         0|            0|            0|  0.00%|                raise ValueError('Style must be one of: %s' % ','.join(
  1993|         0|            0|            0|  0.00%|                                 _STYLES.keys()))
  1994|         0|            0|            0|  0.00%|            fs = kwargs.pop("format", _STYLES[style][1])
  1995|         0|            0|            0|  0.00%|            fmt = Formatter(fs, dfs, style)
  1996|         0|            0|            0|  0.00%|            for h in handlers:
  1997|         0|            0|            0|  0.00%|                if h.formatter is None:
  1998|         0|            0|            0|  0.00%|                    h.setFormatter(fmt)
  1999|         0|            0|            0|  0.00%|                root.addHandler(h)
  2000|         0|            0|            0|  0.00%|            level = kwargs.pop("level", None)
  2001|         0|            0|            0|  0.00%|            if level is not None:
  2002|         0|            0|            0|  0.00%|                root.setLevel(level)
  2003|         0|            0|            0|  0.00%|            if kwargs:
  2004|         0|            0|            0|  0.00%|                keys = ', '.join(kwargs.keys())
  2005|         0|            0|            0|  0.00%|                raise ValueError('Unrecognised argument(s): %s' % keys)
  2006|         0|            0|            0|  0.00%|    finally:
  2007|         0|            0|            0|  0.00%|        _releaseLock()
  2008|         0|            0|            0|  0.00%|
  2009|         0|            0|            0|  0.00%|#---------------------------------------------------------------------------
  2010|         0|            0|            0|  0.00%|# Utility functions at module level.
  2011|         0|            0|            0|  0.00%|# Basically delegate everything to the root logger.
  2012|         0|            0|            0|  0.00%|#---------------------------------------------------------------------------
  2013|         0|            0|            0|  0.00%|
  2014|         0|            0|            0|  0.00%|def getLogger(name=None):
  2015|         0|            0|            0|  0.00%|    """
  2016|         0|            0|            0|  0.00%|    Return a logger with the specified name, creating it if necessary.
  2017|         0|            0|            0|  0.00%|
  2018|         0|            0|            0|  0.00%|    If no name is specified, return the root logger.
  2019|         0|            0|            0|  0.00%|    """
  2020|         0|            0|            0|  0.00%|    if name:
  2021|         0|            0|            0|  0.00%|        return Logger.manager.getLogger(name)
  2022|         0|            0|            0|  0.00%|    else:
  2023|         0|            0|            0|  0.00%|        return root
  2024|         0|            0|            0|  0.00%|
  2025|         0|            0|            0|  0.00%|def critical(msg, *args, **kwargs):
  2026|         0|            0|            0|  0.00%|    """
  2027|         0|            0|            0|  0.00%|    Log a message with severity 'CRITICAL' on the root logger. If the logger
  2028|         0|            0|            0|  0.00%|    has no handlers, call basicConfig() to add a console handler with a
  2029|         0|            0|            0|  0.00%|    pre-defined format.
  2030|         0|            0|            0|  0.00%|    """
  2031|         0|            0|            0|  0.00%|    if len(root.handlers) == 0:
  2032|         0|            0|            0|  0.00%|        basicConfig()
  2033|         0|            0|            0|  0.00%|    root.critical(msg, *args, **kwargs)
  2034|         0|            0|            0|  0.00%|
  2035|         0|            0|            0|  0.00%|fatal = critical
  2036|         0|            0|            0|  0.00%|
  2037|         0|            0|            0|  0.00%|def error(msg, *args, **kwargs):
  2038|         0|            0|            0|  0.00%|    """
  2039|         0|            0|            0|  0.00%|    Log a message with severity 'ERROR' on the root logger. If the logger has
  2040|         0|            0|            0|  0.00%|    no handlers, call basicConfig() to add a console handler with a pre-defined
  2041|         0|            0|            0|  0.00%|    format.
  2042|         0|            0|            0|  0.00%|    """
  2043|         0|            0|            0|  0.00%|    if len(root.handlers) == 0:
  2044|         0|            0|            0|  0.00%|        basicConfig()
  2045|         0|            0|            0|  0.00%|    root.error(msg, *args, **kwargs)
  2046|         0|            0|            0|  0.00%|
  2047|         0|            0|            0|  0.00%|def exception(msg, *args, exc_info=True, **kwargs):
  2048|         0|            0|            0|  0.00%|    """
  2049|         0|            0|            0|  0.00%|    Log a message with severity 'ERROR' on the root logger, with exception
  2050|         0|            0|            0|  0.00%|    information. If the logger has no handlers, basicConfig() is called to add
  2051|         0|            0|            0|  0.00%|    a console handler with a pre-defined format.
  2052|         0|            0|            0|  0.00%|    """
  2053|         0|            0|            0|  0.00%|    error(msg, *args, exc_info=exc_info, **kwargs)
  2054|         0|            0|            0|  0.00%|
  2055|         0|            0|            0|  0.00%|def warning(msg, *args, **kwargs):
  2056|         0|            0|            0|  0.00%|    """
  2057|         0|            0|            0|  0.00%|    Log a message with severity 'WARNING' on the root logger. If the logger has
  2058|         0|            0|            0|  0.00%|    no handlers, call basicConfig() to add a console handler with a pre-defined
  2059|         0|            0|            0|  0.00%|    format.
  2060|         0|            0|            0|  0.00%|    """
  2061|         0|            0|            0|  0.00%|    if len(root.handlers) == 0:
  2062|         0|            0|            0|  0.00%|        basicConfig()
  2063|         0|            0|            0|  0.00%|    root.warning(msg, *args, **kwargs)
  2064|         0|            0|            0|  0.00%|
  2065|         0|            0|            0|  0.00%|def warn(msg, *args, **kwargs):
  2066|         0|            0|            0|  0.00%|    warnings.warn("The 'warn' function is deprecated, "
  2067|         0|            0|            0|  0.00%|        "use 'warning' instead", DeprecationWarning, 2)
  2068|         0|            0|            0|  0.00%|    warning(msg, *args, **kwargs)
  2069|         0|            0|            0|  0.00%|
  2070|         0|            0|            0|  0.00%|def info(msg, *args, **kwargs):
  2071|         0|            0|            0|  0.00%|    """
  2072|         0|            0|            0|  0.00%|    Log a message with severity 'INFO' on the root logger. If the logger has
  2073|         0|            0|            0|  0.00%|    no handlers, call basicConfig() to add a console handler with a pre-defined
  2074|         0|            0|            0|  0.00%|    format.
  2075|         0|            0|            0|  0.00%|    """
  2076|         0|            0|            0|  0.00%|    if len(root.handlers) == 0:
  2077|         0|            0|            0|  0.00%|        basicConfig()
  2078|         0|            0|            0|  0.00%|    root.info(msg, *args, **kwargs)
  2079|         0|            0|            0|  0.00%|
  2080|         0|            0|            0|  0.00%|def debug(msg, *args, **kwargs):
  2081|         0|            0|            0|  0.00%|    """
  2082|         0|            0|            0|  0.00%|    Log a message with severity 'DEBUG' on the root logger. If the logger has
  2083|         0|            0|            0|  0.00%|    no handlers, call basicConfig() to add a console handler with a pre-defined
  2084|         0|            0|            0|  0.00%|    format.
  2085|         0|            0|            0|  0.00%|    """
  2086|         0|            0|            0|  0.00%|    if len(root.handlers) == 0:
  2087|         0|            0|            0|  0.00%|        basicConfig()
  2088|         0|            0|            0|  0.00%|    root.debug(msg, *args, **kwargs)
  2089|         0|            0|            0|  0.00%|
  2090|         0|            0|            0|  0.00%|def log(level, msg, *args, **kwargs):
  2091|         0|            0|            0|  0.00%|    """
  2092|         0|            0|            0|  0.00%|    Log 'msg % args' with the integer severity 'level' on the root logger. If
  2093|         0|            0|            0|  0.00%|    the logger has no handlers, call basicConfig() to add a console handler
  2094|         0|            0|            0|  0.00%|    with a pre-defined format.
  2095|         0|            0|            0|  0.00%|    """
  2096|         0|            0|            0|  0.00%|    if len(root.handlers) == 0:
  2097|         0|            0|            0|  0.00%|        basicConfig()
  2098|         0|            0|            0|  0.00%|    root.log(level, msg, *args, **kwargs)
  2099|         0|            0|            0|  0.00%|
  2100|         0|            0|            0|  0.00%|def disable(level=CRITICAL):
  2101|         0|            0|            0|  0.00%|    """
  2102|         0|            0|            0|  0.00%|    Disable all logging calls of severity 'level' and below.
  2103|         0|            0|            0|  0.00%|    """
  2104|         0|            0|            0|  0.00%|    root.manager.disable = level
  2105|         0|            0|            0|  0.00%|    root.manager._clear_cache()
  2106|         0|            0|            0|  0.00%|
  2107|         0|            0|            0|  0.00%|def shutdown(handlerList=_handlerList):
  2108|         0|            0|            0|  0.00%|    """
  2109|         0|            0|            0|  0.00%|    Perform any cleanup actions in the logging system (e.g. flushing
  2110|         0|            0|            0|  0.00%|    buffers).
  2111|         0|            0|            0|  0.00%|
  2112|         0|            0|            0|  0.00%|    Should be called at application exit.
  2113|         0|            0|            0|  0.00%|    """
  2114|         0|            0|            0|  0.00%|    for wr in reversed(handlerList[:]):
  2115|         0|            0|            0|  0.00%|        #errors might occur, for example, if files are locked
  2116|         0|            0|            0|  0.00%|        #we just ignore them if raiseExceptions is not set
  2117|         0|            0|            0|  0.00%|        try:
  2118|         0|            0|            0|  0.00%|            h = wr()
  2119|         0|            0|            0|  0.00%|            if h:
  2120|         0|            0|            0|  0.00%|                try:
  2121|         0|            0|            0|  0.00%|                    h.acquire()
  2122|         0|            0|            0|  0.00%|                    h.flush()
  2123|         0|            0|            0|  0.00%|                    h.close()
  2124|         0|            0|            0|  0.00%|                except (OSError, ValueError):
  2125|         0|            0|            0|  0.00%|                    # Ignore errors which might be caused
  2126|         0|            0|            0|  0.00%|                    # because handlers have been closed but
  2127|         0|            0|            0|  0.00%|                    # references to them are still around at
  2128|         0|            0|            0|  0.00%|                    # application exit.
  2129|         0|            0|            0|  0.00%|                    pass
  2130|         0|            0|            0|  0.00%|                finally:
  2131|         0|            0|            0|  0.00%|                    h.release()
  2132|         0|            0|            0|  0.00%|        except: # ignore everything, as we're shutting down
  2133|         0|            0|            0|  0.00%|            if raiseExceptions:
  2134|         0|            0|            0|  0.00%|                raise
  2135|         0|            0|            0|  0.00%|            #else, swallow
  2136|         0|            0|            0|  0.00%|
  2137|         0|            0|            0|  0.00%|#Let's try and shutdown automatically on application exit...
  2138|         0|            0|            0|  0.00%|import atexit
  2139|         0|            0|            0|  0.00%|atexit.register(shutdown)
  2140|         0|            0|            0|  0.00%|
  2141|         0|            0|            0|  0.00%|# Null handler
  2142|         0|            0|            0|  0.00%|
  2143|         0|            0|            0|  0.00%|class NullHandler(Handler):
  2144|         0|            0|            0|  0.00%|    """
  2145|         0|            0|            0|  0.00%|    This handler does nothing. It's intended to be used to avoid the
  2146|         0|            0|            0|  0.00%|    "No handlers could be found for logger XXX" one-off warning. This is
  2147|         0|            0|            0|  0.00%|    important for library code, which may contain code to log events. If a user
  2148|         0|            0|            0|  0.00%|    of the library does not configure logging, the one-off warning might be
  2149|         0|            0|            0|  0.00%|    produced; to avoid this, the library developer simply needs to instantiate
  2150|         0|            0|            0|  0.00%|    a NullHandler and add it to the top-level logger of the library module or
  2151|         0|            0|            0|  0.00%|    package.
  2152|         0|            0|            0|  0.00%|    """
  2153|         0|            0|            0|  0.00%|    def handle(self, record):
  2154|         0|            0|            0|  0.00%|        """Stub."""
  2155|         0|            0|            0|  0.00%|
  2156|         0|            0|            0|  0.00%|    def emit(self, record):
  2157|         0|            0|            0|  0.00%|        """Stub."""
  2158|         0|            0|            0|  0.00%|
  2159|         0|            0|            0|  0.00%|    def createLock(self):
  2160|         0|            0|            0|  0.00%|        self.lock = None
  2161|         0|            0|            0|  0.00%|
  2162|         0|            0|            0|  0.00%|# Warnings integration
  2163|         0|            0|            0|  0.00%|
  2164|         0|            0|            0|  0.00%|_warnings_showwarning = None
  2165|         0|            0|            0|  0.00%|
  2166|         0|            0|            0|  0.00%|def _showwarning(message, category, filename, lineno, file=None, line=None):
  2167|         0|            0|            0|  0.00%|    """
  2168|         0|            0|            0|  0.00%|    Implementation of showwarnings which redirects to logging, which will first
  2169|         0|            0|            0|  0.00%|    check to see if the file parameter is None. If a file is specified, it will
  2170|         0|            0|            0|  0.00%|    delegate to the original warnings implementation of showwarning. Otherwise,
  2171|         0|            0|            0|  0.00%|    it will call warnings.formatwarning and will log the resulting string to a
  2172|         0|            0|            0|  0.00%|    warnings logger named "py.warnings" with level logging.WARNING.
  2173|         0|            0|            0|  0.00%|    """
  2174|         0|            0|            0|  0.00%|    if file is not None:
  2175|         0|            0|            0|  0.00%|        if _warnings_showwarning is not None:
  2176|         0|            0|            0|  0.00%|            _warnings_showwarning(message, category, filename, lineno, file, line)
  2177|         0|            0|            0|  0.00%|    else:
  2178|         0|            0|            0|  0.00%|        s = warnings.formatwarning(message, category, filename, lineno, line)
  2179|         0|            0|            0|  0.00%|        logger = getLogger("py.warnings")
  2180|         0|            0|            0|  0.00%|        if not logger.handlers:
  2181|         0|            0|            0|  0.00%|            logger.addHandler(NullHandler())
  2182|         0|            0|            0|  0.00%|        logger.warning("%s", s)
  2183|         0|            0|            0|  0.00%|
  2184|         0|            0|            0|  0.00%|def captureWarnings(capture):
  2185|         0|            0|            0|  0.00%|    """
  2186|         0|            0|            0|  0.00%|    If capture is true, redirect all warnings to the logging package.
  2187|         0|            0|            0|  0.00%|    If capture is False, ensure that warnings are not redirected to logging
  2188|         0|            0|            0|  0.00%|    but to their original destinations.
  2189|         0|            0|            0|  0.00%|    """
  2190|         0|            0|            0|  0.00%|    global _warnings_showwarning
  2191|         0|            0|            0|  0.00%|    if capture:
  2192|         0|            0|            0|  0.00%|        if _warnings_showwarning is None:
  2193|         0|            0|            0|  0.00%|            _warnings_showwarning = warnings.showwarning
  2194|         0|            0|            0|  0.00%|            warnings.showwarning = _showwarning
  2195|         0|            0|            0|  0.00%|    else:
  2196|         0|            0|            0|  0.00%|        if _warnings_showwarning is not None:
  2197|         0|            0|            0|  0.00%|            warnings.showwarning = _warnings_showwarning
  2198|         0|            0|            0|  0.00%|            _warnings_showwarning = None
File: /opt/conda/lib/python3.8/site-packages/torch/nn/_reduction.py
File duration: 0.00252485s (0.00%)
Line #|      Hits|         Time| Time per hit|      %|Source code
------+----------+-------------+-------------+-------+-----------
     1|         0|            0|            0|  0.00%|from typing import Optional
     2|         0|            0|            0|  0.00%|import warnings
     3|         0|            0|            0|  0.00%|
     4|         0|            0|            0|  0.00%|# NB: Keep this file in sync with enums in aten/src/ATen/core/Reduction.h
     5|         0|            0|            0|  0.00%|
     6|         0|            0|            0|  0.00%|
     7|       100|  0.000658751|  6.58751e-06|  0.00%|def get_enum(reduction: str) -> int:
     8|       100|  0.000591993|  5.91993e-06|  0.00%|    if reduction == 'none':
     9|         0|            0|            0|  0.00%|        ret = 0
    10|       100|  0.000458241|  4.58241e-06|  0.00%|    elif reduction == 'mean':
    11|       100|  0.000415802|  4.15802e-06|  0.00%|        ret = 1
    12|         0|            0|            0|  0.00%|    elif reduction == 'elementwise_mean':
    13|         0|            0|            0|  0.00%|        warnings.warn("reduction='elementwise_mean' is deprecated, please use reduction='mean' instead.")
    14|         0|            0|            0|  0.00%|        ret = 1
    15|         0|            0|            0|  0.00%|    elif reduction == 'sum':
    16|         0|            0|            0|  0.00%|        ret = 2
    17|         0|            0|            0|  0.00%|    else:
    18|         0|            0|            0|  0.00%|        ret = -1  # TODO: remove once JIT exceptions support control flow
    19|         0|            0|            0|  0.00%|        raise ValueError("{} is not a valid value for reduction".format(reduction))
    20|       100|  0.000400066|  4.00066e-06|  0.00%|    return ret
    21|         0|            0|            0|  0.00%|
    22|         0|            0|            0|  0.00%|# In order to support previous versions, accept boolean size_average and reduce
    23|         0|            0|            0|  0.00%|# and convert them into the new constants for now
    24|         0|            0|            0|  0.00%|
    25|         0|            0|            0|  0.00%|
    26|         0|            0|            0|  0.00%|# We use these functions in torch/legacy as well, in which case we'll silence the warning
    27|         0|            0|            0|  0.00%|def legacy_get_string(size_average: Optional[bool], reduce: Optional[bool], emit_warning: bool = True) -> str:
    28|         0|            0|            0|  0.00%|    warning = "size_average and reduce args will be deprecated, please use reduction='{}' instead."
    29|         0|            0|            0|  0.00%|
    30|         0|            0|            0|  0.00%|    if size_average is None:
    31|         0|            0|            0|  0.00%|        size_average = True
    32|         0|            0|            0|  0.00%|    if reduce is None:
    33|         0|            0|            0|  0.00%|        reduce = True
    34|         0|            0|            0|  0.00%|
    35|         0|            0|            0|  0.00%|    if size_average and reduce:
    36|         0|            0|            0|  0.00%|        ret = 'mean'
    37|         0|            0|            0|  0.00%|    elif reduce:
    38|         0|            0|            0|  0.00%|        ret = 'sum'
    39|         0|            0|            0|  0.00%|    else:
    40|         0|            0|            0|  0.00%|        ret = 'none'
    41|         0|            0|            0|  0.00%|    if emit_warning:
    42|         0|            0|            0|  0.00%|        warnings.warn(warning.format(ret))
    43|         0|            0|            0|  0.00%|    return ret
    44|         0|            0|            0|  0.00%|
    45|         0|            0|            0|  0.00%|
    46|         0|            0|            0|  0.00%|def legacy_get_enum(size_average: Optional[bool], reduce: Optional[bool], emit_warning: bool = True) -> int:
    47|         0|            0|            0|  0.00%|    return get_enum(legacy_get_string(size_average, reduce, emit_warning))
File: /opt/conda/lib/python3.8/linecache.py
File duration: 0.00208473s (0.00%)
Line #|      Hits|         Time| Time per hit|      %|Source code
------+----------+-------------+-------------+-------+-----------
     1|         0|            0|            0|  0.00%|"""Cache lines from Python source files.
     2|         0|            0|            0|  0.00%|
     3|         0|            0|            0|  0.00%|This is intended to read lines from modules imported -- hence if a filename
     4|         0|            0|            0|  0.00%|is not found, it will look down the module search path for a file by
     5|         0|            0|            0|  0.00%|that name.
     6|         0|            0|            0|  0.00%|"""
     7|         0|            0|            0|  0.00%|
     8|         0|            0|            0|  0.00%|import functools
     9|         0|            0|            0|  0.00%|import sys
    10|         0|            0|            0|  0.00%|import os
    11|         0|            0|            0|  0.00%|import tokenize
    12|         0|            0|            0|  0.00%|
    13|         0|            0|            0|  0.00%|__all__ = ["getline", "clearcache", "checkcache"]
    14|         0|            0|            0|  0.00%|
    15|         1|  2.31266e-05|  2.31266e-05|  0.00%|def getline(filename, lineno, module_globals=None):
    16|         1|  6.86646e-05|  6.86646e-05|  0.00%|    lines = getlines(filename, module_globals)
(call)|         1|   0.00373077|   0.00373077|  0.01%|# /opt/conda/lib/python3.8/linecache.py:37 getlines
    17|         1|  8.82149e-06|  8.82149e-06|  0.00%|    if 1 <= lineno <= len(lines):
    18|         1|  6.19888e-06|  6.19888e-06|  0.00%|        return lines[lineno-1]
    19|         0|            0|            0|  0.00%|    else:
    20|         0|            0|            0|  0.00%|        return ''
    21|         0|            0|            0|  0.00%|
    22|         0|            0|            0|  0.00%|
    23|         0|            0|            0|  0.00%|# The cache
    24|         0|            0|            0|  0.00%|
    25|         0|            0|            0|  0.00%|# The cache. Maps filenames to either a thunk which will provide source code,
    26|         0|            0|            0|  0.00%|# or a tuple (size, mtime, lines, fullname) once loaded.
    27|         0|            0|            0|  0.00%|cache = {}
    28|         0|            0|            0|  0.00%|
    29|         0|            0|            0|  0.00%|
    30|         0|            0|            0|  0.00%|def clearcache():
    31|         0|            0|            0|  0.00%|    """Clear the cache entirely."""
    32|         0|            0|            0|  0.00%|
    33|         0|            0|            0|  0.00%|    global cache
    34|         0|            0|            0|  0.00%|    cache = {}
    35|         0|            0|            0|  0.00%|
    36|         0|            0|            0|  0.00%|
    37|         1|  2.38419e-05|  2.38419e-05|  0.00%|def getlines(filename, module_globals=None):
    38|         0|            0|            0|  0.00%|    """Get the lines for a Python source file from the cache.
    39|         0|            0|            0|  0.00%|    Update the cache if it doesn't contain an entry for this file already."""
    40|         0|            0|            0|  0.00%|
    41|         1|  8.58307e-06|  8.58307e-06|  0.00%|    if filename in cache:
    42|         0|            0|            0|  0.00%|        entry = cache[filename]
    43|         0|            0|            0|  0.00%|        if len(entry) != 1:
    44|         0|            0|            0|  0.00%|            return cache[filename][2]
    45|         0|            0|            0|  0.00%|
    46|         1|   1.3113e-05|   1.3113e-05|  0.00%|    try:
    47|         1|  4.43459e-05|  4.43459e-05|  0.00%|        return updatecache(filename, module_globals)
(call)|         1|   0.00364089|   0.00364089|  0.01%|# /opt/conda/lib/python3.8/linecache.py:82 updatecache
    48|         0|            0|            0|  0.00%|    except MemoryError:
    49|         0|            0|            0|  0.00%|        clearcache()
    50|         0|            0|            0|  0.00%|        return []
    51|         0|            0|            0|  0.00%|
    52|         0|            0|            0|  0.00%|
    53|         0|            0|            0|  0.00%|def checkcache(filename=None):
    54|         0|            0|            0|  0.00%|    """Discard cache entries that are out of date.
    55|         0|            0|            0|  0.00%|    (This is not checked upon each call!)"""
    56|         0|            0|            0|  0.00%|
    57|         0|            0|            0|  0.00%|    if filename is None:
    58|         0|            0|            0|  0.00%|        filenames = list(cache.keys())
    59|         0|            0|            0|  0.00%|    else:
    60|         0|            0|            0|  0.00%|        if filename in cache:
    61|         0|            0|            0|  0.00%|            filenames = [filename]
    62|         0|            0|            0|  0.00%|        else:
    63|         0|            0|            0|  0.00%|            return
    64|         0|            0|            0|  0.00%|
    65|         0|            0|            0|  0.00%|    for filename in filenames:
    66|         0|            0|            0|  0.00%|        entry = cache[filename]
    67|         0|            0|            0|  0.00%|        if len(entry) == 1:
    68|         0|            0|            0|  0.00%|            # lazy cache entry, leave it lazy.
    69|         0|            0|            0|  0.00%|            continue
    70|         0|            0|            0|  0.00%|        size, mtime, lines, fullname = entry
    71|         0|            0|            0|  0.00%|        if mtime is None:
    72|         0|            0|            0|  0.00%|            continue   # no-op for files loaded via a __loader__
    73|         0|            0|            0|  0.00%|        try:
    74|         0|            0|            0|  0.00%|            stat = os.stat(fullname)
    75|         0|            0|            0|  0.00%|        except OSError:
    76|         0|            0|            0|  0.00%|            cache.pop(filename, None)
    77|         0|            0|            0|  0.00%|            continue
    78|         0|            0|            0|  0.00%|        if size != stat.st_size or mtime != stat.st_mtime:
    79|         0|            0|            0|  0.00%|            cache.pop(filename, None)
    80|         0|            0|            0|  0.00%|
    81|         0|            0|            0|  0.00%|
    82|         1|  2.00272e-05|  2.00272e-05|  0.00%|def updatecache(filename, module_globals=None):
    83|         0|            0|            0|  0.00%|    """Update a cache entry and return its list of lines.
    84|         0|            0|            0|  0.00%|    If something's wrong, print a message, discard the cache entry,
    85|         0|            0|            0|  0.00%|    and return an empty list."""
    86|         0|            0|            0|  0.00%|
    87|         1|   1.4782e-05|   1.4782e-05|  0.00%|    if filename in cache:
    88|         0|            0|            0|  0.00%|        if len(cache[filename]) != 1:
    89|         0|            0|            0|  0.00%|            cache.pop(filename, None)
    90|         1|  1.38283e-05|  1.38283e-05|  0.00%|    if not filename or (filename.startswith('<') and filename.endswith('>')):
    91|         0|            0|            0|  0.00%|        return []
    92|         0|            0|            0|  0.00%|
    93|         1|  7.15256e-06|  7.15256e-06|  0.00%|    fullname = filename
    94|         1|  6.91414e-06|  6.91414e-06|  0.00%|    try:
    95|         1|  0.000154018|  0.000154018|  0.00%|        stat = os.stat(fullname)
    96|         0|            0|            0|  0.00%|    except OSError:
    97|         0|            0|            0|  0.00%|        basename = filename
    98|         0|            0|            0|  0.00%|
    99|         0|            0|            0|  0.00%|        # Realise a lazy loader based lookup if there is one
   100|         0|            0|            0|  0.00%|        # otherwise try to lookup right now.
   101|         0|            0|            0|  0.00%|        if lazycache(filename, module_globals):
   102|         0|            0|            0|  0.00%|            try:
   103|         0|            0|            0|  0.00%|                data = cache[filename][0]()
   104|         0|            0|            0|  0.00%|            except (ImportError, OSError):
   105|         0|            0|            0|  0.00%|                pass
   106|         0|            0|            0|  0.00%|            else:
   107|         0|            0|            0|  0.00%|                if data is None:
   108|         0|            0|            0|  0.00%|                    # No luck, the PEP302 loader cannot find the source
   109|         0|            0|            0|  0.00%|                    # for this module.
   110|         0|            0|            0|  0.00%|                    return []
   111|         0|            0|            0|  0.00%|                cache[filename] = (
   112|         0|            0|            0|  0.00%|                    len(data), None,
   113|         0|            0|            0|  0.00%|                    [line+'\n' for line in data.splitlines()], fullname
   114|         0|            0|            0|  0.00%|                )
   115|         0|            0|            0|  0.00%|                return cache[filename][2]
   116|         0|            0|            0|  0.00%|
   117|         0|            0|            0|  0.00%|        # Try looking through the module search path, which is only useful
   118|         0|            0|            0|  0.00%|        # when handling a relative filename.
   119|         0|            0|            0|  0.00%|        if os.path.isabs(filename):
   120|         0|            0|            0|  0.00%|            return []
   121|         0|            0|            0|  0.00%|
   122|         0|            0|            0|  0.00%|        for dirname in sys.path:
   123|         0|            0|            0|  0.00%|            try:
   124|         0|            0|            0|  0.00%|                fullname = os.path.join(dirname, basename)
   125|         0|            0|            0|  0.00%|            except (TypeError, AttributeError):
   126|         0|            0|            0|  0.00%|                # Not sufficiently string-like to do anything useful with.
   127|         0|            0|            0|  0.00%|                continue
   128|         0|            0|            0|  0.00%|            try:
   129|         0|            0|            0|  0.00%|                stat = os.stat(fullname)
   130|         0|            0|            0|  0.00%|                break
   131|         0|            0|            0|  0.00%|            except OSError:
   132|         0|            0|            0|  0.00%|                pass
   133|         0|            0|            0|  0.00%|        else:
   134|         0|            0|            0|  0.00%|            return []
   135|         1|  1.09673e-05|  1.09673e-05|  0.00%|    try:
   136|         1|  7.48634e-05|  7.48634e-05|  0.00%|        with tokenize.open(fullname) as fp:
(call)|         1|  0.000828743|  0.000828743|  0.00%|# /opt/conda/lib/python3.8/tokenize.py:388 open
   137|         1|   0.00152755|   0.00152755|  0.00%|            lines = fp.readlines()
(call)|        27|   0.00092411|  3.42263e-05|  0.00%|# /opt/conda/lib/python3.8/codecs.py:319 decode
   138|         0|            0|            0|  0.00%|    except OSError:
   139|         0|            0|            0|  0.00%|        return []
   140|         1|  1.81198e-05|  1.81198e-05|  0.00%|    if lines and not lines[-1].endswith('\n'):
   141|         0|            0|            0|  0.00%|        lines[-1] += '\n'
   142|         1|  9.05991e-06|  9.05991e-06|  0.00%|    size, mtime = stat.st_size, stat.st_mtime
   143|         1|  2.21729e-05|  2.21729e-05|  0.00%|    cache[filename] = size, mtime, lines, fullname
   144|         1|  8.58307e-06|  8.58307e-06|  0.00%|    return lines
   145|         0|            0|            0|  0.00%|
   146|         0|            0|            0|  0.00%|
   147|         0|            0|            0|  0.00%|def lazycache(filename, module_globals):
   148|         0|            0|            0|  0.00%|    """Seed the cache for filename with module_globals.
   149|         0|            0|            0|  0.00%|
   150|         0|            0|            0|  0.00%|    The module loader will be asked for the source only when getlines is
   151|         0|            0|            0|  0.00%|    called, not immediately.
   152|         0|            0|            0|  0.00%|
   153|         0|            0|            0|  0.00%|    If there is an entry in the cache already, it is not altered.
   154|         0|            0|            0|  0.00%|
   155|         0|            0|            0|  0.00%|    :return: True if a lazy load is registered in the cache,
   156|         0|            0|            0|  0.00%|        otherwise False. To register such a load a module loader with a
   157|         0|            0|            0|  0.00%|        get_source method must be found, the filename must be a cachable
   158|         0|            0|            0|  0.00%|        filename, and the filename must not be already cached.
   159|         0|            0|            0|  0.00%|    """
   160|         0|            0|            0|  0.00%|    if filename in cache:
   161|         0|            0|            0|  0.00%|        if len(cache[filename]) == 1:
   162|         0|            0|            0|  0.00%|            return True
   163|         0|            0|            0|  0.00%|        else:
   164|         0|            0|            0|  0.00%|            return False
   165|         0|            0|            0|  0.00%|    if not filename or (filename.startswith('<') and filename.endswith('>')):
   166|         0|            0|            0|  0.00%|        return False
   167|         0|            0|            0|  0.00%|    # Try for a __loader__, if available
   168|         0|            0|            0|  0.00%|    if module_globals and '__loader__' in module_globals:
   169|         0|            0|            0|  0.00%|        name = module_globals.get('__name__')
   170|         0|            0|            0|  0.00%|        loader = module_globals['__loader__']
   171|         0|            0|            0|  0.00%|        get_source = getattr(loader, 'get_source', None)
   172|         0|            0|            0|  0.00%|
   173|         0|            0|            0|  0.00%|        if name and get_source:
   174|         0|            0|            0|  0.00%|            get_lines = functools.partial(get_source, name)
   175|         0|            0|            0|  0.00%|            cache[filename] = (get_lines,)
   176|         0|            0|            0|  0.00%|            return True
   177|         0|            0|            0|  0.00%|    return False
File: <string>
File duration: 0.00169754s (0.00%)
Line #|      Hits|         Time| Time per hit|      %|Source code
------+----------+-------------+-------------+-------+-----------
     1|       216|   0.00169754|  7.85898e-06|  0.00%|
File: /opt/conda/lib/python3.8/site-packages/torch/nn/parameter.py
File duration: 0.00141168s (0.00%)
Line #|      Hits|         Time| Time per hit|      %|Source code
------+----------+-------------+-------------+-------+-----------
     1|         0|            0|            0|  0.00%|import torch
     2|         0|            0|            0|  0.00%|from torch._C import _disabled_torch_function_impl
     3|         0|            0|            0|  0.00%|from collections import OrderedDict
     4|         0|            0|            0|  0.00%|
     5|         0|            0|            0|  0.00%|
     6|         0|            0|            0|  0.00%|class Parameter(torch.Tensor):
     7|         0|            0|            0|  0.00%|    r"""A kind of Tensor that is to be considered a module parameter.
     8|         0|            0|            0|  0.00%|
     9|         0|            0|            0|  0.00%|    Parameters are :class:`~torch.Tensor` subclasses, that have a
    10|         0|            0|            0|  0.00%|    very special property when used with :class:`Module` s - when they're
    11|         0|            0|            0|  0.00%|    assigned as Module attributes they are automatically added to the list of
    12|         0|            0|            0|  0.00%|    its parameters, and will appear e.g. in :meth:`~Module.parameters` iterator.
    13|         0|            0|            0|  0.00%|    Assigning a Tensor doesn't have such effect. This is because one might
    14|         0|            0|            0|  0.00%|    want to cache some temporary state, like last hidden state of the RNN, in
    15|         0|            0|            0|  0.00%|    the model. If there was no such class as :class:`Parameter`, these
    16|         0|            0|            0|  0.00%|    temporaries would get registered too.
    17|         0|            0|            0|  0.00%|
    18|         0|            0|            0|  0.00%|    Args:
    19|         0|            0|            0|  0.00%|        data (Tensor): parameter tensor.
    20|         0|            0|            0|  0.00%|        requires_grad (bool, optional): if the parameter requires gradient. See
    21|         0|            0|            0|  0.00%|            :ref:`locally-disable-grad-doc` for more details. Default: `True`
    22|         0|            0|            0|  0.00%|    """
    23|         0|            0|            0|  0.00%|    def __new__(cls, data=None, requires_grad=True):
    24|         0|            0|            0|  0.00%|        if data is None:
    25|         0|            0|            0|  0.00%|            data = torch.tensor([])
    26|         0|            0|            0|  0.00%|        return torch.Tensor._make_subclass(cls, data, requires_grad)
    27|         0|            0|            0|  0.00%|
    28|         0|            0|            0|  0.00%|    def __deepcopy__(self, memo):
    29|         0|            0|            0|  0.00%|        if id(self) in memo:
    30|         0|            0|            0|  0.00%|            return memo[id(self)]
    31|         0|            0|            0|  0.00%|        else:
    32|         0|            0|            0|  0.00%|            result = type(self)(self.data.clone(memory_format=torch.preserve_format), self.requires_grad)
    33|         0|            0|            0|  0.00%|            memo[id(self)] = result
    34|         0|            0|            0|  0.00%|            return result
    35|         0|            0|            0|  0.00%|
    36|         0|            0|            0|  0.00%|    def __repr__(self):
    37|         0|            0|            0|  0.00%|        return 'Parameter containing:\n' + super(Parameter, self).__repr__()
    38|         0|            0|            0|  0.00%|
    39|         0|            0|            0|  0.00%|    def __reduce_ex__(self, proto):
    40|         0|            0|            0|  0.00%|        # See Note [Don't serialize hooks]
    41|         0|            0|            0|  0.00%|        return (
    42|         0|            0|            0|  0.00%|            torch._utils._rebuild_parameter,
    43|         0|            0|            0|  0.00%|            (self.data, self.requires_grad, OrderedDict())
    44|         0|            0|            0|  0.00%|        )
    45|         0|            0|            0|  0.00%|
    46|         0|            0|            0|  0.00%|    __torch_function__ = _disabled_torch_function_impl
    47|         0|            0|            0|  0.00%|
    48|         0|            0|            0|  0.00%|
    49|         0|            0|            0|  0.00%|class UninitializedTensorMixin:
    50|         0|            0|            0|  0.00%|    _allowed_methods = [
    51|         0|            0|            0|  0.00%|        torch.Tensor.__hash__,
    52|         0|            0|            0|  0.00%|        torch.Tensor.size,
    53|         0|            0|            0|  0.00%|        torch.Tensor.copy_,
    54|         0|            0|            0|  0.00%|        torch.Tensor.is_floating_point,
    55|         0|            0|            0|  0.00%|        torch.Tensor.half,
    56|         0|            0|            0|  0.00%|        torch.Tensor.float,
    57|         0|            0|            0|  0.00%|        torch.Tensor.double,
    58|         0|            0|            0|  0.00%|        torch.Tensor.char,
    59|         0|            0|            0|  0.00%|        torch.Tensor.short,
    60|         0|            0|            0|  0.00%|        torch.Tensor.int,
    61|         0|            0|            0|  0.00%|        torch.Tensor.long,
    62|         0|            0|            0|  0.00%|        torch.Tensor.cuda,
    63|         0|            0|            0|  0.00%|        torch.Tensor.cpu,
    64|         0|            0|            0|  0.00%|        torch.Tensor.to,
    65|         0|            0|            0|  0.00%|        torch.Tensor.get_device,
    66|         0|            0|            0|  0.00%|        torch._has_compatible_shallow_copy_type,
    67|         0|            0|            0|  0.00%|    ]
    68|         0|            0|            0|  0.00%|
    69|         0|            0|            0|  0.00%|    def materialize(self, shape, device=None, dtype=None):
    70|         0|            0|            0|  0.00%|        r"""Create a Parameter or Tensor with the same properties of the uninitialized one.
    71|         0|            0|            0|  0.00%|        Given a shape, it materializes a parameter in the same device
    72|         0|            0|            0|  0.00%|        and with the same `dtype` as the current one or the specified ones in the
    73|         0|            0|            0|  0.00%|        arguments.
    74|         0|            0|            0|  0.00%|
    75|         0|            0|            0|  0.00%|        Args:
    76|         0|            0|            0|  0.00%|            shape : (tuple): the shape for the materialized tensor.
    77|         0|            0|            0|  0.00%|            device (:class:`torch.device`): the desired device of the parameters
    78|         0|            0|            0|  0.00%|                and buffers in this module. Optional.
    79|         0|            0|            0|  0.00%|            dtype (:class:`torch.dtype`): the desired floating point type of
    80|         0|            0|            0|  0.00%|                the floating point parameters and buffers in this module. Optional.
    81|         0|            0|            0|  0.00%|        """
    82|         0|            0|            0|  0.00%|        if device is None:
    83|         0|            0|            0|  0.00%|            device = self.data.device
    84|         0|            0|            0|  0.00%|        if dtype is None:
    85|         0|            0|            0|  0.00%|            dtype = self.data.dtype
    86|         0|            0|            0|  0.00%|        self.data = torch.empty(shape, device=device, dtype=dtype)
    87|         0|            0|            0|  0.00%|        self.__class__ = self.cls_to_become
    88|         0|            0|            0|  0.00%|
    89|         0|            0|            0|  0.00%|    @property
    90|         0|            0|            0|  0.00%|    def shape(self):
    91|         0|            0|            0|  0.00%|        raise RuntimeError(
    92|         0|            0|            0|  0.00%|            'Can\'t access the shape of an uninitialized parameter or buffer. '
    93|         0|            0|            0|  0.00%|            'This error usually happens in `load_state_dict` when trying to load '
    94|         0|            0|            0|  0.00%|            'an uninitialized parameter into an initialized one. '
    95|         0|            0|            0|  0.00%|            'Call `forward` to initialize the parameters before accessing their attributes.')
    96|         0|            0|            0|  0.00%|
    97|         0|            0|            0|  0.00%|    def share_memory_(self):
    98|         0|            0|            0|  0.00%|        raise RuntimeError(
    99|         0|            0|            0|  0.00%|            'Can\'t share memory on an uninitialized parameter or buffer. '
   100|         0|            0|            0|  0.00%|            'Call `forward` to initialize the parameters before calling '
   101|         0|            0|            0|  0.00%|            '`module.share_memory()`.')
   102|         0|            0|            0|  0.00%|
   103|         0|            0|            0|  0.00%|    def __repr__(self):
   104|         0|            0|            0|  0.00%|        return f'<{self.__class__.__name__}>'
   105|         0|            0|            0|  0.00%|
   106|         0|            0|            0|  0.00%|    def __reduce_ex__(self, proto):
   107|         0|            0|            0|  0.00%|        # See Note [Don't serialize hooks]
   108|         0|            0|            0|  0.00%|        return (
   109|         0|            0|            0|  0.00%|            self.__class__,
   110|         0|            0|            0|  0.00%|            (self.requires_grad,)
   111|         0|            0|            0|  0.00%|        )
   112|         0|            0|            0|  0.00%|
   113|         0|            0|            0|  0.00%|    @classmethod
   114|         0|            0|            0|  0.00%|    def __torch_function__(cls, func, types, args=(), kwargs=None):
   115|         0|            0|            0|  0.00%|        # method-wrapper is to detect access to Tensor properties that are
   116|         0|            0|            0|  0.00%|        # wrapped in descriptors
   117|         0|            0|            0|  0.00%|        if func in cls._allowed_methods or func.__class__.__name__ == 'method-wrapper':
   118|         0|            0|            0|  0.00%|            if kwargs is None:
   119|         0|            0|            0|  0.00%|                kwargs = {}
   120|         0|            0|            0|  0.00%|            return super().__torch_function__(func, types, args, kwargs)
   121|         0|            0|            0|  0.00%|        raise ValueError(
   122|         0|            0|            0|  0.00%|            'Attempted to use an uninitialized parameter in {}. '
   123|         0|            0|            0|  0.00%|            'This error happens when you are using a `LazyModule` or '
   124|         0|            0|            0|  0.00%|            'explicitly manipulating `torch.nn.parameter.{}` '
   125|         0|            0|            0|  0.00%|            'objects. When using LazyModules Call `forward` with a dummy batch '
   126|         0|            0|            0|  0.00%|            'to initialize the parameters before calling torch functions'.format(func, cls.__name__))
   127|         0|            0|            0|  0.00%|
   128|         0|            0|            0|  0.00%|
   129|       122|  0.000605106|  4.95989e-06|  0.00%|def is_lazy(param):
   130|       122|   0.00080657|  6.61123e-06|  0.00%|    return isinstance(param, UninitializedTensorMixin)
   131|         0|            0|            0|  0.00%|
   132|         0|            0|            0|  0.00%|
   133|         0|            0|            0|  0.00%|class UninitializedParameter(UninitializedTensorMixin, Parameter):
   134|         0|            0|            0|  0.00%|    r"""A parameter that is not initialized.
   135|         0|            0|            0|  0.00%|
   136|         0|            0|            0|  0.00%|    Unitialized Parameters are a a special case of :class:`torch.nn.Parameter`
   137|         0|            0|            0|  0.00%|    where the shape of the data is still unknown.
   138|         0|            0|            0|  0.00%|
   139|         0|            0|            0|  0.00%|    Unlike a :class:`torch.nn.Parameter`, uninitialized parameters
   140|         0|            0|            0|  0.00%|    hold no data and attempting to access some properties, like their shape,
   141|         0|            0|            0|  0.00%|    will throw a runtime error. The only operations that can be performed on a uninitialized
   142|         0|            0|            0|  0.00%|    parameter are changing its datatype, moving it to a different device and
   143|         0|            0|            0|  0.00%|    converting it to a regular :class:`torch.nn.Parameter`.
   144|         0|            0|            0|  0.00%|
   145|         0|            0|            0|  0.00%|    The default device or dtype to use when the parameter is materialized can be set
   146|         0|            0|            0|  0.00%|    during construction using e.g. ``device='cuda'``.
   147|         0|            0|            0|  0.00%|    """
   148|         0|            0|            0|  0.00%|
   149|         0|            0|            0|  0.00%|    cls_to_become = Parameter
   150|         0|            0|            0|  0.00%|
   151|         0|            0|            0|  0.00%|    def __new__(cls, requires_grad=True, device=None, dtype=None) -> None:
   152|         0|            0|            0|  0.00%|        factory_kwargs = {'device': device, 'dtype': dtype}
   153|         0|            0|            0|  0.00%|        data = torch.tensor([], **factory_kwargs)
   154|         0|            0|            0|  0.00%|        return torch.Tensor._make_subclass(cls, data, requires_grad)
   155|         0|            0|            0|  0.00%|
   156|         0|            0|            0|  0.00%|
   157|         0|            0|            0|  0.00%|class UninitializedBuffer(UninitializedTensorMixin, torch.Tensor):
   158|         0|            0|            0|  0.00%|    r"""A buffer that is not initialized.
   159|         0|            0|            0|  0.00%|
   160|         0|            0|            0|  0.00%|    Unitialized Buffer is a a special case of :class:`torch.Tensor`
   161|         0|            0|            0|  0.00%|    where the shape of the data is still unknown.
   162|         0|            0|            0|  0.00%|
   163|         0|            0|            0|  0.00%|    Unlike a :class:`torch.Tensor`, uninitialized parameters
   164|         0|            0|            0|  0.00%|    hold no data and attempting to access some properties, like their shape,
   165|         0|            0|            0|  0.00%|    will throw a runtime error. The only operations that can be performed on a uninitialized
   166|         0|            0|            0|  0.00%|    parameter are changing its datatype, moving it to a different device and
   167|         0|            0|            0|  0.00%|    converting it to a regular :class:`torch.Tensor`.
   168|         0|            0|            0|  0.00%|
   169|         0|            0|            0|  0.00%|    The default device or dtype to use when the buffer is materialized can be set
   170|         0|            0|            0|  0.00%|    during construction using e.g. ``device='cuda'``.
   171|         0|            0|            0|  0.00%|    """
   172|         0|            0|            0|  0.00%|
   173|         0|            0|            0|  0.00%|    cls_to_become = torch.Tensor
   174|         0|            0|            0|  0.00%|
   175|         0|            0|            0|  0.00%|    def __new__(cls, requires_grad=False, device=None, dtype=None) -> None:
   176|         0|            0|            0|  0.00%|        factory_kwargs = {'device': device, 'dtype': dtype}
   177|         0|            0|            0|  0.00%|        data = torch.tensor([], **factory_kwargs)
   178|         0|            0|            0|  0.00%|        return torch.Tensor._make_subclass(cls, data, requires_grad)
File: /opt/conda/lib/python3.8/codecs.py
File duration: 0.00103211s (0.00%)
Line #|      Hits|         Time| Time per hit|      %|Source code
------+----------+-------------+-------------+-------+-----------
     1|         0|            0|            0|  0.00%|""" codecs -- Python Codec Registry, API and helpers.
     2|         0|            0|            0|  0.00%|
     3|         0|            0|            0|  0.00%|
     4|         0|            0|            0|  0.00%|Written by Marc-Andre Lemburg (mal@lemburg.com).
     5|         0|            0|            0|  0.00%|
     6|         0|            0|            0|  0.00%|(c) Copyright CNRI, All Rights Reserved. NO WARRANTY.
     7|         0|            0|            0|  0.00%|
     8|         0|            0|            0|  0.00%|"""
     9|         0|            0|            0|  0.00%|
    10|         0|            0|            0|  0.00%|import builtins
    11|         0|            0|            0|  0.00%|import sys
    12|         0|            0|            0|  0.00%|
    13|         0|            0|            0|  0.00%|### Registry and builtin stateless codec functions
    14|         0|            0|            0|  0.00%|
    15|         0|            0|            0|  0.00%|try:
    16|         0|            0|            0|  0.00%|    from _codecs import *
    17|         0|            0|            0|  0.00%|except ImportError as why:
    18|         0|            0|            0|  0.00%|    raise SystemError('Failed to load the builtin codecs: %s' % why)
    19|         0|            0|            0|  0.00%|
    20|         0|            0|            0|  0.00%|__all__ = ["register", "lookup", "open", "EncodedFile", "BOM", "BOM_BE",
    21|         0|            0|            0|  0.00%|           "BOM_LE", "BOM32_BE", "BOM32_LE", "BOM64_BE", "BOM64_LE",
    22|         0|            0|            0|  0.00%|           "BOM_UTF8", "BOM_UTF16", "BOM_UTF16_LE", "BOM_UTF16_BE",
    23|         0|            0|            0|  0.00%|           "BOM_UTF32", "BOM_UTF32_LE", "BOM_UTF32_BE",
    24|         0|            0|            0|  0.00%|           "CodecInfo", "Codec", "IncrementalEncoder", "IncrementalDecoder",
    25|         0|            0|            0|  0.00%|           "StreamReader", "StreamWriter",
    26|         0|            0|            0|  0.00%|           "StreamReaderWriter", "StreamRecoder",
    27|         0|            0|            0|  0.00%|           "getencoder", "getdecoder", "getincrementalencoder",
    28|         0|            0|            0|  0.00%|           "getincrementaldecoder", "getreader", "getwriter",
    29|         0|            0|            0|  0.00%|           "encode", "decode", "iterencode", "iterdecode",
    30|         0|            0|            0|  0.00%|           "strict_errors", "ignore_errors", "replace_errors",
    31|         0|            0|            0|  0.00%|           "xmlcharrefreplace_errors",
    32|         0|            0|            0|  0.00%|           "backslashreplace_errors", "namereplace_errors",
    33|         0|            0|            0|  0.00%|           "register_error", "lookup_error"]
    34|         0|            0|            0|  0.00%|
    35|         0|            0|            0|  0.00%|### Constants
    36|         0|            0|            0|  0.00%|
    37|         0|            0|            0|  0.00%|#
    38|         0|            0|            0|  0.00%|# Byte Order Mark (BOM = ZERO WIDTH NO-BREAK SPACE = U+FEFF)
    39|         0|            0|            0|  0.00%|# and its possible byte string values
    40|         0|            0|            0|  0.00%|# for UTF8/UTF16/UTF32 output and little/big endian machines
    41|         0|            0|            0|  0.00%|#
    42|         0|            0|            0|  0.00%|
    43|         0|            0|            0|  0.00%|# UTF-8
    44|         0|            0|            0|  0.00%|BOM_UTF8 = b'\xef\xbb\xbf'
    45|         0|            0|            0|  0.00%|
    46|         0|            0|            0|  0.00%|# UTF-16, little endian
    47|         0|            0|            0|  0.00%|BOM_LE = BOM_UTF16_LE = b'\xff\xfe'
    48|         0|            0|            0|  0.00%|
    49|         0|            0|            0|  0.00%|# UTF-16, big endian
    50|         0|            0|            0|  0.00%|BOM_BE = BOM_UTF16_BE = b'\xfe\xff'
    51|         0|            0|            0|  0.00%|
    52|         0|            0|            0|  0.00%|# UTF-32, little endian
    53|         0|            0|            0|  0.00%|BOM_UTF32_LE = b'\xff\xfe\x00\x00'
    54|         0|            0|            0|  0.00%|
    55|         0|            0|            0|  0.00%|# UTF-32, big endian
    56|         0|            0|            0|  0.00%|BOM_UTF32_BE = b'\x00\x00\xfe\xff'
    57|         0|            0|            0|  0.00%|
    58|         0|            0|            0|  0.00%|if sys.byteorder == 'little':
    59|         0|            0|            0|  0.00%|
    60|         0|            0|            0|  0.00%|    # UTF-16, native endianness
    61|         0|            0|            0|  0.00%|    BOM = BOM_UTF16 = BOM_UTF16_LE
    62|         0|            0|            0|  0.00%|
    63|         0|            0|            0|  0.00%|    # UTF-32, native endianness
    64|         0|            0|            0|  0.00%|    BOM_UTF32 = BOM_UTF32_LE
    65|         0|            0|            0|  0.00%|
    66|         0|            0|            0|  0.00%|else:
    67|         0|            0|            0|  0.00%|
    68|         0|            0|            0|  0.00%|    # UTF-16, native endianness
    69|         0|            0|            0|  0.00%|    BOM = BOM_UTF16 = BOM_UTF16_BE
    70|         0|            0|            0|  0.00%|
    71|         0|            0|            0|  0.00%|    # UTF-32, native endianness
    72|         0|            0|            0|  0.00%|    BOM_UTF32 = BOM_UTF32_BE
    73|         0|            0|            0|  0.00%|
    74|         0|            0|            0|  0.00%|# Old broken names (don't use in new code)
    75|         0|            0|            0|  0.00%|BOM32_LE = BOM_UTF16_LE
    76|         0|            0|            0|  0.00%|BOM32_BE = BOM_UTF16_BE
    77|         0|            0|            0|  0.00%|BOM64_LE = BOM_UTF32_LE
    78|         0|            0|            0|  0.00%|BOM64_BE = BOM_UTF32_BE
    79|         0|            0|            0|  0.00%|
    80|         0|            0|            0|  0.00%|
    81|         0|            0|            0|  0.00%|### Codec base classes (defining the API)
    82|         0|            0|            0|  0.00%|
    83|         0|            0|            0|  0.00%|class CodecInfo(tuple):
    84|         0|            0|            0|  0.00%|    """Codec details when looking up the codec registry"""
    85|         0|            0|            0|  0.00%|
    86|         0|            0|            0|  0.00%|    # Private API to allow Python 3.4 to blacklist the known non-Unicode
    87|         0|            0|            0|  0.00%|    # codecs in the standard library. A more general mechanism to
    88|         0|            0|            0|  0.00%|    # reliably distinguish test encodings from other codecs will hopefully
    89|         0|            0|            0|  0.00%|    # be defined for Python 3.5
    90|         0|            0|            0|  0.00%|    #
    91|         0|            0|            0|  0.00%|    # See http://bugs.python.org/issue19619
    92|         0|            0|            0|  0.00%|    _is_text_encoding = True # Assume codecs are text encodings by default
    93|         0|            0|            0|  0.00%|
    94|         0|            0|            0|  0.00%|    def __new__(cls, encode, decode, streamreader=None, streamwriter=None,
    95|         0|            0|            0|  0.00%|        incrementalencoder=None, incrementaldecoder=None, name=None,
    96|         0|            0|            0|  0.00%|        *, _is_text_encoding=None):
    97|         0|            0|            0|  0.00%|        self = tuple.__new__(cls, (encode, decode, streamreader, streamwriter))
    98|         0|            0|            0|  0.00%|        self.name = name
    99|         0|            0|            0|  0.00%|        self.encode = encode
   100|         0|            0|            0|  0.00%|        self.decode = decode
   101|         0|            0|            0|  0.00%|        self.incrementalencoder = incrementalencoder
   102|         0|            0|            0|  0.00%|        self.incrementaldecoder = incrementaldecoder
   103|         0|            0|            0|  0.00%|        self.streamwriter = streamwriter
   104|         0|            0|            0|  0.00%|        self.streamreader = streamreader
   105|         0|            0|            0|  0.00%|        if _is_text_encoding is not None:
   106|         0|            0|            0|  0.00%|            self._is_text_encoding = _is_text_encoding
   107|         0|            0|            0|  0.00%|        return self
   108|         0|            0|            0|  0.00%|
   109|         0|            0|            0|  0.00%|    def __repr__(self):
   110|         0|            0|            0|  0.00%|        return "<%s.%s object for encoding %s at %#x>" % \
   111|         0|            0|            0|  0.00%|                (self.__class__.__module__, self.__class__.__qualname__,
   112|         0|            0|            0|  0.00%|                 self.name, id(self))
   113|         0|            0|            0|  0.00%|
   114|         0|            0|            0|  0.00%|class Codec:
   115|         0|            0|            0|  0.00%|
   116|         0|            0|            0|  0.00%|    """ Defines the interface for stateless encoders/decoders.
   117|         0|            0|            0|  0.00%|
   118|         0|            0|            0|  0.00%|        The .encode()/.decode() methods may use different error
   119|         0|            0|            0|  0.00%|        handling schemes by providing the errors argument. These
   120|         0|            0|            0|  0.00%|        string values are predefined:
   121|         0|            0|            0|  0.00%|
   122|         0|            0|            0|  0.00%|         'strict' - raise a ValueError error (or a subclass)
   123|         0|            0|            0|  0.00%|         'ignore' - ignore the character and continue with the next
   124|         0|            0|            0|  0.00%|         'replace' - replace with a suitable replacement character;
   125|         0|            0|            0|  0.00%|                    Python will use the official U+FFFD REPLACEMENT
   126|         0|            0|            0|  0.00%|                    CHARACTER for the builtin Unicode codecs on
   127|         0|            0|            0|  0.00%|                    decoding and '?' on encoding.
   128|         0|            0|            0|  0.00%|         'surrogateescape' - replace with private code points U+DCnn.
   129|         0|            0|            0|  0.00%|         'xmlcharrefreplace' - Replace with the appropriate XML
   130|         0|            0|            0|  0.00%|                               character reference (only for encoding).
   131|         0|            0|            0|  0.00%|         'backslashreplace'  - Replace with backslashed escape sequences.
   132|         0|            0|            0|  0.00%|         'namereplace'       - Replace with \\N{...} escape sequences
   133|         0|            0|            0|  0.00%|                               (only for encoding).
   134|         0|            0|            0|  0.00%|
   135|         0|            0|            0|  0.00%|        The set of allowed values can be extended via register_error.
   136|         0|            0|            0|  0.00%|
   137|         0|            0|            0|  0.00%|    """
   138|         0|            0|            0|  0.00%|    def encode(self, input, errors='strict'):
   139|         0|            0|            0|  0.00%|
   140|         0|            0|            0|  0.00%|        """ Encodes the object input and returns a tuple (output
   141|         0|            0|            0|  0.00%|            object, length consumed).
   142|         0|            0|            0|  0.00%|
   143|         0|            0|            0|  0.00%|            errors defines the error handling to apply. It defaults to
   144|         0|            0|            0|  0.00%|            'strict' handling.
   145|         0|            0|            0|  0.00%|
   146|         0|            0|            0|  0.00%|            The method may not store state in the Codec instance. Use
   147|         0|            0|            0|  0.00%|            StreamWriter for codecs which have to keep state in order to
   148|         0|            0|            0|  0.00%|            make encoding efficient.
   149|         0|            0|            0|  0.00%|
   150|         0|            0|            0|  0.00%|            The encoder must be able to handle zero length input and
   151|         0|            0|            0|  0.00%|            return an empty object of the output object type in this
   152|         0|            0|            0|  0.00%|            situation.
   153|         0|            0|            0|  0.00%|
   154|         0|            0|            0|  0.00%|        """
   155|         0|            0|            0|  0.00%|        raise NotImplementedError
   156|         0|            0|            0|  0.00%|
   157|         0|            0|            0|  0.00%|    def decode(self, input, errors='strict'):
   158|         0|            0|            0|  0.00%|
   159|         0|            0|            0|  0.00%|        """ Decodes the object input and returns a tuple (output
   160|         0|            0|            0|  0.00%|            object, length consumed).
   161|         0|            0|            0|  0.00%|
   162|         0|            0|            0|  0.00%|            input must be an object which provides the bf_getreadbuf
   163|         0|            0|            0|  0.00%|            buffer slot. Python strings, buffer objects and memory
   164|         0|            0|            0|  0.00%|            mapped files are examples of objects providing this slot.
   165|         0|            0|            0|  0.00%|
   166|         0|            0|            0|  0.00%|            errors defines the error handling to apply. It defaults to
   167|         0|            0|            0|  0.00%|            'strict' handling.
   168|         0|            0|            0|  0.00%|
   169|         0|            0|            0|  0.00%|            The method may not store state in the Codec instance. Use
   170|         0|            0|            0|  0.00%|            StreamReader for codecs which have to keep state in order to
   171|         0|            0|            0|  0.00%|            make decoding efficient.
   172|         0|            0|            0|  0.00%|
   173|         0|            0|            0|  0.00%|            The decoder must be able to handle zero length input and
   174|         0|            0|            0|  0.00%|            return an empty object of the output object type in this
   175|         0|            0|            0|  0.00%|            situation.
   176|         0|            0|            0|  0.00%|
   177|         0|            0|            0|  0.00%|        """
   178|         0|            0|            0|  0.00%|        raise NotImplementedError
   179|         0|            0|            0|  0.00%|
   180|         0|            0|            0|  0.00%|class IncrementalEncoder(object):
   181|         0|            0|            0|  0.00%|    """
   182|         0|            0|            0|  0.00%|    An IncrementalEncoder encodes an input in multiple steps. The input can
   183|         0|            0|            0|  0.00%|    be passed piece by piece to the encode() method. The IncrementalEncoder
   184|         0|            0|            0|  0.00%|    remembers the state of the encoding process between calls to encode().
   185|         0|            0|            0|  0.00%|    """
   186|         0|            0|            0|  0.00%|    def __init__(self, errors='strict'):
   187|         0|            0|            0|  0.00%|        """
   188|         0|            0|            0|  0.00%|        Creates an IncrementalEncoder instance.
   189|         0|            0|            0|  0.00%|
   190|         0|            0|            0|  0.00%|        The IncrementalEncoder may use different error handling schemes by
   191|         0|            0|            0|  0.00%|        providing the errors keyword argument. See the module docstring
   192|         0|            0|            0|  0.00%|        for a list of possible values.
   193|         0|            0|            0|  0.00%|        """
   194|         0|            0|            0|  0.00%|        self.errors = errors
   195|         0|            0|            0|  0.00%|        self.buffer = ""
   196|         0|            0|            0|  0.00%|
   197|         0|            0|            0|  0.00%|    def encode(self, input, final=False):
   198|         0|            0|            0|  0.00%|        """
   199|         0|            0|            0|  0.00%|        Encodes input and returns the resulting object.
   200|         0|            0|            0|  0.00%|        """
   201|         0|            0|            0|  0.00%|        raise NotImplementedError
   202|         0|            0|            0|  0.00%|
   203|         0|            0|            0|  0.00%|    def reset(self):
   204|         0|            0|            0|  0.00%|        """
   205|         0|            0|            0|  0.00%|        Resets the encoder to the initial state.
   206|         0|            0|            0|  0.00%|        """
   207|         0|            0|            0|  0.00%|
   208|         0|            0|            0|  0.00%|    def getstate(self):
   209|         0|            0|            0|  0.00%|        """
   210|         0|            0|            0|  0.00%|        Return the current state of the encoder.
   211|         0|            0|            0|  0.00%|        """
   212|         0|            0|            0|  0.00%|        return 0
   213|         0|            0|            0|  0.00%|
   214|         0|            0|            0|  0.00%|    def setstate(self, state):
   215|         0|            0|            0|  0.00%|        """
   216|         0|            0|            0|  0.00%|        Set the current state of the encoder. state must have been
   217|         0|            0|            0|  0.00%|        returned by getstate().
   218|         0|            0|            0|  0.00%|        """
   219|         0|            0|            0|  0.00%|
   220|         0|            0|            0|  0.00%|class BufferedIncrementalEncoder(IncrementalEncoder):
   221|         0|            0|            0|  0.00%|    """
   222|         0|            0|            0|  0.00%|    This subclass of IncrementalEncoder can be used as the baseclass for an
   223|         0|            0|            0|  0.00%|    incremental encoder if the encoder must keep some of the output in a
   224|         0|            0|            0|  0.00%|    buffer between calls to encode().
   225|         0|            0|            0|  0.00%|    """
   226|         0|            0|            0|  0.00%|    def __init__(self, errors='strict'):
   227|         0|            0|            0|  0.00%|        IncrementalEncoder.__init__(self, errors)
   228|         0|            0|            0|  0.00%|        # unencoded input that is kept between calls to encode()
   229|         0|            0|            0|  0.00%|        self.buffer = ""
   230|         0|            0|            0|  0.00%|
   231|         0|            0|            0|  0.00%|    def _buffer_encode(self, input, errors, final):
   232|         0|            0|            0|  0.00%|        # Overwrite this method in subclasses: It must encode input
   233|         0|            0|            0|  0.00%|        # and return an (output, length consumed) tuple
   234|         0|            0|            0|  0.00%|        raise NotImplementedError
   235|         0|            0|            0|  0.00%|
   236|         0|            0|            0|  0.00%|    def encode(self, input, final=False):
   237|         0|            0|            0|  0.00%|        # encode input (taking the buffer into account)
   238|         0|            0|            0|  0.00%|        data = self.buffer + input
   239|         0|            0|            0|  0.00%|        (result, consumed) = self._buffer_encode(data, self.errors, final)
   240|         0|            0|            0|  0.00%|        # keep unencoded input until the next call
   241|         0|            0|            0|  0.00%|        self.buffer = data[consumed:]
   242|         0|            0|            0|  0.00%|        return result
   243|         0|            0|            0|  0.00%|
   244|         0|            0|            0|  0.00%|    def reset(self):
   245|         0|            0|            0|  0.00%|        IncrementalEncoder.reset(self)
   246|         0|            0|            0|  0.00%|        self.buffer = ""
   247|         0|            0|            0|  0.00%|
   248|         0|            0|            0|  0.00%|    def getstate(self):
   249|         0|            0|            0|  0.00%|        return self.buffer or 0
   250|         0|            0|            0|  0.00%|
   251|         0|            0|            0|  0.00%|    def setstate(self, state):
   252|         0|            0|            0|  0.00%|        self.buffer = state or ""
   253|         0|            0|            0|  0.00%|
   254|         0|            0|            0|  0.00%|class IncrementalDecoder(object):
   255|         0|            0|            0|  0.00%|    """
   256|         0|            0|            0|  0.00%|    An IncrementalDecoder decodes an input in multiple steps. The input can
   257|         0|            0|            0|  0.00%|    be passed piece by piece to the decode() method. The IncrementalDecoder
   258|         0|            0|            0|  0.00%|    remembers the state of the decoding process between calls to decode().
   259|         0|            0|            0|  0.00%|    """
   260|         1|  1.40667e-05|  1.40667e-05|  0.00%|    def __init__(self, errors='strict'):
   261|         0|            0|            0|  0.00%|        """
   262|         0|            0|            0|  0.00%|        Create an IncrementalDecoder instance.
   263|         0|            0|            0|  0.00%|
   264|         0|            0|            0|  0.00%|        The IncrementalDecoder may use different error handling schemes by
   265|         0|            0|            0|  0.00%|        providing the errors keyword argument. See the module docstring
   266|         0|            0|            0|  0.00%|        for a list of possible values.
   267|         0|            0|            0|  0.00%|        """
   268|         1|   1.5974e-05|   1.5974e-05|  0.00%|        self.errors = errors
   269|         0|            0|            0|  0.00%|
   270|         0|            0|            0|  0.00%|    def decode(self, input, final=False):
   271|         0|            0|            0|  0.00%|        """
   272|         0|            0|            0|  0.00%|        Decode input and returns the resulting object.
   273|         0|            0|            0|  0.00%|        """
   274|         0|            0|            0|  0.00%|        raise NotImplementedError
   275|         0|            0|            0|  0.00%|
   276|         0|            0|            0|  0.00%|    def reset(self):
   277|         0|            0|            0|  0.00%|        """
   278|         0|            0|            0|  0.00%|        Reset the decoder to the initial state.
   279|         0|            0|            0|  0.00%|        """
   280|         0|            0|            0|  0.00%|
   281|         0|            0|            0|  0.00%|    def getstate(self):
   282|         0|            0|            0|  0.00%|        """
   283|         0|            0|            0|  0.00%|        Return the current state of the decoder.
   284|         0|            0|            0|  0.00%|
   285|         0|            0|            0|  0.00%|        This must be a (buffered_input, additional_state_info) tuple.
   286|         0|            0|            0|  0.00%|        buffered_input must be a bytes object containing bytes that
   287|         0|            0|            0|  0.00%|        were passed to decode() that have not yet been converted.
   288|         0|            0|            0|  0.00%|        additional_state_info must be a non-negative integer
   289|         0|            0|            0|  0.00%|        representing the state of the decoder WITHOUT yet having
   290|         0|            0|            0|  0.00%|        processed the contents of buffered_input.  In the initial state
   291|         0|            0|            0|  0.00%|        and after reset(), getstate() must return (b"", 0).
   292|         0|            0|            0|  0.00%|        """
   293|         0|            0|            0|  0.00%|        return (b"", 0)
   294|         0|            0|            0|  0.00%|
   295|         0|            0|            0|  0.00%|    def setstate(self, state):
   296|         0|            0|            0|  0.00%|        """
   297|         0|            0|            0|  0.00%|        Set the current state of the decoder.
   298|         0|            0|            0|  0.00%|
   299|         0|            0|            0|  0.00%|        state must have been returned by getstate().  The effect of
   300|         0|            0|            0|  0.00%|        setstate((b"", 0)) must be equivalent to reset().
   301|         0|            0|            0|  0.00%|        """
   302|         0|            0|            0|  0.00%|
   303|         0|            0|            0|  0.00%|class BufferedIncrementalDecoder(IncrementalDecoder):
   304|         0|            0|            0|  0.00%|    """
   305|         0|            0|            0|  0.00%|    This subclass of IncrementalDecoder can be used as the baseclass for an
   306|         0|            0|            0|  0.00%|    incremental decoder if the decoder must be able to handle incomplete
   307|         0|            0|            0|  0.00%|    byte sequences.
   308|         0|            0|            0|  0.00%|    """
   309|         1|   1.5974e-05|   1.5974e-05|  0.00%|    def __init__(self, errors='strict'):
   310|         1|  5.48363e-05|  5.48363e-05|  0.00%|        IncrementalDecoder.__init__(self, errors)
(call)|         1|  3.00407e-05|  3.00407e-05|  0.00%|# /opt/conda/lib/python3.8/codecs.py:260 __init__
   311|         0|            0|            0|  0.00%|        # undecoded input that is kept between calls to decode()
   312|         1|  7.15256e-06|  7.15256e-06|  0.00%|        self.buffer = b""
   313|         0|            0|            0|  0.00%|
   314|         0|            0|            0|  0.00%|    def _buffer_decode(self, input, errors, final):
   315|         0|            0|            0|  0.00%|        # Overwrite this method in subclasses: It must decode input
   316|         0|            0|            0|  0.00%|        # and return an (output, length consumed) tuple
   317|         0|            0|            0|  0.00%|        raise NotImplementedError
   318|         0|            0|            0|  0.00%|
   319|        27|  0.000189781|  7.02893e-06|  0.00%|    def decode(self, input, final=False):
   320|         0|            0|            0|  0.00%|        # decode input (taking the buffer into account)
   321|        27|   0.00014782|   5.4748e-06|  0.00%|        data = self.buffer + input
   322|        27|   0.00033617|  1.24507e-05|  0.00%|        (result, consumed) = self._buffer_decode(data, self.errors, final)
   323|         0|            0|            0|  0.00%|        # keep undecoded input until the next call
   324|        27|  0.000142097|  5.26287e-06|  0.00%|        self.buffer = data[consumed:]
   325|        27|  0.000108242|  4.00896e-06|  0.00%|        return result
   326|         0|            0|            0|  0.00%|
   327|         0|            0|            0|  0.00%|    def reset(self):
   328|         0|            0|            0|  0.00%|        IncrementalDecoder.reset(self)
   329|         0|            0|            0|  0.00%|        self.buffer = b""
   330|         0|            0|            0|  0.00%|
   331|         0|            0|            0|  0.00%|    def getstate(self):
   332|         0|            0|            0|  0.00%|        # additional state info is always 0
   333|         0|            0|            0|  0.00%|        return (self.buffer, 0)
   334|         0|            0|            0|  0.00%|
   335|         0|            0|            0|  0.00%|    def setstate(self, state):
   336|         0|            0|            0|  0.00%|        # ignore additional state info
   337|         0|            0|            0|  0.00%|        self.buffer = state[0]
   338|         0|            0|            0|  0.00%|
   339|         0|            0|            0|  0.00%|#
   340|         0|            0|            0|  0.00%|# The StreamWriter and StreamReader class provide generic working
   341|         0|            0|            0|  0.00%|# interfaces which can be used to implement new encoding submodules
   342|         0|            0|            0|  0.00%|# very easily. See encodings/utf_8.py for an example on how this is
   343|         0|            0|            0|  0.00%|# done.
   344|         0|            0|            0|  0.00%|#
   345|         0|            0|            0|  0.00%|
   346|         0|            0|            0|  0.00%|class StreamWriter(Codec):
   347|         0|            0|            0|  0.00%|
   348|         0|            0|            0|  0.00%|    def __init__(self, stream, errors='strict'):
   349|         0|            0|            0|  0.00%|
   350|         0|            0|            0|  0.00%|        """ Creates a StreamWriter instance.
   351|         0|            0|            0|  0.00%|
   352|         0|            0|            0|  0.00%|            stream must be a file-like object open for writing.
   353|         0|            0|            0|  0.00%|
   354|         0|            0|            0|  0.00%|            The StreamWriter may use different error handling
   355|         0|            0|            0|  0.00%|            schemes by providing the errors keyword argument. These
   356|         0|            0|            0|  0.00%|            parameters are predefined:
   357|         0|            0|            0|  0.00%|
   358|         0|            0|            0|  0.00%|             'strict' - raise a ValueError (or a subclass)
   359|         0|            0|            0|  0.00%|             'ignore' - ignore the character and continue with the next
   360|         0|            0|            0|  0.00%|             'replace'- replace with a suitable replacement character
   361|         0|            0|            0|  0.00%|             'xmlcharrefreplace' - Replace with the appropriate XML
   362|         0|            0|            0|  0.00%|                                   character reference.
   363|         0|            0|            0|  0.00%|             'backslashreplace'  - Replace with backslashed escape
   364|         0|            0|            0|  0.00%|                                   sequences.
   365|         0|            0|            0|  0.00%|             'namereplace'       - Replace with \\N{...} escape sequences.
   366|         0|            0|            0|  0.00%|
   367|         0|            0|            0|  0.00%|            The set of allowed parameter values can be extended via
   368|         0|            0|            0|  0.00%|            register_error.
   369|         0|            0|            0|  0.00%|        """
   370|         0|            0|            0|  0.00%|        self.stream = stream
   371|         0|            0|            0|  0.00%|        self.errors = errors
   372|         0|            0|            0|  0.00%|
   373|         0|            0|            0|  0.00%|    def write(self, object):
   374|         0|            0|            0|  0.00%|
   375|         0|            0|            0|  0.00%|        """ Writes the object's contents encoded to self.stream.
   376|         0|            0|            0|  0.00%|        """
   377|         0|            0|            0|  0.00%|        data, consumed = self.encode(object, self.errors)
   378|         0|            0|            0|  0.00%|        self.stream.write(data)
   379|         0|            0|            0|  0.00%|
   380|         0|            0|            0|  0.00%|    def writelines(self, list):
   381|         0|            0|            0|  0.00%|
   382|         0|            0|            0|  0.00%|        """ Writes the concatenated list of strings to the stream
   383|         0|            0|            0|  0.00%|            using .write().
   384|         0|            0|            0|  0.00%|        """
   385|         0|            0|            0|  0.00%|        self.write(''.join(list))
   386|         0|            0|            0|  0.00%|
   387|         0|            0|            0|  0.00%|    def reset(self):
   388|         0|            0|            0|  0.00%|
   389|         0|            0|            0|  0.00%|        """ Flushes and resets the codec buffers used for keeping state.
   390|         0|            0|            0|  0.00%|
   391|         0|            0|            0|  0.00%|            Calling this method should ensure that the data on the
   392|         0|            0|            0|  0.00%|            output is put into a clean state, that allows appending
   393|         0|            0|            0|  0.00%|            of new fresh data without having to rescan the whole
   394|         0|            0|            0|  0.00%|            stream to recover state.
   395|         0|            0|            0|  0.00%|
   396|         0|            0|            0|  0.00%|        """
   397|         0|            0|            0|  0.00%|        pass
   398|         0|            0|            0|  0.00%|
   399|         0|            0|            0|  0.00%|    def seek(self, offset, whence=0):
   400|         0|            0|            0|  0.00%|        self.stream.seek(offset, whence)
   401|         0|            0|            0|  0.00%|        if whence == 0 and offset == 0:
   402|         0|            0|            0|  0.00%|            self.reset()
   403|         0|            0|            0|  0.00%|
   404|         0|            0|            0|  0.00%|    def __getattr__(self, name,
   405|         0|            0|            0|  0.00%|                    getattr=getattr):
   406|         0|            0|            0|  0.00%|
   407|         0|            0|            0|  0.00%|        """ Inherit all other methods from the underlying stream.
   408|         0|            0|            0|  0.00%|        """
   409|         0|            0|            0|  0.00%|        return getattr(self.stream, name)
   410|         0|            0|            0|  0.00%|
   411|         0|            0|            0|  0.00%|    def __enter__(self):
   412|         0|            0|            0|  0.00%|        return self
   413|         0|            0|            0|  0.00%|
   414|         0|            0|            0|  0.00%|    def __exit__(self, type, value, tb):
   415|         0|            0|            0|  0.00%|        self.stream.close()
   416|         0|            0|            0|  0.00%|
   417|         0|            0|            0|  0.00%|###
   418|         0|            0|            0|  0.00%|
   419|         0|            0|            0|  0.00%|class StreamReader(Codec):
   420|         0|            0|            0|  0.00%|
   421|         0|            0|            0|  0.00%|    charbuffertype = str
   422|         0|            0|            0|  0.00%|
   423|         0|            0|            0|  0.00%|    def __init__(self, stream, errors='strict'):
   424|         0|            0|            0|  0.00%|
   425|         0|            0|            0|  0.00%|        """ Creates a StreamReader instance.
   426|         0|            0|            0|  0.00%|
   427|         0|            0|            0|  0.00%|            stream must be a file-like object open for reading.
   428|         0|            0|            0|  0.00%|
   429|         0|            0|            0|  0.00%|            The StreamReader may use different error handling
   430|         0|            0|            0|  0.00%|            schemes by providing the errors keyword argument. These
   431|         0|            0|            0|  0.00%|            parameters are predefined:
   432|         0|            0|            0|  0.00%|
   433|         0|            0|            0|  0.00%|             'strict' - raise a ValueError (or a subclass)
   434|         0|            0|            0|  0.00%|             'ignore' - ignore the character and continue with the next
   435|         0|            0|            0|  0.00%|             'replace'- replace with a suitable replacement character
   436|         0|            0|            0|  0.00%|             'backslashreplace' - Replace with backslashed escape sequences;
   437|         0|            0|            0|  0.00%|
   438|         0|            0|            0|  0.00%|            The set of allowed parameter values can be extended via
   439|         0|            0|            0|  0.00%|            register_error.
   440|         0|            0|            0|  0.00%|        """
   441|         0|            0|            0|  0.00%|        self.stream = stream
   442|         0|            0|            0|  0.00%|        self.errors = errors
   443|         0|            0|            0|  0.00%|        self.bytebuffer = b""
   444|         0|            0|            0|  0.00%|        self._empty_charbuffer = self.charbuffertype()
   445|         0|            0|            0|  0.00%|        self.charbuffer = self._empty_charbuffer
   446|         0|            0|            0|  0.00%|        self.linebuffer = None
   447|         0|            0|            0|  0.00%|
   448|         0|            0|            0|  0.00%|    def decode(self, input, errors='strict'):
   449|         0|            0|            0|  0.00%|        raise NotImplementedError
   450|         0|            0|            0|  0.00%|
   451|         0|            0|            0|  0.00%|    def read(self, size=-1, chars=-1, firstline=False):
   452|         0|            0|            0|  0.00%|
   453|         0|            0|            0|  0.00%|        """ Decodes data from the stream self.stream and returns the
   454|         0|            0|            0|  0.00%|            resulting object.
   455|         0|            0|            0|  0.00%|
   456|         0|            0|            0|  0.00%|            chars indicates the number of decoded code points or bytes to
   457|         0|            0|            0|  0.00%|            return. read() will never return more data than requested,
   458|         0|            0|            0|  0.00%|            but it might return less, if there is not enough available.
   459|         0|            0|            0|  0.00%|
   460|         0|            0|            0|  0.00%|            size indicates the approximate maximum number of decoded
   461|         0|            0|            0|  0.00%|            bytes or code points to read for decoding. The decoder
   462|         0|            0|            0|  0.00%|            can modify this setting as appropriate. The default value
   463|         0|            0|            0|  0.00%|            -1 indicates to read and decode as much as possible.  size
   464|         0|            0|            0|  0.00%|            is intended to prevent having to decode huge files in one
   465|         0|            0|            0|  0.00%|            step.
   466|         0|            0|            0|  0.00%|
   467|         0|            0|            0|  0.00%|            If firstline is true, and a UnicodeDecodeError happens
   468|         0|            0|            0|  0.00%|            after the first line terminator in the input only the first line
   469|         0|            0|            0|  0.00%|            will be returned, the rest of the input will be kept until the
   470|         0|            0|            0|  0.00%|            next call to read().
   471|         0|            0|            0|  0.00%|
   472|         0|            0|            0|  0.00%|            The method should use a greedy read strategy, meaning that
   473|         0|            0|            0|  0.00%|            it should read as much data as is allowed within the
   474|         0|            0|            0|  0.00%|            definition of the encoding and the given size, e.g.  if
   475|         0|            0|            0|  0.00%|            optional encoding endings or state markers are available
   476|         0|            0|            0|  0.00%|            on the stream, these should be read too.
   477|         0|            0|            0|  0.00%|        """
   478|         0|            0|            0|  0.00%|        # If we have lines cached, first merge them back into characters
   479|         0|            0|            0|  0.00%|        if self.linebuffer:
   480|         0|            0|            0|  0.00%|            self.charbuffer = self._empty_charbuffer.join(self.linebuffer)
   481|         0|            0|            0|  0.00%|            self.linebuffer = None
   482|         0|            0|            0|  0.00%|
   483|         0|            0|            0|  0.00%|        if chars < 0:
   484|         0|            0|            0|  0.00%|            # For compatibility with other read() methods that take a
   485|         0|            0|            0|  0.00%|            # single argument
   486|         0|            0|            0|  0.00%|            chars = size
   487|         0|            0|            0|  0.00%|
   488|         0|            0|            0|  0.00%|        # read until we get the required number of characters (if available)
   489|         0|            0|            0|  0.00%|        while True:
   490|         0|            0|            0|  0.00%|            # can the request be satisfied from the character buffer?
   491|         0|            0|            0|  0.00%|            if chars >= 0:
   492|         0|            0|            0|  0.00%|                if len(self.charbuffer) >= chars:
   493|         0|            0|            0|  0.00%|                    break
   494|         0|            0|            0|  0.00%|            # we need more data
   495|         0|            0|            0|  0.00%|            if size < 0:
   496|         0|            0|            0|  0.00%|                newdata = self.stream.read()
   497|         0|            0|            0|  0.00%|            else:
   498|         0|            0|            0|  0.00%|                newdata = self.stream.read(size)
   499|         0|            0|            0|  0.00%|            # decode bytes (those remaining from the last call included)
   500|         0|            0|            0|  0.00%|            data = self.bytebuffer + newdata
   501|         0|            0|            0|  0.00%|            if not data:
   502|         0|            0|            0|  0.00%|                break
   503|         0|            0|            0|  0.00%|            try:
   504|         0|            0|            0|  0.00%|                newchars, decodedbytes = self.decode(data, self.errors)
   505|         0|            0|            0|  0.00%|            except UnicodeDecodeError as exc:
   506|         0|            0|            0|  0.00%|                if firstline:
   507|         0|            0|            0|  0.00%|                    newchars, decodedbytes = \
   508|         0|            0|            0|  0.00%|                        self.decode(data[:exc.start], self.errors)
   509|         0|            0|            0|  0.00%|                    lines = newchars.splitlines(keepends=True)
   510|         0|            0|            0|  0.00%|                    if len(lines)<=1:
   511|         0|            0|            0|  0.00%|                        raise
   512|         0|            0|            0|  0.00%|                else:
   513|         0|            0|            0|  0.00%|                    raise
   514|         0|            0|            0|  0.00%|            # keep undecoded bytes until the next call
   515|         0|            0|            0|  0.00%|            self.bytebuffer = data[decodedbytes:]
   516|         0|            0|            0|  0.00%|            # put new characters in the character buffer
   517|         0|            0|            0|  0.00%|            self.charbuffer += newchars
   518|         0|            0|            0|  0.00%|            # there was no data available
   519|         0|            0|            0|  0.00%|            if not newdata:
   520|         0|            0|            0|  0.00%|                break
   521|         0|            0|            0|  0.00%|        if chars < 0:
   522|         0|            0|            0|  0.00%|            # Return everything we've got
   523|         0|            0|            0|  0.00%|            result = self.charbuffer
   524|         0|            0|            0|  0.00%|            self.charbuffer = self._empty_charbuffer
   525|         0|            0|            0|  0.00%|        else:
   526|         0|            0|            0|  0.00%|            # Return the first chars characters
   527|         0|            0|            0|  0.00%|            result = self.charbuffer[:chars]
   528|         0|            0|            0|  0.00%|            self.charbuffer = self.charbuffer[chars:]
   529|         0|            0|            0|  0.00%|        return result
   530|         0|            0|            0|  0.00%|
   531|         0|            0|            0|  0.00%|    def readline(self, size=None, keepends=True):
   532|         0|            0|            0|  0.00%|
   533|         0|            0|            0|  0.00%|        """ Read one line from the input stream and return the
   534|         0|            0|            0|  0.00%|            decoded data.
   535|         0|            0|            0|  0.00%|
   536|         0|            0|            0|  0.00%|            size, if given, is passed as size argument to the
   537|         0|            0|            0|  0.00%|            read() method.
   538|         0|            0|            0|  0.00%|
   539|         0|            0|            0|  0.00%|        """
   540|         0|            0|            0|  0.00%|        # If we have lines cached from an earlier read, return
   541|         0|            0|            0|  0.00%|        # them unconditionally
   542|         0|            0|            0|  0.00%|        if self.linebuffer:
   543|         0|            0|            0|  0.00%|            line = self.linebuffer[0]
   544|         0|            0|            0|  0.00%|            del self.linebuffer[0]
   545|         0|            0|            0|  0.00%|            if len(self.linebuffer) == 1:
   546|         0|            0|            0|  0.00%|                # revert to charbuffer mode; we might need more data
   547|         0|            0|            0|  0.00%|                # next time
   548|         0|            0|            0|  0.00%|                self.charbuffer = self.linebuffer[0]
   549|         0|            0|            0|  0.00%|                self.linebuffer = None
   550|         0|            0|            0|  0.00%|            if not keepends:
   551|         0|            0|            0|  0.00%|                line = line.splitlines(keepends=False)[0]
   552|         0|            0|            0|  0.00%|            return line
   553|         0|            0|            0|  0.00%|
   554|         0|            0|            0|  0.00%|        readsize = size or 72
   555|         0|            0|            0|  0.00%|        line = self._empty_charbuffer
   556|         0|            0|            0|  0.00%|        # If size is given, we call read() only once
   557|         0|            0|            0|  0.00%|        while True:
   558|         0|            0|            0|  0.00%|            data = self.read(readsize, firstline=True)
   559|         0|            0|            0|  0.00%|            if data:
   560|         0|            0|            0|  0.00%|                # If we're at a "\r" read one extra character (which might
   561|         0|            0|            0|  0.00%|                # be a "\n") to get a proper line ending. If the stream is
   562|         0|            0|            0|  0.00%|                # temporarily exhausted we return the wrong line ending.
   563|         0|            0|            0|  0.00%|                if (isinstance(data, str) and data.endswith("\r")) or \
   564|         0|            0|            0|  0.00%|                   (isinstance(data, bytes) and data.endswith(b"\r")):
   565|         0|            0|            0|  0.00%|                    data += self.read(size=1, chars=1)
   566|         0|            0|            0|  0.00%|
   567|         0|            0|            0|  0.00%|            line += data
   568|         0|            0|            0|  0.00%|            lines = line.splitlines(keepends=True)
   569|         0|            0|            0|  0.00%|            if lines:
   570|         0|            0|            0|  0.00%|                if len(lines) > 1:
   571|         0|            0|            0|  0.00%|                    # More than one line result; the first line is a full line
   572|         0|            0|            0|  0.00%|                    # to return
   573|         0|            0|            0|  0.00%|                    line = lines[0]
   574|         0|            0|            0|  0.00%|                    del lines[0]
   575|         0|            0|            0|  0.00%|                    if len(lines) > 1:
   576|         0|            0|            0|  0.00%|                        # cache the remaining lines
   577|         0|            0|            0|  0.00%|                        lines[-1] += self.charbuffer
   578|         0|            0|            0|  0.00%|                        self.linebuffer = lines
   579|         0|            0|            0|  0.00%|                        self.charbuffer = None
   580|         0|            0|            0|  0.00%|                    else:
   581|         0|            0|            0|  0.00%|                        # only one remaining line, put it back into charbuffer
   582|         0|            0|            0|  0.00%|                        self.charbuffer = lines[0] + self.charbuffer
   583|         0|            0|            0|  0.00%|                    if not keepends:
   584|         0|            0|            0|  0.00%|                        line = line.splitlines(keepends=False)[0]
   585|         0|            0|            0|  0.00%|                    break
   586|         0|            0|            0|  0.00%|                line0withend = lines[0]
   587|         0|            0|            0|  0.00%|                line0withoutend = lines[0].splitlines(keepends=False)[0]
   588|         0|            0|            0|  0.00%|                if line0withend != line0withoutend: # We really have a line end
   589|         0|            0|            0|  0.00%|                    # Put the rest back together and keep it until the next call
   590|         0|            0|            0|  0.00%|                    self.charbuffer = self._empty_charbuffer.join(lines[1:]) + \
   591|         0|            0|            0|  0.00%|                                      self.charbuffer
   592|         0|            0|            0|  0.00%|                    if keepends:
   593|         0|            0|            0|  0.00%|                        line = line0withend
   594|         0|            0|            0|  0.00%|                    else:
   595|         0|            0|            0|  0.00%|                        line = line0withoutend
   596|         0|            0|            0|  0.00%|                    break
   597|         0|            0|            0|  0.00%|            # we didn't get anything or this was our only try
   598|         0|            0|            0|  0.00%|            if not data or size is not None:
   599|         0|            0|            0|  0.00%|                if line and not keepends:
   600|         0|            0|            0|  0.00%|                    line = line.splitlines(keepends=False)[0]
   601|         0|            0|            0|  0.00%|                break
   602|         0|            0|            0|  0.00%|            if readsize < 8000:
   603|         0|            0|            0|  0.00%|                readsize *= 2
   604|         0|            0|            0|  0.00%|        return line
   605|         0|            0|            0|  0.00%|
   606|         0|            0|            0|  0.00%|    def readlines(self, sizehint=None, keepends=True):
   607|         0|            0|            0|  0.00%|
   608|         0|            0|            0|  0.00%|        """ Read all lines available on the input stream
   609|         0|            0|            0|  0.00%|            and return them as a list.
   610|         0|            0|            0|  0.00%|
   611|         0|            0|            0|  0.00%|            Line breaks are implemented using the codec's decoder
   612|         0|            0|            0|  0.00%|            method and are included in the list entries.
   613|         0|            0|            0|  0.00%|
   614|         0|            0|            0|  0.00%|            sizehint, if given, is ignored since there is no efficient
   615|         0|            0|            0|  0.00%|            way to finding the true end-of-line.
   616|         0|            0|            0|  0.00%|
   617|         0|            0|            0|  0.00%|        """
   618|         0|            0|            0|  0.00%|        data = self.read()
   619|         0|            0|            0|  0.00%|        return data.splitlines(keepends)
   620|         0|            0|            0|  0.00%|
   621|         0|            0|            0|  0.00%|    def reset(self):
   622|         0|            0|            0|  0.00%|
   623|         0|            0|            0|  0.00%|        """ Resets the codec buffers used for keeping state.
   624|         0|            0|            0|  0.00%|
   625|         0|            0|            0|  0.00%|            Note that no stream repositioning should take place.
   626|         0|            0|            0|  0.00%|            This method is primarily intended to be able to recover
   627|         0|            0|            0|  0.00%|            from decoding errors.
   628|         0|            0|            0|  0.00%|
   629|         0|            0|            0|  0.00%|        """
   630|         0|            0|            0|  0.00%|        self.bytebuffer = b""
   631|         0|            0|            0|  0.00%|        self.charbuffer = self._empty_charbuffer
   632|         0|            0|            0|  0.00%|        self.linebuffer = None
   633|         0|            0|            0|  0.00%|
   634|         0|            0|            0|  0.00%|    def seek(self, offset, whence=0):
   635|         0|            0|            0|  0.00%|        """ Set the input stream's current position.
   636|         0|            0|            0|  0.00%|
   637|         0|            0|            0|  0.00%|            Resets the codec buffers used for keeping state.
   638|         0|            0|            0|  0.00%|        """
   639|         0|            0|            0|  0.00%|        self.stream.seek(offset, whence)
   640|         0|            0|            0|  0.00%|        self.reset()
   641|         0|            0|            0|  0.00%|
   642|         0|            0|            0|  0.00%|    def __next__(self):
   643|         0|            0|            0|  0.00%|
   644|         0|            0|            0|  0.00%|        """ Return the next decoded line from the input stream."""
   645|         0|            0|            0|  0.00%|        line = self.readline()
   646|         0|            0|            0|  0.00%|        if line:
   647|         0|            0|            0|  0.00%|            return line
   648|         0|            0|            0|  0.00%|        raise StopIteration
   649|         0|            0|            0|  0.00%|
   650|         0|            0|            0|  0.00%|    def __iter__(self):
   651|         0|            0|            0|  0.00%|        return self
   652|         0|            0|            0|  0.00%|
   653|         0|            0|            0|  0.00%|    def __getattr__(self, name,
   654|         0|            0|            0|  0.00%|                    getattr=getattr):
   655|         0|            0|            0|  0.00%|
   656|         0|            0|            0|  0.00%|        """ Inherit all other methods from the underlying stream.
   657|         0|            0|            0|  0.00%|        """
   658|         0|            0|            0|  0.00%|        return getattr(self.stream, name)
   659|         0|            0|            0|  0.00%|
   660|         0|            0|            0|  0.00%|    def __enter__(self):
   661|         0|            0|            0|  0.00%|        return self
   662|         0|            0|            0|  0.00%|
   663|         0|            0|            0|  0.00%|    def __exit__(self, type, value, tb):
   664|         0|            0|            0|  0.00%|        self.stream.close()
   665|         0|            0|            0|  0.00%|
   666|         0|            0|            0|  0.00%|###
   667|         0|            0|            0|  0.00%|
   668|         0|            0|            0|  0.00%|class StreamReaderWriter:
   669|         0|            0|            0|  0.00%|
   670|         0|            0|            0|  0.00%|    """ StreamReaderWriter instances allow wrapping streams which
   671|         0|            0|            0|  0.00%|        work in both read and write modes.
   672|         0|            0|            0|  0.00%|
   673|         0|            0|            0|  0.00%|        The design is such that one can use the factory functions
   674|         0|            0|            0|  0.00%|        returned by the codec.lookup() function to construct the
   675|         0|            0|            0|  0.00%|        instance.
   676|         0|            0|            0|  0.00%|
   677|         0|            0|            0|  0.00%|    """
   678|         0|            0|            0|  0.00%|    # Optional attributes set by the file wrappers below
   679|         0|            0|            0|  0.00%|    encoding = 'unknown'
   680|         0|            0|            0|  0.00%|
   681|         0|            0|            0|  0.00%|    def __init__(self, stream, Reader, Writer, errors='strict'):
   682|         0|            0|            0|  0.00%|
   683|         0|            0|            0|  0.00%|        """ Creates a StreamReaderWriter instance.
   684|         0|            0|            0|  0.00%|
   685|         0|            0|            0|  0.00%|            stream must be a Stream-like object.
   686|         0|            0|            0|  0.00%|
   687|         0|            0|            0|  0.00%|            Reader, Writer must be factory functions or classes
   688|         0|            0|            0|  0.00%|            providing the StreamReader, StreamWriter interface resp.
   689|         0|            0|            0|  0.00%|
   690|         0|            0|            0|  0.00%|            Error handling is done in the same way as defined for the
   691|         0|            0|            0|  0.00%|            StreamWriter/Readers.
   692|         0|            0|            0|  0.00%|
   693|         0|            0|            0|  0.00%|        """
   694|         0|            0|            0|  0.00%|        self.stream = stream
   695|         0|            0|            0|  0.00%|        self.reader = Reader(stream, errors)
   696|         0|            0|            0|  0.00%|        self.writer = Writer(stream, errors)
   697|         0|            0|            0|  0.00%|        self.errors = errors
   698|         0|            0|            0|  0.00%|
   699|         0|            0|            0|  0.00%|    def read(self, size=-1):
   700|         0|            0|            0|  0.00%|
   701|         0|            0|            0|  0.00%|        return self.reader.read(size)
   702|         0|            0|            0|  0.00%|
   703|         0|            0|            0|  0.00%|    def readline(self, size=None):
   704|         0|            0|            0|  0.00%|
   705|         0|            0|            0|  0.00%|        return self.reader.readline(size)
   706|         0|            0|            0|  0.00%|
   707|         0|            0|            0|  0.00%|    def readlines(self, sizehint=None):
   708|         0|            0|            0|  0.00%|
   709|         0|            0|            0|  0.00%|        return self.reader.readlines(sizehint)
   710|         0|            0|            0|  0.00%|
   711|         0|            0|            0|  0.00%|    def __next__(self):
   712|         0|            0|            0|  0.00%|
   713|         0|            0|            0|  0.00%|        """ Return the next decoded line from the input stream."""
   714|         0|            0|            0|  0.00%|        return next(self.reader)
   715|         0|            0|            0|  0.00%|
   716|         0|            0|            0|  0.00%|    def __iter__(self):
   717|         0|            0|            0|  0.00%|        return self
   718|         0|            0|            0|  0.00%|
   719|         0|            0|            0|  0.00%|    def write(self, data):
   720|         0|            0|            0|  0.00%|
   721|         0|            0|            0|  0.00%|        return self.writer.write(data)
   722|         0|            0|            0|  0.00%|
   723|         0|            0|            0|  0.00%|    def writelines(self, list):
   724|         0|            0|            0|  0.00%|
   725|         0|            0|            0|  0.00%|        return self.writer.writelines(list)
   726|         0|            0|            0|  0.00%|
   727|         0|            0|            0|  0.00%|    def reset(self):
   728|         0|            0|            0|  0.00%|
   729|         0|            0|            0|  0.00%|        self.reader.reset()
   730|         0|            0|            0|  0.00%|        self.writer.reset()
   731|         0|            0|            0|  0.00%|
   732|         0|            0|            0|  0.00%|    def seek(self, offset, whence=0):
   733|         0|            0|            0|  0.00%|        self.stream.seek(offset, whence)
   734|         0|            0|            0|  0.00%|        self.reader.reset()
   735|         0|            0|            0|  0.00%|        if whence == 0 and offset == 0:
   736|         0|            0|            0|  0.00%|            self.writer.reset()
   737|         0|            0|            0|  0.00%|
   738|         0|            0|            0|  0.00%|    def __getattr__(self, name,
   739|         0|            0|            0|  0.00%|                    getattr=getattr):
   740|         0|            0|            0|  0.00%|
   741|         0|            0|            0|  0.00%|        """ Inherit all other methods from the underlying stream.
   742|         0|            0|            0|  0.00%|        """
   743|         0|            0|            0|  0.00%|        return getattr(self.stream, name)
   744|         0|            0|            0|  0.00%|
   745|         0|            0|            0|  0.00%|    # these are needed to make "with StreamReaderWriter(...)" work properly
   746|         0|            0|            0|  0.00%|
   747|         0|            0|            0|  0.00%|    def __enter__(self):
   748|         0|            0|            0|  0.00%|        return self
   749|         0|            0|            0|  0.00%|
   750|         0|            0|            0|  0.00%|    def __exit__(self, type, value, tb):
   751|         0|            0|            0|  0.00%|        self.stream.close()
   752|         0|            0|            0|  0.00%|
   753|         0|            0|            0|  0.00%|###
   754|         0|            0|            0|  0.00%|
   755|         0|            0|            0|  0.00%|class StreamRecoder:
   756|         0|            0|            0|  0.00%|
   757|         0|            0|            0|  0.00%|    """ StreamRecoder instances translate data from one encoding to another.
   758|         0|            0|            0|  0.00%|
   759|         0|            0|            0|  0.00%|        They use the complete set of APIs returned by the
   760|         0|            0|            0|  0.00%|        codecs.lookup() function to implement their task.
   761|         0|            0|            0|  0.00%|
   762|         0|            0|            0|  0.00%|        Data written to the StreamRecoder is first decoded into an
   763|         0|            0|            0|  0.00%|        intermediate format (depending on the "decode" codec) and then
   764|         0|            0|            0|  0.00%|        written to the underlying stream using an instance of the provided
   765|         0|            0|            0|  0.00%|        Writer class.
   766|         0|            0|            0|  0.00%|
   767|         0|            0|            0|  0.00%|        In the other direction, data is read from the underlying stream using
   768|         0|            0|            0|  0.00%|        a Reader instance and then encoded and returned to the caller.
   769|         0|            0|            0|  0.00%|
   770|         0|            0|            0|  0.00%|    """
   771|         0|            0|            0|  0.00%|    # Optional attributes set by the file wrappers below
   772|         0|            0|            0|  0.00%|    data_encoding = 'unknown'
   773|         0|            0|            0|  0.00%|    file_encoding = 'unknown'
   774|         0|            0|            0|  0.00%|
   775|         0|            0|            0|  0.00%|    def __init__(self, stream, encode, decode, Reader, Writer,
   776|         0|            0|            0|  0.00%|                 errors='strict'):
   777|         0|            0|            0|  0.00%|
   778|         0|            0|            0|  0.00%|        """ Creates a StreamRecoder instance which implements a two-way
   779|         0|            0|            0|  0.00%|            conversion: encode and decode work on the frontend (the
   780|         0|            0|            0|  0.00%|            data visible to .read() and .write()) while Reader and Writer
   781|         0|            0|            0|  0.00%|            work on the backend (the data in stream).
   782|         0|            0|            0|  0.00%|
   783|         0|            0|            0|  0.00%|            You can use these objects to do transparent
   784|         0|            0|            0|  0.00%|            transcodings from e.g. latin-1 to utf-8 and back.
   785|         0|            0|            0|  0.00%|
   786|         0|            0|            0|  0.00%|            stream must be a file-like object.
   787|         0|            0|            0|  0.00%|
   788|         0|            0|            0|  0.00%|            encode and decode must adhere to the Codec interface; Reader and
   789|         0|            0|            0|  0.00%|            Writer must be factory functions or classes providing the
   790|         0|            0|            0|  0.00%|            StreamReader and StreamWriter interfaces resp.
   791|         0|            0|            0|  0.00%|
   792|         0|            0|            0|  0.00%|            Error handling is done in the same way as defined for the
   793|         0|            0|            0|  0.00%|            StreamWriter/Readers.
   794|         0|            0|            0|  0.00%|
   795|         0|            0|            0|  0.00%|        """
   796|         0|            0|            0|  0.00%|        self.stream = stream
   797|         0|            0|            0|  0.00%|        self.encode = encode
   798|         0|            0|            0|  0.00%|        self.decode = decode
   799|         0|            0|            0|  0.00%|        self.reader = Reader(stream, errors)
   800|         0|            0|            0|  0.00%|        self.writer = Writer(stream, errors)
   801|         0|            0|            0|  0.00%|        self.errors = errors
   802|         0|            0|            0|  0.00%|
   803|         0|            0|            0|  0.00%|    def read(self, size=-1):
   804|         0|            0|            0|  0.00%|
   805|         0|            0|            0|  0.00%|        data = self.reader.read(size)
   806|         0|            0|            0|  0.00%|        data, bytesencoded = self.encode(data, self.errors)
   807|         0|            0|            0|  0.00%|        return data
   808|         0|            0|            0|  0.00%|
   809|         0|            0|            0|  0.00%|    def readline(self, size=None):
   810|         0|            0|            0|  0.00%|
   811|         0|            0|            0|  0.00%|        if size is None:
   812|         0|            0|            0|  0.00%|            data = self.reader.readline()
   813|         0|            0|            0|  0.00%|        else:
   814|         0|            0|            0|  0.00%|            data = self.reader.readline(size)
   815|         0|            0|            0|  0.00%|        data, bytesencoded = self.encode(data, self.errors)
   816|         0|            0|            0|  0.00%|        return data
   817|         0|            0|            0|  0.00%|
   818|         0|            0|            0|  0.00%|    def readlines(self, sizehint=None):
   819|         0|            0|            0|  0.00%|
   820|         0|            0|            0|  0.00%|        data = self.reader.read()
   821|         0|            0|            0|  0.00%|        data, bytesencoded = self.encode(data, self.errors)
   822|         0|            0|            0|  0.00%|        return data.splitlines(keepends=True)
   823|         0|            0|            0|  0.00%|
   824|         0|            0|            0|  0.00%|    def __next__(self):
   825|         0|            0|            0|  0.00%|
   826|         0|            0|            0|  0.00%|        """ Return the next decoded line from the input stream."""
   827|         0|            0|            0|  0.00%|        data = next(self.reader)
   828|         0|            0|            0|  0.00%|        data, bytesencoded = self.encode(data, self.errors)
   829|         0|            0|            0|  0.00%|        return data
   830|         0|            0|            0|  0.00%|
   831|         0|            0|            0|  0.00%|    def __iter__(self):
   832|         0|            0|            0|  0.00%|        return self
   833|         0|            0|            0|  0.00%|
   834|         0|            0|            0|  0.00%|    def write(self, data):
   835|         0|            0|            0|  0.00%|
   836|         0|            0|            0|  0.00%|        data, bytesdecoded = self.decode(data, self.errors)
   837|         0|            0|            0|  0.00%|        return self.writer.write(data)
   838|         0|            0|            0|  0.00%|
   839|         0|            0|            0|  0.00%|    def writelines(self, list):
   840|         0|            0|            0|  0.00%|
   841|         0|            0|            0|  0.00%|        data = b''.join(list)
   842|         0|            0|            0|  0.00%|        data, bytesdecoded = self.decode(data, self.errors)
   843|         0|            0|            0|  0.00%|        return self.writer.write(data)
   844|         0|            0|            0|  0.00%|
   845|         0|            0|            0|  0.00%|    def reset(self):
   846|         0|            0|            0|  0.00%|
   847|         0|            0|            0|  0.00%|        self.reader.reset()
   848|         0|            0|            0|  0.00%|        self.writer.reset()
   849|         0|            0|            0|  0.00%|
   850|         0|            0|            0|  0.00%|    def seek(self, offset, whence=0):
   851|         0|            0|            0|  0.00%|        # Seeks must be propagated to both the readers and writers
   852|         0|            0|            0|  0.00%|        # as they might need to reset their internal buffers.
   853|         0|            0|            0|  0.00%|        self.reader.seek(offset, whence)
   854|         0|            0|            0|  0.00%|        self.writer.seek(offset, whence)
   855|         0|            0|            0|  0.00%|
   856|         0|            0|            0|  0.00%|    def __getattr__(self, name,
   857|         0|            0|            0|  0.00%|                    getattr=getattr):
   858|         0|            0|            0|  0.00%|
   859|         0|            0|            0|  0.00%|        """ Inherit all other methods from the underlying stream.
   860|         0|            0|            0|  0.00%|        """
   861|         0|            0|            0|  0.00%|        return getattr(self.stream, name)
   862|         0|            0|            0|  0.00%|
   863|         0|            0|            0|  0.00%|    def __enter__(self):
   864|         0|            0|            0|  0.00%|        return self
   865|         0|            0|            0|  0.00%|
   866|         0|            0|            0|  0.00%|    def __exit__(self, type, value, tb):
   867|         0|            0|            0|  0.00%|        self.stream.close()
   868|         0|            0|            0|  0.00%|
   869|         0|            0|            0|  0.00%|### Shortcuts
   870|         0|            0|            0|  0.00%|
   871|         0|            0|            0|  0.00%|def open(filename, mode='r', encoding=None, errors='strict', buffering=-1):
   872|         0|            0|            0|  0.00%|
   873|         0|            0|            0|  0.00%|    """ Open an encoded file using the given mode and return
   874|         0|            0|            0|  0.00%|        a wrapped version providing transparent encoding/decoding.
   875|         0|            0|            0|  0.00%|
   876|         0|            0|            0|  0.00%|        Note: The wrapped version will only accept the object format
   877|         0|            0|            0|  0.00%|        defined by the codecs, i.e. Unicode objects for most builtin
   878|         0|            0|            0|  0.00%|        codecs. Output is also codec dependent and will usually be
   879|         0|            0|            0|  0.00%|        Unicode as well.
   880|         0|            0|            0|  0.00%|
   881|         0|            0|            0|  0.00%|        Underlying encoded files are always opened in binary mode.
   882|         0|            0|            0|  0.00%|        The default file mode is 'r', meaning to open the file in read mode.
   883|         0|            0|            0|  0.00%|
   884|         0|            0|            0|  0.00%|        encoding specifies the encoding which is to be used for the
   885|         0|            0|            0|  0.00%|        file.
   886|         0|            0|            0|  0.00%|
   887|         0|            0|            0|  0.00%|        errors may be given to define the error handling. It defaults
   888|         0|            0|            0|  0.00%|        to 'strict' which causes ValueErrors to be raised in case an
   889|         0|            0|            0|  0.00%|        encoding error occurs.
   890|         0|            0|            0|  0.00%|
   891|         0|            0|            0|  0.00%|        buffering has the same meaning as for the builtin open() API.
   892|         0|            0|            0|  0.00%|        It defaults to -1 which means that the default buffer size will
   893|         0|            0|            0|  0.00%|        be used.
   894|         0|            0|            0|  0.00%|
   895|         0|            0|            0|  0.00%|        The returned wrapped file object provides an extra attribute
   896|         0|            0|            0|  0.00%|        .encoding which allows querying the used encoding. This
   897|         0|            0|            0|  0.00%|        attribute is only available if an encoding was specified as
   898|         0|            0|            0|  0.00%|        parameter.
   899|         0|            0|            0|  0.00%|
   900|         0|            0|            0|  0.00%|    """
   901|         0|            0|            0|  0.00%|    if encoding is not None and \
   902|         0|            0|            0|  0.00%|       'b' not in mode:
   903|         0|            0|            0|  0.00%|        # Force opening of the file in binary mode
   904|         0|            0|            0|  0.00%|        mode = mode + 'b'
   905|         0|            0|            0|  0.00%|    file = builtins.open(filename, mode, buffering)
   906|         0|            0|            0|  0.00%|    if encoding is None:
   907|         0|            0|            0|  0.00%|        return file
   908|         0|            0|            0|  0.00%|
   909|         0|            0|            0|  0.00%|    try:
   910|         0|            0|            0|  0.00%|        info = lookup(encoding)
   911|         0|            0|            0|  0.00%|        srw = StreamReaderWriter(file, info.streamreader, info.streamwriter, errors)
   912|         0|            0|            0|  0.00%|        # Add attributes to simplify introspection
   913|         0|            0|            0|  0.00%|        srw.encoding = encoding
   914|         0|            0|            0|  0.00%|        return srw
   915|         0|            0|            0|  0.00%|    except:
   916|         0|            0|            0|  0.00%|        file.close()
   917|         0|            0|            0|  0.00%|        raise
   918|         0|            0|            0|  0.00%|
   919|         0|            0|            0|  0.00%|def EncodedFile(file, data_encoding, file_encoding=None, errors='strict'):
   920|         0|            0|            0|  0.00%|
   921|         0|            0|            0|  0.00%|    """ Return a wrapped version of file which provides transparent
   922|         0|            0|            0|  0.00%|        encoding translation.
   923|         0|            0|            0|  0.00%|
   924|         0|            0|            0|  0.00%|        Data written to the wrapped file is decoded according
   925|         0|            0|            0|  0.00%|        to the given data_encoding and then encoded to the underlying
   926|         0|            0|            0|  0.00%|        file using file_encoding. The intermediate data type
   927|         0|            0|            0|  0.00%|        will usually be Unicode but depends on the specified codecs.
   928|         0|            0|            0|  0.00%|
   929|         0|            0|            0|  0.00%|        Bytes read from the file are decoded using file_encoding and then
   930|         0|            0|            0|  0.00%|        passed back to the caller encoded using data_encoding.
   931|         0|            0|            0|  0.00%|
   932|         0|            0|            0|  0.00%|        If file_encoding is not given, it defaults to data_encoding.
   933|         0|            0|            0|  0.00%|
   934|         0|            0|            0|  0.00%|        errors may be given to define the error handling. It defaults
   935|         0|            0|            0|  0.00%|        to 'strict' which causes ValueErrors to be raised in case an
   936|         0|            0|            0|  0.00%|        encoding error occurs.
   937|         0|            0|            0|  0.00%|
   938|         0|            0|            0|  0.00%|        The returned wrapped file object provides two extra attributes
   939|         0|            0|            0|  0.00%|        .data_encoding and .file_encoding which reflect the given
   940|         0|            0|            0|  0.00%|        parameters of the same name. The attributes can be used for
   941|         0|            0|            0|  0.00%|        introspection by Python programs.
   942|         0|            0|            0|  0.00%|
   943|         0|            0|            0|  0.00%|    """
   944|         0|            0|            0|  0.00%|    if file_encoding is None:
   945|         0|            0|            0|  0.00%|        file_encoding = data_encoding
   946|         0|            0|            0|  0.00%|    data_info = lookup(data_encoding)
   947|         0|            0|            0|  0.00%|    file_info = lookup(file_encoding)
   948|         0|            0|            0|  0.00%|    sr = StreamRecoder(file, data_info.encode, data_info.decode,
   949|         0|            0|            0|  0.00%|                       file_info.streamreader, file_info.streamwriter, errors)
   950|         0|            0|            0|  0.00%|    # Add attributes to simplify introspection
   951|         0|            0|            0|  0.00%|    sr.data_encoding = data_encoding
   952|         0|            0|            0|  0.00%|    sr.file_encoding = file_encoding
   953|         0|            0|            0|  0.00%|    return sr
   954|         0|            0|            0|  0.00%|
   955|         0|            0|            0|  0.00%|### Helpers for codec lookup
   956|         0|            0|            0|  0.00%|
   957|         0|            0|            0|  0.00%|def getencoder(encoding):
   958|         0|            0|            0|  0.00%|
   959|         0|            0|            0|  0.00%|    """ Lookup up the codec for the given encoding and return
   960|         0|            0|            0|  0.00%|        its encoder function.
   961|         0|            0|            0|  0.00%|
   962|         0|            0|            0|  0.00%|        Raises a LookupError in case the encoding cannot be found.
   963|         0|            0|            0|  0.00%|
   964|         0|            0|            0|  0.00%|    """
   965|         0|            0|            0|  0.00%|    return lookup(encoding).encode
   966|         0|            0|            0|  0.00%|
   967|         0|            0|            0|  0.00%|def getdecoder(encoding):
   968|         0|            0|            0|  0.00%|
   969|         0|            0|            0|  0.00%|    """ Lookup up the codec for the given encoding and return
   970|         0|            0|            0|  0.00%|        its decoder function.
   971|         0|            0|            0|  0.00%|
   972|         0|            0|            0|  0.00%|        Raises a LookupError in case the encoding cannot be found.
   973|         0|            0|            0|  0.00%|
   974|         0|            0|            0|  0.00%|    """
   975|         0|            0|            0|  0.00%|    return lookup(encoding).decode
   976|         0|            0|            0|  0.00%|
   977|         0|            0|            0|  0.00%|def getincrementalencoder(encoding):
   978|         0|            0|            0|  0.00%|
   979|         0|            0|            0|  0.00%|    """ Lookup up the codec for the given encoding and return
   980|         0|            0|            0|  0.00%|        its IncrementalEncoder class or factory function.
   981|         0|            0|            0|  0.00%|
   982|         0|            0|            0|  0.00%|        Raises a LookupError in case the encoding cannot be found
   983|         0|            0|            0|  0.00%|        or the codecs doesn't provide an incremental encoder.
   984|         0|            0|            0|  0.00%|
   985|         0|            0|            0|  0.00%|    """
   986|         0|            0|            0|  0.00%|    encoder = lookup(encoding).incrementalencoder
   987|         0|            0|            0|  0.00%|    if encoder is None:
   988|         0|            0|            0|  0.00%|        raise LookupError(encoding)
   989|         0|            0|            0|  0.00%|    return encoder
   990|         0|            0|            0|  0.00%|
   991|         0|            0|            0|  0.00%|def getincrementaldecoder(encoding):
   992|         0|            0|            0|  0.00%|
   993|         0|            0|            0|  0.00%|    """ Lookup up the codec for the given encoding and return
   994|         0|            0|            0|  0.00%|        its IncrementalDecoder class or factory function.
   995|         0|            0|            0|  0.00%|
   996|         0|            0|            0|  0.00%|        Raises a LookupError in case the encoding cannot be found
   997|         0|            0|            0|  0.00%|        or the codecs doesn't provide an incremental decoder.
   998|         0|            0|            0|  0.00%|
   999|         0|            0|            0|  0.00%|    """
  1000|         0|            0|            0|  0.00%|    decoder = lookup(encoding).incrementaldecoder
  1001|         0|            0|            0|  0.00%|    if decoder is None:
  1002|         0|            0|            0|  0.00%|        raise LookupError(encoding)
  1003|         0|            0|            0|  0.00%|    return decoder
  1004|         0|            0|            0|  0.00%|
  1005|         0|            0|            0|  0.00%|def getreader(encoding):
  1006|         0|            0|            0|  0.00%|
  1007|         0|            0|            0|  0.00%|    """ Lookup up the codec for the given encoding and return
  1008|         0|            0|            0|  0.00%|        its StreamReader class or factory function.
  1009|         0|            0|            0|  0.00%|
  1010|         0|            0|            0|  0.00%|        Raises a LookupError in case the encoding cannot be found.
  1011|         0|            0|            0|  0.00%|
  1012|         0|            0|            0|  0.00%|    """
  1013|         0|            0|            0|  0.00%|    return lookup(encoding).streamreader
  1014|         0|            0|            0|  0.00%|
  1015|         0|            0|            0|  0.00%|def getwriter(encoding):
  1016|         0|            0|            0|  0.00%|
  1017|         0|            0|            0|  0.00%|    """ Lookup up the codec for the given encoding and return
  1018|         0|            0|            0|  0.00%|        its StreamWriter class or factory function.
  1019|         0|            0|            0|  0.00%|
  1020|         0|            0|            0|  0.00%|        Raises a LookupError in case the encoding cannot be found.
  1021|         0|            0|            0|  0.00%|
  1022|         0|            0|            0|  0.00%|    """
  1023|         0|            0|            0|  0.00%|    return lookup(encoding).streamwriter
  1024|         0|            0|            0|  0.00%|
  1025|         0|            0|            0|  0.00%|def iterencode(iterator, encoding, errors='strict', **kwargs):
  1026|         0|            0|            0|  0.00%|    """
  1027|         0|            0|            0|  0.00%|    Encoding iterator.
  1028|         0|            0|            0|  0.00%|
  1029|         0|            0|            0|  0.00%|    Encodes the input strings from the iterator using an IncrementalEncoder.
  1030|         0|            0|            0|  0.00%|
  1031|         0|            0|            0|  0.00%|    errors and kwargs are passed through to the IncrementalEncoder
  1032|         0|            0|            0|  0.00%|    constructor.
  1033|         0|            0|            0|  0.00%|    """
  1034|         0|            0|            0|  0.00%|    encoder = getincrementalencoder(encoding)(errors, **kwargs)
  1035|         0|            0|            0|  0.00%|    for input in iterator:
  1036|         0|            0|            0|  0.00%|        output = encoder.encode(input)
  1037|         0|            0|            0|  0.00%|        if output:
  1038|         0|            0|            0|  0.00%|            yield output
  1039|         0|            0|            0|  0.00%|    output = encoder.encode("", True)
  1040|         0|            0|            0|  0.00%|    if output:
  1041|         0|            0|            0|  0.00%|        yield output
  1042|         0|            0|            0|  0.00%|
  1043|         0|            0|            0|  0.00%|def iterdecode(iterator, encoding, errors='strict', **kwargs):
  1044|         0|            0|            0|  0.00%|    """
  1045|         0|            0|            0|  0.00%|    Decoding iterator.
  1046|         0|            0|            0|  0.00%|
  1047|         0|            0|            0|  0.00%|    Decodes the input strings from the iterator using an IncrementalDecoder.
  1048|         0|            0|            0|  0.00%|
  1049|         0|            0|            0|  0.00%|    errors and kwargs are passed through to the IncrementalDecoder
  1050|         0|            0|            0|  0.00%|    constructor.
  1051|         0|            0|            0|  0.00%|    """
  1052|         0|            0|            0|  0.00%|    decoder = getincrementaldecoder(encoding)(errors, **kwargs)
  1053|         0|            0|            0|  0.00%|    for input in iterator:
  1054|         0|            0|            0|  0.00%|        output = decoder.decode(input)
  1055|         0|            0|            0|  0.00%|        if output:
  1056|         0|            0|            0|  0.00%|            yield output
  1057|         0|            0|            0|  0.00%|    output = decoder.decode(b"", True)
  1058|         0|            0|            0|  0.00%|    if output:
  1059|         0|            0|            0|  0.00%|        yield output
  1060|         0|            0|            0|  0.00%|
  1061|         0|            0|            0|  0.00%|### Helpers for charmap-based codecs
  1062|         0|            0|            0|  0.00%|
  1063|         0|            0|            0|  0.00%|def make_identity_dict(rng):
  1064|         0|            0|            0|  0.00%|
  1065|         0|            0|            0|  0.00%|    """ make_identity_dict(rng) -> dict
  1066|         0|            0|            0|  0.00%|
  1067|         0|            0|            0|  0.00%|        Return a dictionary where elements of the rng sequence are
  1068|         0|            0|            0|  0.00%|        mapped to themselves.
  1069|         0|            0|            0|  0.00%|
  1070|         0|            0|            0|  0.00%|    """
  1071|         0|            0|            0|  0.00%|    return {i:i for i in rng}
  1072|         0|            0|            0|  0.00%|
  1073|         0|            0|            0|  0.00%|def make_encoding_map(decoding_map):
  1074|         0|            0|            0|  0.00%|
  1075|         0|            0|            0|  0.00%|    """ Creates an encoding map from a decoding map.
  1076|         0|            0|            0|  0.00%|
  1077|         0|            0|            0|  0.00%|        If a target mapping in the decoding map occurs multiple
  1078|         0|            0|            0|  0.00%|        times, then that target is mapped to None (undefined mapping),
  1079|         0|            0|            0|  0.00%|        causing an exception when encountered by the charmap codec
  1080|         0|            0|            0|  0.00%|        during translation.
  1081|         0|            0|            0|  0.00%|
  1082|         0|            0|            0|  0.00%|        One example where this happens is cp875.py which decodes
  1083|         0|            0|            0|  0.00%|        multiple character to \\u001a.
  1084|         0|            0|            0|  0.00%|
  1085|         0|            0|            0|  0.00%|    """
  1086|         0|            0|            0|  0.00%|    m = {}
  1087|         0|            0|            0|  0.00%|    for k,v in decoding_map.items():
  1088|         0|            0|            0|  0.00%|        if not v in m:
  1089|         0|            0|            0|  0.00%|            m[v] = k
  1090|         0|            0|            0|  0.00%|        else:
  1091|         0|            0|            0|  0.00%|            m[v] = None
  1092|         0|            0|            0|  0.00%|    return m
  1093|         0|            0|            0|  0.00%|
  1094|         0|            0|            0|  0.00%|### error handlers
  1095|         0|            0|            0|  0.00%|
  1096|         0|            0|            0|  0.00%|try:
  1097|         0|            0|            0|  0.00%|    strict_errors = lookup_error("strict")
  1098|         0|            0|            0|  0.00%|    ignore_errors = lookup_error("ignore")
  1099|         0|            0|            0|  0.00%|    replace_errors = lookup_error("replace")
  1100|         0|            0|            0|  0.00%|    xmlcharrefreplace_errors = lookup_error("xmlcharrefreplace")
  1101|         0|            0|            0|  0.00%|    backslashreplace_errors = lookup_error("backslashreplace")
  1102|         0|            0|            0|  0.00%|    namereplace_errors = lookup_error("namereplace")
  1103|         0|            0|            0|  0.00%|except LookupError:
  1104|         0|            0|            0|  0.00%|    # In --disable-unicode builds, these error handler are missing
  1105|         0|            0|            0|  0.00%|    strict_errors = None
  1106|         0|            0|            0|  0.00%|    ignore_errors = None
  1107|         0|            0|            0|  0.00%|    replace_errors = None
  1108|         0|            0|            0|  0.00%|    xmlcharrefreplace_errors = None
  1109|         0|            0|            0|  0.00%|    backslashreplace_errors = None
  1110|         0|            0|            0|  0.00%|    namereplace_errors = None
  1111|         0|            0|            0|  0.00%|
  1112|         0|            0|            0|  0.00%|# Tell modulefinder that using codecs probably needs the encodings
  1113|         0|            0|            0|  0.00%|# package
  1114|         0|            0|            0|  0.00%|_false = 0
  1115|         0|            0|            0|  0.00%|if _false:
  1116|         0|            0|            0|  0.00%|    import encodings
  1117|         0|            0|            0|  0.00%|
  1118|         0|            0|            0|  0.00%|### Tests
  1119|         0|            0|            0|  0.00%|
  1120|         0|            0|            0|  0.00%|if __name__ == '__main__':
  1121|         0|            0|            0|  0.00%|
  1122|         0|            0|            0|  0.00%|    # Make stdout translate Latin-1 output into UTF-8 output
  1123|         0|            0|            0|  0.00%|    sys.stdout = EncodedFile(sys.stdout, 'latin-1', 'utf-8')
  1124|         0|            0|            0|  0.00%|
  1125|         0|            0|            0|  0.00%|    # Have stdin translate Latin-1 input into UTF-8 input
  1126|         0|            0|            0|  0.00%|    sys.stdin = EncodedFile(sys.stdin, 'utf-8', 'latin-1')
File: /opt/conda/lib/python3.8/_weakrefset.py
File duration: 0.000740051s (0.00%)
Line #|      Hits|         Time| Time per hit|      %|Source code
------+----------+-------------+-------------+-------+-----------
     1|         0|            0|            0|  0.00%|# Access WeakSet through the weakref module.
     2|         0|            0|            0|  0.00%|# This code is separated-out because it is needed
     3|         0|            0|            0|  0.00%|# by abc.py to load everything else at startup.
     4|         0|            0|            0|  0.00%|
     5|         0|            0|            0|  0.00%|from _weakref import ref
     6|         0|            0|            0|  0.00%|
     7|         0|            0|            0|  0.00%|__all__ = ['WeakSet']
     8|         0|            0|            0|  0.00%|
     9|         0|            0|            0|  0.00%|
    10|         0|            0|            0|  0.00%|class _IterationGuard:
    11|         0|            0|            0|  0.00%|    # This context manager registers itself in the current iterators of the
    12|         0|            0|            0|  0.00%|    # weak container, such as to delay all removals until the context manager
    13|         0|            0|            0|  0.00%|    # exits.
    14|         0|            0|            0|  0.00%|    # This technique should be relatively thread-safe (since sets are).
    15|         0|            0|            0|  0.00%|
    16|         0|            0|            0|  0.00%|    def __init__(self, weakcontainer):
    17|         0|            0|            0|  0.00%|        # Don't create cycles
    18|         0|            0|            0|  0.00%|        self.weakcontainer = ref(weakcontainer)
    19|         0|            0|            0|  0.00%|
    20|         0|            0|            0|  0.00%|    def __enter__(self):
    21|         0|            0|            0|  0.00%|        w = self.weakcontainer()
    22|         0|            0|            0|  0.00%|        if w is not None:
    23|         0|            0|            0|  0.00%|            w._iterating.add(self)
    24|         0|            0|            0|  0.00%|        return self
    25|         0|            0|            0|  0.00%|
    26|         0|            0|            0|  0.00%|    def __exit__(self, e, t, b):
    27|         0|            0|            0|  0.00%|        w = self.weakcontainer()
    28|         0|            0|            0|  0.00%|        if w is not None:
    29|         0|            0|            0|  0.00%|            s = w._iterating
    30|         0|            0|            0|  0.00%|            s.remove(self)
    31|         0|            0|            0|  0.00%|            if not s:
    32|         0|            0|            0|  0.00%|                w._commit_removals()
    33|         0|            0|            0|  0.00%|
    34|         0|            0|            0|  0.00%|
    35|         0|            0|            0|  0.00%|class WeakSet:
    36|         0|            0|            0|  0.00%|    def __init__(self, data=None):
    37|         0|            0|            0|  0.00%|        self.data = set()
    38|        11|  8.27312e-05|  7.52102e-06|  0.00%|        def _remove(item, selfref=ref(self)):
    39|        11|  6.62804e-05|  6.02549e-06|  0.00%|            self = selfref()
    40|        11|  4.41074e-05|  4.00977e-06|  0.00%|            if self is not None:
    41|        11|  4.57764e-05|  4.16149e-06|  0.00%|                if self._iterating:
    42|         0|            0|            0|  0.00%|                    self._pending_removals.append(item)
    43|         0|            0|            0|  0.00%|                else:
    44|        11|  7.12872e-05|  6.48065e-06|  0.00%|                    self.data.discard(item)
    45|         0|            0|            0|  0.00%|        self._remove = _remove
    46|         0|            0|            0|  0.00%|        # A list of keys to be removed
    47|         0|            0|            0|  0.00%|        self._pending_removals = []
    48|         0|            0|            0|  0.00%|        self._iterating = set()
    49|         0|            0|            0|  0.00%|        if data is not None:
    50|         0|            0|            0|  0.00%|            self.update(data)
    51|         0|            0|            0|  0.00%|
    52|         0|            0|            0|  0.00%|    def _commit_removals(self):
    53|         0|            0|            0|  0.00%|        l = self._pending_removals
    54|         0|            0|            0|  0.00%|        discard = self.data.discard
    55|         0|            0|            0|  0.00%|        while l:
    56|         0|            0|            0|  0.00%|            discard(l.pop())
    57|         0|            0|            0|  0.00%|
    58|         0|            0|            0|  0.00%|    def __iter__(self):
    59|         0|            0|            0|  0.00%|        with _IterationGuard(self):
    60|         0|            0|            0|  0.00%|            for itemref in self.data:
    61|         0|            0|            0|  0.00%|                item = itemref()
    62|         0|            0|            0|  0.00%|                if item is not None:
    63|         0|            0|            0|  0.00%|                    # Caveat: the iterator will keep a strong reference to
    64|         0|            0|            0|  0.00%|                    # `item` until it is resumed or closed.
    65|         0|            0|            0|  0.00%|                    yield item
    66|         0|            0|            0|  0.00%|
    67|         0|            0|            0|  0.00%|    def __len__(self):
    68|         0|            0|            0|  0.00%|        return len(self.data) - len(self._pending_removals)
    69|         0|            0|            0|  0.00%|
    70|         0|            0|            0|  0.00%|    def __contains__(self, item):
    71|         0|            0|            0|  0.00%|        try:
    72|         0|            0|            0|  0.00%|            wr = ref(item)
    73|         0|            0|            0|  0.00%|        except TypeError:
    74|         0|            0|            0|  0.00%|            return False
    75|         0|            0|            0|  0.00%|        return wr in self.data
    76|         0|            0|            0|  0.00%|
    77|         0|            0|            0|  0.00%|    def __reduce__(self):
    78|         0|            0|            0|  0.00%|        return (self.__class__, (list(self),),
    79|         0|            0|            0|  0.00%|                getattr(self, '__dict__', None))
    80|         0|            0|            0|  0.00%|
    81|        16|  0.000102997|   6.4373e-06|  0.00%|    def add(self, item):
    82|        16|  0.000108242|  6.76513e-06|  0.00%|        if self._pending_removals:
    83|         0|            0|            0|  0.00%|            self._commit_removals()
    84|        16|   0.00021863|  1.36644e-05|  0.00%|        self.data.add(ref(item, self._remove))
    85|         0|            0|            0|  0.00%|
    86|         0|            0|            0|  0.00%|    def clear(self):
    87|         0|            0|            0|  0.00%|        if self._pending_removals:
    88|         0|            0|            0|  0.00%|            self._commit_removals()
    89|         0|            0|            0|  0.00%|        self.data.clear()
    90|         0|            0|            0|  0.00%|
    91|         0|            0|            0|  0.00%|    def copy(self):
    92|         0|            0|            0|  0.00%|        return self.__class__(self)
    93|         0|            0|            0|  0.00%|
    94|         0|            0|            0|  0.00%|    def pop(self):
    95|         0|            0|            0|  0.00%|        if self._pending_removals:
    96|         0|            0|            0|  0.00%|            self._commit_removals()
    97|         0|            0|            0|  0.00%|        while True:
    98|         0|            0|            0|  0.00%|            try:
    99|         0|            0|            0|  0.00%|                itemref = self.data.pop()
   100|         0|            0|            0|  0.00%|            except KeyError:
   101|         0|            0|            0|  0.00%|                raise KeyError('pop from empty WeakSet') from None
   102|         0|            0|            0|  0.00%|            item = itemref()
   103|         0|            0|            0|  0.00%|            if item is not None:
   104|         0|            0|            0|  0.00%|                return item
   105|         0|            0|            0|  0.00%|
   106|         0|            0|            0|  0.00%|    def remove(self, item):
   107|         0|            0|            0|  0.00%|        if self._pending_removals:
   108|         0|            0|            0|  0.00%|            self._commit_removals()
   109|         0|            0|            0|  0.00%|        self.data.remove(ref(item))
   110|         0|            0|            0|  0.00%|
   111|         0|            0|            0|  0.00%|    def discard(self, item):
   112|         0|            0|            0|  0.00%|        if self._pending_removals:
   113|         0|            0|            0|  0.00%|            self._commit_removals()
   114|         0|            0|            0|  0.00%|        self.data.discard(ref(item))
   115|         0|            0|            0|  0.00%|
   116|         0|            0|            0|  0.00%|    def update(self, other):
   117|         0|            0|            0|  0.00%|        if self._pending_removals:
   118|         0|            0|            0|  0.00%|            self._commit_removals()
   119|         0|            0|            0|  0.00%|        for element in other:
   120|         0|            0|            0|  0.00%|            self.add(element)
   121|         0|            0|            0|  0.00%|
   122|         0|            0|            0|  0.00%|    def __ior__(self, other):
   123|         0|            0|            0|  0.00%|        self.update(other)
   124|         0|            0|            0|  0.00%|        return self
   125|         0|            0|            0|  0.00%|
   126|         0|            0|            0|  0.00%|    def difference(self, other):
   127|         0|            0|            0|  0.00%|        newset = self.copy()
   128|         0|            0|            0|  0.00%|        newset.difference_update(other)
   129|         0|            0|            0|  0.00%|        return newset
   130|         0|            0|            0|  0.00%|    __sub__ = difference
   131|         0|            0|            0|  0.00%|
   132|         0|            0|            0|  0.00%|    def difference_update(self, other):
   133|         0|            0|            0|  0.00%|        self.__isub__(other)
   134|         0|            0|            0|  0.00%|    def __isub__(self, other):
   135|         0|            0|            0|  0.00%|        if self._pending_removals:
   136|         0|            0|            0|  0.00%|            self._commit_removals()
   137|         0|            0|            0|  0.00%|        if self is other:
   138|         0|            0|            0|  0.00%|            self.data.clear()
   139|         0|            0|            0|  0.00%|        else:
   140|         0|            0|            0|  0.00%|            self.data.difference_update(ref(item) for item in other)
   141|         0|            0|            0|  0.00%|        return self
   142|         0|            0|            0|  0.00%|
   143|         0|            0|            0|  0.00%|    def intersection(self, other):
   144|         0|            0|            0|  0.00%|        return self.__class__(item for item in other if item in self)
   145|         0|            0|            0|  0.00%|    __and__ = intersection
   146|         0|            0|            0|  0.00%|
   147|         0|            0|            0|  0.00%|    def intersection_update(self, other):
   148|         0|            0|            0|  0.00%|        self.__iand__(other)
   149|         0|            0|            0|  0.00%|    def __iand__(self, other):
   150|         0|            0|            0|  0.00%|        if self._pending_removals:
   151|         0|            0|            0|  0.00%|            self._commit_removals()
   152|         0|            0|            0|  0.00%|        self.data.intersection_update(ref(item) for item in other)
   153|         0|            0|            0|  0.00%|        return self
   154|         0|            0|            0|  0.00%|
   155|         0|            0|            0|  0.00%|    def issubset(self, other):
   156|         0|            0|            0|  0.00%|        return self.data.issubset(ref(item) for item in other)
   157|         0|            0|            0|  0.00%|    __le__ = issubset
   158|         0|            0|            0|  0.00%|
   159|         0|            0|            0|  0.00%|    def __lt__(self, other):
   160|         0|            0|            0|  0.00%|        return self.data < set(map(ref, other))
   161|         0|            0|            0|  0.00%|
   162|         0|            0|            0|  0.00%|    def issuperset(self, other):
   163|         0|            0|            0|  0.00%|        return self.data.issuperset(ref(item) for item in other)
   164|         0|            0|            0|  0.00%|    __ge__ = issuperset
   165|         0|            0|            0|  0.00%|
   166|         0|            0|            0|  0.00%|    def __gt__(self, other):
   167|         0|            0|            0|  0.00%|        return self.data > set(map(ref, other))
   168|         0|            0|            0|  0.00%|
   169|         0|            0|            0|  0.00%|    def __eq__(self, other):
   170|         0|            0|            0|  0.00%|        if not isinstance(other, self.__class__):
   171|         0|            0|            0|  0.00%|            return NotImplemented
   172|         0|            0|            0|  0.00%|        return self.data == set(map(ref, other))
   173|         0|            0|            0|  0.00%|
   174|         0|            0|            0|  0.00%|    def symmetric_difference(self, other):
   175|         0|            0|            0|  0.00%|        newset = self.copy()
   176|         0|            0|            0|  0.00%|        newset.symmetric_difference_update(other)
   177|         0|            0|            0|  0.00%|        return newset
   178|         0|            0|            0|  0.00%|    __xor__ = symmetric_difference
   179|         0|            0|            0|  0.00%|
   180|         0|            0|            0|  0.00%|    def symmetric_difference_update(self, other):
   181|         0|            0|            0|  0.00%|        self.__ixor__(other)
   182|         0|            0|            0|  0.00%|    def __ixor__(self, other):
   183|         0|            0|            0|  0.00%|        if self._pending_removals:
   184|         0|            0|            0|  0.00%|            self._commit_removals()
   185|         0|            0|            0|  0.00%|        if self is other:
   186|         0|            0|            0|  0.00%|            self.data.clear()
   187|         0|            0|            0|  0.00%|        else:
   188|         0|            0|            0|  0.00%|            self.data.symmetric_difference_update(ref(item, self._remove) for item in other)
   189|         0|            0|            0|  0.00%|        return self
   190|         0|            0|            0|  0.00%|
   191|         0|            0|            0|  0.00%|    def union(self, other):
   192|         0|            0|            0|  0.00%|        return self.__class__(e for s in (self, other) for e in s)
   193|         0|            0|            0|  0.00%|    __or__ = union
   194|         0|            0|            0|  0.00%|
   195|         0|            0|            0|  0.00%|    def isdisjoint(self, other):
   196|         0|            0|            0|  0.00%|        return len(self.intersection(other)) == 0
   197|         0|            0|            0|  0.00%|
   198|         0|            0|            0|  0.00%|    def __repr__(self):
   199|         0|            0|            0|  0.00%|        return repr(self.data)
File: /opt/conda/lib/python3.8/tokenize.py
File duration: 0.000720739s (0.00%)
Line #|      Hits|         Time| Time per hit|      %|Source code
------+----------+-------------+-------------+-------+-----------
     1|         0|            0|            0|  0.00%|"""Tokenization help for Python programs.
     2|         0|            0|            0|  0.00%|
     3|         0|            0|            0|  0.00%|tokenize(readline) is a generator that breaks a stream of bytes into
     4|         0|            0|            0|  0.00%|Python tokens.  It decodes the bytes according to PEP-0263 for
     5|         0|            0|            0|  0.00%|determining source file encoding.
     6|         0|            0|            0|  0.00%|
     7|         0|            0|            0|  0.00%|It accepts a readline-like method which is called repeatedly to get the
     8|         0|            0|            0|  0.00%|next line of input (or b"" for EOF).  It generates 5-tuples with these
     9|         0|            0|            0|  0.00%|members:
    10|         0|            0|            0|  0.00%|
    11|         0|            0|            0|  0.00%|    the token type (see token.py)
    12|         0|            0|            0|  0.00%|    the token (a string)
    13|         0|            0|            0|  0.00%|    the starting (row, column) indices of the token (a 2-tuple of ints)
    14|         0|            0|            0|  0.00%|    the ending (row, column) indices of the token (a 2-tuple of ints)
    15|         0|            0|            0|  0.00%|    the original line (string)
    16|         0|            0|            0|  0.00%|
    17|         0|            0|            0|  0.00%|It is designed to match the working of the Python tokenizer exactly, except
    18|         0|            0|            0|  0.00%|that it produces COMMENT tokens for comments and gives type OP for all
    19|         0|            0|            0|  0.00%|operators.  Additionally, all token lists start with an ENCODING token
    20|         0|            0|            0|  0.00%|which tells you which encoding was used to decode the bytes stream.
    21|         0|            0|            0|  0.00%|"""
    22|         0|            0|            0|  0.00%|
    23|         0|            0|            0|  0.00%|__author__ = 'Ka-Ping Yee <ping@lfw.org>'
    24|         0|            0|            0|  0.00%|__credits__ = ('GvR, ESR, Tim Peters, Thomas Wouters, Fred Drake, '
    25|         0|            0|            0|  0.00%|               'Skip Montanaro, Raymond Hettinger, Trent Nelson, '
    26|         0|            0|            0|  0.00%|               'Michael Foord')
    27|         0|            0|            0|  0.00%|from builtins import open as _builtin_open
    28|         0|            0|            0|  0.00%|from codecs import lookup, BOM_UTF8
    29|         0|            0|            0|  0.00%|import collections
    30|         0|            0|            0|  0.00%|from io import TextIOWrapper
    31|         0|            0|            0|  0.00%|import itertools as _itertools
    32|         0|            0|            0|  0.00%|import re
    33|         0|            0|            0|  0.00%|import sys
    34|         0|            0|            0|  0.00%|from token import *
    35|         0|            0|            0|  0.00%|from token import EXACT_TOKEN_TYPES
    36|         0|            0|            0|  0.00%|
    37|         0|            0|            0|  0.00%|cookie_re = re.compile(r'^[ \t\f]*#.*?coding[:=][ \t]*([-\w.]+)', re.ASCII)
    38|         0|            0|            0|  0.00%|blank_re = re.compile(br'^[ \t\f]*(?:[#\r\n]|$)', re.ASCII)
    39|         0|            0|            0|  0.00%|
    40|         0|            0|            0|  0.00%|import token
    41|         0|            0|            0|  0.00%|__all__ = token.__all__ + ["tokenize", "generate_tokens", "detect_encoding",
    42|         0|            0|            0|  0.00%|                           "untokenize", "TokenInfo"]
    43|         0|            0|            0|  0.00%|del token
    44|         0|            0|            0|  0.00%|
    45|         0|            0|            0|  0.00%|class TokenInfo(collections.namedtuple('TokenInfo', 'type string start end line')):
    46|         0|            0|            0|  0.00%|    def __repr__(self):
    47|         0|            0|            0|  0.00%|        annotated_type = '%d (%s)' % (self.type, tok_name[self.type])
    48|         0|            0|            0|  0.00%|        return ('TokenInfo(type=%s, string=%r, start=%r, end=%r, line=%r)' %
    49|         0|            0|            0|  0.00%|                self._replace(type=annotated_type))
    50|         0|            0|            0|  0.00%|
    51|         0|            0|            0|  0.00%|    @property
    52|         0|            0|            0|  0.00%|    def exact_type(self):
    53|         0|            0|            0|  0.00%|        if self.type == OP and self.string in EXACT_TOKEN_TYPES:
    54|         0|            0|            0|  0.00%|            return EXACT_TOKEN_TYPES[self.string]
    55|         0|            0|            0|  0.00%|        else:
    56|         0|            0|            0|  0.00%|            return self.type
    57|         0|            0|            0|  0.00%|
    58|         0|            0|            0|  0.00%|def group(*choices): return '(' + '|'.join(choices) + ')'
    59|         0|            0|            0|  0.00%|def any(*choices): return group(*choices) + '*'
    60|         0|            0|            0|  0.00%|def maybe(*choices): return group(*choices) + '?'
    61|         0|            0|            0|  0.00%|
    62|         0|            0|            0|  0.00%|# Note: we use unicode matching for names ("\w") but ascii matching for
    63|         0|            0|            0|  0.00%|# number literals.
    64|         0|            0|            0|  0.00%|Whitespace = r'[ \f\t]*'
    65|         0|            0|            0|  0.00%|Comment = r'#[^\r\n]*'
    66|         0|            0|            0|  0.00%|Ignore = Whitespace + any(r'\\\r?\n' + Whitespace) + maybe(Comment)
    67|         0|            0|            0|  0.00%|Name = r'\w+'
    68|         0|            0|            0|  0.00%|
    69|         0|            0|            0|  0.00%|Hexnumber = r'0[xX](?:_?[0-9a-fA-F])+'
    70|         0|            0|            0|  0.00%|Binnumber = r'0[bB](?:_?[01])+'
    71|         0|            0|            0|  0.00%|Octnumber = r'0[oO](?:_?[0-7])+'
    72|         0|            0|            0|  0.00%|Decnumber = r'(?:0(?:_?0)*|[1-9](?:_?[0-9])*)'
    73|         0|            0|            0|  0.00%|Intnumber = group(Hexnumber, Binnumber, Octnumber, Decnumber)
    74|         0|            0|            0|  0.00%|Exponent = r'[eE][-+]?[0-9](?:_?[0-9])*'
    75|         0|            0|            0|  0.00%|Pointfloat = group(r'[0-9](?:_?[0-9])*\.(?:[0-9](?:_?[0-9])*)?',
    76|         0|            0|            0|  0.00%|                   r'\.[0-9](?:_?[0-9])*') + maybe(Exponent)
    77|         0|            0|            0|  0.00%|Expfloat = r'[0-9](?:_?[0-9])*' + Exponent
    78|         0|            0|            0|  0.00%|Floatnumber = group(Pointfloat, Expfloat)
    79|         0|            0|            0|  0.00%|Imagnumber = group(r'[0-9](?:_?[0-9])*[jJ]', Floatnumber + r'[jJ]')
    80|         0|            0|            0|  0.00%|Number = group(Imagnumber, Floatnumber, Intnumber)
    81|         0|            0|            0|  0.00%|
    82|         0|            0|            0|  0.00%|# Return the empty string, plus all of the valid string prefixes.
    83|         0|            0|            0|  0.00%|def _all_string_prefixes():
    84|         0|            0|            0|  0.00%|    # The valid string prefixes. Only contain the lower case versions,
    85|         0|            0|            0|  0.00%|    #  and don't contain any permutations (include 'fr', but not
    86|         0|            0|            0|  0.00%|    #  'rf'). The various permutations will be generated.
    87|         0|            0|            0|  0.00%|    _valid_string_prefixes = ['b', 'r', 'u', 'f', 'br', 'fr']
    88|         0|            0|            0|  0.00%|    # if we add binary f-strings, add: ['fb', 'fbr']
    89|         0|            0|            0|  0.00%|    result = {''}
    90|         0|            0|            0|  0.00%|    for prefix in _valid_string_prefixes:
    91|         0|            0|            0|  0.00%|        for t in _itertools.permutations(prefix):
    92|         0|            0|            0|  0.00%|            # create a list with upper and lower versions of each
    93|         0|            0|            0|  0.00%|            #  character
    94|         0|            0|            0|  0.00%|            for u in _itertools.product(*[(c, c.upper()) for c in t]):
    95|         0|            0|            0|  0.00%|                result.add(''.join(u))
    96|         0|            0|            0|  0.00%|    return result
    97|         0|            0|            0|  0.00%|
    98|         0|            0|            0|  0.00%|def _compile(expr):
    99|         0|            0|            0|  0.00%|    return re.compile(expr, re.UNICODE)
   100|         0|            0|            0|  0.00%|
   101|         0|            0|            0|  0.00%|# Note that since _all_string_prefixes includes the empty string,
   102|         0|            0|            0|  0.00%|#  StringPrefix can be the empty string (making it optional).
   103|         0|            0|            0|  0.00%|StringPrefix = group(*_all_string_prefixes())
   104|         0|            0|            0|  0.00%|
   105|         0|            0|            0|  0.00%|# Tail end of ' string.
   106|         0|            0|            0|  0.00%|Single = r"[^'\\]*(?:\\.[^'\\]*)*'"
   107|         0|            0|            0|  0.00%|# Tail end of " string.
   108|         0|            0|            0|  0.00%|Double = r'[^"\\]*(?:\\.[^"\\]*)*"'
   109|         0|            0|            0|  0.00%|# Tail end of ''' string.
   110|         0|            0|            0|  0.00%|Single3 = r"[^'\\]*(?:(?:\\.|'(?!''))[^'\\]*)*'''"
   111|         0|            0|            0|  0.00%|# Tail end of """ string.
   112|         0|            0|            0|  0.00%|Double3 = r'[^"\\]*(?:(?:\\.|"(?!""))[^"\\]*)*"""'
   113|         0|            0|            0|  0.00%|Triple = group(StringPrefix + "'''", StringPrefix + '"""')
   114|         0|            0|            0|  0.00%|# Single-line ' or " string.
   115|         0|            0|            0|  0.00%|String = group(StringPrefix + r"'[^\n'\\]*(?:\\.[^\n'\\]*)*'",
   116|         0|            0|            0|  0.00%|               StringPrefix + r'"[^\n"\\]*(?:\\.[^\n"\\]*)*"')
   117|         0|            0|            0|  0.00%|
   118|         0|            0|            0|  0.00%|# Sorting in reverse order puts the long operators before their prefixes.
   119|         0|            0|            0|  0.00%|# Otherwise if = came before ==, == would get recognized as two instances
   120|         0|            0|            0|  0.00%|# of =.
   121|         0|            0|            0|  0.00%|Special = group(*map(re.escape, sorted(EXACT_TOKEN_TYPES, reverse=True)))
   122|         0|            0|            0|  0.00%|Funny = group(r'\r?\n', Special)
   123|         0|            0|            0|  0.00%|
   124|         0|            0|            0|  0.00%|PlainToken = group(Number, Funny, String, Name)
   125|         0|            0|            0|  0.00%|Token = Ignore + PlainToken
   126|         0|            0|            0|  0.00%|
   127|         0|            0|            0|  0.00%|# First (or only) line of ' or " string.
   128|         0|            0|            0|  0.00%|ContStr = group(StringPrefix + r"'[^\n'\\]*(?:\\.[^\n'\\]*)*" +
   129|         0|            0|            0|  0.00%|                group("'", r'\\\r?\n'),
   130|         0|            0|            0|  0.00%|                StringPrefix + r'"[^\n"\\]*(?:\\.[^\n"\\]*)*' +
   131|         0|            0|            0|  0.00%|                group('"', r'\\\r?\n'))
   132|         0|            0|            0|  0.00%|PseudoExtras = group(r'\\\r?\n|\Z', Comment, Triple)
   133|         0|            0|            0|  0.00%|PseudoToken = Whitespace + group(PseudoExtras, Number, Funny, ContStr, Name)
   134|         0|            0|            0|  0.00%|
   135|         0|            0|            0|  0.00%|# For a given string prefix plus quotes, endpats maps it to a regex
   136|         0|            0|            0|  0.00%|#  to match the remainder of that string. _prefix can be empty, for
   137|         0|            0|            0|  0.00%|#  a normal single or triple quoted string (with no prefix).
   138|         0|            0|            0|  0.00%|endpats = {}
   139|         0|            0|            0|  0.00%|for _prefix in _all_string_prefixes():
   140|         0|            0|            0|  0.00%|    endpats[_prefix + "'"] = Single
   141|         0|            0|            0|  0.00%|    endpats[_prefix + '"'] = Double
   142|         0|            0|            0|  0.00%|    endpats[_prefix + "'''"] = Single3
   143|         0|            0|            0|  0.00%|    endpats[_prefix + '"""'] = Double3
   144|         0|            0|            0|  0.00%|
   145|         0|            0|            0|  0.00%|# A set of all of the single and triple quoted string prefixes,
   146|         0|            0|            0|  0.00%|#  including the opening quotes.
   147|         0|            0|            0|  0.00%|single_quoted = set()
   148|         0|            0|            0|  0.00%|triple_quoted = set()
   149|         0|            0|            0|  0.00%|for t in _all_string_prefixes():
   150|         0|            0|            0|  0.00%|    for u in (t + '"', t + "'"):
   151|         0|            0|            0|  0.00%|        single_quoted.add(u)
   152|         0|            0|            0|  0.00%|    for u in (t + '"""', t + "'''"):
   153|         0|            0|            0|  0.00%|        triple_quoted.add(u)
   154|         0|            0|            0|  0.00%|
   155|         0|            0|            0|  0.00%|tabsize = 8
   156|         0|            0|            0|  0.00%|
   157|         0|            0|            0|  0.00%|class TokenError(Exception): pass
   158|         0|            0|            0|  0.00%|
   159|         0|            0|            0|  0.00%|class StopTokenizing(Exception): pass
   160|         0|            0|            0|  0.00%|
   161|         0|            0|            0|  0.00%|
   162|         0|            0|            0|  0.00%|class Untokenizer:
   163|         0|            0|            0|  0.00%|
   164|         0|            0|            0|  0.00%|    def __init__(self):
   165|         0|            0|            0|  0.00%|        self.tokens = []
   166|         0|            0|            0|  0.00%|        self.prev_row = 1
   167|         0|            0|            0|  0.00%|        self.prev_col = 0
   168|         0|            0|            0|  0.00%|        self.encoding = None
   169|         0|            0|            0|  0.00%|
   170|         0|            0|            0|  0.00%|    def add_whitespace(self, start):
   171|         0|            0|            0|  0.00%|        row, col = start
   172|         0|            0|            0|  0.00%|        if row < self.prev_row or row == self.prev_row and col < self.prev_col:
   173|         0|            0|            0|  0.00%|            raise ValueError("start ({},{}) precedes previous end ({},{})"
   174|         0|            0|            0|  0.00%|                             .format(row, col, self.prev_row, self.prev_col))
   175|         0|            0|            0|  0.00%|        row_offset = row - self.prev_row
   176|         0|            0|            0|  0.00%|        if row_offset:
   177|         0|            0|            0|  0.00%|            self.tokens.append("\\\n" * row_offset)
   178|         0|            0|            0|  0.00%|            self.prev_col = 0
   179|         0|            0|            0|  0.00%|        col_offset = col - self.prev_col
   180|         0|            0|            0|  0.00%|        if col_offset:
   181|         0|            0|            0|  0.00%|            self.tokens.append(" " * col_offset)
   182|         0|            0|            0|  0.00%|
   183|         0|            0|            0|  0.00%|    def untokenize(self, iterable):
   184|         0|            0|            0|  0.00%|        it = iter(iterable)
   185|         0|            0|            0|  0.00%|        indents = []
   186|         0|            0|            0|  0.00%|        startline = False
   187|         0|            0|            0|  0.00%|        for t in it:
   188|         0|            0|            0|  0.00%|            if len(t) == 2:
   189|         0|            0|            0|  0.00%|                self.compat(t, it)
   190|         0|            0|            0|  0.00%|                break
   191|         0|            0|            0|  0.00%|            tok_type, token, start, end, line = t
   192|         0|            0|            0|  0.00%|            if tok_type == ENCODING:
   193|         0|            0|            0|  0.00%|                self.encoding = token
   194|         0|            0|            0|  0.00%|                continue
   195|         0|            0|            0|  0.00%|            if tok_type == ENDMARKER:
   196|         0|            0|            0|  0.00%|                break
   197|         0|            0|            0|  0.00%|            if tok_type == INDENT:
   198|         0|            0|            0|  0.00%|                indents.append(token)
   199|         0|            0|            0|  0.00%|                continue
   200|         0|            0|            0|  0.00%|            elif tok_type == DEDENT:
   201|         0|            0|            0|  0.00%|                indents.pop()
   202|         0|            0|            0|  0.00%|                self.prev_row, self.prev_col = end
   203|         0|            0|            0|  0.00%|                continue
   204|         0|            0|            0|  0.00%|            elif tok_type in (NEWLINE, NL):
   205|         0|            0|            0|  0.00%|                startline = True
   206|         0|            0|            0|  0.00%|            elif startline and indents:
   207|         0|            0|            0|  0.00%|                indent = indents[-1]
   208|         0|            0|            0|  0.00%|                if start[1] >= len(indent):
   209|         0|            0|            0|  0.00%|                    self.tokens.append(indent)
   210|         0|            0|            0|  0.00%|                    self.prev_col = len(indent)
   211|         0|            0|            0|  0.00%|                startline = False
   212|         0|            0|            0|  0.00%|            self.add_whitespace(start)
   213|         0|            0|            0|  0.00%|            self.tokens.append(token)
   214|         0|            0|            0|  0.00%|            self.prev_row, self.prev_col = end
   215|         0|            0|            0|  0.00%|            if tok_type in (NEWLINE, NL):
   216|         0|            0|            0|  0.00%|                self.prev_row += 1
   217|         0|            0|            0|  0.00%|                self.prev_col = 0
   218|         0|            0|            0|  0.00%|        return "".join(self.tokens)
   219|         0|            0|            0|  0.00%|
   220|         0|            0|            0|  0.00%|    def compat(self, token, iterable):
   221|         0|            0|            0|  0.00%|        indents = []
   222|         0|            0|            0|  0.00%|        toks_append = self.tokens.append
   223|         0|            0|            0|  0.00%|        startline = token[0] in (NEWLINE, NL)
   224|         0|            0|            0|  0.00%|        prevstring = False
   225|         0|            0|            0|  0.00%|
   226|         0|            0|            0|  0.00%|        for tok in _itertools.chain([token], iterable):
   227|         0|            0|            0|  0.00%|            toknum, tokval = tok[:2]
   228|         0|            0|            0|  0.00%|            if toknum == ENCODING:
   229|         0|            0|            0|  0.00%|                self.encoding = tokval
   230|         0|            0|            0|  0.00%|                continue
   231|         0|            0|            0|  0.00%|
   232|         0|            0|            0|  0.00%|            if toknum in (NAME, NUMBER):
   233|         0|            0|            0|  0.00%|                tokval += ' '
   234|         0|            0|            0|  0.00%|
   235|         0|            0|            0|  0.00%|            # Insert a space between two consecutive strings
   236|         0|            0|            0|  0.00%|            if toknum == STRING:
   237|         0|            0|            0|  0.00%|                if prevstring:
   238|         0|            0|            0|  0.00%|                    tokval = ' ' + tokval
   239|         0|            0|            0|  0.00%|                prevstring = True
   240|         0|            0|            0|  0.00%|            else:
   241|         0|            0|            0|  0.00%|                prevstring = False
   242|         0|            0|            0|  0.00%|
   243|         0|            0|            0|  0.00%|            if toknum == INDENT:
   244|         0|            0|            0|  0.00%|                indents.append(tokval)
   245|         0|            0|            0|  0.00%|                continue
   246|         0|            0|            0|  0.00%|            elif toknum == DEDENT:
   247|         0|            0|            0|  0.00%|                indents.pop()
   248|         0|            0|            0|  0.00%|                continue
   249|         0|            0|            0|  0.00%|            elif toknum in (NEWLINE, NL):
   250|         0|            0|            0|  0.00%|                startline = True
   251|         0|            0|            0|  0.00%|            elif startline and indents:
   252|         0|            0|            0|  0.00%|                toks_append(indents[-1])
   253|         0|            0|            0|  0.00%|                startline = False
   254|         0|            0|            0|  0.00%|            toks_append(tokval)
   255|         0|            0|            0|  0.00%|
   256|         0|            0|            0|  0.00%|
   257|         0|            0|            0|  0.00%|def untokenize(iterable):
   258|         0|            0|            0|  0.00%|    """Transform tokens back into Python source code.
   259|         0|            0|            0|  0.00%|    It returns a bytes object, encoded using the ENCODING
   260|         0|            0|            0|  0.00%|    token, which is the first token sequence output by tokenize.
   261|         0|            0|            0|  0.00%|
   262|         0|            0|            0|  0.00%|    Each element returned by the iterable must be a token sequence
   263|         0|            0|            0|  0.00%|    with at least two elements, a token number and token value.  If
   264|         0|            0|            0|  0.00%|    only two tokens are passed, the resulting output is poor.
   265|         0|            0|            0|  0.00%|
   266|         0|            0|            0|  0.00%|    Round-trip invariant for full input:
   267|         0|            0|            0|  0.00%|        Untokenized source will match input source exactly
   268|         0|            0|            0|  0.00%|
   269|         0|            0|            0|  0.00%|    Round-trip invariant for limited input:
   270|         0|            0|            0|  0.00%|        # Output bytes will tokenize back to the input
   271|         0|            0|            0|  0.00%|        t1 = [tok[:2] for tok in tokenize(f.readline)]
   272|         0|            0|            0|  0.00%|        newcode = untokenize(t1)
   273|         0|            0|            0|  0.00%|        readline = BytesIO(newcode).readline
   274|         0|            0|            0|  0.00%|        t2 = [tok[:2] for tok in tokenize(readline)]
   275|         0|            0|            0|  0.00%|        assert t1 == t2
   276|         0|            0|            0|  0.00%|    """
   277|         0|            0|            0|  0.00%|    ut = Untokenizer()
   278|         0|            0|            0|  0.00%|    out = ut.untokenize(iterable)
   279|         0|            0|            0|  0.00%|    if ut.encoding is not None:
   280|         0|            0|            0|  0.00%|        out = out.encode(ut.encoding)
   281|         0|            0|            0|  0.00%|    return out
   282|         0|            0|            0|  0.00%|
   283|         0|            0|            0|  0.00%|
   284|         0|            0|            0|  0.00%|def _get_normal_name(orig_enc):
   285|         0|            0|            0|  0.00%|    """Imitates get_normal_name in tokenizer.c."""
   286|         0|            0|            0|  0.00%|    # Only care about the first 12 characters.
   287|         0|            0|            0|  0.00%|    enc = orig_enc[:12].lower().replace("_", "-")
   288|         0|            0|            0|  0.00%|    if enc == "utf-8" or enc.startswith("utf-8-"):
   289|         0|            0|            0|  0.00%|        return "utf-8"
   290|         0|            0|            0|  0.00%|    if enc in ("latin-1", "iso-8859-1", "iso-latin-1") or \
   291|         0|            0|            0|  0.00%|       enc.startswith(("latin-1-", "iso-8859-1-", "iso-latin-1-")):
   292|         0|            0|            0|  0.00%|        return "iso-8859-1"
   293|         0|            0|            0|  0.00%|    return orig_enc
   294|         0|            0|            0|  0.00%|
   295|         1|  5.88894e-05|  5.88894e-05|  0.00%|def detect_encoding(readline):
   296|         0|            0|            0|  0.00%|    """
   297|         0|            0|            0|  0.00%|    The detect_encoding() function is used to detect the encoding that should
   298|         0|            0|            0|  0.00%|    be used to decode a Python source file.  It requires one argument, readline,
   299|         0|            0|            0|  0.00%|    in the same way as the tokenize() generator.
   300|         0|            0|            0|  0.00%|
   301|         0|            0|            0|  0.00%|    It will call readline a maximum of twice, and return the encoding used
   302|         0|            0|            0|  0.00%|    (as a string) and a list of any lines (left as bytes) it has read in.
   303|         0|            0|            0|  0.00%|
   304|         0|            0|            0|  0.00%|    It detects the encoding from the presence of a utf-8 bom or an encoding
   305|         0|            0|            0|  0.00%|    cookie as specified in pep-0263.  If both a bom and a cookie are present,
   306|         0|            0|            0|  0.00%|    but disagree, a SyntaxError will be raised.  If the encoding cookie is an
   307|         0|            0|            0|  0.00%|    invalid charset, raise a SyntaxError.  Note that if a utf-8 bom is found,
   308|         0|            0|            0|  0.00%|    'utf-8-sig' is returned.
   309|         0|            0|            0|  0.00%|
   310|         0|            0|            0|  0.00%|    If no encoding is specified, then the default of 'utf-8' will be returned.
   311|         0|            0|            0|  0.00%|    """
   312|         1|  1.07288e-05|  1.07288e-05|  0.00%|    try:
   313|         1|  8.82149e-06|  8.82149e-06|  0.00%|        filename = readline.__self__.name
   314|         0|            0|            0|  0.00%|    except AttributeError:
   315|         0|            0|            0|  0.00%|        filename = None
   316|         1|  7.86781e-06|  7.86781e-06|  0.00%|    bom_found = False
   317|         1|  7.39098e-06|  7.39098e-06|  0.00%|    encoding = None
   318|         1|  6.91414e-06|  6.91414e-06|  0.00%|    default = 'utf-8'
   319|         2|  1.57356e-05|  7.86781e-06|  0.00%|    def read_or_stop():
   320|         1|  5.72205e-06|  5.72205e-06|  0.00%|        try:
   321|         1|  4.05312e-05|  4.05312e-05|  0.00%|            return readline()
   322|         0|            0|            0|  0.00%|        except StopIteration:
   323|         0|            0|            0|  0.00%|            return b''
   324|         0|            0|            0|  0.00%|
   325|         2|  1.71661e-05|  8.58307e-06|  0.00%|    def find_cookie(line):
   326|         1|  6.91414e-06|  6.91414e-06|  0.00%|        try:
   327|         0|            0|            0|  0.00%|            # Decode as UTF-8. Either the line is an encoding declaration,
   328|         0|            0|            0|  0.00%|            # in which case it should be pure ASCII, or it must be UTF-8
   329|         0|            0|            0|  0.00%|            # per default encoding.
   330|         1|  1.40667e-05|  1.40667e-05|  0.00%|            line_string = line.decode('utf-8')
   331|         0|            0|            0|  0.00%|        except UnicodeDecodeError:
   332|         0|            0|            0|  0.00%|            msg = "invalid or missing encoding declaration"
   333|         0|            0|            0|  0.00%|            if filename is not None:
   334|         0|            0|            0|  0.00%|                msg = '{} for {!r}'.format(msg, filename)
   335|         0|            0|            0|  0.00%|            raise SyntaxError(msg)
   336|         0|            0|            0|  0.00%|
   337|         1|  1.54972e-05|  1.54972e-05|  0.00%|        match = cookie_re.match(line_string)
   338|         1|  7.39098e-06|  7.39098e-06|  0.00%|        if not match:
   339|         1|  6.91414e-06|  6.91414e-06|  0.00%|            return None
   340|         0|            0|            0|  0.00%|        encoding = _get_normal_name(match.group(1))
   341|         0|            0|            0|  0.00%|        try:
   342|         0|            0|            0|  0.00%|            codec = lookup(encoding)
   343|         0|            0|            0|  0.00%|        except LookupError:
   344|         0|            0|            0|  0.00%|            # This behaviour mimics the Python interpreter
   345|         0|            0|            0|  0.00%|            if filename is None:
   346|         0|            0|            0|  0.00%|                msg = "unknown encoding: " + encoding
   347|         0|            0|            0|  0.00%|            else:
   348|         0|            0|            0|  0.00%|                msg = "unknown encoding for {!r}: {}".format(filename,
   349|         0|            0|            0|  0.00%|                        encoding)
   350|         0|            0|            0|  0.00%|            raise SyntaxError(msg)
   351|         0|            0|            0|  0.00%|
   352|         0|            0|            0|  0.00%|        if bom_found:
   353|         0|            0|            0|  0.00%|            if encoding != 'utf-8':
   354|         0|            0|            0|  0.00%|                # This behaviour mimics the Python interpreter
   355|         0|            0|            0|  0.00%|                if filename is None:
   356|         0|            0|            0|  0.00%|                    msg = 'encoding problem: utf-8'
   357|         0|            0|            0|  0.00%|                else:
   358|         0|            0|            0|  0.00%|                    msg = 'encoding problem for {!r}: utf-8'.format(filename)
   359|         0|            0|            0|  0.00%|                raise SyntaxError(msg)
   360|         0|            0|            0|  0.00%|            encoding += '-sig'
   361|         0|            0|            0|  0.00%|        return encoding
   362|         0|            0|            0|  0.00%|
   363|         1|  4.95911e-05|  4.95911e-05|  0.00%|    first = read_or_stop()
(call)|         1|  5.43594e-05|  5.43594e-05|  0.00%|# /opt/conda/lib/python3.8/tokenize.py:319 read_or_stop
   364|         1|  3.14713e-05|  3.14713e-05|  0.00%|    if first.startswith(BOM_UTF8):
   365|         0|            0|            0|  0.00%|        bom_found = True
   366|         0|            0|            0|  0.00%|        first = first[3:]
   367|         0|            0|            0|  0.00%|        default = 'utf-8-sig'
   368|         1|  9.77516e-06|  9.77516e-06|  0.00%|    if not first:
   369|         0|            0|            0|  0.00%|        return default, []
   370|         0|            0|            0|  0.00%|
   371|         1|   3.6478e-05|   3.6478e-05|  0.00%|    encoding = find_cookie(first)
(call)|         1|  6.07967e-05|  6.07967e-05|  0.00%|# /opt/conda/lib/python3.8/tokenize.py:325 find_cookie
   372|         1|  9.77516e-06|  9.77516e-06|  0.00%|    if encoding:
   373|         0|            0|            0|  0.00%|        return encoding, [first]
   374|         1|  1.35899e-05|  1.35899e-05|  0.00%|    if not blank_re.match(first):
   375|         1|  8.34465e-06|  8.34465e-06|  0.00%|        return default, [first]
   376|         0|            0|            0|  0.00%|
   377|         0|            0|            0|  0.00%|    second = read_or_stop()
   378|         0|            0|            0|  0.00%|    if not second:
   379|         0|            0|            0|  0.00%|        return default, [first]
   380|         0|            0|            0|  0.00%|
   381|         0|            0|            0|  0.00%|    encoding = find_cookie(second)
   382|         0|            0|            0|  0.00%|    if encoding:
   383|         0|            0|            0|  0.00%|        return encoding, [first, second]
   384|         0|            0|            0|  0.00%|
   385|         0|            0|            0|  0.00%|    return default, [first, second]
   386|         0|            0|            0|  0.00%|
   387|         0|            0|            0|  0.00%|
   388|         1|  1.12057e-05|  1.12057e-05|  0.00%|def open(filename):
   389|         0|            0|            0|  0.00%|    """Open a file in read only mode using the encoding detected by
   390|         0|            0|            0|  0.00%|    detect_encoding().
   391|         0|            0|            0|  0.00%|    """
   392|         1|  7.62939e-05|  7.62939e-05|  0.00%|    buffer = _builtin_open(filename, 'rb')
   393|         1|   1.0252e-05|   1.0252e-05|  0.00%|    try:
   394|         1|  9.77516e-05|  9.77516e-05|  0.00%|        encoding, lines = detect_encoding(buffer.readline)
(call)|         1|  0.000389576|  0.000389576|  0.00%|# /opt/conda/lib/python3.8/tokenize.py:295 detect_encoding
   395|         1|  1.38283e-05|  1.38283e-05|  0.00%|        buffer.seek(0)
   396|         1|  0.000102282|  0.000102282|  0.00%|        text = TextIOWrapper(buffer, encoding, line_buffering=True)
(call)|         1|  0.000108004|  0.000108004|  0.00%|# /opt/conda/lib/python3.8/codecs.py:309 __init__
   397|         1|  1.28746e-05|  1.28746e-05|  0.00%|        text.mode = 'r'
   398|         1|  6.67572e-06|  6.67572e-06|  0.00%|        return text
   399|         0|            0|            0|  0.00%|    except:
   400|         0|            0|            0|  0.00%|        buffer.close()
   401|         0|            0|            0|  0.00%|        raise
   402|         0|            0|            0|  0.00%|
   403|         0|            0|            0|  0.00%|
   404|         0|            0|            0|  0.00%|def tokenize(readline):
   405|         0|            0|            0|  0.00%|    """
   406|         0|            0|            0|  0.00%|    The tokenize() generator requires one argument, readline, which
   407|         0|            0|            0|  0.00%|    must be a callable object which provides the same interface as the
   408|         0|            0|            0|  0.00%|    readline() method of built-in file objects.  Each call to the function
   409|         0|            0|            0|  0.00%|    should return one line of input as bytes.  Alternatively, readline
   410|         0|            0|            0|  0.00%|    can be a callable function terminating with StopIteration:
   411|         0|            0|            0|  0.00%|        readline = open(myfile, 'rb').__next__  # Example of alternate readline
   412|         0|            0|            0|  0.00%|
   413|         0|            0|            0|  0.00%|    The generator produces 5-tuples with these members: the token type; the
   414|         0|            0|            0|  0.00%|    token string; a 2-tuple (srow, scol) of ints specifying the row and
   415|         0|            0|            0|  0.00%|    column where the token begins in the source; a 2-tuple (erow, ecol) of
   416|         0|            0|            0|  0.00%|    ints specifying the row and column where the token ends in the source;
   417|         0|            0|            0|  0.00%|    and the line on which the token was found.  The line passed is the
   418|         0|            0|            0|  0.00%|    physical line.
   419|         0|            0|            0|  0.00%|
   420|         0|            0|            0|  0.00%|    The first token sequence will always be an ENCODING token
   421|         0|            0|            0|  0.00%|    which tells you which encoding was used to decode the bytes stream.
   422|         0|            0|            0|  0.00%|    """
   423|         0|            0|            0|  0.00%|    encoding, consumed = detect_encoding(readline)
   424|         0|            0|            0|  0.00%|    empty = _itertools.repeat(b"")
   425|         0|            0|            0|  0.00%|    rl_gen = _itertools.chain(consumed, iter(readline, b""), empty)
   426|         0|            0|            0|  0.00%|    return _tokenize(rl_gen.__next__, encoding)
   427|         0|            0|            0|  0.00%|
   428|         0|            0|            0|  0.00%|
   429|         0|            0|            0|  0.00%|def _tokenize(readline, encoding):
   430|         0|            0|            0|  0.00%|    lnum = parenlev = continued = 0
   431|         0|            0|            0|  0.00%|    numchars = '0123456789'
   432|         0|            0|            0|  0.00%|    contstr, needcont = '', 0
   433|         0|            0|            0|  0.00%|    contline = None
   434|         0|            0|            0|  0.00%|    indents = [0]
   435|         0|            0|            0|  0.00%|
   436|         0|            0|            0|  0.00%|    if encoding is not None:
   437|         0|            0|            0|  0.00%|        if encoding == "utf-8-sig":
   438|         0|            0|            0|  0.00%|            # BOM will already have been stripped.
   439|         0|            0|            0|  0.00%|            encoding = "utf-8"
   440|         0|            0|            0|  0.00%|        yield TokenInfo(ENCODING, encoding, (0, 0), (0, 0), '')
   441|         0|            0|            0|  0.00%|    last_line = b''
   442|         0|            0|            0|  0.00%|    line = b''
   443|         0|            0|            0|  0.00%|    while True:                                # loop over lines in stream
   444|         0|            0|            0|  0.00%|        try:
   445|         0|            0|            0|  0.00%|            # We capture the value of the line variable here because
   446|         0|            0|            0|  0.00%|            # readline uses the empty string '' to signal end of input,
   447|         0|            0|            0|  0.00%|            # hence `line` itself will always be overwritten at the end
   448|         0|            0|            0|  0.00%|            # of this loop.
   449|         0|            0|            0|  0.00%|            last_line = line
   450|         0|            0|            0|  0.00%|            line = readline()
   451|         0|            0|            0|  0.00%|        except StopIteration:
   452|         0|            0|            0|  0.00%|            line = b''
   453|         0|            0|            0|  0.00%|
   454|         0|            0|            0|  0.00%|        if encoding is not None:
   455|         0|            0|            0|  0.00%|            line = line.decode(encoding)
   456|         0|            0|            0|  0.00%|        lnum += 1
   457|         0|            0|            0|  0.00%|        pos, max = 0, len(line)
   458|         0|            0|            0|  0.00%|
   459|         0|            0|            0|  0.00%|        if contstr:                            # continued string
   460|         0|            0|            0|  0.00%|            if not line:
   461|         0|            0|            0|  0.00%|                raise TokenError("EOF in multi-line string", strstart)
   462|         0|            0|            0|  0.00%|            endmatch = endprog.match(line)
   463|         0|            0|            0|  0.00%|            if endmatch:
   464|         0|            0|            0|  0.00%|                pos = end = endmatch.end(0)
   465|         0|            0|            0|  0.00%|                yield TokenInfo(STRING, contstr + line[:end],
   466|         0|            0|            0|  0.00%|                       strstart, (lnum, end), contline + line)
   467|         0|            0|            0|  0.00%|                contstr, needcont = '', 0
   468|         0|            0|            0|  0.00%|                contline = None
   469|         0|            0|            0|  0.00%|            elif needcont and line[-2:] != '\\\n' and line[-3:] != '\\\r\n':
   470|         0|            0|            0|  0.00%|                yield TokenInfo(ERRORTOKEN, contstr + line,
   471|         0|            0|            0|  0.00%|                           strstart, (lnum, len(line)), contline)
   472|         0|            0|            0|  0.00%|                contstr = ''
   473|         0|            0|            0|  0.00%|                contline = None
   474|         0|            0|            0|  0.00%|                continue
   475|         0|            0|            0|  0.00%|            else:
   476|         0|            0|            0|  0.00%|                contstr = contstr + line
   477|         0|            0|            0|  0.00%|                contline = contline + line
   478|         0|            0|            0|  0.00%|                continue
   479|         0|            0|            0|  0.00%|
   480|         0|            0|            0|  0.00%|        elif parenlev == 0 and not continued:  # new statement
   481|         0|            0|            0|  0.00%|            if not line: break
   482|         0|            0|            0|  0.00%|            column = 0
   483|         0|            0|            0|  0.00%|            while pos < max:                   # measure leading whitespace
   484|         0|            0|            0|  0.00%|                if line[pos] == ' ':
   485|         0|            0|            0|  0.00%|                    column += 1
   486|         0|            0|            0|  0.00%|                elif line[pos] == '\t':
   487|         0|            0|            0|  0.00%|                    column = (column//tabsize + 1)*tabsize
   488|         0|            0|            0|  0.00%|                elif line[pos] == '\f':
   489|         0|            0|            0|  0.00%|                    column = 0
   490|         0|            0|            0|  0.00%|                else:
   491|         0|            0|            0|  0.00%|                    break
   492|         0|            0|            0|  0.00%|                pos += 1
   493|         0|            0|            0|  0.00%|            if pos == max:
   494|         0|            0|            0|  0.00%|                break
   495|         0|            0|            0|  0.00%|
   496|         0|            0|            0|  0.00%|            if line[pos] in '#\r\n':           # skip comments or blank lines
   497|         0|            0|            0|  0.00%|                if line[pos] == '#':
   498|         0|            0|            0|  0.00%|                    comment_token = line[pos:].rstrip('\r\n')
   499|         0|            0|            0|  0.00%|                    yield TokenInfo(COMMENT, comment_token,
   500|         0|            0|            0|  0.00%|                           (lnum, pos), (lnum, pos + len(comment_token)), line)
   501|         0|            0|            0|  0.00%|                    pos += len(comment_token)
   502|         0|            0|            0|  0.00%|
   503|         0|            0|            0|  0.00%|                yield TokenInfo(NL, line[pos:],
   504|         0|            0|            0|  0.00%|                           (lnum, pos), (lnum, len(line)), line)
   505|         0|            0|            0|  0.00%|                continue
   506|         0|            0|            0|  0.00%|
   507|         0|            0|            0|  0.00%|            if column > indents[-1]:           # count indents or dedents
   508|         0|            0|            0|  0.00%|                indents.append(column)
   509|         0|            0|            0|  0.00%|                yield TokenInfo(INDENT, line[:pos], (lnum, 0), (lnum, pos), line)
   510|         0|            0|            0|  0.00%|            while column < indents[-1]:
   511|         0|            0|            0|  0.00%|                if column not in indents:
   512|         0|            0|            0|  0.00%|                    raise IndentationError(
   513|         0|            0|            0|  0.00%|                        "unindent does not match any outer indentation level",
   514|         0|            0|            0|  0.00%|                        ("<tokenize>", lnum, pos, line))
   515|         0|            0|            0|  0.00%|                indents = indents[:-1]
   516|         0|            0|            0|  0.00%|
   517|         0|            0|            0|  0.00%|                yield TokenInfo(DEDENT, '', (lnum, pos), (lnum, pos), line)
   518|         0|            0|            0|  0.00%|
   519|         0|            0|            0|  0.00%|        else:                                  # continued statement
   520|         0|            0|            0|  0.00%|            if not line:
   521|         0|            0|            0|  0.00%|                raise TokenError("EOF in multi-line statement", (lnum, 0))
   522|         0|            0|            0|  0.00%|            continued = 0
   523|         0|            0|            0|  0.00%|
   524|         0|            0|            0|  0.00%|        while pos < max:
   525|         0|            0|            0|  0.00%|            pseudomatch = _compile(PseudoToken).match(line, pos)
   526|         0|            0|            0|  0.00%|            if pseudomatch:                                # scan for tokens
   527|         0|            0|            0|  0.00%|                start, end = pseudomatch.span(1)
   528|         0|            0|            0|  0.00%|                spos, epos, pos = (lnum, start), (lnum, end), end
   529|         0|            0|            0|  0.00%|                if start == end:
   530|         0|            0|            0|  0.00%|                    continue
   531|         0|            0|            0|  0.00%|                token, initial = line[start:end], line[start]
   532|         0|            0|            0|  0.00%|
   533|         0|            0|            0|  0.00%|                if (initial in numchars or                 # ordinary number
   534|         0|            0|            0|  0.00%|                    (initial == '.' and token != '.' and token != '...')):
   535|         0|            0|            0|  0.00%|                    yield TokenInfo(NUMBER, token, spos, epos, line)
   536|         0|            0|            0|  0.00%|                elif initial in '\r\n':
   537|         0|            0|            0|  0.00%|                    if parenlev > 0:
   538|         0|            0|            0|  0.00%|                        yield TokenInfo(NL, token, spos, epos, line)
   539|         0|            0|            0|  0.00%|                    else:
   540|         0|            0|            0|  0.00%|                        yield TokenInfo(NEWLINE, token, spos, epos, line)
   541|         0|            0|            0|  0.00%|
   542|         0|            0|            0|  0.00%|                elif initial == '#':
   543|         0|            0|            0|  0.00%|                    assert not token.endswith("\n")
   544|         0|            0|            0|  0.00%|                    yield TokenInfo(COMMENT, token, spos, epos, line)
   545|         0|            0|            0|  0.00%|
   546|         0|            0|            0|  0.00%|                elif token in triple_quoted:
   547|         0|            0|            0|  0.00%|                    endprog = _compile(endpats[token])
   548|         0|            0|            0|  0.00%|                    endmatch = endprog.match(line, pos)
   549|         0|            0|            0|  0.00%|                    if endmatch:                           # all on one line
   550|         0|            0|            0|  0.00%|                        pos = endmatch.end(0)
   551|         0|            0|            0|  0.00%|                        token = line[start:pos]
   552|         0|            0|            0|  0.00%|                        yield TokenInfo(STRING, token, spos, (lnum, pos), line)
   553|         0|            0|            0|  0.00%|                    else:
   554|         0|            0|            0|  0.00%|                        strstart = (lnum, start)           # multiple lines
   555|         0|            0|            0|  0.00%|                        contstr = line[start:]
   556|         0|            0|            0|  0.00%|                        contline = line
   557|         0|            0|            0|  0.00%|                        break
   558|         0|            0|            0|  0.00%|
   559|         0|            0|            0|  0.00%|                # Check up to the first 3 chars of the token to see if
   560|         0|            0|            0|  0.00%|                #  they're in the single_quoted set. If so, they start
   561|         0|            0|            0|  0.00%|                #  a string.
   562|         0|            0|            0|  0.00%|                # We're using the first 3, because we're looking for
   563|         0|            0|            0|  0.00%|                #  "rb'" (for example) at the start of the token. If
   564|         0|            0|            0|  0.00%|                #  we switch to longer prefixes, this needs to be
   565|         0|            0|            0|  0.00%|                #  adjusted.
   566|         0|            0|            0|  0.00%|                # Note that initial == token[:1].
   567|         0|            0|            0|  0.00%|                # Also note that single quote checking must come after
   568|         0|            0|            0|  0.00%|                #  triple quote checking (above).
   569|         0|            0|            0|  0.00%|                elif (initial in single_quoted or
   570|         0|            0|            0|  0.00%|                      token[:2] in single_quoted or
   571|         0|            0|            0|  0.00%|                      token[:3] in single_quoted):
   572|         0|            0|            0|  0.00%|                    if token[-1] == '\n':                  # continued string
   573|         0|            0|            0|  0.00%|                        strstart = (lnum, start)
   574|         0|            0|            0|  0.00%|                        # Again, using the first 3 chars of the
   575|         0|            0|            0|  0.00%|                        #  token. This is looking for the matching end
   576|         0|            0|            0|  0.00%|                        #  regex for the correct type of quote
   577|         0|            0|            0|  0.00%|                        #  character. So it's really looking for
   578|         0|            0|            0|  0.00%|                        #  endpats["'"] or endpats['"'], by trying to
   579|         0|            0|            0|  0.00%|                        #  skip string prefix characters, if any.
   580|         0|            0|            0|  0.00%|                        endprog = _compile(endpats.get(initial) or
   581|         0|            0|            0|  0.00%|                                           endpats.get(token[1]) or
   582|         0|            0|            0|  0.00%|                                           endpats.get(token[2]))
   583|         0|            0|            0|  0.00%|                        contstr, needcont = line[start:], 1
   584|         0|            0|            0|  0.00%|                        contline = line
   585|         0|            0|            0|  0.00%|                        break
   586|         0|            0|            0|  0.00%|                    else:                                  # ordinary string
   587|         0|            0|            0|  0.00%|                        yield TokenInfo(STRING, token, spos, epos, line)
   588|         0|            0|            0|  0.00%|
   589|         0|            0|            0|  0.00%|                elif initial.isidentifier():               # ordinary name
   590|         0|            0|            0|  0.00%|                    yield TokenInfo(NAME, token, spos, epos, line)
   591|         0|            0|            0|  0.00%|                elif initial == '\\':                      # continued stmt
   592|         0|            0|            0|  0.00%|                    continued = 1
   593|         0|            0|            0|  0.00%|                else:
   594|         0|            0|            0|  0.00%|                    if initial in '([{':
   595|         0|            0|            0|  0.00%|                        parenlev += 1
   596|         0|            0|            0|  0.00%|                    elif initial in ')]}':
   597|         0|            0|            0|  0.00%|                        parenlev -= 1
   598|         0|            0|            0|  0.00%|                    yield TokenInfo(OP, token, spos, epos, line)
   599|         0|            0|            0|  0.00%|            else:
   600|         0|            0|            0|  0.00%|                yield TokenInfo(ERRORTOKEN, line[pos],
   601|         0|            0|            0|  0.00%|                           (lnum, pos), (lnum, pos+1), line)
   602|         0|            0|            0|  0.00%|                pos += 1
   603|         0|            0|            0|  0.00%|
   604|         0|            0|            0|  0.00%|    # Add an implicit NEWLINE if the input doesn't end in one
   605|         0|            0|            0|  0.00%|    if last_line and last_line[-1] not in '\r\n':
   606|         0|            0|            0|  0.00%|        yield TokenInfo(NEWLINE, '', (lnum - 1, len(last_line)), (lnum - 1, len(last_line) + 1), '')
   607|         0|            0|            0|  0.00%|    for indent in indents[1:]:                 # pop remaining indent levels
   608|         0|            0|            0|  0.00%|        yield TokenInfo(DEDENT, '', (lnum, 0), (lnum, 0), '')
   609|         0|            0|            0|  0.00%|    yield TokenInfo(ENDMARKER, '', (lnum, 0), (lnum, 0), '')
   610|         0|            0|            0|  0.00%|
   611|         0|            0|            0|  0.00%|
   612|         0|            0|            0|  0.00%|def generate_tokens(readline):
   613|         0|            0|            0|  0.00%|    """Tokenize a source reading Python code as unicode strings.
   614|         0|            0|            0|  0.00%|
   615|         0|            0|            0|  0.00%|    This has the same API as tokenize(), except that it expects the *readline*
   616|         0|            0|            0|  0.00%|    callable to return str objects instead of bytes.
   617|         0|            0|            0|  0.00%|    """
   618|         0|            0|            0|  0.00%|    return _tokenize(readline, None)
   619|         0|            0|            0|  0.00%|
   620|         0|            0|            0|  0.00%|def main():
   621|         0|            0|            0|  0.00%|    import argparse
   622|         0|            0|            0|  0.00%|
   623|         0|            0|            0|  0.00%|    # Helper error handling routines
   624|         0|            0|            0|  0.00%|    def perror(message):
   625|         0|            0|            0|  0.00%|        sys.stderr.write(message)
   626|         0|            0|            0|  0.00%|        sys.stderr.write('\n')
   627|         0|            0|            0|  0.00%|
   628|         0|            0|            0|  0.00%|    def error(message, filename=None, location=None):
   629|         0|            0|            0|  0.00%|        if location:
   630|         0|            0|            0|  0.00%|            args = (filename,) + location + (message,)
   631|         0|            0|            0|  0.00%|            perror("%s:%d:%d: error: %s" % args)
   632|         0|            0|            0|  0.00%|        elif filename:
   633|         0|            0|            0|  0.00%|            perror("%s: error: %s" % (filename, message))
   634|         0|            0|            0|  0.00%|        else:
   635|         0|            0|            0|  0.00%|            perror("error: %s" % message)
   636|         0|            0|            0|  0.00%|        sys.exit(1)
   637|         0|            0|            0|  0.00%|
   638|         0|            0|            0|  0.00%|    # Parse the arguments and options
   639|         0|            0|            0|  0.00%|    parser = argparse.ArgumentParser(prog='python -m tokenize')
   640|         0|            0|            0|  0.00%|    parser.add_argument(dest='filename', nargs='?',
   641|         0|            0|            0|  0.00%|                        metavar='filename.py',
   642|         0|            0|            0|  0.00%|                        help='the file to tokenize; defaults to stdin')
   643|         0|            0|            0|  0.00%|    parser.add_argument('-e', '--exact', dest='exact', action='store_true',
   644|         0|            0|            0|  0.00%|                        help='display token names using the exact type')
   645|         0|            0|            0|  0.00%|    args = parser.parse_args()
   646|         0|            0|            0|  0.00%|
   647|         0|            0|            0|  0.00%|    try:
   648|         0|            0|            0|  0.00%|        # Tokenize the input
   649|         0|            0|            0|  0.00%|        if args.filename:
   650|         0|            0|            0|  0.00%|            filename = args.filename
   651|         0|            0|            0|  0.00%|            with _builtin_open(filename, 'rb') as f:
   652|         0|            0|            0|  0.00%|                tokens = list(tokenize(f.readline))
   653|         0|            0|            0|  0.00%|        else:
   654|         0|            0|            0|  0.00%|            filename = "<stdin>"
   655|         0|            0|            0|  0.00%|            tokens = _tokenize(sys.stdin.readline, None)
   656|         0|            0|            0|  0.00%|
   657|         0|            0|            0|  0.00%|        # Output the tokenization
   658|         0|            0|            0|  0.00%|        for token in tokens:
   659|         0|            0|            0|  0.00%|            token_type = token.type
   660|         0|            0|            0|  0.00%|            if args.exact:
   661|         0|            0|            0|  0.00%|                token_type = token.exact_type
   662|         0|            0|            0|  0.00%|            token_range = "%d,%d-%d,%d:" % (token.start + token.end)
   663|         0|            0|            0|  0.00%|            print("%-20s%-15s%-15r" %
   664|         0|            0|            0|  0.00%|                  (token_range, tok_name[token_type], token.string))
   665|         0|            0|            0|  0.00%|    except IndentationError as err:
   666|         0|            0|            0|  0.00%|        line, column = err.args[1][1:3]
   667|         0|            0|            0|  0.00%|        error(err.args[0], filename, (line, column))
   668|         0|            0|            0|  0.00%|    except TokenError as err:
   669|         0|            0|            0|  0.00%|        line, column = err.args[1]
   670|         0|            0|            0|  0.00%|        error(err.args[0], filename, (line, column))
   671|         0|            0|            0|  0.00%|    except SyntaxError as err:
   672|         0|            0|            0|  0.00%|        error(err, filename)
   673|         0|            0|            0|  0.00%|    except OSError as err:
   674|         0|            0|            0|  0.00%|        error(err)
   675|         0|            0|            0|  0.00%|    except KeyboardInterrupt:
   676|         0|            0|            0|  0.00%|        print("interrupted\n")
   677|         0|            0|            0|  0.00%|    except Exception as err:
   678|         0|            0|            0|  0.00%|        perror("unexpected error: %s" % err)
   679|         0|            0|            0|  0.00%|        raise
   680|         0|            0|            0|  0.00%|
   681|         0|            0|            0|  0.00%|if __name__ == "__main__":
   682|         0|            0|            0|  0.00%|    main()
File: /opt/conda/lib/python3.8/warnings.py
File duration: 0.000696182s (0.00%)
Line #|      Hits|         Time| Time per hit|      %|Source code
------+----------+-------------+-------------+-------+-----------
     1|         0|            0|            0|  0.00%|"""Python part of the warnings subsystem."""
     2|         0|            0|            0|  0.00%|
     3|         0|            0|            0|  0.00%|import sys
     4|         0|            0|            0|  0.00%|
     5|         0|            0|            0|  0.00%|
     6|         0|            0|            0|  0.00%|__all__ = ["warn", "warn_explicit", "showwarning",
     7|         0|            0|            0|  0.00%|           "formatwarning", "filterwarnings", "simplefilter",
     8|         0|            0|            0|  0.00%|           "resetwarnings", "catch_warnings"]
     9|         0|            0|            0|  0.00%|
    10|         0|            0|            0|  0.00%|def showwarning(message, category, filename, lineno, file=None, line=None):
    11|         0|            0|            0|  0.00%|    """Hook to write a warning to a file; replace if you like."""
    12|         0|            0|            0|  0.00%|    msg = WarningMessage(message, category, filename, lineno, file, line)
    13|         0|            0|            0|  0.00%|    _showwarnmsg_impl(msg)
    14|         0|            0|            0|  0.00%|
    15|         0|            0|            0|  0.00%|def formatwarning(message, category, filename, lineno, line=None):
    16|         0|            0|            0|  0.00%|    """Function to format a warning the standard way."""
    17|         0|            0|            0|  0.00%|    msg = WarningMessage(message, category, filename, lineno, None, line)
    18|         0|            0|            0|  0.00%|    return _formatwarnmsg_impl(msg)
    19|         0|            0|            0|  0.00%|
    20|         1|   2.0504e-05|   2.0504e-05|  0.00%|def _showwarnmsg_impl(msg):
    21|         1|  7.86781e-06|  7.86781e-06|  0.00%|    file = msg.file
    22|         1|  5.72205e-06|  5.72205e-06|  0.00%|    if file is None:
    23|         1|  6.91414e-06|  6.91414e-06|  0.00%|        file = sys.stderr
    24|         1|  5.72205e-06|  5.72205e-06|  0.00%|        if file is None:
    25|         0|            0|            0|  0.00%|            # sys.stderr is None when run with pythonw.exe:
    26|         0|            0|            0|  0.00%|            # warnings get lost
    27|         0|            0|            0|  0.00%|            return
    28|         1|   1.7643e-05|   1.7643e-05|  0.00%|    text = _formatwarnmsg(msg)
(call)|         1|   0.00413322|   0.00413322|  0.01%|# /opt/conda/lib/python3.8/warnings.py:117 _formatwarnmsg
    29|         1|  5.72205e-06|  5.72205e-06|  0.00%|    try:
    30|         1|  8.01086e-05|  8.01086e-05|  0.00%|        file.write(text)
    31|         0|            0|            0|  0.00%|    except OSError:
    32|         0|            0|            0|  0.00%|        # the file (probably stderr) is invalid - this warning gets lost.
    33|         0|            0|            0|  0.00%|        pass
    34|         0|            0|            0|  0.00%|
    35|         1|  2.43187e-05|  2.43187e-05|  0.00%|def _formatwarnmsg_impl(msg):
    36|         1|  9.05991e-06|  9.05991e-06|  0.00%|    category = msg.category.__name__
    37|         1|  3.95775e-05|  3.95775e-05|  0.00%|    s =  f"{msg.filename}:{msg.lineno}: {category}: {msg.message}\n"
    38|         0|            0|            0|  0.00%|
    39|         1|  8.82149e-06|  8.82149e-06|  0.00%|    if msg.line is None:
    40|         1|  6.67572e-06|  6.67572e-06|  0.00%|        try:
    41|         1|  5.14984e-05|  5.14984e-05|  0.00%|            import linecache
    42|         1|  5.79357e-05|  5.79357e-05|  0.00%|            line = linecache.getline(msg.filename, msg.lineno)
(call)|         1|   0.00383759|   0.00383759|  0.01%|# /opt/conda/lib/python3.8/linecache.py:15 getline
    43|         0|            0|            0|  0.00%|        except Exception:
    44|         0|            0|            0|  0.00%|            # When a warning is logged during Python shutdown, linecache
    45|         0|            0|            0|  0.00%|            # and the import machinery don't work anymore
    46|         0|            0|            0|  0.00%|            line = None
    47|         0|            0|            0|  0.00%|            linecache = None
    48|         0|            0|            0|  0.00%|    else:
    49|         0|            0|            0|  0.00%|        line = msg.line
    50|         1|  7.15256e-06|  7.15256e-06|  0.00%|    if line:
    51|         1|  1.28746e-05|  1.28746e-05|  0.00%|        line = line.strip()
    52|         1|  9.05991e-06|  9.05991e-06|  0.00%|        s += "  %s\n" % line
    53|         0|            0|            0|  0.00%|
    54|         1|  1.50204e-05|  1.50204e-05|  0.00%|    if msg.source is not None:
    55|         0|            0|            0|  0.00%|        try:
    56|         0|            0|            0|  0.00%|            import tracemalloc
    57|         0|            0|            0|  0.00%|        # Logging a warning should not raise a new exception:
    58|         0|            0|            0|  0.00%|        # catch Exception, not only ImportError and RecursionError.
    59|         0|            0|            0|  0.00%|        except Exception:
    60|         0|            0|            0|  0.00%|            # don't suggest to enable tracemalloc if it's not available
    61|         0|            0|            0|  0.00%|            tracing = True
    62|         0|            0|            0|  0.00%|            tb = None
    63|         0|            0|            0|  0.00%|        else:
    64|         0|            0|            0|  0.00%|            tracing = tracemalloc.is_tracing()
    65|         0|            0|            0|  0.00%|            try:
    66|         0|            0|            0|  0.00%|                tb = tracemalloc.get_object_traceback(msg.source)
    67|         0|            0|            0|  0.00%|            except Exception:
    68|         0|            0|            0|  0.00%|                # When a warning is logged during Python shutdown, tracemalloc
    69|         0|            0|            0|  0.00%|                # and the import machinery don't work anymore
    70|         0|            0|            0|  0.00%|                tb = None
    71|         0|            0|            0|  0.00%|
    72|         0|            0|            0|  0.00%|        if tb is not None:
    73|         0|            0|            0|  0.00%|            s += 'Object allocated at (most recent call last):\n'
    74|         0|            0|            0|  0.00%|            for frame in tb:
    75|         0|            0|            0|  0.00%|                s += ('  File "%s", lineno %s\n'
    76|         0|            0|            0|  0.00%|                      % (frame.filename, frame.lineno))
    77|         0|            0|            0|  0.00%|
    78|         0|            0|            0|  0.00%|                try:
    79|         0|            0|            0|  0.00%|                    if linecache is not None:
    80|         0|            0|            0|  0.00%|                        line = linecache.getline(frame.filename, frame.lineno)
    81|         0|            0|            0|  0.00%|                    else:
    82|         0|            0|            0|  0.00%|                        line = None
    83|         0|            0|            0|  0.00%|                except Exception:
    84|         0|            0|            0|  0.00%|                    line = None
    85|         0|            0|            0|  0.00%|                if line:
    86|         0|            0|            0|  0.00%|                    line = line.strip()
    87|         0|            0|            0|  0.00%|                    s += '    %s\n' % line
    88|         0|            0|            0|  0.00%|        elif not tracing:
    89|         0|            0|            0|  0.00%|            s += (f'{category}: Enable tracemalloc to get the object '
    90|         0|            0|            0|  0.00%|                  f'allocation traceback\n')
    91|         1|  6.67572e-06|  6.67572e-06|  0.00%|    return s
    92|         0|            0|            0|  0.00%|
    93|         0|            0|            0|  0.00%|# Keep a reference to check if the function was replaced
    94|         0|            0|            0|  0.00%|_showwarning_orig = showwarning
    95|         0|            0|            0|  0.00%|
    96|         1|  3.48091e-05|  3.48091e-05|  0.00%|def _showwarnmsg(msg):
    97|         0|            0|            0|  0.00%|    """Hook to write a warning to a file; replace if you like."""
    98|         1|  7.86781e-06|  7.86781e-06|  0.00%|    try:
    99|         1|  7.39098e-06|  7.39098e-06|  0.00%|        sw = showwarning
   100|         0|            0|            0|  0.00%|    except NameError:
   101|         0|            0|            0|  0.00%|        pass
   102|         0|            0|            0|  0.00%|    else:
   103|         1|  1.26362e-05|  1.26362e-05|  0.00%|        if sw is not _showwarning_orig:
   104|         0|            0|            0|  0.00%|            # warnings.showwarning() was replaced
   105|         0|            0|            0|  0.00%|            if not callable(sw):
   106|         0|            0|            0|  0.00%|                raise TypeError("warnings.showwarning() must be set to a "
   107|         0|            0|            0|  0.00%|                                "function or method")
   108|         0|            0|            0|  0.00%|
   109|         0|            0|            0|  0.00%|            sw(msg.message, msg.category, msg.filename, msg.lineno,
   110|         0|            0|            0|  0.00%|               msg.file, msg.line)
   111|         0|            0|            0|  0.00%|            return
   112|         1|   4.1008e-05|   4.1008e-05|  0.00%|    _showwarnmsg_impl(msg)
(call)|         1|   0.00428343|   0.00428343|  0.01%|# /opt/conda/lib/python3.8/warnings.py:20 _showwarnmsg_impl
   113|         0|            0|            0|  0.00%|
   114|         0|            0|            0|  0.00%|# Keep a reference to check if the function was replaced
   115|         0|            0|            0|  0.00%|_formatwarning_orig = formatwarning
   116|         0|            0|            0|  0.00%|
   117|         1|  6.91414e-06|  6.91414e-06|  0.00%|def _formatwarnmsg(msg):
   118|         0|            0|            0|  0.00%|    """Function to format a warning the standard way."""
   119|         1|  5.96046e-06|  5.96046e-06|  0.00%|    try:
   120|         1|  5.72205e-06|  5.72205e-06|  0.00%|        fw = formatwarning
   121|         0|            0|            0|  0.00%|    except NameError:
   122|         0|            0|            0|  0.00%|        pass
   123|         0|            0|            0|  0.00%|    else:
   124|         1|  5.72205e-06|  5.72205e-06|  0.00%|        if fw is not _formatwarning_orig:
   125|         0|            0|            0|  0.00%|            # warnings.formatwarning() was replaced
   126|         0|            0|            0|  0.00%|            return fw(msg.message, msg.category,
   127|         0|            0|            0|  0.00%|                      msg.filename, msg.lineno, msg.line)
   128|         1|  2.26498e-05|  2.26498e-05|  0.00%|    return _formatwarnmsg_impl(msg)
(call)|         1|   0.00408626|   0.00408626|  0.01%|# /opt/conda/lib/python3.8/warnings.py:35 _formatwarnmsg_impl
   129|         0|            0|            0|  0.00%|
   130|         0|            0|            0|  0.00%|def filterwarnings(action, message="", category=Warning, module="", lineno=0,
   131|         0|            0|            0|  0.00%|                   append=False):
   132|         0|            0|            0|  0.00%|    """Insert an entry into the list of warnings filters (at the front).
   133|         0|            0|            0|  0.00%|
   134|         0|            0|            0|  0.00%|    'action' -- one of "error", "ignore", "always", "default", "module",
   135|         0|            0|            0|  0.00%|                or "once"
   136|         0|            0|            0|  0.00%|    'message' -- a regex that the warning message must match
   137|         0|            0|            0|  0.00%|    'category' -- a class that the warning must be a subclass of
   138|         0|            0|            0|  0.00%|    'module' -- a regex that the module name must match
   139|         0|            0|            0|  0.00%|    'lineno' -- an integer line number, 0 matches all warnings
   140|         0|            0|            0|  0.00%|    'append' -- if true, append to the list of filters
   141|         0|            0|            0|  0.00%|    """
   142|         0|            0|            0|  0.00%|    assert action in ("error", "ignore", "always", "default", "module",
   143|         0|            0|            0|  0.00%|                      "once"), "invalid action: %r" % (action,)
   144|         0|            0|            0|  0.00%|    assert isinstance(message, str), "message must be a string"
   145|         0|            0|            0|  0.00%|    assert isinstance(category, type), "category must be a class"
   146|         0|            0|            0|  0.00%|    assert issubclass(category, Warning), "category must be a Warning subclass"
   147|         0|            0|            0|  0.00%|    assert isinstance(module, str), "module must be a string"
   148|         0|            0|            0|  0.00%|    assert isinstance(lineno, int) and lineno >= 0, \
   149|         0|            0|            0|  0.00%|           "lineno must be an int >= 0"
   150|         0|            0|            0|  0.00%|
   151|         0|            0|            0|  0.00%|    if message or module:
   152|         0|            0|            0|  0.00%|        import re
   153|         0|            0|            0|  0.00%|
   154|         0|            0|            0|  0.00%|    if message:
   155|         0|            0|            0|  0.00%|        message = re.compile(message, re.I)
   156|         0|            0|            0|  0.00%|    else:
   157|         0|            0|            0|  0.00%|        message = None
   158|         0|            0|            0|  0.00%|    if module:
   159|         0|            0|            0|  0.00%|        module = re.compile(module)
   160|         0|            0|            0|  0.00%|    else:
   161|         0|            0|            0|  0.00%|        module = None
   162|         0|            0|            0|  0.00%|
   163|         0|            0|            0|  0.00%|    _add_filter(action, message, category, module, lineno, append=append)
   164|         0|            0|            0|  0.00%|
   165|         0|            0|            0|  0.00%|def simplefilter(action, category=Warning, lineno=0, append=False):
   166|         0|            0|            0|  0.00%|    """Insert a simple entry into the list of warnings filters (at the front).
   167|         0|            0|            0|  0.00%|
   168|         0|            0|            0|  0.00%|    A simple filter matches all modules and messages.
   169|         0|            0|            0|  0.00%|    'action' -- one of "error", "ignore", "always", "default", "module",
   170|         0|            0|            0|  0.00%|                or "once"
   171|         0|            0|            0|  0.00%|    'category' -- a class that the warning must be a subclass of
   172|         0|            0|            0|  0.00%|    'lineno' -- an integer line number, 0 matches all warnings
   173|         0|            0|            0|  0.00%|    'append' -- if true, append to the list of filters
   174|         0|            0|            0|  0.00%|    """
   175|         0|            0|            0|  0.00%|    assert action in ("error", "ignore", "always", "default", "module",
   176|         0|            0|            0|  0.00%|                      "once"), "invalid action: %r" % (action,)
   177|         0|            0|            0|  0.00%|    assert isinstance(lineno, int) and lineno >= 0, \
   178|         0|            0|            0|  0.00%|           "lineno must be an int >= 0"
   179|         0|            0|            0|  0.00%|    _add_filter(action, None, category, None, lineno, append=append)
   180|         0|            0|            0|  0.00%|
   181|         0|            0|            0|  0.00%|def _add_filter(*item, append):
   182|         0|            0|            0|  0.00%|    # Remove possible duplicate filters, so new one will be placed
   183|         0|            0|            0|  0.00%|    # in correct place. If append=True and duplicate exists, do nothing.
   184|         0|            0|            0|  0.00%|    if not append:
   185|         0|            0|            0|  0.00%|        try:
   186|         0|            0|            0|  0.00%|            filters.remove(item)
   187|         0|            0|            0|  0.00%|        except ValueError:
   188|         0|            0|            0|  0.00%|            pass
   189|         0|            0|            0|  0.00%|        filters.insert(0, item)
   190|         0|            0|            0|  0.00%|    else:
   191|         0|            0|            0|  0.00%|        if item not in filters:
   192|         0|            0|            0|  0.00%|            filters.append(item)
   193|         0|            0|            0|  0.00%|    _filters_mutated()
   194|         0|            0|            0|  0.00%|
   195|         0|            0|            0|  0.00%|def resetwarnings():
   196|         0|            0|            0|  0.00%|    """Clear the list of warning filters, so that no filters are active."""
   197|         0|            0|            0|  0.00%|    filters[:] = []
   198|         0|            0|            0|  0.00%|    _filters_mutated()
   199|         0|            0|            0|  0.00%|
   200|         0|            0|            0|  0.00%|class _OptionError(Exception):
   201|         0|            0|            0|  0.00%|    """Exception used by option processing helpers."""
   202|         0|            0|            0|  0.00%|    pass
   203|         0|            0|            0|  0.00%|
   204|         0|            0|            0|  0.00%|# Helper to process -W options passed via sys.warnoptions
   205|         0|            0|            0|  0.00%|def _processoptions(args):
   206|         0|            0|            0|  0.00%|    for arg in args:
   207|         0|            0|            0|  0.00%|        try:
   208|         0|            0|            0|  0.00%|            _setoption(arg)
   209|         0|            0|            0|  0.00%|        except _OptionError as msg:
   210|         0|            0|            0|  0.00%|            print("Invalid -W option ignored:", msg, file=sys.stderr)
   211|         0|            0|            0|  0.00%|
   212|         0|            0|            0|  0.00%|# Helper for _processoptions()
   213|         0|            0|            0|  0.00%|def _setoption(arg):
   214|         0|            0|            0|  0.00%|    parts = arg.split(':')
   215|         0|            0|            0|  0.00%|    if len(parts) > 5:
   216|         0|            0|            0|  0.00%|        raise _OptionError("too many fields (max 5): %r" % (arg,))
   217|         0|            0|            0|  0.00%|    while len(parts) < 5:
   218|         0|            0|            0|  0.00%|        parts.append('')
   219|         0|            0|            0|  0.00%|    action, message, category, module, lineno = [s.strip()
   220|         0|            0|            0|  0.00%|                                                 for s in parts]
   221|         0|            0|            0|  0.00%|    action = _getaction(action)
   222|         0|            0|            0|  0.00%|    category = _getcategory(category)
   223|         0|            0|            0|  0.00%|    if message or module:
   224|         0|            0|            0|  0.00%|        import re
   225|         0|            0|            0|  0.00%|    if message:
   226|         0|            0|            0|  0.00%|        message = re.escape(message)
   227|         0|            0|            0|  0.00%|    if module:
   228|         0|            0|            0|  0.00%|        module = re.escape(module) + r'\Z'
   229|         0|            0|            0|  0.00%|    if lineno:
   230|         0|            0|            0|  0.00%|        try:
   231|         0|            0|            0|  0.00%|            lineno = int(lineno)
   232|         0|            0|            0|  0.00%|            if lineno < 0:
   233|         0|            0|            0|  0.00%|                raise ValueError
   234|         0|            0|            0|  0.00%|        except (ValueError, OverflowError):
   235|         0|            0|            0|  0.00%|            raise _OptionError("invalid lineno %r" % (lineno,)) from None
   236|         0|            0|            0|  0.00%|    else:
   237|         0|            0|            0|  0.00%|        lineno = 0
   238|         0|            0|            0|  0.00%|    filterwarnings(action, message, category, module, lineno)
   239|         0|            0|            0|  0.00%|
   240|         0|            0|            0|  0.00%|# Helper for _setoption()
   241|         0|            0|            0|  0.00%|def _getaction(action):
   242|         0|            0|            0|  0.00%|    if not action:
   243|         0|            0|            0|  0.00%|        return "default"
   244|         0|            0|            0|  0.00%|    if action == "all": return "always" # Alias
   245|         0|            0|            0|  0.00%|    for a in ('default', 'always', 'ignore', 'module', 'once', 'error'):
   246|         0|            0|            0|  0.00%|        if a.startswith(action):
   247|         0|            0|            0|  0.00%|            return a
   248|         0|            0|            0|  0.00%|    raise _OptionError("invalid action: %r" % (action,))
   249|         0|            0|            0|  0.00%|
   250|         0|            0|            0|  0.00%|# Helper for _setoption()
   251|         0|            0|            0|  0.00%|def _getcategory(category):
   252|         0|            0|            0|  0.00%|    if not category:
   253|         0|            0|            0|  0.00%|        return Warning
   254|         0|            0|            0|  0.00%|    if '.' not in category:
   255|         0|            0|            0|  0.00%|        import builtins as m
   256|         0|            0|            0|  0.00%|        klass = category
   257|         0|            0|            0|  0.00%|    else:
   258|         0|            0|            0|  0.00%|        module, _, klass = category.rpartition('.')
   259|         0|            0|            0|  0.00%|        try:
   260|         0|            0|            0|  0.00%|            m = __import__(module, None, None, [klass])
   261|         0|            0|            0|  0.00%|        except ImportError:
   262|         0|            0|            0|  0.00%|            raise _OptionError("invalid module name: %r" % (module,)) from None
   263|         0|            0|            0|  0.00%|    try:
   264|         0|            0|            0|  0.00%|        cat = getattr(m, klass)
   265|         0|            0|            0|  0.00%|    except AttributeError:
   266|         0|            0|            0|  0.00%|        raise _OptionError("unknown warning category: %r" % (category,)) from None
   267|         0|            0|            0|  0.00%|    if not issubclass(cat, Warning):
   268|         0|            0|            0|  0.00%|        raise _OptionError("invalid warning category: %r" % (category,))
   269|         0|            0|            0|  0.00%|    return cat
   270|         0|            0|            0|  0.00%|
   271|         0|            0|            0|  0.00%|
   272|         0|            0|            0|  0.00%|def _is_internal_frame(frame):
   273|         0|            0|            0|  0.00%|    """Signal whether the frame is an internal CPython implementation detail."""
   274|         0|            0|            0|  0.00%|    filename = frame.f_code.co_filename
   275|         0|            0|            0|  0.00%|    return 'importlib' in filename and '_bootstrap' in filename
   276|         0|            0|            0|  0.00%|
   277|         0|            0|            0|  0.00%|
   278|         0|            0|            0|  0.00%|def _next_external_frame(frame):
   279|         0|            0|            0|  0.00%|    """Find the next frame that doesn't involve CPython internals."""
   280|         0|            0|            0|  0.00%|    frame = frame.f_back
   281|         0|            0|            0|  0.00%|    while frame is not None and _is_internal_frame(frame):
   282|         0|            0|            0|  0.00%|        frame = frame.f_back
   283|         0|            0|            0|  0.00%|    return frame
   284|         0|            0|            0|  0.00%|
   285|         0|            0|            0|  0.00%|
   286|         0|            0|            0|  0.00%|# Code typically replaced by _warnings
   287|         0|            0|            0|  0.00%|def warn(message, category=None, stacklevel=1, source=None):
   288|         0|            0|            0|  0.00%|    """Issue a warning, or maybe ignore it or raise an exception."""
   289|         0|            0|            0|  0.00%|    # Check if message is already a Warning object
   290|         0|            0|            0|  0.00%|    if isinstance(message, Warning):
   291|         0|            0|            0|  0.00%|        category = message.__class__
   292|         0|            0|            0|  0.00%|    # Check category argument
   293|         0|            0|            0|  0.00%|    if category is None:
   294|         0|            0|            0|  0.00%|        category = UserWarning
   295|         0|            0|            0|  0.00%|    if not (isinstance(category, type) and issubclass(category, Warning)):
   296|         0|            0|            0|  0.00%|        raise TypeError("category must be a Warning subclass, "
   297|         0|            0|            0|  0.00%|                        "not '{:s}'".format(type(category).__name__))
   298|         0|            0|            0|  0.00%|    # Get context information
   299|         0|            0|            0|  0.00%|    try:
   300|         0|            0|            0|  0.00%|        if stacklevel <= 1 or _is_internal_frame(sys._getframe(1)):
   301|         0|            0|            0|  0.00%|            # If frame is too small to care or if the warning originated in
   302|         0|            0|            0|  0.00%|            # internal code, then do not try to hide any frames.
   303|         0|            0|            0|  0.00%|            frame = sys._getframe(stacklevel)
   304|         0|            0|            0|  0.00%|        else:
   305|         0|            0|            0|  0.00%|            frame = sys._getframe(1)
   306|         0|            0|            0|  0.00%|            # Look for one frame less since the above line starts us off.
   307|         0|            0|            0|  0.00%|            for x in range(stacklevel-1):
   308|         0|            0|            0|  0.00%|                frame = _next_external_frame(frame)
   309|         0|            0|            0|  0.00%|                if frame is None:
   310|         0|            0|            0|  0.00%|                    raise ValueError
   311|         0|            0|            0|  0.00%|    except ValueError:
   312|         0|            0|            0|  0.00%|        globals = sys.__dict__
   313|         0|            0|            0|  0.00%|        filename = "sys"
   314|         0|            0|            0|  0.00%|        lineno = 1
   315|         0|            0|            0|  0.00%|    else:
   316|         0|            0|            0|  0.00%|        globals = frame.f_globals
   317|         0|            0|            0|  0.00%|        filename = frame.f_code.co_filename
   318|         0|            0|            0|  0.00%|        lineno = frame.f_lineno
   319|         0|            0|            0|  0.00%|    if '__name__' in globals:
   320|         0|            0|            0|  0.00%|        module = globals['__name__']
   321|         0|            0|            0|  0.00%|    else:
   322|         0|            0|            0|  0.00%|        module = "<string>"
   323|         0|            0|            0|  0.00%|    registry = globals.setdefault("__warningregistry__", {})
   324|         0|            0|            0|  0.00%|    warn_explicit(message, category, filename, lineno, module, registry,
   325|         0|            0|            0|  0.00%|                  globals, source)
   326|         0|            0|            0|  0.00%|
   327|         0|            0|            0|  0.00%|def warn_explicit(message, category, filename, lineno,
   328|         0|            0|            0|  0.00%|                  module=None, registry=None, module_globals=None,
   329|         0|            0|            0|  0.00%|                  source=None):
   330|         0|            0|            0|  0.00%|    lineno = int(lineno)
   331|         0|            0|            0|  0.00%|    if module is None:
   332|         0|            0|            0|  0.00%|        module = filename or "<unknown>"
   333|         0|            0|            0|  0.00%|        if module[-3:].lower() == ".py":
   334|         0|            0|            0|  0.00%|            module = module[:-3] # XXX What about leading pathname?
   335|         0|            0|            0|  0.00%|    if registry is None:
   336|         0|            0|            0|  0.00%|        registry = {}
   337|         0|            0|            0|  0.00%|    if registry.get('version', 0) != _filters_version:
   338|         0|            0|            0|  0.00%|        registry.clear()
   339|         0|            0|            0|  0.00%|        registry['version'] = _filters_version
   340|         0|            0|            0|  0.00%|    if isinstance(message, Warning):
   341|         0|            0|            0|  0.00%|        text = str(message)
   342|         0|            0|            0|  0.00%|        category = message.__class__
   343|         0|            0|            0|  0.00%|    else:
   344|         0|            0|            0|  0.00%|        text = message
   345|         0|            0|            0|  0.00%|        message = category(message)
   346|         0|            0|            0|  0.00%|    key = (text, category, lineno)
   347|         0|            0|            0|  0.00%|    # Quick test for common case
   348|         0|            0|            0|  0.00%|    if registry.get(key):
   349|         0|            0|            0|  0.00%|        return
   350|         0|            0|            0|  0.00%|    # Search the filters
   351|         0|            0|            0|  0.00%|    for item in filters:
   352|         0|            0|            0|  0.00%|        action, msg, cat, mod, ln = item
   353|         0|            0|            0|  0.00%|        if ((msg is None or msg.match(text)) and
   354|         0|            0|            0|  0.00%|            issubclass(category, cat) and
   355|         0|            0|            0|  0.00%|            (mod is None or mod.match(module)) and
   356|         0|            0|            0|  0.00%|            (ln == 0 or lineno == ln)):
   357|         0|            0|            0|  0.00%|            break
   358|         0|            0|            0|  0.00%|    else:
   359|         0|            0|            0|  0.00%|        action = defaultaction
   360|         0|            0|            0|  0.00%|    # Early exit actions
   361|         0|            0|            0|  0.00%|    if action == "ignore":
   362|         0|            0|            0|  0.00%|        return
   363|         0|            0|            0|  0.00%|
   364|         0|            0|            0|  0.00%|    # Prime the linecache for formatting, in case the
   365|         0|            0|            0|  0.00%|    # "file" is actually in a zipfile or something.
   366|         0|            0|            0|  0.00%|    import linecache
   367|         0|            0|            0|  0.00%|    linecache.getlines(filename, module_globals)
   368|         0|            0|            0|  0.00%|
   369|         0|            0|            0|  0.00%|    if action == "error":
   370|         0|            0|            0|  0.00%|        raise message
   371|         0|            0|            0|  0.00%|    # Other actions
   372|         0|            0|            0|  0.00%|    if action == "once":
   373|         0|            0|            0|  0.00%|        registry[key] = 1
   374|         0|            0|            0|  0.00%|        oncekey = (text, category)
   375|         0|            0|            0|  0.00%|        if onceregistry.get(oncekey):
   376|         0|            0|            0|  0.00%|            return
   377|         0|            0|            0|  0.00%|        onceregistry[oncekey] = 1
   378|         0|            0|            0|  0.00%|    elif action == "always":
   379|         0|            0|            0|  0.00%|        pass
   380|         0|            0|            0|  0.00%|    elif action == "module":
   381|         0|            0|            0|  0.00%|        registry[key] = 1
   382|         0|            0|            0|  0.00%|        altkey = (text, category, 0)
   383|         0|            0|            0|  0.00%|        if registry.get(altkey):
   384|         0|            0|            0|  0.00%|            return
   385|         0|            0|            0|  0.00%|        registry[altkey] = 1
   386|         0|            0|            0|  0.00%|    elif action == "default":
   387|         0|            0|            0|  0.00%|        registry[key] = 1
   388|         0|            0|            0|  0.00%|    else:
   389|         0|            0|            0|  0.00%|        # Unrecognized actions are errors
   390|         0|            0|            0|  0.00%|        raise RuntimeError(
   391|         0|            0|            0|  0.00%|              "Unrecognized action (%r) in warnings.filters:\n %s" %
   392|         0|            0|            0|  0.00%|              (action, item))
   393|         0|            0|            0|  0.00%|    # Print message and context
   394|         0|            0|            0|  0.00%|    msg = WarningMessage(message, category, filename, lineno, source)
   395|         0|            0|            0|  0.00%|    _showwarnmsg(msg)
   396|         0|            0|            0|  0.00%|
   397|         0|            0|            0|  0.00%|
   398|         0|            0|            0|  0.00%|class WarningMessage(object):
   399|         0|            0|            0|  0.00%|
   400|         0|            0|            0|  0.00%|    _WARNING_DETAILS = ("message", "category", "filename", "lineno", "file",
   401|         0|            0|            0|  0.00%|                        "line", "source")
   402|         0|            0|            0|  0.00%|
   403|         1|  2.88486e-05|  2.88486e-05|  0.00%|    def __init__(self, message, category, filename, lineno, file=None,
   404|         0|            0|            0|  0.00%|                 line=None, source=None):
   405|         1|  3.74317e-05|  3.74317e-05|  0.00%|        self.message = message
   406|         1|  8.82149e-06|  8.82149e-06|  0.00%|        self.category = category
   407|         1|  6.19888e-06|  6.19888e-06|  0.00%|        self.filename = filename
   408|         1|   6.4373e-06|   6.4373e-06|  0.00%|        self.lineno = lineno
   409|         1|   2.0504e-05|   2.0504e-05|  0.00%|        self.file = file
   410|         1|  1.62125e-05|  1.62125e-05|  0.00%|        self.line = line
   411|         1|  1.28746e-05|  1.28746e-05|  0.00%|        self.source = source
   412|         1|  9.29832e-06|  9.29832e-06|  0.00%|        self._category_name = category.__name__ if category else None
   413|         0|            0|            0|  0.00%|
   414|         0|            0|            0|  0.00%|    def __str__(self):
   415|         0|            0|            0|  0.00%|        return ("{message : %r, category : %r, filename : %r, lineno : %s, "
   416|         0|            0|            0|  0.00%|                    "line : %r}" % (self.message, self._category_name,
   417|         0|            0|            0|  0.00%|                                    self.filename, self.lineno, self.line))
   418|         0|            0|            0|  0.00%|
   419|         0|            0|            0|  0.00%|
   420|         0|            0|            0|  0.00%|class catch_warnings(object):
   421|         0|            0|            0|  0.00%|
   422|         0|            0|            0|  0.00%|    """A context manager that copies and restores the warnings filter upon
   423|         0|            0|            0|  0.00%|    exiting the context.
   424|         0|            0|            0|  0.00%|
   425|         0|            0|            0|  0.00%|    The 'record' argument specifies whether warnings should be captured by a
   426|         0|            0|            0|  0.00%|    custom implementation of warnings.showwarning() and be appended to a list
   427|         0|            0|            0|  0.00%|    returned by the context manager. Otherwise None is returned by the context
   428|         0|            0|            0|  0.00%|    manager. The objects appended to the list are arguments whose attributes
   429|         0|            0|            0|  0.00%|    mirror the arguments to showwarning().
   430|         0|            0|            0|  0.00%|
   431|         0|            0|            0|  0.00%|    The 'module' argument is to specify an alternative module to the module
   432|         0|            0|            0|  0.00%|    named 'warnings' and imported under that name. This argument is only useful
   433|         0|            0|            0|  0.00%|    when testing the warnings module itself.
   434|         0|            0|            0|  0.00%|
   435|         0|            0|            0|  0.00%|    """
   436|         0|            0|            0|  0.00%|
   437|         0|            0|            0|  0.00%|    def __init__(self, *, record=False, module=None):
   438|         0|            0|            0|  0.00%|        """Specify whether to record warnings and if an alternative module
   439|         0|            0|            0|  0.00%|        should be used other than sys.modules['warnings'].
   440|         0|            0|            0|  0.00%|
   441|         0|            0|            0|  0.00%|        For compatibility with Python 3.0, please consider all arguments to be
   442|         0|            0|            0|  0.00%|        keyword-only.
   443|         0|            0|            0|  0.00%|
   444|         0|            0|            0|  0.00%|        """
   445|         0|            0|            0|  0.00%|        self._record = record
   446|         0|            0|            0|  0.00%|        self._module = sys.modules['warnings'] if module is None else module
   447|         0|            0|            0|  0.00%|        self._entered = False
   448|         0|            0|            0|  0.00%|
   449|         0|            0|            0|  0.00%|    def __repr__(self):
   450|         0|            0|            0|  0.00%|        args = []
   451|         0|            0|            0|  0.00%|        if self._record:
   452|         0|            0|            0|  0.00%|            args.append("record=True")
   453|         0|            0|            0|  0.00%|        if self._module is not sys.modules['warnings']:
   454|         0|            0|            0|  0.00%|            args.append("module=%r" % self._module)
   455|         0|            0|            0|  0.00%|        name = type(self).__name__
   456|         0|            0|            0|  0.00%|        return "%s(%s)" % (name, ", ".join(args))
   457|         0|            0|            0|  0.00%|
   458|         0|            0|            0|  0.00%|    def __enter__(self):
   459|         0|            0|            0|  0.00%|        if self._entered:
   460|         0|            0|            0|  0.00%|            raise RuntimeError("Cannot enter %r twice" % self)
   461|         0|            0|            0|  0.00%|        self._entered = True
   462|         0|            0|            0|  0.00%|        self._filters = self._module.filters
   463|         0|            0|            0|  0.00%|        self._module.filters = self._filters[:]
   464|         0|            0|            0|  0.00%|        self._module._filters_mutated()
   465|         0|            0|            0|  0.00%|        self._showwarning = self._module.showwarning
   466|         0|            0|            0|  0.00%|        self._showwarnmsg_impl = self._module._showwarnmsg_impl
   467|         0|            0|            0|  0.00%|        if self._record:
   468|         0|            0|            0|  0.00%|            log = []
   469|         0|            0|            0|  0.00%|            self._module._showwarnmsg_impl = log.append
   470|         0|            0|            0|  0.00%|            # Reset showwarning() to the default implementation to make sure
   471|         0|            0|            0|  0.00%|            # that _showwarnmsg() calls _showwarnmsg_impl()
   472|         0|            0|            0|  0.00%|            self._module.showwarning = self._module._showwarning_orig
   473|         0|            0|            0|  0.00%|            return log
   474|         0|            0|            0|  0.00%|        else:
   475|         0|            0|            0|  0.00%|            return None
   476|         0|            0|            0|  0.00%|
   477|         0|            0|            0|  0.00%|    def __exit__(self, *exc_info):
   478|         0|            0|            0|  0.00%|        if not self._entered:
   479|         0|            0|            0|  0.00%|            raise RuntimeError("Cannot exit %r without entering first" % self)
   480|         0|            0|            0|  0.00%|        self._module.filters = self._filters
   481|         0|            0|            0|  0.00%|        self._module._filters_mutated()
   482|         0|            0|            0|  0.00%|        self._module.showwarning = self._showwarning
   483|         0|            0|            0|  0.00%|        self._module._showwarnmsg_impl = self._showwarnmsg_impl
   484|         0|            0|            0|  0.00%|
   485|         0|            0|            0|  0.00%|
   486|         0|            0|            0|  0.00%|# Private utility function called by _PyErr_WarnUnawaitedCoroutine
   487|         0|            0|            0|  0.00%|def _warn_unawaited_coroutine(coro):
   488|         0|            0|            0|  0.00%|    msg_lines = [
   489|         0|            0|            0|  0.00%|        f"coroutine '{coro.__qualname__}' was never awaited\n"
   490|         0|            0|            0|  0.00%|    ]
   491|         0|            0|            0|  0.00%|    if coro.cr_origin is not None:
   492|         0|            0|            0|  0.00%|        import linecache, traceback
   493|         0|            0|            0|  0.00%|        def extract():
   494|         0|            0|            0|  0.00%|            for filename, lineno, funcname in reversed(coro.cr_origin):
   495|         0|            0|            0|  0.00%|                line = linecache.getline(filename, lineno)
   496|         0|            0|            0|  0.00%|                yield (filename, lineno, funcname, line)
   497|         0|            0|            0|  0.00%|        msg_lines.append("Coroutine created at (most recent call last)\n")
   498|         0|            0|            0|  0.00%|        msg_lines += traceback.format_list(list(extract()))
   499|         0|            0|            0|  0.00%|    msg = "".join(msg_lines).rstrip("\n")
   500|         0|            0|            0|  0.00%|    # Passing source= here means that if the user happens to have tracemalloc
   501|         0|            0|            0|  0.00%|    # enabled and tracking where the coroutine was created, the warning will
   502|         0|            0|            0|  0.00%|    # contain that traceback. This does mean that if they have *both*
   503|         0|            0|            0|  0.00%|    # coroutine origin tracking *and* tracemalloc enabled, they'll get two
   504|         0|            0|            0|  0.00%|    # partially-redundant tracebacks. If we wanted to be clever we could
   505|         0|            0|            0|  0.00%|    # probably detect this case and avoid it, but for now we don't bother.
   506|         0|            0|            0|  0.00%|    warn(msg, category=RuntimeWarning, stacklevel=2, source=coro)
   507|         0|            0|            0|  0.00%|
   508|         0|            0|            0|  0.00%|
   509|         0|            0|            0|  0.00%|# filters contains a sequence of filter 5-tuples
   510|         0|            0|            0|  0.00%|# The components of the 5-tuple are:
   511|         0|            0|            0|  0.00%|# - an action: error, ignore, always, default, module, or once
   512|         0|            0|            0|  0.00%|# - a compiled regex that must match the warning message
   513|         0|            0|            0|  0.00%|# - a class representing the warning category
   514|         0|            0|            0|  0.00%|# - a compiled regex that must match the module that is being warned
   515|         0|            0|            0|  0.00%|# - a line number for the line being warning, or 0 to mean any line
   516|         0|            0|            0|  0.00%|# If either if the compiled regexs are None, match anything.
   517|         0|            0|            0|  0.00%|try:
   518|         0|            0|            0|  0.00%|    from _warnings import (filters, _defaultaction, _onceregistry,
   519|         0|            0|            0|  0.00%|                           warn, warn_explicit, _filters_mutated)
   520|         0|            0|            0|  0.00%|    defaultaction = _defaultaction
   521|         0|            0|            0|  0.00%|    onceregistry = _onceregistry
   522|         0|            0|            0|  0.00%|    _warnings_defaults = True
   523|         0|            0|            0|  0.00%|except ImportError:
   524|         0|            0|            0|  0.00%|    filters = []
   525|         0|            0|            0|  0.00%|    defaultaction = "default"
   526|         0|            0|            0|  0.00%|    onceregistry = {}
   527|         0|            0|            0|  0.00%|
   528|         0|            0|            0|  0.00%|    _filters_version = 1
   529|         0|            0|            0|  0.00%|
   530|         0|            0|            0|  0.00%|    def _filters_mutated():
   531|         0|            0|            0|  0.00%|        global _filters_version
   532|         0|            0|            0|  0.00%|        _filters_version += 1
   533|         0|            0|            0|  0.00%|
   534|         0|            0|            0|  0.00%|    _warnings_defaults = False
   535|         0|            0|            0|  0.00%|
   536|         0|            0|            0|  0.00%|
   537|         0|            0|            0|  0.00%|# Module initialization
   538|         0|            0|            0|  0.00%|_processoptions(sys.warnoptions)
   539|         0|            0|            0|  0.00%|if not _warnings_defaults:
   540|         0|            0|            0|  0.00%|    # Several warning categories are ignored by default in regular builds
   541|         0|            0|            0|  0.00%|    if not hasattr(sys, 'gettotalrefcount'):
   542|         0|            0|            0|  0.00%|        filterwarnings("default", category=DeprecationWarning,
   543|         0|            0|            0|  0.00%|                       module="__main__", append=1)
   544|         0|            0|            0|  0.00%|        simplefilter("ignore", category=DeprecationWarning, append=1)
   545|         0|            0|            0|  0.00%|        simplefilter("ignore", category=PendingDeprecationWarning, append=1)
   546|         0|            0|            0|  0.00%|        simplefilter("ignore", category=ImportWarning, append=1)
   547|         0|            0|            0|  0.00%|        simplefilter("ignore", category=ResourceWarning, append=1)
   548|         0|            0|            0|  0.00%|
   549|         0|            0|            0|  0.00%|del _warnings_defaults
File: /opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/signal_handling.py
File duration: 0.000629425s (0.00%)
Line #|      Hits|         Time| Time per hit|      %|Source code
------+----------+-------------+-------------+-------+-----------
     1|         0|            0|            0|  0.00%|r""""Signal handling for multiprocessing data loading.
     2|         0|            0|            0|  0.00%|
     3|         0|            0|            0|  0.00%|NOTE [ Signal handling in multiprocessing data loading ]
     4|         0|            0|            0|  0.00%|
     5|         0|            0|            0|  0.00%|In cases like DataLoader, if a worker process dies due to bus error/segfault
     6|         0|            0|            0|  0.00%|or just hang, the main process will hang waiting for data. This is difficult
     7|         0|            0|            0|  0.00%|to avoid on PyTorch side as it can be caused by limited shm, or other
     8|         0|            0|            0|  0.00%|libraries users call in the workers. In this file and `DataLoader.cpp`, we make
     9|         0|            0|            0|  0.00%|our best effort to provide some error message to users when such unfortunate
    10|         0|            0|            0|  0.00%|events happen.
    11|         0|            0|            0|  0.00%|
    12|         0|            0|            0|  0.00%|When a _BaseDataLoaderIter starts worker processes, their pids are registered in a
    13|         0|            0|            0|  0.00%|defined in `DataLoader.cpp`: id(_BaseDataLoaderIter) => Collection[ Worker pids ]
    14|         0|            0|            0|  0.00%|via `_set_worker_pids`.
    15|         0|            0|            0|  0.00%|
    16|         0|            0|            0|  0.00%|When an error happens in a worker process, the main process received a SIGCHLD,
    17|         0|            0|            0|  0.00%|and Python will eventually call the handler registered below
    18|         0|            0|            0|  0.00%|(in `_set_SIGCHLD_handler`). In the handler, the `_error_if_any_worker_fails`
    19|         0|            0|            0|  0.00%|call checks all registered worker pids and raise proper error message to
    20|         0|            0|            0|  0.00%|prevent main process from hanging waiting for data from worker.
    21|         0|            0|            0|  0.00%|
    22|         0|            0|            0|  0.00%|Additionally, at the beginning of each worker's `_utils.worker._worker_loop`,
    23|         0|            0|            0|  0.00%|`_set_worker_signal_handlers` is called to register critical signal handlers
    24|         0|            0|            0|  0.00%|(e.g., for SIGSEGV, SIGBUS, SIGFPE, SIGTERM) in C, which just prints an error
    25|         0|            0|            0|  0.00%|message to stderr before triggering the default handler. So a message will also
    26|         0|            0|            0|  0.00%|be printed from the worker process when it is killed by such signals.
    27|         0|            0|            0|  0.00%|
    28|         0|            0|            0|  0.00%|See NOTE [ Data Loader Multiprocessing Shutdown Logic ] for the reasoning of
    29|         0|            0|            0|  0.00%|this signal handling design and other mechanism we implement to make our
    30|         0|            0|            0|  0.00%|multiprocessing data loading robust to errors.
    31|         0|            0|            0|  0.00%|"""
    32|         0|            0|            0|  0.00%|
    33|         0|            0|            0|  0.00%|import signal
    34|         0|            0|            0|  0.00%|import threading
    35|         0|            0|            0|  0.00%|from . import IS_WINDOWS
    36|         0|            0|            0|  0.00%|
    37|         0|            0|            0|  0.00%|# Some of the following imported functions are not used in this file, but are to
    38|         0|            0|            0|  0.00%|# be used `_utils.signal_handling.XXXXX`.
    39|         0|            0|            0|  0.00%|from torch._C import _set_worker_pids, _remove_worker_pids  # noqa: F401
    40|         0|            0|            0|  0.00%|from torch._C import _error_if_any_worker_fails, _set_worker_signal_handlers  # noqa: F401
    41|         0|            0|            0|  0.00%|
    42|         0|            0|            0|  0.00%|_SIGCHLD_handler_set = False
    43|         0|            0|            0|  0.00%|r"""Whether SIGCHLD handler is set for DataLoader worker failures. Only one
    44|         0|            0|            0|  0.00%|handler needs to be set for all DataLoaders in a process."""
    45|         0|            0|            0|  0.00%|
    46|         0|            0|            0|  0.00%|
    47|         2|  4.22001e-05|     2.11e-05|  0.00%|def _set_SIGCHLD_handler():
    48|         0|            0|            0|  0.00%|    # Windows doesn't support SIGCHLD handler
    49|         2|  0.000102282|  5.11408e-05|  0.00%|    if IS_WINDOWS:
    50|         0|            0|            0|  0.00%|        return
    51|         0|            0|            0|  0.00%|    # can't set signal in child threads
    52|         2|  0.000133038|  6.65188e-05|  0.00%|    if not isinstance(threading.current_thread(), threading._MainThread):  # type: ignore[attr-defined]
(call)|         2|  0.000111818|  5.59092e-05|  0.00%|# /opt/conda/lib/python3.8/threading.py:1306 current_thread
    53|         0|            0|            0|  0.00%|        return
    54|         0|            0|            0|  0.00%|    global _SIGCHLD_handler_set
    55|         2|  1.35899e-05|  6.79493e-06|  0.00%|    if _SIGCHLD_handler_set:
    56|         2|  1.07288e-05|  5.36442e-06|  0.00%|        return
    57|         0|            0|            0|  0.00%|    previous_handler = signal.getsignal(signal.SIGCHLD)
    58|         0|            0|            0|  0.00%|    if not callable(previous_handler):
    59|         0|            0|            0|  0.00%|        # This doesn't catch default handler, but SIGCHLD default handler is a
    60|         0|            0|            0|  0.00%|        # no-op.
    61|         0|            0|            0|  0.00%|        previous_handler = None
    62|         0|            0|            0|  0.00%|
    63|         5|  0.000103712|  2.07424e-05|  0.00%|    def handler(signum, frame):
    64|         0|            0|            0|  0.00%|        # This following call uses `waitid` with WNOHANG from C side. Therefore,
    65|         0|            0|            0|  0.00%|        # Python can still get and update the process status successfully.
    66|         5|  0.000193834|  3.87669e-05|  0.00%|        _error_if_any_worker_fails()
(call)|         1|  3.05176e-05|  3.05176e-05|  0.00%|# /opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/signal_handling.py:63 handler
    67|         5|  3.00407e-05|  6.00815e-06|  0.00%|        if previous_handler is not None:
    68|         0|            0|            0|  0.00%|            assert callable(previous_handler)
    69|         0|            0|            0|  0.00%|            previous_handler(signum, frame)
    70|         0|            0|            0|  0.00%|
    71|         0|            0|            0|  0.00%|    signal.signal(signal.SIGCHLD, handler)
    72|         0|            0|            0|  0.00%|    _SIGCHLD_handler_set = True
File: /opt/conda/lib/python3.8/site-packages/torchvision/datasets/folder.py
File duration: 0.000169992s (0.00%)
Line #|      Hits|         Time| Time per hit|      %|Source code
------+----------+-------------+-------------+-------+-----------
     1|         0|            0|            0|  0.00%|from .vision import VisionDataset
     2|         0|            0|            0|  0.00%|
     3|         0|            0|            0|  0.00%|from PIL import Image
     4|         0|            0|            0|  0.00%|
     5|         0|            0|            0|  0.00%|import os
     6|         0|            0|            0|  0.00%|import os.path
     7|         0|            0|            0|  0.00%|from typing import Any, Callable, cast, Dict, List, Optional, Tuple
     8|         0|            0|            0|  0.00%|
     9|         0|            0|            0|  0.00%|
    10|         0|            0|            0|  0.00%|def has_file_allowed_extension(filename: str, extensions: Tuple[str, ...]) -> bool:
    11|         0|            0|            0|  0.00%|    """Checks if a file is an allowed extension.
    12|         0|            0|            0|  0.00%|
    13|         0|            0|            0|  0.00%|    Args:
    14|         0|            0|            0|  0.00%|        filename (string): path to a file
    15|         0|            0|            0|  0.00%|        extensions (tuple of strings): extensions to consider (lowercase)
    16|         0|            0|            0|  0.00%|
    17|         0|            0|            0|  0.00%|    Returns:
    18|         0|            0|            0|  0.00%|        bool: True if the filename ends with one of given extensions
    19|         0|            0|            0|  0.00%|    """
    20|         0|            0|            0|  0.00%|    return filename.lower().endswith(extensions)
    21|         0|            0|            0|  0.00%|
    22|         0|            0|            0|  0.00%|
    23|         0|            0|            0|  0.00%|def is_image_file(filename: str) -> bool:
    24|         0|            0|            0|  0.00%|    """Checks if a file is an allowed image extension.
    25|         0|            0|            0|  0.00%|
    26|         0|            0|            0|  0.00%|    Args:
    27|         0|            0|            0|  0.00%|        filename (string): path to a file
    28|         0|            0|            0|  0.00%|
    29|         0|            0|            0|  0.00%|    Returns:
    30|         0|            0|            0|  0.00%|        bool: True if the filename ends with a known image extension
    31|         0|            0|            0|  0.00%|    """
    32|         0|            0|            0|  0.00%|    return has_file_allowed_extension(filename, IMG_EXTENSIONS)
    33|         0|            0|            0|  0.00%|
    34|         0|            0|            0|  0.00%|
    35|         0|            0|            0|  0.00%|def find_classes(directory: str) -> Tuple[List[str], Dict[str, int]]:
    36|         0|            0|            0|  0.00%|    """Finds the class folders in a dataset.
    37|         0|            0|            0|  0.00%|
    38|         0|            0|            0|  0.00%|    See :class:`DatasetFolder` for details.
    39|         0|            0|            0|  0.00%|    """
    40|         0|            0|            0|  0.00%|    classes = sorted(entry.name for entry in os.scandir(directory) if entry.is_dir())
    41|         0|            0|            0|  0.00%|    if not classes:
    42|         0|            0|            0|  0.00%|        raise FileNotFoundError(f"Couldn't find any class folder in {directory}.")
    43|         0|            0|            0|  0.00%|
    44|         0|            0|            0|  0.00%|    class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}
    45|         0|            0|            0|  0.00%|    return classes, class_to_idx
    46|         0|            0|            0|  0.00%|
    47|         0|            0|            0|  0.00%|
    48|         0|            0|            0|  0.00%|def make_dataset(
    49|         0|            0|            0|  0.00%|    directory: str,
    50|         0|            0|            0|  0.00%|    class_to_idx: Optional[Dict[str, int]] = None,
    51|         0|            0|            0|  0.00%|    extensions: Optional[Tuple[str, ...]] = None,
    52|         0|            0|            0|  0.00%|    is_valid_file: Optional[Callable[[str], bool]] = None,
    53|         0|            0|            0|  0.00%|) -> List[Tuple[str, int]]:
    54|         0|            0|            0|  0.00%|    """Generates a list of samples of a form (path_to_sample, class).
    55|         0|            0|            0|  0.00%|
    56|         0|            0|            0|  0.00%|    See :class:`DatasetFolder` for details.
    57|         0|            0|            0|  0.00%|
    58|         0|            0|            0|  0.00%|    Note: The class_to_idx parameter is here optional and will use the logic of the ``find_classes`` function
    59|         0|            0|            0|  0.00%|    by default.
    60|         0|            0|            0|  0.00%|    """
    61|         0|            0|            0|  0.00%|    directory = os.path.expanduser(directory)
    62|         0|            0|            0|  0.00%|
    63|         0|            0|            0|  0.00%|    if class_to_idx is None:
    64|         0|            0|            0|  0.00%|        _, class_to_idx = find_classes(directory)
    65|         0|            0|            0|  0.00%|    elif not class_to_idx:
    66|         0|            0|            0|  0.00%|        raise ValueError("'class_to_index' must have at least one entry to collect any samples.")
    67|         0|            0|            0|  0.00%|
    68|         0|            0|            0|  0.00%|    both_none = extensions is None and is_valid_file is None
    69|         0|            0|            0|  0.00%|    both_something = extensions is not None and is_valid_file is not None
    70|         0|            0|            0|  0.00%|    if both_none or both_something:
    71|         0|            0|            0|  0.00%|        raise ValueError("Both extensions and is_valid_file cannot be None or not None at the same time")
    72|         0|            0|            0|  0.00%|
    73|         0|            0|            0|  0.00%|    if extensions is not None:
    74|         0|            0|            0|  0.00%|
    75|         0|            0|            0|  0.00%|        def is_valid_file(x: str) -> bool:
    76|         0|            0|            0|  0.00%|            return has_file_allowed_extension(x, cast(Tuple[str, ...], extensions))
    77|         0|            0|            0|  0.00%|
    78|         0|            0|            0|  0.00%|    is_valid_file = cast(Callable[[str], bool], is_valid_file)
    79|         0|            0|            0|  0.00%|
    80|         0|            0|            0|  0.00%|    instances = []
    81|         0|            0|            0|  0.00%|    available_classes = set()
    82|         0|            0|            0|  0.00%|    for target_class in sorted(class_to_idx.keys()):
    83|         0|            0|            0|  0.00%|        class_index = class_to_idx[target_class]
    84|         0|            0|            0|  0.00%|        target_dir = os.path.join(directory, target_class)
    85|         0|            0|            0|  0.00%|        if not os.path.isdir(target_dir):
    86|         0|            0|            0|  0.00%|            continue
    87|         0|            0|            0|  0.00%|        for root, _, fnames in sorted(os.walk(target_dir, followlinks=True)):
    88|         0|            0|            0|  0.00%|            for fname in sorted(fnames):
    89|         0|            0|            0|  0.00%|                path = os.path.join(root, fname)
    90|         0|            0|            0|  0.00%|                if is_valid_file(path):
    91|         0|            0|            0|  0.00%|                    item = path, class_index
    92|         0|            0|            0|  0.00%|                    instances.append(item)
    93|         0|            0|            0|  0.00%|
    94|         0|            0|            0|  0.00%|                    if target_class not in available_classes:
    95|         0|            0|            0|  0.00%|                        available_classes.add(target_class)
    96|         0|            0|            0|  0.00%|
    97|         0|            0|            0|  0.00%|    empty_classes = set(class_to_idx.keys()) - available_classes
    98|         0|            0|            0|  0.00%|    if empty_classes:
    99|         0|            0|            0|  0.00%|        msg = f"Found no valid file for the classes {', '.join(sorted(empty_classes))}. "
   100|         0|            0|            0|  0.00%|        if extensions is not None:
   101|         0|            0|            0|  0.00%|            msg += f"Supported extensions are: {', '.join(extensions)}"
   102|         0|            0|            0|  0.00%|        raise FileNotFoundError(msg)
   103|         0|            0|            0|  0.00%|
   104|         0|            0|            0|  0.00%|    return instances
   105|         0|            0|            0|  0.00%|
   106|         0|            0|            0|  0.00%|
   107|         0|            0|            0|  0.00%|class DatasetFolder(VisionDataset):
   108|         0|            0|            0|  0.00%|    """A generic data loader.
   109|         0|            0|            0|  0.00%|
   110|         0|            0|            0|  0.00%|    This default directory structure can be customized by overriding the
   111|         0|            0|            0|  0.00%|    :meth:`find_classes` method.
   112|         0|            0|            0|  0.00%|
   113|         0|            0|            0|  0.00%|    Args:
   114|         0|            0|            0|  0.00%|        root (string): Root directory path.
   115|         0|            0|            0|  0.00%|        loader (callable): A function to load a sample given its path.
   116|         0|            0|            0|  0.00%|        extensions (tuple[string]): A list of allowed extensions.
   117|         0|            0|            0|  0.00%|            both extensions and is_valid_file should not be passed.
   118|         0|            0|            0|  0.00%|        transform (callable, optional): A function/transform that takes in
   119|         0|            0|            0|  0.00%|            a sample and returns a transformed version.
   120|         0|            0|            0|  0.00%|            E.g, ``transforms.RandomCrop`` for images.
   121|         0|            0|            0|  0.00%|        target_transform (callable, optional): A function/transform that takes
   122|         0|            0|            0|  0.00%|            in the target and transforms it.
   123|         0|            0|            0|  0.00%|        is_valid_file (callable, optional): A function that takes path of a file
   124|         0|            0|            0|  0.00%|            and check if the file is a valid file (used to check of corrupt files)
   125|         0|            0|            0|  0.00%|            both extensions and is_valid_file should not be passed.
   126|         0|            0|            0|  0.00%|
   127|         0|            0|            0|  0.00%|     Attributes:
   128|         0|            0|            0|  0.00%|        classes (list): List of the class names sorted alphabetically.
   129|         0|            0|            0|  0.00%|        class_to_idx (dict): Dict with items (class_name, class_index).
   130|         0|            0|            0|  0.00%|        samples (list): List of (sample path, class_index) tuples
   131|         0|            0|            0|  0.00%|        targets (list): The class_index value for each image in the dataset
   132|         0|            0|            0|  0.00%|    """
   133|         0|            0|            0|  0.00%|
   134|         0|            0|            0|  0.00%|    def __init__(
   135|         0|            0|            0|  0.00%|            self,
   136|         0|            0|            0|  0.00%|            root: str,
   137|         0|            0|            0|  0.00%|            loader: Callable[[str], Any],
   138|         0|            0|            0|  0.00%|            extensions: Optional[Tuple[str, ...]] = None,
   139|         0|            0|            0|  0.00%|            transform: Optional[Callable] = None,
   140|         0|            0|            0|  0.00%|            target_transform: Optional[Callable] = None,
   141|         0|            0|            0|  0.00%|            is_valid_file: Optional[Callable[[str], bool]] = None,
   142|         0|            0|            0|  0.00%|    ) -> None:
   143|         0|            0|            0|  0.00%|        super(DatasetFolder, self).__init__(root, transform=transform,
   144|         0|            0|            0|  0.00%|                                            target_transform=target_transform)
   145|         0|            0|            0|  0.00%|        classes, class_to_idx = self.find_classes(self.root)
   146|         0|            0|            0|  0.00%|        samples = self.make_dataset(self.root, class_to_idx, extensions, is_valid_file)
   147|         0|            0|            0|  0.00%|
   148|         0|            0|            0|  0.00%|        self.loader = loader
   149|         0|            0|            0|  0.00%|        self.extensions = extensions
   150|         0|            0|            0|  0.00%|
   151|         0|            0|            0|  0.00%|        self.classes = classes
   152|         0|            0|            0|  0.00%|        self.class_to_idx = class_to_idx
   153|         0|            0|            0|  0.00%|        self.samples = samples
   154|         0|            0|            0|  0.00%|        self.targets = [s[1] for s in samples]
   155|         0|            0|            0|  0.00%|
   156|         0|            0|            0|  0.00%|    @staticmethod
   157|         0|            0|            0|  0.00%|    def make_dataset(
   158|         0|            0|            0|  0.00%|        directory: str,
   159|         0|            0|            0|  0.00%|        class_to_idx: Dict[str, int],
   160|         0|            0|            0|  0.00%|        extensions: Optional[Tuple[str, ...]] = None,
   161|         0|            0|            0|  0.00%|        is_valid_file: Optional[Callable[[str], bool]] = None,
   162|         0|            0|            0|  0.00%|    ) -> List[Tuple[str, int]]:
   163|         0|            0|            0|  0.00%|        """Generates a list of samples of a form (path_to_sample, class).
   164|         0|            0|            0|  0.00%|
   165|         0|            0|            0|  0.00%|        This can be overridden to e.g. read files from a compressed zip file instead of from the disk.
   166|         0|            0|            0|  0.00%|
   167|         0|            0|            0|  0.00%|        Args:
   168|         0|            0|            0|  0.00%|            directory (str): root dataset directory, corresponding to ``self.root``.
   169|         0|            0|            0|  0.00%|            class_to_idx (Dict[str, int]): Dictionary mapping class name to class index.
   170|         0|            0|            0|  0.00%|            extensions (optional): A list of allowed extensions.
   171|         0|            0|            0|  0.00%|                Either extensions or is_valid_file should be passed. Defaults to None.
   172|         0|            0|            0|  0.00%|            is_valid_file (optional): A function that takes path of a file
   173|         0|            0|            0|  0.00%|                and checks if the file is a valid file
   174|         0|            0|            0|  0.00%|                (used to check of corrupt files) both extensions and
   175|         0|            0|            0|  0.00%|                is_valid_file should not be passed. Defaults to None.
   176|         0|            0|            0|  0.00%|
   177|         0|            0|            0|  0.00%|        Raises:
   178|         0|            0|            0|  0.00%|            ValueError: In case ``class_to_idx`` is empty.
   179|         0|            0|            0|  0.00%|            ValueError: In case ``extensions`` and ``is_valid_file`` are None or both are not None.
   180|         0|            0|            0|  0.00%|            FileNotFoundError: In case no valid file was found for any class.
   181|         0|            0|            0|  0.00%|
   182|         0|            0|            0|  0.00%|        Returns:
   183|         0|            0|            0|  0.00%|            List[Tuple[str, int]]: samples of a form (path_to_sample, class)
   184|         0|            0|            0|  0.00%|        """
   185|         0|            0|            0|  0.00%|        if class_to_idx is None:
   186|         0|            0|            0|  0.00%|            # prevent potential bug since make_dataset() would use the class_to_idx logic of the
   187|         0|            0|            0|  0.00%|            # find_classes() function, instead of using that of the find_classes() method, which
   188|         0|            0|            0|  0.00%|            # is potentially overridden and thus could have a different logic.
   189|         0|            0|            0|  0.00%|            raise ValueError(
   190|         0|            0|            0|  0.00%|                "The class_to_idx parameter cannot be None."
   191|         0|            0|            0|  0.00%|            )
   192|         0|            0|            0|  0.00%|        return make_dataset(directory, class_to_idx, extensions=extensions, is_valid_file=is_valid_file)
   193|         0|            0|            0|  0.00%|
   194|         0|            0|            0|  0.00%|    def find_classes(self, directory: str) -> Tuple[List[str], Dict[str, int]]:
   195|         0|            0|            0|  0.00%|        """Find the class folders in a dataset structured as follows::
   196|         0|            0|            0|  0.00%|
   197|         0|            0|            0|  0.00%|            directory/
   198|         0|            0|            0|  0.00%|            âââ class_x
   199|         0|            0|            0|  0.00%|            â   âââ xxx.ext
   200|         0|            0|            0|  0.00%|            â   âââ xxy.ext
   201|         0|            0|            0|  0.00%|            â   âââ ...
   202|         0|            0|            0|  0.00%|            â       âââ xxz.ext
   203|         0|            0|            0|  0.00%|            âââ class_y
   204|         0|            0|            0|  0.00%|                âââ 123.ext
   205|         0|            0|            0|  0.00%|                âââ nsdf3.ext
   206|         0|            0|            0|  0.00%|                âââ ...
   207|         0|            0|            0|  0.00%|                âââ asd932_.ext
   208|         0|            0|            0|  0.00%|
   209|         0|            0|            0|  0.00%|        This method can be overridden to only consider
   210|         0|            0|            0|  0.00%|        a subset of classes, or to adapt to a different dataset directory structure.
   211|         0|            0|            0|  0.00%|
   212|         0|            0|            0|  0.00%|        Args:
   213|         0|            0|            0|  0.00%|            directory(str): Root directory path, corresponding to ``self.root``
   214|         0|            0|            0|  0.00%|
   215|         0|            0|            0|  0.00%|        Raises:
   216|         0|            0|            0|  0.00%|            FileNotFoundError: If ``dir`` has no class folders.
   217|         0|            0|            0|  0.00%|
   218|         0|            0|            0|  0.00%|        Returns:
   219|         0|            0|            0|  0.00%|            (Tuple[List[str], Dict[str, int]]): List of all classes and dictionary mapping each class to an index.
   220|         0|            0|            0|  0.00%|        """
   221|         0|            0|            0|  0.00%|        return find_classes(directory)
   222|         0|            0|            0|  0.00%|
   223|         0|            0|            0|  0.00%|    def __getitem__(self, index: int) -> Tuple[Any, Any]:
   224|         0|            0|            0|  0.00%|        """
   225|         0|            0|            0|  0.00%|        Args:
   226|         0|            0|            0|  0.00%|            index (int): Index
   227|         0|            0|            0|  0.00%|
   228|         0|            0|            0|  0.00%|        Returns:
   229|         0|            0|            0|  0.00%|            tuple: (sample, target) where target is class_index of the target class.
   230|         0|            0|            0|  0.00%|        """
   231|         0|            0|            0|  0.00%|        path, target = self.samples[index]
   232|         0|            0|            0|  0.00%|        sample = self.loader(path)
   233|         0|            0|            0|  0.00%|        if self.transform is not None:
   234|         0|            0|            0|  0.00%|            sample = self.transform(sample)
   235|         0|            0|            0|  0.00%|        if self.target_transform is not None:
   236|         0|            0|            0|  0.00%|            target = self.target_transform(target)
   237|         0|            0|            0|  0.00%|
   238|         0|            0|            0|  0.00%|        return sample, target
   239|         0|            0|            0|  0.00%|
   240|         2|  4.05312e-05|  2.02656e-05|  0.00%|    def __len__(self) -> int:
   241|         2|  0.000129461|  6.47306e-05|  0.00%|        return len(self.samples)
   242|         0|            0|            0|  0.00%|
   243|         0|            0|            0|  0.00%|
   244|         0|            0|            0|  0.00%|IMG_EXTENSIONS = ('.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm', '.tif', '.tiff', '.webp')
   245|         0|            0|            0|  0.00%|
   246|         0|            0|            0|  0.00%|
   247|         0|            0|            0|  0.00%|def pil_loader(path: str) -> Image.Image:
   248|         0|            0|            0|  0.00%|    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)
   249|         0|            0|            0|  0.00%|    with open(path, 'rb') as f:
   250|         0|            0|            0|  0.00%|        img = Image.open(f)
   251|         0|            0|            0|  0.00%|        return img.convert('RGB')
   252|         0|            0|            0|  0.00%|
   253|         0|            0|            0|  0.00%|
   254|         0|            0|            0|  0.00%|# TODO: specify the return type
   255|         0|            0|            0|  0.00%|def accimage_loader(path: str) -> Any:
   256|         0|            0|            0|  0.00%|    import accimage
   257|         0|            0|            0|  0.00%|    try:
   258|         0|            0|            0|  0.00%|        return accimage.Image(path)
   259|         0|            0|            0|  0.00%|    except IOError:
   260|         0|            0|            0|  0.00%|        # Potentially a decoding problem, fall back to PIL.Image
   261|         0|            0|            0|  0.00%|        return pil_loader(path)
   262|         0|            0|            0|  0.00%|
   263|         0|            0|            0|  0.00%|
   264|         0|            0|            0|  0.00%|def default_loader(path: str) -> Any:
   265|         0|            0|            0|  0.00%|    from torchvision import get_image_backend
   266|         0|            0|            0|  0.00%|    if get_image_backend() == 'accimage':
   267|         0|            0|            0|  0.00%|        return accimage_loader(path)
   268|         0|            0|            0|  0.00%|    else:
   269|         0|            0|            0|  0.00%|        return pil_loader(path)
   270|         0|            0|            0|  0.00%|
   271|         0|            0|            0|  0.00%|
   272|         0|            0|            0|  0.00%|class ImageFolder(DatasetFolder):
   273|         0|            0|            0|  0.00%|    """A generic data loader where the images are arranged in this way by default: ::
   274|         0|            0|            0|  0.00%|
   275|         0|            0|            0|  0.00%|        root/dog/xxx.png
   276|         0|            0|            0|  0.00%|        root/dog/xxy.png
   277|         0|            0|            0|  0.00%|        root/dog/[...]/xxz.png
   278|         0|            0|            0|  0.00%|
   279|         0|            0|            0|  0.00%|        root/cat/123.png
   280|         0|            0|            0|  0.00%|        root/cat/nsdf3.png
   281|         0|            0|            0|  0.00%|        root/cat/[...]/asd932_.png
   282|         0|            0|            0|  0.00%|
   283|         0|            0|            0|  0.00%|    This class inherits from :class:`~torchvision.datasets.DatasetFolder` so
   284|         0|            0|            0|  0.00%|    the same methods can be overridden to customize the dataset.
   285|         0|            0|            0|  0.00%|
   286|         0|            0|            0|  0.00%|    Args:
   287|         0|            0|            0|  0.00%|        root (string): Root directory path.
   288|         0|            0|            0|  0.00%|        transform (callable, optional): A function/transform that  takes in an PIL image
   289|         0|            0|            0|  0.00%|            and returns a transformed version. E.g, ``transforms.RandomCrop``
   290|         0|            0|            0|  0.00%|        target_transform (callable, optional): A function/transform that takes in the
   291|         0|            0|            0|  0.00%|            target and transforms it.
   292|         0|            0|            0|  0.00%|        loader (callable, optional): A function to load an image given its path.
   293|         0|            0|            0|  0.00%|        is_valid_file (callable, optional): A function that takes path of an Image file
   294|         0|            0|            0|  0.00%|            and check if the file is a valid file (used to check of corrupt files)
   295|         0|            0|            0|  0.00%|
   296|         0|            0|            0|  0.00%|     Attributes:
   297|         0|            0|            0|  0.00%|        classes (list): List of the class names sorted alphabetically.
   298|         0|            0|            0|  0.00%|        class_to_idx (dict): Dict with items (class_name, class_index).
   299|         0|            0|            0|  0.00%|        imgs (list): List of (image path, class_index) tuples
   300|         0|            0|            0|  0.00%|    """
   301|         0|            0|            0|  0.00%|
   302|         0|            0|            0|  0.00%|    def __init__(
   303|         0|            0|            0|  0.00%|            self,
   304|         0|            0|            0|  0.00%|            root: str,
   305|         0|            0|            0|  0.00%|            transform: Optional[Callable] = None,
   306|         0|            0|            0|  0.00%|            target_transform: Optional[Callable] = None,
   307|         0|            0|            0|  0.00%|            loader: Callable[[str], Any] = default_loader,
   308|         0|            0|            0|  0.00%|            is_valid_file: Optional[Callable[[str], bool]] = None,
   309|         0|            0|            0|  0.00%|    ):
   310|         0|            0|            0|  0.00%|        super(ImageFolder, self).__init__(root, loader, IMG_EXTENSIONS if is_valid_file is None else None,
   311|         0|            0|            0|  0.00%|                                          transform=transform,
   312|         0|            0|            0|  0.00%|                                          target_transform=target_transform,
   313|         0|            0|            0|  0.00%|                                          is_valid_file=is_valid_file)
   314|         0|            0|            0|  0.00%|        self.imgs = self.samples
File: <string>_0
File duration: 3.24249e-05s (0.00%)
Line #|      Hits|         Time| Time per hit|      %|Source code
------+----------+-------------+-------------+-------+-----------
     1|         2|  3.24249e-05|  1.62125e-05|  0.00%|
